{"translation": {"code": "void membersDisplayString ( StringBuilder sb , String indent ) { for ( Type field : fields ) { field . writeToStringBuilder ( sb , indent ) ; if ( field . isPrimitive ( ) ) { sb . append ( \";\" ) ; } sb . append ( \"\\n\" ) ; } }", "nl": "appends a display string for of the members of this group to sb"}}
{"translation": {"code": "public void endBlock ( ) throws IOException { state = state . endBlock ( ) ; LOG . debug ( \"{}: end block\" , out . getPos ( ) ) ; currentBlock . setRowCount ( currentRecordCount ) ; blocks . add ( currentBlock ) ; columnIndexes . add ( currentColumnIndexes ) ; offsetIndexes . add ( currentOffsetIndexes ) ; currentColumnIndexes = null ; currentOffsetIndexes = null ; currentBlock = null ; }", "nl": "ends a block once all column chunks have been written"}}
{"translation": {"code": "public void start ( ) throws IOException { state = state . start ( ) ; LOG . debug ( \"{}: start\" , out . getPos ( ) ) ; out . write ( MAGIC ) ; }", "nl": "start the file"}}
{"translation": {"code": "public PageReadStore readNextRowGroup ( ) throws IOException { if ( currentBlock == blocks . size ( ) ) { return null ; } BlockMetaData block = blocks . get ( currentBlock ) ; if ( block . getRowCount ( ) == 0 ) { throw new RuntimeException ( \"Illegal row group of 0 rows\" ) ; } this . currentRowGroup = new ColumnChunkPageReadStore ( block . getRowCount ( ) ) ; // prepare the list of consecutive parts to read them in one scan List < ConsecutivePartList > allParts = new ArrayList < ConsecutivePartList > ( ) ; ConsecutivePartList currentParts = null ; for ( ColumnChunkMetaData mc : block . getColumns ( ) ) { ColumnPath pathKey = mc . getPath ( ) ; BenchmarkCounter . incrementTotalBytes ( mc . getTotalSize ( ) ) ; ColumnDescriptor columnDescriptor = paths . get ( pathKey ) ; if ( columnDescriptor != null ) { long startingPos = mc . getStartingPos ( ) ; // first part or not consecutive => new list if ( currentParts == null || currentParts . endPos ( ) != startingPos ) { currentParts = new ConsecutivePartList ( startingPos ) ; allParts . add ( currentParts ) ; } currentParts . addChunk ( new ChunkDescriptor ( columnDescriptor , mc , startingPos , ( int ) mc . getTotalSize ( ) ) ) ; } } // actually read all the chunks ChunkListBuilder builder = new ChunkListBuilder ( ) ; for ( ConsecutivePartList consecutiveChunks : allParts ) { consecutiveChunks . readAll ( f , builder ) ; } for ( Chunk chunk : builder . build ( ) ) { currentRowGroup . addColumn ( chunk . descriptor . col , chunk . readAllPages ( ) ) ; } // avoid re-reading bytes the dictionary reader is used after this call if ( nextDictionaryReader != null ) { nextDictionaryReader . setRowGroup ( currentRowGroup ) ; } advanceToNextBlock ( ) ; return currentRowGroup ; }", "nl": "Reads all the columns requested from the row group at the current file position ."}}
{"translation": {"code": "public PageReadStore readNextFilteredRowGroup ( ) throws IOException { if ( currentBlock == blocks . size ( ) ) { return null ; } if ( ! options . useColumnIndexFilter ( ) ) { return readNextRowGroup ( ) ; } BlockMetaData block = blocks . get ( currentBlock ) ; if ( block . getRowCount ( ) == 0 ) { throw new RuntimeException ( \"Illegal row group of 0 rows\" ) ; } ColumnIndexStore ciStore = getColumnIndexStore ( currentBlock ) ; RowRanges rowRanges = getRowRanges ( currentBlock ) ; long rowCount = rowRanges . rowCount ( ) ; if ( rowCount == 0 ) { // There are no matching rows -> skipping this row-group advanceToNextBlock ( ) ; return readNextFilteredRowGroup ( ) ; } if ( rowCount == block . getRowCount ( ) ) { // All rows are matching -> fall back to the non-filtering path return readNextRowGroup ( ) ; } this . currentRowGroup = new ColumnChunkPageReadStore ( rowRanges ) ; // prepare the list of consecutive parts to read them in one scan ChunkListBuilder builder = new ChunkListBuilder ( ) ; List < ConsecutivePartList > allParts = new ArrayList < ConsecutivePartList > ( ) ; ConsecutivePartList currentParts = null ; for ( ColumnChunkMetaData mc : block . getColumns ( ) ) { ColumnPath pathKey = mc . getPath ( ) ; ColumnDescriptor columnDescriptor = paths . get ( pathKey ) ; if ( columnDescriptor != null ) { OffsetIndex offsetIndex = ciStore . getOffsetIndex ( mc . getPath ( ) ) ; OffsetIndex filteredOffsetIndex = filterOffsetIndex ( offsetIndex , rowRanges , block . getRowCount ( ) ) ; for ( OffsetRange range : calculateOffsetRanges ( filteredOffsetIndex , mc , offsetIndex . getOffset ( 0 ) ) ) { BenchmarkCounter . incrementTotalBytes ( range . getLength ( ) ) ; long startingPos = range . getOffset ( ) ; // first part or not consecutive => new list if ( currentParts == null || currentParts . endPos ( ) != startingPos ) { currentParts = new ConsecutivePartList ( startingPos ) ; allParts . add ( currentParts ) ; } ChunkDescriptor chunkDescriptor = new ChunkDescriptor ( columnDescriptor , mc , startingPos , ( int ) range . getLength ( ) ) ; currentParts . addChunk ( chunkDescriptor ) ; builder . setOffsetIndex ( chunkDescriptor , filteredOffsetIndex ) ; } } } // actually read all the chunks for ( ConsecutivePartList consecutiveChunks : allParts ) { consecutiveChunks . readAll ( f , builder ) ; } for ( Chunk chunk : builder . build ( ) ) { currentRowGroup . addColumn ( chunk . descriptor . col , chunk . readAllPages ( ) ) ; } // avoid re-reading bytes the dictionary reader is used after this call if ( nextDictionaryReader != null ) { nextDictionaryReader . setRowGroup ( currentRowGroup ) ; } advanceToNextBlock ( ) ; return currentRowGroup ; }", "nl": "Reads all the columns requested from the row group at the current file position . It may skip specific pages based on the column indexes according to the actual filter . As the rows are not aligned among the pages of the different columns row synchronization might be required . See the documentation of the class SynchronizingColumnReader for details ."}}
{"translation": {"code": "public void end ( Map < String , String > extraMetaData ) throws IOException { state = state . end ( ) ; serializeColumnIndexes ( columnIndexes , blocks , out ) ; serializeOffsetIndexes ( offsetIndexes , blocks , out ) ; LOG . debug ( \"{}: end\" , out . getPos ( ) ) ; this . footer = new ParquetMetadata ( new FileMetaData ( schema , extraMetaData , Version . FULL_VERSION ) , blocks ) ; serializeFooter ( footer , out ) ; out . close ( ) ; }", "nl": "ends a file once all blocks have been written . closes the file ."}}
{"translation": {"code": "private static TupleSummaryData sumUp ( Schema schema , Tuple t ) throws ExecException { TupleSummaryData summaryData = new TupleSummaryData ( ) ; DataBag bag = ( DataBag ) t . get ( 0 ) ; for ( Tuple tuple : bag ) { summaryData . addTuple ( schema , tuple ) ; } return summaryData ; }", "nl": "The input tuple contains a bag of Tuples to sum up"}}
{"translation": {"code": "private static TupleSummaryData merge ( Tuple t ) throws IOException { TupleSummaryData summaryData = new TupleSummaryData ( ) ; DataBag bag = ( DataBag ) t . get ( 0 ) ; for ( Tuple tuple : bag ) { summaryData . merge ( getData ( tuple ) ) ; } return summaryData ; }", "nl": "the input tuple contains a bag of string representations of TupleSummaryData"}}
{"translation": {"code": "DictionaryPage readDictionary ( ColumnChunkMetaData meta ) throws IOException { if ( ! meta . getEncodings ( ) . contains ( Encoding . PLAIN_DICTIONARY ) && ! meta . getEncodings ( ) . contains ( Encoding . RLE_DICTIONARY ) ) { return null ; } // TODO: this should use getDictionaryPageOffset() but it isn't reliable. if ( f . getPos ( ) != meta . getStartingPos ( ) ) { f . seek ( meta . getStartingPos ( ) ) ; } PageHeader pageHeader = Util . readPageHeader ( f ) ; if ( ! pageHeader . isSetDictionary_page_header ( ) ) { return null ; // TODO: should this complain? } DictionaryPage compressedPage = readCompressedDictionary ( pageHeader , f ) ; BytesInputDecompressor decompressor = options . getCodecFactory ( ) . getDecompressor ( meta . getCodec ( ) ) ; return new DictionaryPage ( decompressor . decompress ( compressedPage . getBytes ( ) , compressedPage . getUncompressedSize ( ) ) , compressedPage . getDictionarySize ( ) , compressedPage . getEncoding ( ) ) ; }", "nl": "Reads and decompresses a dictionary page for the given column chunk ."}}
{"translation": {"code": "public void startColumn ( ColumnDescriptor descriptor , long valueCount , CompressionCodecName compressionCodecName ) throws IOException { state = state . startColumn ( ) ; encodingStatsBuilder . clear ( ) ; currentEncodings = new HashSet < Encoding > ( ) ; currentChunkPath = ColumnPath . get ( descriptor . getPath ( ) ) ; currentChunkType = descriptor . getPrimitiveType ( ) ; currentChunkCodec = compressionCodecName ; currentChunkValueCount = valueCount ; currentChunkFirstDataPage = out . getPos ( ) ; compressedLength = 0 ; uncompressedLength = 0 ; // The statistics will be copied from the first one added at writeDataPage(s) so we have the correct typed one currentStatistics = null ; columnIndexBuilder = ColumnIndexBuilder . getBuilder ( currentChunkType , columnIndexTruncateLength ) ; offsetIndexBuilder = OffsetIndexBuilder . getBuilder ( ) ; firstPageOffset = - 1 ; }", "nl": "start a column inside a block"}}
{"translation": {"code": "public void startBlock ( long recordCount ) throws IOException { state = state . startBlock ( ) ; LOG . debug ( \"{}: start block\" , out . getPos ( ) ) ; //    out.write(MAGIC); // TODO: add a magic delimiter alignment . alignForRowGroup ( out ) ; currentBlock = new BlockMetaData ( ) ; currentRecordCount = recordCount ; currentColumnIndexes = new ArrayList <> ( ) ; currentOffsetIndexes = new ArrayList <> ( ) ; }", "nl": "start a block"}}
{"translation": {"code": "public static int checkedCast ( long value ) { int valueI = ( int ) value ; if ( valueI != value ) { throw new IllegalArgumentException ( String . format ( \"Overflow casting %d to an int\" , value ) ) ; } return valueI ; }", "nl": "Cast value to a an int or throw an exception if there is an overflow ."}}
{"translation": {"code": "@ Deprecated public static List < Footer > readFooters ( Configuration configuration , FileStatus pathStatus ) throws IOException { return readFooters ( configuration , pathStatus , false ) ; }", "nl": "this always returns the row groups"}}
{"translation": {"code": "@ Deprecated public static List < Footer > readSummaryFile ( Configuration configuration , FileStatus summaryStatus ) throws IOException { final Path parent = summaryStatus . getPath ( ) . getParent ( ) ; ParquetMetadata mergedFooters = readFooter ( configuration , summaryStatus , filter ( false ) ) ; return footersFromSummaryFile ( parent , mergedFooters ) ; }", "nl": "Specifically reads a given summary file"}}
{"translation": {"code": "public void writeDataPage ( int valueCount , int uncompressedPageSize , BytesInput bytes , Statistics statistics , long rowCount , Encoding rlEncoding , Encoding dlEncoding , Encoding valuesEncoding ) throws IOException { long beforeHeader = out . getPos ( ) ; innerWriteDataPage ( valueCount , uncompressedPageSize , bytes , statistics , rlEncoding , dlEncoding , valuesEncoding ) ; offsetIndexBuilder . add ( ( int ) ( out . getPos ( ) - beforeHeader ) , rowCount ) ; }", "nl": "Writes a single page"}}
{"translation": {"code": "void writeColumnChunk ( ColumnDescriptor descriptor , long valueCount , CompressionCodecName compressionCodecName , DictionaryPage dictionaryPage , BytesInput bytes , long uncompressedTotalPageSize , long compressedTotalPageSize , Statistics < ? > totalStats , ColumnIndexBuilder columnIndexBuilder , OffsetIndexBuilder offsetIndexBuilder , Set < Encoding > rlEncodings , Set < Encoding > dlEncodings , List < Encoding > dataEncodings ) throws IOException { startColumn ( descriptor , valueCount , compressionCodecName ) ; state = state . write ( ) ; if ( dictionaryPage != null ) { writeDictionaryPage ( dictionaryPage ) ; } LOG . debug ( \"{}: write data pages\" , out . getPos ( ) ) ; long headersSize = bytes . size ( ) - compressedTotalPageSize ; this . uncompressedLength += uncompressedTotalPageSize + headersSize ; this . compressedLength += compressedTotalPageSize + headersSize ; LOG . debug ( \"{}: write data pages content\" , out . getPos ( ) ) ; firstPageOffset = out . getPos ( ) ; bytes . writeAllTo ( out ) ; encodingStatsBuilder . addDataEncodings ( dataEncodings ) ; if ( rlEncodings . isEmpty ( ) ) { encodingStatsBuilder . withV2Pages ( ) ; } currentEncodings . addAll ( rlEncodings ) ; currentEncodings . addAll ( dlEncodings ) ; currentEncodings . addAll ( dataEncodings ) ; currentStatistics = totalStats ; this . columnIndexBuilder = columnIndexBuilder ; this . offsetIndexBuilder = offsetIndexBuilder ; endColumn ( ) ; }", "nl": "Writes a column chunk at once"}}
{"translation": {"code": "private static GroupType listWrapper ( Repetition repetition , String alias , LogicalTypeAnnotation logicalTypeAnnotation , Type nested ) { if ( ! nested . isRepetition ( Repetition . REPEATED ) ) { throw new IllegalArgumentException ( \"Nested type should be repeated: \" + nested ) ; } return new GroupType ( repetition , alias , logicalTypeAnnotation , nested ) ; }", "nl": "to preserve the difference between empty list and null when optional"}}
{"translation": {"code": "public static void setSchema ( Job job , MessageType schema ) { GroupWriteSupport . setSchema ( schema , ContextUtil . getConfiguration ( job ) ) ; }", "nl": "set the schema being written to the job conf"}}
{"translation": {"code": "public Map < String , String > toExtraMetaData ( ) { final Map < String , String > map = new HashMap < String , String > ( ) ; map . put ( THRIFT_CLASS , getThriftClass ( ) . getName ( ) ) ; map . put ( THRIFT_DESCRIPTOR , descriptor . toJSON ( ) ) ; return map ; }", "nl": "generates a map of key values to store in the footer"}}
{"translation": {"code": "public static ThriftMetaData fromExtraMetaData ( Map < String , String > extraMetaData ) { final String thriftClassName = extraMetaData . get ( THRIFT_CLASS ) ; final String thriftDescriptorString = extraMetaData . get ( THRIFT_DESCRIPTOR ) ; if ( thriftClassName == null || thriftDescriptorString == null ) { return null ; } final StructType descriptor = parseDescriptor ( thriftDescriptorString ) ; return new ThriftMetaData ( thriftClassName , descriptor ) ; }", "nl": "Reads ThriftMetadata from the parquet file footer ."}}
{"translation": {"code": "public MessageType convert ( StructType struct ) { MessageType messageType = ThriftSchemaConvertVisitor . convert ( struct , fieldProjectionFilter , true ) ; fieldProjectionFilter . assertNoUnmatchedPatterns ( ) ; return messageType ; }", "nl": "struct is assumed to contain valid structOrUnionType metadata when used with this method . This method may throw if structOrUnionType is unknown ."}}
{"translation": {"code": "@ Override public void readOne ( TProtocol in , TProtocol out ) throws TException { readOneStruct ( in , out ) ; }", "nl": "reads one record from in and writes it to out exceptions are not recoverable as record might be halfway written"}}
{"translation": {"code": "@ Override public void close ( ) throws IOException { try { recordWriter . close ( taskAttemptContext ) ; } catch ( InterruptedException e ) { Thread . interrupted ( ) ; throw new IOException ( \"The thread was interrupted\" , e ) ; } }", "nl": "close the file"}}
{"translation": {"code": "@ Deprecated public static void writeMetadataFile ( Configuration configuration , Path outputPath , List < Footer > footers ) throws IOException { writeMetadataFile ( configuration , outputPath , footers , JobSummaryLevel . ALL ) ; }", "nl": "writes a _metadata and _common_metadata file"}}
{"translation": {"code": "static GlobalMetaData mergeInto ( FileMetaData toMerge , GlobalMetaData mergedMetadata ) { return mergeInto ( toMerge , mergedMetadata , true ) ; }", "nl": "Will return the result of merging toMerge into mergedMetadata"}}
{"translation": {"code": "public void writeDictionaryPage ( DictionaryPage dictionaryPage ) throws IOException { state = state . write ( ) ; LOG . debug ( \"{}: write dictionary page: {} values\" , out . getPos ( ) , dictionaryPage . getDictionarySize ( ) ) ; currentChunkDictionaryPageOffset = out . getPos ( ) ; int uncompressedSize = dictionaryPage . getUncompressedSize ( ) ; int compressedPageSize = ( int ) dictionaryPage . getBytes ( ) . size ( ) ; // TODO: fix casts metadataConverter . writeDictionaryPageHeader ( uncompressedSize , compressedPageSize , dictionaryPage . getDictionarySize ( ) , dictionaryPage . getEncoding ( ) , out ) ; long headerSize = out . getPos ( ) - currentChunkDictionaryPageOffset ; this . uncompressedLength += uncompressedSize + headerSize ; this . compressedLength += compressedPageSize + headerSize ; LOG . debug ( \"{}: write dictionary page content {}\" , out . getPos ( ) , compressedPageSize ) ; dictionaryPage . getBytes ( ) . writeAllTo ( out ) ; encodingStatsBuilder . addDictEncoding ( dictionaryPage . getEncoding ( ) ) ; currentEncodings . add ( dictionaryPage . getEncoding ( ) ) ; }", "nl": "writes a dictionary page page"}}
{"translation": {"code": "private void writeValue ( Type type , Schema avroSchema , Object value ) { Schema nonNullAvroSchema = AvroSchemaConverter . getNonNull ( avroSchema ) ; LogicalType logicalType = nonNullAvroSchema . getLogicalType ( ) ; if ( logicalType != null ) { Conversion < ? > conversion = model . getConversionByClass ( value . getClass ( ) , logicalType ) ; writeValueWithoutConversion ( type , nonNullAvroSchema , convert ( nonNullAvroSchema , logicalType , conversion , value ) ) ; } else { writeValueWithoutConversion ( type , nonNullAvroSchema , value ) ; } }", "nl": "Calls an appropriate write method based on the value . Value MUST not be null ."}}
{"translation": {"code": "@ SuppressWarnings ( \"unchecked\" ) private void writeValueWithoutConversion ( Type type , Schema avroSchema , Object value ) { switch ( avroSchema . getType ( ) ) { case BOOLEAN : recordConsumer . addBoolean ( ( Boolean ) value ) ; break ; case INT : if ( value instanceof Character ) { recordConsumer . addInteger ( ( Character ) value ) ; } else { recordConsumer . addInteger ( ( ( Number ) value ) . intValue ( ) ) ; } break ; case LONG : recordConsumer . addLong ( ( ( Number ) value ) . longValue ( ) ) ; break ; case FLOAT : recordConsumer . addFloat ( ( ( Number ) value ) . floatValue ( ) ) ; break ; case DOUBLE : recordConsumer . addDouble ( ( ( Number ) value ) . doubleValue ( ) ) ; break ; case FIXED : recordConsumer . addBinary ( Binary . fromReusedByteArray ( ( ( GenericFixed ) value ) . bytes ( ) ) ) ; break ; case BYTES : if ( value instanceof byte [ ] ) { recordConsumer . addBinary ( Binary . fromReusedByteArray ( ( byte [ ] ) value ) ) ; } else { recordConsumer . addBinary ( Binary . fromReusedByteBuffer ( ( ByteBuffer ) value ) ) ; } break ; case STRING : recordConsumer . addBinary ( fromAvroString ( value ) ) ; break ; case RECORD : writeRecord ( type . asGroupType ( ) , avroSchema , value ) ; break ; case ENUM : recordConsumer . addBinary ( Binary . fromString ( value . toString ( ) ) ) ; break ; case ARRAY : listWriter . writeList ( type . asGroupType ( ) , avroSchema , value ) ; break ; case MAP : writeMap ( type . asGroupType ( ) , avroSchema , ( Map < CharSequence , ? > ) value ) ; break ; case UNION : writeUnion ( type . asGroupType ( ) , avroSchema , value ) ; break ; }", "nl": "Calls an appropriate write method based on the value . Value must not be null and the schema must not be nullable ."}}
{"translation": {"code": "public static void setSchema ( Job job , Schema schema ) { AvroWriteSupport . setSchema ( ContextUtil . getConfiguration ( job ) , schema ) ; }", "nl": "Set the Avro schema to use for writing . The schema is translated into a Parquet schema so that the records can be written in Parquet format . It is also stored in the Parquet metadata so that records can be reconstructed as Avro objects at read time without specifying a read schema ."}}
{"translation": {"code": "private void init ( final JobConf job ) { final String plan = HiveConf . getVar ( job , HiveConf . ConfVars . PLAN ) ; if ( mrwork == null && plan != null && plan . length ( ) > 0 ) { mrwork = Utilities . getMapRedWork ( job ) ; pathToPartitionInfo . clear ( ) ; for ( final Map . Entry < String , PartitionDesc > entry : mrwork . getPathToPartitionInfo ( ) . entrySet ( ) ) { pathToPartitionInfo . put ( new Path ( entry . getKey ( ) ) . toUri ( ) . getPath ( ) . toString ( ) , entry . getValue ( ) ) ; } } }", "nl": "Initialize the mrwork variable in order to get all the partition and start to update the jobconf"}}
{"translation": {"code": "private static GroupType convertStructType ( final String name , final StructTypeInfo typeInfo ) { final List < String > columnNames = typeInfo . getAllStructFieldNames ( ) ; final List < TypeInfo > columnTypes = typeInfo . getAllStructFieldTypeInfos ( ) ; return new GroupType ( Repetition . OPTIONAL , name , convertTypes ( columnNames , columnTypes ) ) ; }", "nl": "An optional group containing multiple elements"}}
{"translation": {"code": "private static GroupType convertArrayType ( final String name , final ListTypeInfo typeInfo ) { final TypeInfo subType = typeInfo . getListElementTypeInfo ( ) ; return listWrapper ( name , listType ( ) , new GroupType ( Repetition . REPEATED , ParquetHiveSerDe . ARRAY . toString ( ) , convertType ( \"array_element\" , subType ) ) ) ; }", "nl": "1 anonymous element array_element"}}
{"translation": {"code": "public void writeInt ( int value ) throws IOException { input [ inputSize ] = value ; ++ inputSize ; if ( inputSize == VALUES_WRITTEN_AT_A_TIME ) { pack ( ) ; if ( packedPosition == slabSize ) { slabs . add ( BytesInput . from ( packed ) ) ; totalFullSlabSize += slabSize ; if ( slabSize < bitWidth * MAX_SLAB_SIZE_MULT ) { slabSize *= 2 ; } initPackedSlab ( ) ; } } }", "nl": "writes an int using the requested number of bits . accepts only values less than 2^bitWidth"}}
{"translation": {"code": "public List < Footer > getFooters ( Configuration configuration , Collection < FileStatus > statuses ) throws IOException { LOG . debug ( \"reading {} files\" , statuses . size ( ) ) ; boolean taskSideMetaData = isTaskSideMetaData ( configuration ) ; return ParquetFileReader . readAllFootersInParallelUsingSummaryFiles ( configuration , statuses , taskSideMetaData ) ; }", "nl": "the footers for the files"}}
{"translation": {"code": "public static Schema getNonNull ( Schema schema ) { if ( schema . getType ( ) . equals ( Schema . Type . UNION ) ) { List < Schema > schemas = schema . getTypes ( ) ; if ( schemas . size ( ) == 2 ) { if ( schemas . get ( 0 ) . getType ( ) . equals ( Schema . Type . NULL ) ) { return schemas . get ( 1 ) ; } else if ( schemas . get ( 1 ) . getType ( ) . equals ( Schema . Type . NULL ) ) { return schemas . get ( 0 ) ; } else { return schema ; } } else { return schema ; } } else { return schema ; } }", "nl": "Given a schema check to see if it is a union of a null type and a regular schema and then return the non - null sub - schema . Otherwise return the given schema ."}}
{"translation": {"code": "public static TaskAttemptContext newTaskAttemptContext ( Configuration conf , TaskAttemptID taskAttemptId ) { try { return ( TaskAttemptContext ) TASK_CONTEXT_CONSTRUCTOR . newInstance ( conf , taskAttemptId ) ; } catch ( InstantiationException e ) { throw new IllegalArgumentException ( \"Can't instantiate TaskAttemptContext\" , e ) ; } catch ( IllegalAccessException e ) { throw new IllegalArgumentException ( \"Can't instantiate TaskAttemptContext\" , e ) ; } catch ( InvocationTargetException e ) { throw new IllegalArgumentException ( \"Can't instantiate TaskAttemptContext\" , e ) ; } }", "nl": "Creates TaskAttemptContext from a JobConf and jobId using the correct constructor for based on Hadoop version ."}}
{"translation": {"code": "private void addSlab ( int minimumSize ) { int nextSlabSize ; if ( bytesUsed == 0 ) { nextSlabSize = initialSlabSize ; } else if ( bytesUsed > maxCapacityHint / 5 ) { // to avoid an overhead of up to twice the needed size, we get linear when approaching target page size nextSlabSize = maxCapacityHint / 5 ; } else { // double the size every time nextSlabSize = bytesUsed ; } if ( nextSlabSize < minimumSize ) { LOG . debug ( \"slab size {} too small for value of size {}. Bumping up slab size\" , nextSlabSize , minimumSize ) ; nextSlabSize = minimumSize ; } LOG . debug ( \"used {} slabs, adding new slab of size {}\" , slabs . size ( ) , nextSlabSize ) ; this . currentSlab = allocator . allocate ( nextSlabSize ) ; this . slabs . add ( currentSlab ) ; this . bytesAllocated += nextSlabSize ; this . currentSlabIndex = 0 ; }", "nl": "the new slab is guaranteed to be at least minimumSize"}}
{"translation": {"code": "public void setByte ( long index , byte value ) { checkArgument ( index < bytesUsed , \"Index: \" + index + \" is >= the current size of: \" + bytesUsed ) ; long seen = 0 ; for ( int i = 0 ; i < slabs . size ( ) ; i ++ ) { ByteBuffer slab = slabs . get ( i ) ; if ( index < seen + slab . limit ( ) ) { // ok found index slab . put ( ( int ) ( index - seen ) , value ) ; break ; } seen += slab . limit ( ) ; } }", "nl": "Replace the byte stored at position index in this stream with value"}}
{"translation": {"code": "private void endPreviousBitPackedRun ( ) { if ( bitPackedRunHeaderPointer == - 1 ) { // we're not currently in a bit-packed-run return ; } // create bit-packed-header, which needs to fit in 1 byte byte bitPackHeader = ( byte ) ( ( bitPackedGroupCount << 1 ) | 1 ) ; // update this byte baos . setByte ( bitPackedRunHeaderPointer , bitPackHeader ) ; // mark that this run is over bitPackedRunHeaderPointer = - 1 ; // reset the number of groups bitPackedGroupCount = 0 ; }", "nl": "If we are currently writing a bit - packed - run update the bit - packed - header and consider this run to be over"}}
{"translation": {"code": "public static final UnboundRecordFilter column ( final String columnPath , final ColumnPredicates . Predicate predicate ) { checkNotNull ( columnPath , \"columnPath\" ) ; checkNotNull ( predicate , \"predicate\" ) ; return new UnboundRecordFilter ( ) { final String [ ] filterPath = columnPath . split ( \"\\\\.\" ) ; @ Override public RecordFilter bind ( Iterable < ColumnReader > readers ) { for ( ColumnReader reader : readers ) { if ( Arrays . equals ( reader . getDescriptor ( ) . getPath ( ) , filterPath ) ) { return new ColumnRecordFilter ( reader , predicate ) ; } } throw new IllegalArgumentException ( \"Column \" + columnPath + \" does not exist.\" ) ; } } ; }", "nl": "Factory method for record filter which applies the supplied predicate to the specified column . Note that if searching for a repeated sub - attribute it will only ever match against the first instance of it in the object ."}}
{"translation": {"code": "public static final UnboundRecordFilter page ( final long startPos , final long pageSize ) { return new UnboundRecordFilter ( ) { @ Override public RecordFilter bind ( Iterable < ColumnReader > readers ) { return new PagedRecordFilter ( startPos , pageSize ) ; } } ; }", "nl": "Returns builder for creating a paged query ."}}
{"translation": {"code": "private void skipToMatch ( ) { while ( recordsRead < recordCount && ! recordFilter . isMatch ( ) ) { State currentState = getState ( 0 ) ; do { ColumnReader columnReader = currentState . column ; // currentLevel = depth + 1 at this point // set the current value if ( columnReader . getCurrentDefinitionLevel ( ) >= currentState . maxDefinitionLevel ) { columnReader . skip ( ) ; } columnReader . consume ( ) ; // Based on repetition level work out next state to go to int nextR = currentState . maxRepetitionLevel == 0 ? 0 : columnReader . getCurrentRepetitionLevel ( ) ; currentState = currentState . getNextState ( nextR ) ; } while ( currentState != null ) ; ++ recordsRead ; } }", "nl": "Skips forwards until the filter finds the first match . Returns false if none found ."}}
{"translation": {"code": "private void checkEnum ( ThriftType expectedType , int i ) { if ( expectedType . getType ( ) == ThriftTypeID . ENUM ) { ThriftType . EnumType expectedEnumType = ( ThriftType . EnumType ) expectedType ; if ( expectedEnumType . getEnumValueById ( i ) == null ) { throw new DecodingSchemaMismatchException ( \"can not find index \" + i + \" in enum \" + expectedType ) ; } } }", "nl": "In thrift enum values are written as ints this method checks if the enum index is defined ."}}
{"translation": {"code": "private static Object invoke ( Method method , Object obj , Object ... args ) { try { return method . invoke ( obj , args ) ; } catch ( IllegalAccessException e ) { throw new IllegalArgumentException ( \"Can't invoke method \" + method . getName ( ) , e ) ; } catch ( InvocationTargetException e ) { throw new IllegalArgumentException ( \"Can't invoke method \" + method . getName ( ) , e ) ; } }", "nl": "Invokes a method and rethrows any exception as runtime exceptions ."}}
{"translation": {"code": "public void set ( String glob ) { StringBuilder regex = new StringBuilder ( ) ; int setOpen = 0 ; int curlyOpen = 0 ; int len = glob . length ( ) ; hasWildcard = false ; for ( int i = 0 ; i < len ; i ++ ) { char c = glob . charAt ( i ) ; switch ( c ) { case BACKSLASH : if ( ++ i >= len ) { error ( \"Missing escaped character\" , glob , i ) ; } regex . append ( c ) . append ( glob . charAt ( i ) ) ; continue ; case ' ' : case ' ' : case ' ' : case ' ' : case ' ' : case ' ' : // escape regex special chars that are not glob special chars regex . append ( BACKSLASH ) ; break ; case ' ' : if ( i + 1 < len && glob . charAt ( i + 1 ) == ' ' ) { regex . append ( ' ' ) ; i ++ ; break ; } regex . append ( \"[^\" + PATH_SEPARATOR + \"]\" ) ; hasWildcard = true ; break ; case ' ' : regex . append ( ' ' ) ; hasWildcard = true ; continue ; case ' ' : // start of a group regex . append ( \"(?:\" ) ; // non-capturing curlyOpen ++ ; hasWildcard = true ; continue ; case ' ' : regex . append ( curlyOpen > 0 ? ' ' : c ) ; continue ; case ' ' : if ( curlyOpen > 0 ) { // end of a group curlyOpen -- ; regex . append ( \")\" ) ; continue ; } break ; case ' ' : if ( setOpen > 0 ) { error ( \"Unclosed character class\" , glob , i ) ; } setOpen ++ ; hasWildcard = true ; break ; case ' ' : // ^ inside [...] can be unescaped if ( setOpen == 0 ) { regex . append ( BACKSLASH ) ; } break ; case ' ' : // [! needs to be translated to [^ regex . append ( setOpen > 0 && ' ' == glob . charAt ( i - 1 ) ? ' ' : ' ' ) ; continue ; case ' ' : // Many set errors like [][] could not be easily detected here, // as []], []-] and [-] are all valid POSIX glob and java regex. // We'll just let the regex compiler do the real work. setOpen = 0 ; break ; default : } regex . append ( c ) ; } if ( setOpen > 0 ) { error ( \"Unclosed character class\" , glob , len ) ; } if ( curlyOpen > 0 ) { error ( \"Unclosed group\" , glob , len ) ; } compiled = Pattern . compile ( regex . toString ( ) ) ; }", "nl": "Set and compile a glob pattern"}}
{"translation": {"code": "public static void initCounterFromReporter ( Reporter reporter , Configuration configuration ) { counterLoader = new MapRedCounterLoader ( reporter , configuration ) ; loadCounters ( ) ; }", "nl": "Init counters in hadoop s mapred API which is used by cascading and Hive ."}}
{"translation": {"code": "@ Deprecated public Map < String , String > getMergedKeyValueMetaData ( ) { if ( mergedKeyValueMetadata == null ) { Map < String , String > mergedKeyValues = new HashMap < String , String > ( ) ; for ( Entry < String , Set < String > > entry : keyValueMetadata . entrySet ( ) ) { if ( entry . getValue ( ) . size ( ) > 1 ) { throw new RuntimeException ( \"could not merge metadata: key \" + entry . getKey ( ) + \" has conflicting values: \" + entry . getValue ( ) ) ; } mergedKeyValues . put ( entry . getKey ( ) , entry . getValue ( ) . iterator ( ) . next ( ) ) ; } mergedKeyValueMetadata = mergedKeyValues ; } return mergedKeyValueMetadata ; }", "nl": "If there is a conflicting value when reading from multiple files an exception will be thrown"}}
{"translation": {"code": "List < Type > mergeFields ( GroupType toMerge , boolean strict ) { List < Type > newFields = new ArrayList < Type > ( ) ; // merge existing fields for ( Type type : this . getFields ( ) ) { Type merged ; if ( toMerge . containsField ( type . getName ( ) ) ) { Type fieldToMerge = toMerge . getType ( type . getName ( ) ) ; if ( type . getLogicalTypeAnnotation ( ) != null && ! type . getLogicalTypeAnnotation ( ) . equals ( fieldToMerge . getLogicalTypeAnnotation ( ) ) ) { throw new IncompatibleSchemaModificationException ( \"cannot merge logical type \" + fieldToMerge . getLogicalTypeAnnotation ( ) + \" into \" + type . getLogicalTypeAnnotation ( ) ) ; } merged = type . union ( fieldToMerge , strict ) ; } else { merged = type ; } newFields . add ( merged ) ; } // add new fields for ( Type type : toMerge . getFields ( ) ) { if ( ! this . containsField ( type . getName ( ) ) ) { newFields . add ( type ) ; } } return newFields ; }", "nl": "produces the list of fields resulting from merging toMerge into the fields of this"}}
{"translation": {"code": "private void checkSet ( Iterator < TProtocol > eventIter , ThriftField setFieldDefinition ) throws TException { TSet thriftSet = acceptProtocol ( eventIter . next ( ) ) . readSetBegin ( ) ; ThriftField elementFieldDefinition = ( ( ThriftType . SetType ) setFieldDefinition . getType ( ) ) . getValues ( ) ; int setSize = thriftSet . size ; for ( int i = 0 ; i < setSize ; i ++ ) { checkField ( thriftSet . elemType , eventIter , elementFieldDefinition ) ; } acceptProtocol ( eventIter . next ( ) ) . readSetEnd ( ) ; }", "nl": "check each element of the Set make sure all the element contain required fields"}}
{"translation": {"code": "public List < TProtocol > amendMissingRequiredFields ( StructType recordThriftType ) throws TException { Iterator < TProtocol > protocolIter = rootEvents . iterator ( ) ; checkStruct ( protocolIter , recordThriftType ) ; return fixedEvents ; }", "nl": "Given a thrift definition protocols events it checks all the required fields and create default value if a required field is missing"}}
{"translation": {"code": "private void allocateValuesBuffer ( ) { int totalMiniBlockCount = ( int ) Math . ceil ( ( double ) totalValueCount / config . miniBlockSizeInValues ) ; //+ 1 because first value written to header is also stored in values buffer valuesBuffer = new long [ totalMiniBlockCount * config . miniBlockSizeInValues + 1 ] ; }", "nl": "the value buffer is allocated so that the size of it is multiple of mini block because when writing data is flushed on a mini block basis"}}
{"translation": {"code": "@ Override public void initFromPage ( int valueCount , ByteBufferInputStream stream ) throws IOException { this . in = stream ; long startPos = in . position ( ) ; this . config = DeltaBinaryPackingConfig . readConfig ( in ) ; this . totalValueCount = BytesUtils . readUnsignedVarInt ( in ) ; allocateValuesBuffer ( ) ; bitWidths = new int [ config . miniBlockNumInABlock ] ; //read first value from header valuesBuffer [ valuesBuffered ++ ] = BytesUtils . readZigZagVarLong ( in ) ; while ( valuesBuffered < totalValueCount ) { //values Buffered could be more than totalValueCount, since we flush on a mini block basis loadNewBlockToBuffer ( ) ; } updateNextOffset ( ( int ) ( in . position ( ) - startPos ) ) ; }", "nl": "eagerly loads all the data into memory"}}
{"translation": {"code": "private void validatedMapping ( Descriptor descriptor , GroupType parquetSchema ) { List < FieldDescriptor > allFields = descriptor . getFields ( ) ; for ( FieldDescriptor fieldDescriptor : allFields ) { String fieldName = fieldDescriptor . getName ( ) ; int fieldIndex = fieldDescriptor . getIndex ( ) ; int parquetIndex = parquetSchema . getFieldIndex ( fieldName ) ; if ( fieldIndex != parquetIndex ) { String message = \"FieldIndex mismatch name=\" + fieldName + \": \" + fieldIndex + \" != \" + parquetIndex ; throw new IncompatibleSchemaModificationException ( message ) ; } } }", "nl": "validates mapping between protobuffer fields and parquet fields ."}}
{"translation": {"code": "private String serializeDescriptor ( Class < ? extends Message > protoClass ) { Descriptor descriptor = Protobufs . getMessageDescriptor ( protoClass ) ; DescriptorProtos . DescriptorProto asProto = descriptor . toProto ( ) ; return TextFormat . printToString ( asProto ) ; }", "nl": "Returns message descriptor as JSON String"}}
{"translation": {"code": "@ Override public void write ( T record ) { recordConsumer . startMessage ( ) ; try { messageWriter . writeTopLevelMessage ( record ) ; } catch ( RuntimeException e ) { Message m = ( record instanceof Message . Builder ) ? ( ( Message . Builder ) record ) . build ( ) : ( Message ) record ; LOG . error ( \"Cannot write message \" + e . getMessage ( ) + \" : \" + m ) ; throw e ; } recordConsumer . endMessage ( ) ; }", "nl": "Writes Protocol buffer to parquet file ."}}
{"translation": {"code": "protected ParquetInputSplit getSplit ( final InputSplit oldSplit , final JobConf conf ) throws IOException { if ( oldSplit instanceof FileSplit ) { FileSplit fileSplit = ( FileSplit ) oldSplit ; final long splitStart = fileSplit . getStart ( ) ; final long splitLength = fileSplit . getLength ( ) ; final Path finalPath = fileSplit . getPath ( ) ; final JobConf cloneJob = hiveBinding . pushProjectionsAndFilters ( conf , finalPath . getParent ( ) ) ; final ParquetMetadata parquetMetadata = ParquetFileReader . readFooter ( cloneJob , finalPath , SKIP_ROW_GROUPS ) ; final FileMetaData fileMetaData = parquetMetadata . getFileMetaData ( ) ; final ReadContext readContext = new DataWritableReadSupport ( ) . init ( cloneJob , fileMetaData . getKeyValueMetaData ( ) , fileMetaData . getSchema ( ) ) ; schemaSize = MessageTypeParser . parseMessageType ( readContext . getReadSupportMetadata ( ) . get ( DataWritableReadSupport . HIVE_SCHEMA_KEY ) ) . getFieldCount ( ) ; return new ParquetInputSplit ( finalPath , splitStart , splitStart + splitLength , splitLength , fileSplit . getLocations ( ) , null ) ; } else { throw new IllegalArgumentException ( \"Unknown split type: \" + oldSplit ) ; } }", "nl": "gets a ParquetInputSplit corresponding to a split given by Hive"}}
{"translation": {"code": "@ Deprecated public static Statistics getStatsBasedOnType ( PrimitiveTypeName type ) { switch ( type ) { case INT32 : return new IntStatistics ( ) ; case INT64 : return new LongStatistics ( ) ; case FLOAT : return new FloatStatistics ( ) ; case DOUBLE : return new DoubleStatistics ( ) ; case BOOLEAN : return new BooleanStatistics ( ) ; case BINARY : return new BinaryStatistics ( ) ; case INT96 : return new BinaryStatistics ( ) ; case FIXED_LEN_BYTE_ARRAY : return new BinaryStatistics ( ) ; default : throw new UnknownColumnTypeException ( type ) ; } }", "nl": "Returns the typed statistics object based on the passed type parameter"}}
{"translation": {"code": "public void mergeStatistics ( Statistics stats ) { if ( stats . isEmpty ( ) ) return ; // Merge stats only if they have the same type if ( type . equals ( stats . type ) ) { incrementNumNulls ( stats . getNumNulls ( ) ) ; if ( stats . hasNonNullValue ( ) ) { mergeStatisticsMinMax ( stats ) ; markAsNotEmpty ( ) ; } } else { throw StatisticsClassException . create ( this , stats ) ; } }", "nl": "Method to merge this statistics object with the object passed as parameter . Merging keeps the smallest of min values largest of max values and combines the number of null counts ."}}
{"translation": {"code": "public V getCurrentValue ( final K key ) { V value = cacheMap . get ( key ) ; LOG . debug ( \"Value for '{}' {} in cache\" , key , ( value == null ? \"not \" : \"\" ) ) ; if ( value != null && ! value . isCurrent ( key ) ) { // value is not current; remove it and return null remove ( key ) ; return null ; } return value ; }", "nl": "Returns the value to which the specified key is mapped or null if 1 ) the value is not current or 2 ) this cache contains no mapping for the key ."}}
{"translation": {"code": "public V remove ( final K key ) { V oldValue = cacheMap . remove ( key ) ; if ( oldValue != null ) { LOG . debug ( \"Removed cache entry for '{}'\" , key ) ; } return oldValue ; }", "nl": "Removes the mapping for the specified key from this cache if present ."}}
{"translation": {"code": "public void put ( final K key , final V newValue ) { if ( newValue == null || ! newValue . isCurrent ( key ) ) { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( \"Ignoring new cache entry for '{}' because it is {}\" , key , ( newValue == null ? \"null\" : \"not current\" ) ) ; } return ; } V oldValue = cacheMap . get ( key ) ; if ( oldValue != null && oldValue . isNewerThan ( newValue ) ) { if ( LOG . isWarnEnabled ( ) ) { LOG . warn ( \"Ignoring new cache entry for '{}' because \" + \"existing cache entry is newer\" , key ) ; } return ; } // no existing value or new value is newer than old value oldValue = cacheMap . put ( key , newValue ) ; if ( LOG . isDebugEnabled ( ) ) { if ( oldValue == null ) { LOG . debug ( \"Added new cache entry for '{}'\" , key ) ; } else { LOG . debug ( \"Overwrote existing cache entry for '{}'\" , key ) ; } } }", "nl": "Associates the specified value with the specified key in this cache . The value is only inserted if it is not null and it is considered current . If the cache previously contained a mapping for the key the old value is replaced only if the new value is newer than the old one ."}}
{"translation": {"code": "private MessageType resolveSchemaAccess ( MessageType requestedSchema , MessageType fileSchema , Configuration configuration ) { if ( configuration . getBoolean ( PARQUET_COLUMN_INDEX_ACCESS , false ) ) { final List < String > listColumns = getColumns ( configuration . get ( IOConstants . COLUMNS ) ) ; List < Type > requestedTypes = new ArrayList < Type > ( ) ; for ( Type t : requestedSchema . getFields ( ) ) { int index = listColumns . indexOf ( t . getName ( ) ) ; requestedTypes . add ( fileSchema . getType ( index ) ) ; } requestedSchema = new MessageType ( requestedSchema . getName ( ) , requestedTypes ) ; } return requestedSchema ; }", "nl": "Determine the file column names based on the position within the requested columns and use that as the requested schema ."}}
{"translation": {"code": "public static < T extends Comparable < T > > void assertTypeValid ( Column < T > foundColumn , PrimitiveTypeName primitiveType ) { Class < T > foundColumnType = foundColumn . getColumnType ( ) ; ColumnPath columnPath = foundColumn . getColumnPath ( ) ; Set < PrimitiveTypeName > validTypeDescriptors = classToParquetType . get ( foundColumnType ) ; if ( validTypeDescriptors == null ) { StringBuilder message = new StringBuilder ( ) ; message . append ( \"Column \" ) . append ( columnPath . toDotString ( ) ) . append ( \" was declared as type: \" ) . append ( foundColumnType . getName ( ) ) . append ( \" which is not supported in FilterPredicates.\" ) ; Set < Class < ? > > supportedTypes = parquetTypeToClass . get ( primitiveType ) ; if ( supportedTypes != null ) { message . append ( \" Supported types for this column are: \" ) . append ( supportedTypes ) ; } else { message . append ( \" There are no supported types for columns of \" + primitiveType ) ; } throw new IllegalArgumentException ( message . toString ( ) ) ; } if ( ! validTypeDescriptors . contains ( primitiveType ) ) { StringBuilder message = new StringBuilder ( ) ; message . append ( \"FilterPredicate column: \" ) . append ( columnPath . toDotString ( ) ) . append ( \"'s declared type (\" ) . append ( foundColumnType . getName ( ) ) . append ( \") does not match the schema found in file metadata. Column \" ) . append ( columnPath . toDotString ( ) ) . append ( \" is of type: \" ) . append ( primitiveType ) . append ( \"\\nValid types for this column are: \" ) . append ( parquetTypeToClass . get ( primitiveType ) ) ; throw new IllegalArgumentException ( message . toString ( ) ) ; } }", "nl": "Asserts that foundColumn was declared as a type that is compatible with the type for this column found in the schema of the parquet file ."}}
{"translation": {"code": "public static Filter getFilter ( Configuration conf ) { return FilterCompat . get ( getFilterPredicate ( conf ) , getUnboundRecordFilterInstance ( conf ) ) ; }", "nl": "Returns a non - null Filter which is a wrapper around either a FilterPredicate an UnboundRecordFilter or a no - op filter ."}}
{"translation": {"code": "public static void writeObjectToConfAsBase64 ( String key , Object obj , Configuration conf ) throws IOException { try ( ByteArrayOutputStream baos = new ByteArrayOutputStream ( ) ) { try ( GZIPOutputStream gos = new GZIPOutputStream ( baos ) ; ObjectOutputStream oos = new ObjectOutputStream ( gos ) ) { oos . writeObject ( obj ) ; } conf . set ( key , new String ( Base64 . encodeBase64 ( baos . toByteArray ( ) ) , StandardCharsets . UTF_8 ) ) ; } }", "nl": "Writes an object to a configuration ."}}
{"translation": {"code": "private static void add ( Class < ? > c , PrimitiveTypeName p ) { Set < PrimitiveTypeName > descriptors = classToParquetType . get ( c ) ; if ( descriptors == null ) { descriptors = new HashSet < PrimitiveTypeName > ( ) ; classToParquetType . put ( c , descriptors ) ; } descriptors . add ( p ) ; Set < Class < ? > > classes = parquetTypeToClass . get ( p ) ; if ( classes == null ) { classes = new HashSet < Class < ? > > ( ) ; parquetTypeToClass . put ( p , classes ) ; } classes . add ( c ) ; }", "nl": "set up the mapping in both directions"}}
{"translation": {"code": "public void readMapEntry ( byte keyType , TypedConsumer keyConsumer , byte valueType , TypedConsumer valueConsumer ) throws TException { keyConsumer . read ( protocol , this , keyType ) ; valueConsumer . read ( protocol , this , valueType ) ; }", "nl": "reads a key - value pair"}}
{"translation": {"code": "public void readStruct ( FieldConsumer c ) throws TException { protocol . readStructBegin ( ) ; readStructContent ( c ) ; protocol . readStructEnd ( ) ; }", "nl": "reads a Struct from the underlying protocol and passes the field events to the FieldConsumer"}}
{"translation": {"code": "public static FileMetaData readFileMetaData ( InputStream from , boolean skipRowGroups ) throws IOException { FileMetaData md = new FileMetaData ( ) ; if ( skipRowGroups ) { readFileMetaData ( from , new DefaultFileMetaDataConsumer ( md ) , skipRowGroups ) ; } else { read ( from , md ) ; } return md ; }", "nl": "reads the meta data from the stream"}}
{"translation": {"code": "public static < T extends TBase < T , ? extends TFieldIdEnum > > ListConsumer listOf ( Class < T > c , final Consumer < List < T > > consumer ) { class ListConsumer implements Consumer < T > { List < T > list ; @ Override public void consume ( T t ) { list . add ( t ) ; } } final ListConsumer co = new ListConsumer ( ) ; return new DelegatingListElementsConsumer ( struct ( c , co ) ) { @ Override public void consumeList ( TProtocol protocol , EventBasedThriftReader reader , TList tList ) throws TException { co . list = new ArrayList < T > ( ) ; super . consumeList ( protocol , reader , tList ) ; consumer . consume ( co . list ) ; } } ; }", "nl": "To consume a list of elements"}}
{"translation": {"code": "static < T > List < ParquetInputSplit > generateSplits ( List < BlockMetaData > rowGroupBlocks , BlockLocation [ ] hdfsBlocksArray , FileStatus fileStatus , String requestedSchema , Map < String , String > readSupportMetadata , long minSplitSize , long maxSplitSize ) throws IOException { List < SplitInfo > splitRowGroups = generateSplitInfo ( rowGroupBlocks , hdfsBlocksArray , minSplitSize , maxSplitSize ) ; //generate splits from rowGroups of each split List < ParquetInputSplit > resultSplits = new ArrayList < ParquetInputSplit > ( ) ; for ( SplitInfo splitInfo : splitRowGroups ) { ParquetInputSplit split = splitInfo . getParquetInputSplit ( fileStatus , requestedSchema , readSupportMetadata ) ; resultSplits . add ( split ) ; } return resultSplits ; }", "nl": "groups together all the data blocks for the same HDFS block"}}
{"translation": {"code": "@ Deprecated public void writeDataPageV2Header ( int uncompressedSize , int compressedSize , int valueCount , int nullCount , int rowCount , org . apache . parquet . column . statistics . Statistics statistics , org . apache . parquet . column . Encoding dataEncoding , int rlByteLength , int dlByteLength , OutputStream to ) throws IOException { writePageHeader ( newDataPageV2Header ( uncompressedSize , compressedSize , valueCount , nullCount , rowCount , dataEncoding , rlByteLength , dlByteLength ) , to ) ; }", "nl": "Statistics are no longer saved in page headers"}}
{"translation": {"code": "private void updateAllocation ( ) { long totalAllocations = 0 ; for ( Long allocation : writerList . values ( ) ) { totalAllocations += allocation ; } if ( totalAllocations <= totalMemoryPool ) { scale = 1.0 ; } else { scale = ( double ) totalMemoryPool / totalAllocations ; LOG . warn ( String . format ( \"Total allocation exceeds %.2f%% (%,d bytes) of heap memory\\n\" + \"Scaling row group sizes to %.2f%% for %d writers\" , 100 * memoryPoolRatio , totalMemoryPool , 100 * scale , writerList . size ( ) ) ) ; for ( Runnable callBack : callBacks . values ( ) ) { // we do not really want to start a new thread here. callBack . run ( ) ; } } int maxColCount = 0 ; for ( InternalParquetRecordWriter w : writerList . keySet ( ) ) { maxColCount = Math . max ( w . getSchema ( ) . getColumns ( ) . size ( ) , maxColCount ) ; } for ( Map . Entry < InternalParquetRecordWriter , Long > entry : writerList . entrySet ( ) ) { long newSize = ( long ) Math . floor ( entry . getValue ( ) * scale ) ; if ( scale < 1.0 && minMemoryAllocation > 0 && newSize < minMemoryAllocation ) { throw new ParquetRuntimeException ( String . format ( \"New Memory allocation %d bytes\" + \" is smaller than the minimum allocation size of %d bytes.\" , newSize , minMemoryAllocation ) ) { } ; } entry . getKey ( ) . setRowGroupSizeThreshold ( newSize ) ; LOG . debug ( String . format ( \"Adjust block size from %,d to %,d for writer: %s\" , entry . getValue ( ) , newSize , entry . getKey ( ) ) ) ; } }", "nl": "Update the allocated size of each writer based on the current allocations and pool size ."}}
{"translation": {"code": "synchronized void removeWriter ( InternalParquetRecordWriter writer ) { if ( writerList . containsKey ( writer ) ) { writerList . remove ( writer ) ; } if ( ! writerList . isEmpty ( ) ) { updateAllocation ( ) ; } }", "nl": "Remove the given writer from the memory manager ."}}
{"translation": {"code": "synchronized void addWriter ( InternalParquetRecordWriter writer , Long allocation ) { Long oldValue = writerList . get ( writer ) ; if ( oldValue == null ) { writerList . put ( writer , allocation ) ; } else { throw new IllegalArgumentException ( \"[BUG] The Parquet Memory Manager should not add an \" + \"instance of InternalParquetRecordWriter more than once. The Manager already contains \" + \"the writer: \" + writer ) ; } updateAllocation ( ) ; }", "nl": "Add a new writer and its memory allocation to the memory manager ."}}
{"translation": {"code": "private void calculateBitWidthsForDeltaBlockBuffer ( int miniBlocksToFlush ) { for ( int miniBlockIndex = 0 ; miniBlockIndex < miniBlocksToFlush ; miniBlockIndex ++ ) { int mask = 0 ; int miniStart = miniBlockIndex * config . miniBlockSizeInValues ; //The end of current mini block could be the end of current block(deltaValuesToFlush) buffer when data is not aligned to mini block int miniEnd = Math . min ( ( miniBlockIndex + 1 ) * config . miniBlockSizeInValues , deltaValuesToFlush ) ; for ( int i = miniStart ; i < miniEnd ; i ++ ) { mask |= deltaBlockBuffer [ i ] ; } bitWidths [ miniBlockIndex ] = 32 - Integer . numberOfLeadingZeros ( mask ) ; } }", "nl": "iterate through values in each mini block and calculate the bitWidths of max values ."}}
{"translation": {"code": "private static String annotateMessage ( String message , int pos ) { StringBuilder sb = new StringBuilder ( message ) ; sb . append ( ' ' ) ; for ( int i = 0 ; i < pos ; i ++ ) { sb . append ( ' ' ) ; } sb . append ( ' ' ) ; return sb . toString ( ) ; }", "nl": "for pretty printing which character had the error"}}
{"translation": {"code": "public void registerScaleCallBack ( String callBackName , Runnable callBack ) { Preconditions . checkNotNull ( callBackName , \"callBackName\" ) ; Preconditions . checkNotNull ( callBack , \"callBack\" ) ; if ( callBacks . containsKey ( callBackName ) ) { throw new IllegalArgumentException ( \"The callBackName \" + callBackName + \" is duplicated and has been registered already.\" ) ; } else { callBacks . put ( callBackName , callBack ) ; } }", "nl": "Register callback and deduplicate it if any ."}}
{"translation": {"code": "public static GroupType listOfElements ( Repetition listRepetition , String name , Type elementType ) { Preconditions . checkArgument ( elementType . getName ( ) . equals ( ELEMENT_NAME ) , \"List element type must be named 'element'\" ) ; return listWrapper ( listRepetition , name , LogicalTypeAnnotation . listType ( ) , new GroupType ( Repetition . REPEATED , \"list\" , elementType ) ) ; }", "nl": "Creates a 3 - level list structure annotated with LIST with elements of the given elementType . The repeated level is inserted automatically and the elementType s repetition should be the correct repetition of the elements required for non - null and optional for nullable ."}}
{"translation": {"code": "@ Deprecated public static void writeMergedMetadataFile ( List < Path > files , Path outputPath , Configuration conf ) throws IOException { ParquetMetadata merged = mergeMetadataFiles ( files , conf ) ; writeMetadataFile ( outputPath , merged , outputPath . getFileSystem ( conf ) ) ; }", "nl": "Given a list of metadata files merge them into a single metadata file . Requires that the schemas be compatible and the extraMetaData be exactly equal . This is useful when merging 2 directories of parquet files into a single directory as long as both directories were written with compatible schemas and equal extraMetaData ."}}
{"translation": {"code": "@ Deprecated public static ParquetMetadata mergeMetadataFiles ( List < Path > files , Configuration conf ) throws IOException { Preconditions . checkArgument ( ! files . isEmpty ( ) , \"Cannot merge an empty list of metadata\" ) ; GlobalMetaData globalMetaData = null ; List < BlockMetaData > blocks = new ArrayList < BlockMetaData > ( ) ; for ( Path p : files ) { ParquetMetadata pmd = ParquetFileReader . readFooter ( conf , p , ParquetMetadataConverter . NO_FILTER ) ; FileMetaData fmd = pmd . getFileMetaData ( ) ; globalMetaData = mergeInto ( fmd , globalMetaData , true ) ; blocks . addAll ( pmd . getBlocks ( ) ) ; } // collapse GlobalMetaData into a single FileMetaData, which will throw if they are not compatible return new ParquetMetadata ( globalMetaData . merge ( ) , blocks ) ; }", "nl": "Given a list of metadata files merge them into a single ParquetMetadata Requires that the schemas be compatible and the extraMetadata be exactly equal ."}}
{"translation": {"code": "public static CodecFactory createDirectCodecFactory ( Configuration config , ByteBufferAllocator allocator , int pageSize ) { return new DirectCodecFactory ( config , allocator , pageSize ) ; }", "nl": "Create a codec factory that will provide compressors and decompressors that will work natively with ByteBuffers backed by direct memory ."}}
{"translation": {"code": "public static int readIntLittleEndian ( ByteBuffer in , int offset ) throws IOException { int ch4 = in . get ( offset ) & 0xff ; int ch3 = in . get ( offset + 1 ) & 0xff ; int ch2 = in . get ( offset + 2 ) & 0xff ; int ch1 = in . get ( offset + 3 ) & 0xff ; return ( ( ch1 << 24 ) + ( ch2 << 16 ) + ( ch3 << 8 ) + ( ch4 << 0 ) ) ; }", "nl": "reads an int in little endian at the given position"}}
{"translation": {"code": "private static void copy ( SeekableInputStream from , PositionOutputStream to , long start , long length ) throws IOException { LOG . debug ( \"Copying {} bytes at {} to {}\" , length , start , to . getPos ( ) ) ; from . seek ( start ) ; long bytesCopied = 0 ; byte [ ] buffer = COPY_BUFFER . get ( ) ; while ( bytesCopied < length ) { long bytesLeft = length - bytesCopied ; int bytesRead = from . read ( buffer , 0 , ( buffer . length < bytesLeft ? buffer . length : ( int ) bytesLeft ) ) ; if ( bytesRead < 0 ) { throw new IllegalArgumentException ( \"Unexpected end of input file at \" + start + bytesCopied ) ; } to . write ( buffer , 0 , bytesRead ) ; bytesCopied += bytesRead ; } }", "nl": "Copy from a FS input stream to an output stream . Thread - safe"}}