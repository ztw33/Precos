{"translation": {"code": "def read ( self , data_path : str , * args , * * kwargs ) -> Dict [ str , List [ Tuple [ Any , Any ] ] ] : raise NotImplementedError", "nl": "Reads a file from a path and returns data as a list of tuples of inputs and correct outputs for every data type in train valid and test ."}}
{"translation": {"code": "def register ( name : str = None ) -> type : def decorate ( model_cls : type , reg_name : str = None ) -> type : model_name = reg_name or short_name ( model_cls ) global _REGISTRY cls_name = model_cls . __module__ + ':' + model_cls . __name__ if model_name in _REGISTRY and _REGISTRY [ model_name ] != cls_name : logger . warning ( 'Registry name \"{}\" has been already registered and will be overwritten.' . format ( model_name ) ) _REGISTRY [ model_name ] = cls_name return model_cls return lambda model_cls_name : decorate ( model_cls_name , name )", "nl": "Register classes that could be initialized from JSON configuration file . If name is not passed the class name is converted to snake - case ."}}
{"translation": {"code": "def from_params ( params : Dict , mode : str = 'infer' , serialized : Any = None , * * kwargs ) -> Component : # what is passed in json: config_params = { k : _resolve ( v ) for k , v in params . items ( ) } # get component by reference (if any) if 'ref' in config_params : try : component = _refs [ config_params [ 'ref' ] ] if serialized is not None : component . deserialize ( serialized ) return component except KeyError : e = ConfigError ( 'Component with id \"{id}\" was referenced but not initialized' . format ( id = config_params [ 'ref' ] ) ) log . exception ( e ) raise e elif 'config_path' in config_params : from deeppavlov . core . commands . infer import build_model refs = _refs . copy ( ) _refs . clear ( ) config = parse_config ( expand_path ( config_params [ 'config_path' ] ) ) model = build_model ( config , serialized = serialized ) _refs . clear ( ) _refs . update ( refs ) try : _refs [ config_params [ 'id' ] ] = model except KeyError : pass return model cls_name = config_params . pop ( 'class_name' , None ) if not cls_name : e = ConfigError ( 'Component config has no `class_name` nor `ref` fields' ) log . exception ( e ) raise e cls = get_model ( cls_name ) # find the submodels params recursively config_params = { k : _init_param ( v , mode ) for k , v in config_params . items ( ) } try : spec = inspect . getfullargspec ( cls ) if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : kwargs [ 'mode' ] = mode component = cls ( * * dict ( config_params , * * kwargs ) ) try : _refs [ config_params [ 'id' ] ] = component except KeyError : pass except Exception : log . exception ( \"Exception in {}\" . format ( cls ) ) raise if serialized is not None : component . deserialize ( serialized ) return component", "nl": "Builds and returns the Component from corresponding dictionary of parameters ."}}
{"translation": {"code": "def interact_model ( config : Union [ str , Path , dict ] ) -> None : model = build_model ( config ) while True : args = [ ] for in_x in model . in_x : args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) # check for exit command if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : return pred = model ( * args ) if len ( model . out_params ) > 1 : pred = zip ( * pred ) print ( '>>' , * pred )", "nl": "Start interaction with the model described in corresponding configuration file ."}}
{"translation": {"code": "def get_train_op ( self , loss , learning_rate , optimizer = None , clip_norm = None , learnable_scopes = None , optimizer_scope_name = None , * * kwargs ) : if optimizer_scope_name is None : opt_scope = tf . variable_scope ( 'Optimizer' ) else : opt_scope = tf . variable_scope ( optimizer_scope_name ) with opt_scope : if learnable_scopes is None : variables_to_train = tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES ) else : variables_to_train = [ ] for scope_name in learnable_scopes : variables_to_train . extend ( tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES , scope = scope_name ) ) if optimizer is None : optimizer = tf . train . AdamOptimizer # For batch norm it is necessary to update running averages extra_update_ops = tf . get_collection ( tf . GraphKeys . UPDATE_OPS ) with tf . control_dependencies ( extra_update_ops ) : def clip_if_not_none ( grad ) : if grad is not None : return tf . clip_by_norm ( grad , clip_norm ) opt = optimizer ( learning_rate , * * kwargs ) grads_and_vars = opt . compute_gradients ( loss , var_list = variables_to_train ) if clip_norm is not None : grads_and_vars = [ ( clip_if_not_none ( grad ) , var ) for grad , var in grads_and_vars ] train_op = opt . apply_gradients ( grads_and_vars ) return train_op", "nl": "Get train operation for given loss"}}
{"translation": {"code": "def read_requirements ( ) : reqs_path = os . path . join ( __location__ , 'requirements.txt' ) with open ( reqs_path , encoding = 'utf8' ) as f : reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] names = [ ] links = [ ] for req in reqs : if '://' in req : links . append ( req ) else : names . append ( req ) return { 'install_requires' : names , 'dependency_links' : links }", "nl": "parses requirements from requirements . txt"}}
{"translation": {"code": "def download ( dest_file_path : [ List [ Union [ str , Path ] ] ] , source_url : str , force_download = True ) : if isinstance ( dest_file_path , list ) : dest_file_paths = [ Path ( path ) for path in dest_file_path ] else : dest_file_paths = [ Path ( dest_file_path ) . absolute ( ) ] if not force_download : to_check = list ( dest_file_paths ) dest_file_paths = [ ] for p in to_check : if p . exists ( ) : log . info ( f'File already exists in {p}' ) else : dest_file_paths . append ( p ) if dest_file_paths : cache_dir = os . getenv ( 'DP_CACHE_DIR' ) cached_exists = False if cache_dir : first_dest_path = Path ( cache_dir ) / md5 ( source_url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] cached_exists = first_dest_path . exists ( ) else : first_dest_path = dest_file_paths . pop ( ) if not cached_exists : first_dest_path . parent . mkdir ( parents = True , exist_ok = True ) simple_download ( source_url , first_dest_path ) else : log . info ( f'Found cached {source_url} in {first_dest_path}' ) for dest_path in dest_file_paths : dest_path . parent . mkdir ( parents = True , exist_ok = True ) shutil . copy ( str ( first_dest_path ) , str ( dest_path ) )", "nl": "Download a file from URL to one or several target locations"}}
{"translation": {"code": "def untar ( file_path , extract_folder = None ) : file_path = Path ( file_path ) if extract_folder is None : extract_folder = file_path . parent extract_folder = Path ( extract_folder ) tar = tarfile . open ( file_path ) tar . extractall ( extract_folder ) tar . close ( )", "nl": "Simple tar archive extractor"}}
{"translation": {"code": "def download_decompress ( url : str , download_path : [ Path , str ] , extract_paths = None ) : file_name = Path ( urlparse ( url ) . path ) . name download_path = Path ( download_path ) if extract_paths is None : extract_paths = [ download_path ] elif isinstance ( extract_paths , list ) : extract_paths = [ Path ( path ) for path in extract_paths ] else : extract_paths = [ Path ( extract_paths ) ] cache_dir = os . getenv ( 'DP_CACHE_DIR' ) extracted = False if cache_dir : cache_dir = Path ( cache_dir ) url_hash = md5 ( url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] arch_file_path = cache_dir / url_hash extracted_path = cache_dir / ( url_hash + '_extracted' ) extracted = extracted_path . exists ( ) if not extracted and not arch_file_path . exists ( ) : simple_download ( url , arch_file_path ) else : arch_file_path = download_path / file_name simple_download ( url , arch_file_path ) extracted_path = extract_paths . pop ( ) if not extracted : log . info ( 'Extracting {} archive into {}' . format ( arch_file_path , extracted_path ) ) extracted_path . mkdir ( parents = True , exist_ok = True ) if file_name . endswith ( '.tar.gz' ) : untar ( arch_file_path , extracted_path ) elif file_name . endswith ( '.gz' ) : ungzip ( arch_file_path , extracted_path / Path ( file_name ) . with_suffix ( '' ) . name ) elif file_name . endswith ( '.zip' ) : with zipfile . ZipFile ( arch_file_path , 'r' ) as zip_ref : zip_ref . extractall ( extracted_path ) else : raise RuntimeError ( f'Trying to extract an unknown type of archive {file_name}' ) if not cache_dir : arch_file_path . unlink ( ) for extract_path in extract_paths : for src in extracted_path . iterdir ( ) : dest = extract_path / src . name if src . is_dir ( ) : copytree ( src , dest ) else : extract_path . mkdir ( parents = True , exist_ok = True ) shutil . copy ( str ( src ) , str ( dest ) )", "nl": "Download and extract . tar . gz or . gz file to one or several target locations . The archive is deleted if extraction was successful ."}}
{"translation": {"code": "def parse_config ( config : Union [ str , Path , dict ] ) -> dict : if isinstance ( config , ( str , Path ) ) : config = read_json ( find_config ( config ) ) variables = { 'DEEPPAVLOV_PATH' : os . getenv ( f'DP_DEEPPAVLOV_PATH' , Path ( __file__ ) . parent . parent . parent ) } for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : env_name = f'DP_{name}' if env_name in os . environ : value = os . getenv ( env_name ) variables [ name ] = value . format ( * * variables ) return _parse_config_property ( config , variables )", "nl": "Read config s variables and apply their values to all its properties"}}
{"translation": {"code": "def proba2onehot ( proba : [ list , np . ndarray ] , confident_threshold : float , classes : [ list , np . ndarray ] ) -> np . ndarray : return labels2onehot ( proba2labels ( proba , confident_threshold , classes ) , classes )", "nl": "Convert vectors of probabilities to one - hot representations using confident threshold"}}
{"translation": {"code": "def labels2onehot ( labels : [ List [ str ] , List [ List [ str ] ] , np . ndarray ] , classes : [ list , np . ndarray ] ) -> np . ndarray : n_classes = len ( classes ) y = [ ] for sample in labels : curr = np . zeros ( n_classes ) if isinstance ( sample , list ) : for intent in sample : if intent not in classes : log . warning ( 'Unknown intent {} detected. Assigning no class' . format ( intent ) ) else : curr [ np . where ( np . array ( classes ) == intent ) [ 0 ] ] = 1 else : curr [ np . where ( np . array ( classes ) == sample ) [ 0 ] ] = 1 y . append ( curr ) y = np . asarray ( y ) return y", "nl": "Convert labels to one - hot vectors for multi - class multi - label classification"}}
{"translation": {"code": "def _config_session ( ) : config = tf . ConfigProto ( ) config . gpu_options . allow_growth = True config . gpu_options . visible_device_list = '0' return tf . Session ( config = config )", "nl": "Configure session for particular device"}}
{"translation": {"code": "def _graph_wrap ( func , graph ) : @ wraps ( func ) def _wrapped ( * args , * * kwargs ) : with graph . as_default ( ) : return func ( * args , * * kwargs ) return _wrapped", "nl": "Constructs function encapsulated in the graph ."}}
{"translation": {"code": "def build_model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load_trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : config = parse_config ( config ) if serialized : serialized : list = pickle . loads ( serialized ) if download : deep_download ( config ) import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) model_config = config [ 'chainer' ] model = Chainer ( model_config [ 'in' ] , model_config [ 'out' ] , model_config . get ( 'in_y' ) ) for component_config in model_config [ 'pipe' ] : if load_trained and ( 'fit_on' in component_config or 'in_y' in component_config ) : try : component_config [ 'load_path' ] = component_config [ 'save_path' ] except KeyError : log . warning ( 'No \"save_path\" parameter for the {} component, so \"load_path\" will not be renewed' . format ( component_config . get ( 'class_name' , component_config . get ( 'ref' , 'UNKNOWN' ) ) ) ) if serialized and 'in' in component_config : component_serialized = serialized . pop ( 0 ) else : component_serialized = None component = from_params ( component_config , mode = mode , serialized = component_serialized ) if 'in' in component_config : c_in = component_config [ 'in' ] c_out = component_config [ 'out' ] in_y = component_config . get ( 'in_y' , None ) main = component_config . get ( 'main' , False ) model . append ( component , c_in , c_out , in_y , main ) return model", "nl": "Build and return the model described in corresponding configuration file ."}}
{"translation": {"code": "def accuracy ( y_true : [ list , np . ndarray ] , y_predicted : [ list , np . ndarray ] ) -> float : examples_len = len ( y_true ) correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , y_predicted ) ] ) return correct / examples_len if examples_len else 0", "nl": "Calculate accuracy in terms of absolute coincidence"}}
{"translation": {"code": "def register_metric ( metric_name : str ) -> Callable [ ... , Any ] : def decorate ( fn ) : fn_name = fn . __module__ + ':' + fn . __name__ if metric_name in _REGISTRY and _REGISTRY [ metric_name ] != fn_name : log . warning ( '\"{}\" is already registered as a metric name, the old function will be ignored' . format ( metric_name ) ) _REGISTRY [ metric_name ] = fn_name return fn return decorate", "nl": "Decorator for metric registration ."}}
{"translation": {"code": "def train_evaluate_model_from_config ( config : Union [ str , Path , dict ] , iterator : Union [ DataLearningIterator , DataFittingIterator ] = None , * , to_train : bool = True , evaluation_targets : Optional [ Iterable [ str ] ] = None , to_validate : Optional [ bool ] = None , download : bool = False , start_epoch_num : Optional [ int ] = None , recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : config = parse_config ( config ) if download : deep_download ( config ) if to_train and recursive : for subconfig in get_all_elems_from_json ( config [ 'chainer' ] , 'config_path' ) : log . info ( f'Training \"{subconfig}\"' ) train_evaluate_model_from_config ( subconfig , download = False , recursive = True ) import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) if iterator is None : try : data = read_data_by_config ( config ) except ConfigError as e : to_train = False log . warning ( f'Skipping training. {e.message}' ) else : iterator = get_iterator_from_config ( config , data ) if 'train' not in config : log . warning ( 'Train config is missing. Populating with default values' ) train_config = config . get ( 'train' ) if start_epoch_num is not None : train_config [ 'start_epoch_num' ] = start_epoch_num if 'evaluation_targets' not in train_config and ( 'validate_best' in train_config or 'test_best' in train_config ) : log . warning ( '\"validate_best\" and \"test_best\" parameters are deprecated.' ' Please, use \"evaluation_targets\" list instead' ) train_config [ 'evaluation_targets' ] = [ ] if train_config . pop ( 'validate_best' , True ) : train_config [ 'evaluation_targets' ] . append ( 'valid' ) if train_config . pop ( 'test_best' , True ) : train_config [ 'evaluation_targets' ] . append ( 'test' ) trainer_class = get_model ( train_config . pop ( 'class_name' , 'nn_trainer' ) ) trainer = trainer_class ( config [ 'chainer' ] , * * train_config ) if to_train : trainer . train ( iterator ) res = { } if iterator is not None : if to_validate is not None : if evaluation_targets is None : log . warning ( '\"to_validate\" parameter is deprecated and will be removed in future versions.' ' Please, use \"evaluation_targets\" list instead' ) evaluation_targets = [ 'test' ] if to_validate : evaluation_targets . append ( 'valid' ) else : log . warn ( 'Both \"evaluation_targets\" and \"to_validate\" parameters are specified.' ' \"to_validate\" is deprecated and will be ignored' ) res = trainer . evaluate ( iterator , evaluation_targets , print_reports = True ) trainer . get_chainer ( ) . destroy ( ) res = { k : v [ 'metrics' ] for k , v in res . items ( ) } return res", "nl": "Make training and evaluation of the model described in corresponding configuration file ."}}
{"translation": {"code": "def read_data_by_config ( config : dict ) : dataset_config = config . get ( 'dataset' , None ) if dataset_config : config . pop ( 'dataset' ) ds_type = dataset_config [ 'type' ] if ds_type == 'classification' : reader = { 'class_name' : 'basic_classification_reader' } iterator = { 'class_name' : 'basic_classification_iterator' } config [ 'dataset_reader' ] = { * * dataset_config , * * reader } config [ 'dataset_iterator' ] = { * * dataset_config , * * iterator } else : raise Exception ( \"Unsupported dataset type: {}\" . format ( ds_type ) ) try : reader_config = dict ( config [ 'dataset_reader' ] ) except KeyError : raise ConfigError ( \"No dataset reader is provided in the JSON config.\" ) reader = get_model ( reader_config . pop ( 'class_name' ) ) ( ) data_path = reader_config . pop ( 'data_path' , '' ) if isinstance ( data_path , list ) : data_path = [ expand_path ( x ) for x in data_path ] else : data_path = expand_path ( data_path ) return reader . read ( data_path , * * reader_config )", "nl": "Read data by dataset_reader from specified config ."}}
{"translation": {"code": "def expand_path ( path : Union [ str , Path ] ) -> Path : return Path ( path ) . expanduser ( ) . resolve ( )", "nl": "Convert relative paths to absolute with resolving user directory ."}}
{"translation": {"code": "def load ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise RuntimeError ( 'Your TensorFlow model {} must' ' have sess attribute!' . format ( self . __class__ . __name__ ) ) path = str ( self . load_path . resolve ( ) ) # Check presence of the model files if tf . train . checkpoint_exists ( path ) : log . info ( '[loading model from {}]' . format ( path ) ) # Exclude optimizer variables from saved variables var_list = self . _get_saveable_variables ( exclude_scopes ) saver = tf . train . Saver ( var_list ) saver . restore ( self . sess , path )", "nl": "Load model parameters from self . load_path"}}
{"translation": {"code": "def save ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise RuntimeError ( 'Your TensorFlow model {} must' ' have sess attribute!' . format ( self . __class__ . __name__ ) ) path = str ( self . save_path . resolve ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) var_list = self . _get_saveable_variables ( exclude_scopes ) saver = tf . train . Saver ( var_list ) saver . save ( self . sess , path )", "nl": "Save model parameters to self . save_path"}}
{"translation": {"code": "def stacked_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None , add_l2_losses = False ) : l2_reg = tf . nn . l2_loss if add_l2_losses else None for n_layer , n_hidden in enumerate ( n_hidden_list ) : if use_dilation : dilation_rate = 2 ** n_layer else : dilation_rate = 1 units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) , kernel_regularizer = l2_reg ) if use_batch_norm : assert training_ph is not None units = tf . layers . batch_normalization ( units , training = training_ph ) units = tf . nn . relu ( units ) return units", "nl": "Number of convolutional layers stacked on top of each other"}}
{"translation": {"code": "def stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = False , name = 'RNN_layer' ) : for n , n_hidden in enumerate ( n_hidden_list ) : with tf . variable_scope ( name + '_' + str ( n ) ) : if cell_type == 'gru' : forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) elif cell_type == 'lstm' : forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) else : raise RuntimeError ( 'cell_type must be either gru or lstm' ) ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) if cell_type == 'gru' : last_units = tf . concat ( [ fw , bw ] , axis = 1 ) else : ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) last_units = ( h , c ) return units , last_units", "nl": "Stackted recurrent neural networks GRU or LSTM"}}
{"translation": {"code": "def embedding_layer ( token_indices = None , token_embedding_matrix = None , n_tokens = None , token_embedding_dim = None , name : str = None , trainable = True ) : if token_embedding_matrix is not None : tok_mat = token_embedding_matrix if trainable : Warning ( 'Matrix of embeddings is passed to the embedding_layer, ' 'possibly there is a pre-trained embedding matrix. ' 'Embeddings paramenters are set to Trainable!' ) else : tok_mat = np . random . randn ( n_tokens , token_embedding_dim ) . astype ( np . float32 ) / np . sqrt ( token_embedding_dim ) tok_emb_mat = tf . Variable ( tok_mat , name = name , trainable = trainable ) embedded_tokens = tf . nn . embedding_lookup ( tok_emb_mat , token_indices ) return embedded_tokens", "nl": "Token embedding layer . Create matrix of for token embeddings . Can be initialized with given matrix ( for example pre - trained with word2ve algorithm"}}
{"translation": {"code": "def cudnn_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : gru = tf . contrib . cudnn_rnn . CudnnGRU ( num_layers = n_layers , num_units = n_hidden ) if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = input_initial_h or init_h h , h_last = gru ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , ) ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = tf . squeeze ( h_last , axis = 0 ) [ - 1 ] # extract last layer state # Extract last states if they are provided if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , h_last", "nl": "Fast CuDNN GRU implementation"}}
{"translation": {"code": "def cudnn_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : lstm = tf . contrib . cudnn_rnn . CudnnLSTM ( num_layers = n_layers , num_units = n_hidden ) if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = initial_h or init_h initial_c = initial_c or init_c h , ( h_last , c_last ) = lstm ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , initial_c ) ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = h_last [ - 1 ] c_last = c_last [ - 1 ] # Extract last states if they are provided if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , ( h_last , c_last )", "nl": "Fast CuDNN LSTM implementation"}}
{"translation": {"code": "def cudnn_bi_gru ( units , n_hidden , seq_lengths = None , n_layers = 1 , trainable_initial_states = False , name = 'cudnn_bi_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if seq_lengths is None : seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] with tf . variable_scope ( 'Forward' ) : h_fw , h_last_fw = cudnn_gru_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) with tf . variable_scope ( 'Backward' ) : reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) h_bw , h_last_bw = cudnn_gru_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) return ( h_fw , h_bw ) , ( h_last_fw , h_last_bw )", "nl": "Fast CuDNN Bi - GRU implementation"}}
{"translation": {"code": "def cudnn_bi_lstm ( units , n_hidden , seq_lengths = None , n_layers = 1 , trainable_initial_states = False , name = 'cudnn_bi_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if seq_lengths is None : seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] with tf . variable_scope ( 'Forward' ) : h_fw , ( h_fw_last , c_fw_last ) = cudnn_lstm_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) with tf . variable_scope ( 'Backward' ) : reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) h_bw , ( h_bw_last , c_bw_last ) = cudnn_lstm_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) return ( h_fw , h_bw ) , ( ( h_fw_last , c_fw_last ) , ( h_bw_last , c_bw_last ) )", "nl": "Fast CuDNN Bi - LSTM implementation"}}
{"translation": {"code": "def stacked_highway_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None ) : for n_layer , n_hidden in enumerate ( n_hidden_list ) : input_units = units # Projection if needed if input_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : input_units = tf . layers . dense ( input_units , n_hidden ) if use_dilation : dilation_rate = 2 ** n_layer else : dilation_rate = 1 units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) ) if use_batch_norm : units = tf . layers . batch_normalization ( units , training = training_ph ) sigmoid_gate = tf . layers . dense ( input_units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) input_units = sigmoid_gate * input_units + ( 1 - sigmoid_gate ) * units input_units = tf . nn . relu ( input_units ) units = input_units return units", "nl": "Highway convolutional network . Skip connection with gating mechanism ."}}
{"translation": {"code": "def simple_attention ( memory , att_size , mask , keep_prob = 1.0 , scope = \"simple_attention\" ) : with tf . variable_scope ( scope ) : BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) memory_do = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) logits = tf . layers . dense ( tf . layers . dense ( memory_do , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce_sum ( att_weights * memory , axis = 1 ) return res", "nl": "Simple attention without any conditions ."}}
{"translation": {"code": "def attention ( inputs , state , att_size , mask , scope = \"attention\" ) : with tf . variable_scope ( scope ) : u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce_sum ( att_weights * inputs , axis = 1 ) return res , logits", "nl": "Computes weighted sum of inputs conditioned on state"}}
{"translation": {"code": "def squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : f1_total = 0.0 for ground_truth , prediction in zip ( y_true , y_predicted ) : prediction_tokens = normalize_answer ( prediction ) . split ( ) f1s = [ ] for gt in ground_truth : gt_tokens = normalize_answer ( gt ) . split ( ) if len ( gt_tokens ) == 0 or len ( prediction_tokens ) == 0 : f1s . append ( float ( gt_tokens == prediction_tokens ) ) continue common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) num_same = sum ( common . values ( ) ) if num_same == 0 : f1s . append ( 0.0 ) continue precision = 1.0 * num_same / len ( prediction_tokens ) recall = 1.0 * num_same / len ( gt_tokens ) f1 = ( 2 * precision * recall ) / ( precision + recall ) f1s . append ( f1 ) f1_total += max ( f1s ) return 100 * f1_total / len ( y_true ) if len ( y_true ) > 0 else 0", "nl": "Calculates F - 1 score between y_true and y_predicted F - 1 score uses the best matching y_true answer"}}
{"translation": {"code": "def expand_tile ( units , axis ) : assert axis in ( 1 , 2 ) n_time_steps = K . int_shape ( units ) [ 1 ] repetitions = [ 1 , 1 , 1 , 1 ] repetitions [ axis ] = n_time_steps if axis == 1 : expanded = Reshape ( target_shape = ( ( 1 , ) + K . int_shape ( units ) [ 1 : ] ) ) ( units ) else : expanded = Reshape ( target_shape = ( K . int_shape ( units ) [ 1 : 2 ] + ( 1 , ) + K . int_shape ( units ) [ 2 : ] ) ) ( units ) return K . tile ( expanded , repetitions )", "nl": "Expand and tile tensor along given axis"}}
{"translation": {"code": "def predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : if file_path is None or file_path == '-' : if sys . stdin . isatty ( ) : raise RuntimeError ( 'To process data from terminal please use interact mode' ) f = sys . stdin else : f = open ( file_path , encoding = 'utf8' ) model : Chainer = build_model ( config ) args_count = len ( model . in_x ) while True : batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) if not batch : break args = [ ] for i in range ( args_count ) : args . append ( batch [ i : : args_count ] ) res = model ( * args ) if len ( model . out_params ) == 1 : res = [ res ] for res in zip ( * res ) : res = json . dumps ( res , ensure_ascii = False ) print ( res , flush = True ) if f is not sys . stdin : f . close ( )", "nl": "Make a prediction with the component described in corresponding configuration file ."}}
{"translation": {"code": "def variational_dropout ( units , keep_prob , fixed_mask_dims = ( 1 , ) ) : units_shape = tf . shape ( units ) noise_shape = [ units_shape [ n ] for n in range ( len ( units . shape ) ) ] for dim in fixed_mask_dims : noise_shape [ dim ] = 1 return tf . nn . dropout ( units , keep_prob , noise_shape )", "nl": "Dropout with the same drop mask for all fixed_mask_dims"}}
{"translation": {"code": "def bi_rnn ( units : tf . Tensor , n_hidden : List , cell_type = 'gru' , seq_lengths = None , trainable_initial_states = False , use_peepholes = False , name = 'Bi-' ) : with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : if cell_type == 'gru' : forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) if trainable_initial_states : initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) else : initial_state_fw = initial_state_bw = None elif cell_type == 'lstm' : forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) if trainable_initial_states : initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_fw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_bw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) else : initial_state_fw = initial_state_bw = None else : raise RuntimeError ( 'cell_type must be either \"gru\" or \"lstm\"s' ) ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths , initial_state_fw = initial_state_fw , initial_state_bw = initial_state_bw ) kernels = [ var for var in forward_cell . trainable_variables + backward_cell . trainable_variables if 'kernel' in var . name ] for kernel in kernels : tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw )", "nl": "Bi directional recurrent neural network . GRU or LSTM"}}
{"translation": {"code": "def _make_default_operation_costs ( self , allow_spaces = False ) : self . operation_costs = dict ( ) self . operation_costs [ \"\" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } for a in self . alphabet : current_costs = { c : 1.0 for c in self . alphabet } current_costs [ a ] = 0.0 current_costs [ \"\" ] = 1.0 if allow_spaces : current_costs [ \" \" ] = 1.0 self . operation_costs [ a ] = current_costs # \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0437\u0438\u0446\u0438\u0438 for a , b in itertools . permutations ( self . alphabet , 2 ) : self . operation_costs [ a + b ] = { b + a : 1.0 } # \u043f\u0440\u043e\u0431\u0435\u043b\u044b if allow_spaces : self . operation_costs [ \" \" ] = { c : 1.0 for c in self . alphabet } self . operation_costs [ \" \" ] [ \"\" ] = 1.0", "nl": "sets 1 . 0 cost for every replacement insertion deletion and transposition"}}
{"translation": {"code": "def search ( self , word , d , allow_spaces = True , return_cost = True ) : if not all ( ( c in self . alphabet or ( c == \" \" and self . allow_spaces ) ) for c in word ) : return [ ] # raise ValueError(\"{0} contains an incorrect symbol\".format(word)) return self . _trie_search ( word , d , allow_spaces = allow_spaces , return_cost = return_cost )", "nl": "Finds all dictionary words in d - window from word"}}
{"translation": {"code": "def precompute_future_symbols ( trie , n , allow_spaces = False ) : if n == 0 : return if trie . is_terminated and trie . precompute_symbols : # \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u043f\u043e\u0441\u0447\u0438\u0442\u0430\u043d\u044b return for index , final in enumerate ( trie . final ) : trie . data [ index ] = [ set ( ) for i in range ( n ) ] for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : node_data [ 0 ] = set ( trie . _get_letters ( index ) ) if allow_spaces and final : node_data [ 0 ] . add ( \" \" ) for d in range ( 1 , n ) : for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : children = set ( trie . _get_children ( index ) ) for child in children : node_data [ d ] |= trie . data [ child ] [ d - 1 ] # \u0432 \u0441\u043b\u0443\u0447\u0430\u0435, \u0435\u0441\u043b\u0438 \u0440\u0430\u0437\u0440\u0435\u0448\u0451\u043d \u0432\u043e\u0437\u0432\u0440\u0430\u0442 \u043f\u043e \u043f\u0440\u043e\u0431\u0435\u043b\u0443 \u0432 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0435 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 if allow_spaces and final : node_data [ d ] |= trie . data [ trie . root ] [ d - 1 ] trie . terminated = True", "nl": "Collecting possible continuations of length < = n for every node"}}
{"translation": {"code": "def cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2 , keep_prob = 1.0 , concat_stacked_outputs = False , trainable_initial_states = False , name = 'cudnn_stacked_bi_gru' , reuse = False ) : if seq_lengths is None : seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] outputs = [ units ] with tf . variable_scope ( name , reuse = reuse ) : for n in range ( n_stacks ) : if n == 0 : inputs = outputs [ - 1 ] else : inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = 1 , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) if concat_stacked_outputs : return tf . concat ( outputs [ 1 : ] , axis = 2 ) return outputs [ - 1 ]", "nl": "Fast CuDNN Stacked Bi - GRU implementation"}}
{"translation": {"code": "def cls_from_str ( name : str ) -> type : try : module_name , cls_name = name . split ( ':' ) except ValueError : raise ConfigError ( 'Expected class description in a `module.submodules:ClassName` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import_module ( module_name ) , cls_name )", "nl": "Returns a class object with the name given as a string ."}}
{"translation": {"code": "def read_infile ( infile : Union [ Path , str ] , from_words = False , word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , tag_column : int = TAG_COLUMN , max_sents : int = - 1 , read_only_words : bool = False ) -> List [ Tuple [ List , Union [ List , None ] ] ] : answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] if from_words : word_column , read_only_words = 0 , True with open ( infile , \"r\" , encoding = \"utf8\" ) as fin : for line in fin : line = line . strip ( ) if line . startswith ( \"#\" ) : continue if line == \"\" : if len ( curr_word_sent ) > 0 : if read_only_words : curr_tag_sent = None answer . append ( ( curr_word_sent , curr_tag_sent ) ) curr_tag_sent , curr_word_sent = [ ] , [ ] if len ( answer ) == max_sents : break continue splitted = line . split ( \"\\t\" ) index = splitted [ 0 ] if not from_words and not index . isdigit ( ) : continue curr_word_sent . append ( splitted [ word_column ] ) if not read_only_words : pos , tag = splitted [ pos_column ] , splitted [ tag_column ] tag = pos if tag == \"_\" else \"{},{}\" . format ( pos , tag ) curr_tag_sent . append ( tag ) if len ( curr_word_sent ) > 0 : if read_only_words : curr_tag_sent = None answer . append ( ( curr_word_sent , curr_tag_sent ) ) return answer", "nl": "Reads input file in CONLL - U format"}}
{"translation": {"code": "def to_one_hot ( x , k ) : unit = np . eye ( k , dtype = int ) return unit [ x ]", "nl": "Takes an array of integers and transforms it to an array of one - hot encoded vectors"}}
{"translation": {"code": "def build ( self ) : word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = \"int32\" ) inputs = [ word_inputs ] word_outputs = self . _build_word_cnn ( word_inputs ) if len ( self . word_vectorizers ) > 0 : additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = \"float32\" ) for input_dim , dense_dim in self . word_vectorizers ] inputs . extend ( additional_word_inputs ) additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) compile_args = { \"optimizer\" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , \"loss\" : \"categorical_crossentropy\" , \"metrics\" : [ \"accuracy\" ] } self . model_ = Model ( inputs , outputs ) self . model_ . compile ( * * compile_args ) if self . verbose > 0 : self . model_ . summary ( print_fn = log . info ) return self", "nl": "Builds the network using Keras ."}}
{"translation": {"code": "def process_word ( word : str , to_lower : bool = False , append_case : Optional [ str ] = None ) -> Tuple [ str ] : if all ( x . isupper ( ) for x in word ) and len ( word ) > 1 : uppercase = \"<ALL_UPPER>\" elif word [ 0 ] . isupper ( ) : uppercase = \"<FIRST_UPPER>\" else : uppercase = None if to_lower : word = word . lower ( ) if word . isdigit ( ) : answer = [ \"<DIGIT>\" ] elif word . startswith ( \"http://\" ) or word . startswith ( \"www.\" ) : answer = [ \"<HTTP>\" ] else : answer = list ( word ) if to_lower and uppercase is not None : if append_case == \"first\" : answer = [ uppercase ] + answer elif append_case == \"last\" : answer = answer + [ uppercase ] return tuple ( answer )", "nl": "Converts word to a tuple of symbols optionally converts it to lowercase and adds capitalization label ."}}
{"translation": {"code": "def _build_word_cnn ( self , inputs ) : inputs = kl . Lambda ( kb . one_hot , arguments = { \"num_classes\" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) conv_outputs = [ ] self . char_output_dim_ = 0 for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : curr_output = char_embeddings curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) for _ in range ( self . char_conv_layers - 1 ) : curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) if self . conv_dropout > 0.0 : curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = \"same\" , activation = \"relu\" , data_format = \"channels_last\" ) ( curr_output ) conv_outputs . append ( curr_output ) self . char_output_dim_ += curr_filters_number if len ( conv_outputs ) > 1 : conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) else : conv_output = conv_outputs [ 0 ] highway_input = kl . Lambda ( kb . max , arguments = { \"axis\" : - 2 } ) ( conv_output ) if self . intermediate_dropout > 0.0 : highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) for i in range ( self . char_highway_layers - 1 ) : highway_input = Highway ( activation = \"relu\" ) ( highway_input ) if self . highway_dropout > 0.0 : highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) highway_output = Highway ( activation = \"relu\" ) ( highway_input ) return highway_output", "nl": "Builds word - level network"}}
{"translation": {"code": "def _build_basic_network ( self , word_outputs ) : if self . word_dropout > 0.0 : lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) else : lstm_outputs = word_outputs for j in range ( self . word_lstm_layers - 1 ) : lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = \"softmax\" , activity_regularizer = self . regularizer ) , name = \"p\" ) ( lstm_outputs ) return pre_outputs , lstm_outputs", "nl": "Creates the basic network architecture transforming word embeddings to intermediate outputs"}}
{"translation": {"code": "def train_on_batch ( self , data : List [ Iterable ] , labels : Iterable [ list ] ) -> None : X , Y = self . _transform_batch ( data , labels ) self . model_ . train_on_batch ( X , Y )", "nl": "Trains model on a single batch"}}
{"translation": {"code": "def predict_on_batch ( self , data : Union [ list , tuple ] , return_indexes : bool = False ) -> List [ List [ str ] ] : X = self . _transform_batch ( data ) objects_number , lengths = len ( X [ 0 ] ) , [ len ( elem ) for elem in data [ 0 ] ] Y = self . model_ . predict_on_batch ( X ) labels = np . argmax ( Y , axis = - 1 ) answer : List [ List [ str ] ] = [ None ] * objects_number for i , ( elem , length ) in enumerate ( zip ( labels , lengths ) ) : elem = elem [ : length ] answer [ i ] = elem if return_indexes else self . tags . idxs2toks ( elem ) return answer", "nl": "Makes predictions on a single batch"}}
{"translation": {"code": "def _make_sent_vector ( self , sent : List , bucket_length : int = None ) -> np . ndarray : bucket_length = bucket_length or len ( sent ) answer = np . zeros ( shape = ( bucket_length , MAX_WORD_LENGTH + 2 ) , dtype = np . int32 ) for i , word in enumerate ( sent ) : answer [ i , 0 ] = self . tags . tok2idx ( \"BEGIN\" ) m = min ( len ( word ) , MAX_WORD_LENGTH ) for j , x in enumerate ( word [ - m : ] ) : answer [ i , j + 1 ] = self . symbols . tok2idx ( x ) answer [ i , m + 1 ] = self . tags . tok2idx ( \"END\" ) answer [ i , m + 2 : ] = self . tags . tok2idx ( \"PAD\" ) return answer", "nl": "Transforms a sentence to Numpy array which will be the network input ."}}
{"translation": {"code": "def _make_tags_vector ( self , tags , bucket_length = None ) -> np . ndarray : bucket_length = bucket_length or len ( tags ) answer = np . zeros ( shape = ( bucket_length , ) , dtype = np . int32 ) for i , tag in enumerate ( tags ) : answer [ i ] = self . tags . tok2idx ( tag ) return answer", "nl": "Transforms a sentence of tags to Numpy array which will be the network target ."}}
{"translation": {"code": "def get_model ( name : str ) -> type : if name not in _REGISTRY : if ':' not in name : raise ConfigError ( \"Model {} is not registered.\" . format ( name ) ) return cls_from_str ( name ) return cls_from_str ( _REGISTRY [ name ] )", "nl": "Returns a registered class object with the name given in the string ."}}
{"translation": {"code": "def fn_from_str ( name : str ) -> Callable [ ... , Any ] : try : module_name , fn_name = name . split ( ':' ) except ValueError : raise ConfigError ( 'Expected function description in a `module.submodules:function_name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import_module ( module_name ) , fn_name )", "nl": "Returns a function object with the name given in string ."}}
{"translation": {"code": "def cudnn_compatible_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = input_initial_h or init_h with tf . variable_scope ( 'cudnn_gru' , reuse = reuse ) : def single_cell ( ) : return tf . contrib . cudnn_rnn . CudnnCompatibleGRUCell ( n_hidden ) cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) units = tf . transpose ( units , ( 1 , 0 , 2 ) ) h , h_last = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = tuple ( tf . unstack ( initial_h , axis = 0 ) ) ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = h_last [ - 1 ] # h_last is tuple: n_layers x batch_size x n_hidden # Extract last states if they are provided if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , h_last", "nl": "CuDNN Compatible GRU implementation . It should be used to load models saved with CudnnGRUCell to run on CPU ."}}
{"translation": {"code": "def cudnn_compatible_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = initial_h or init_h initial_c = initial_c or init_c with tf . variable_scope ( 'cudnn_lstm' , reuse = reuse ) : def single_cell ( ) : return tf . contrib . cudnn_rnn . CudnnCompatibleLSTMCell ( n_hidden ) cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) units = tf . transpose ( units , ( 1 , 0 , 2 ) ) init = tuple ( [ tf . nn . rnn_cell . LSTMStateTuple ( ic , ih ) for ih , ic in zip ( tf . unstack ( initial_h , axis = 0 ) , tf . unstack ( initial_c , axis = 0 ) ) ] ) h , state = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = init ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = state [ - 1 ] . h c_last = state [ - 1 ] . c # Extract last states if they are provided if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , ( h_last , c_last )", "nl": "CuDNN Compatible LSTM implementation . It should be used to load models saved with CudnnLSTMCell to run on CPU ."}}
{"translation": {"code": "def compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : matches_by_order = [ 0 ] * max_order possible_matches_by_order = [ 0 ] * max_order reference_length = 0 translation_length = 0 for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : reference_length += min ( len ( r ) for r in references ) translation_length += len ( translation ) merged_ref_ngram_counts = collections . Counter ( ) for reference in references : merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) translation_ngram_counts = _get_ngrams ( translation , max_order ) overlap = translation_ngram_counts & merged_ref_ngram_counts for ngram in overlap : matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] for order in range ( 1 , max_order + 1 ) : possible_matches = len ( translation ) - order + 1 if possible_matches > 0 : possible_matches_by_order [ order - 1 ] += possible_matches precisions = [ 0 ] * max_order for i in range ( 0 , max_order ) : if smooth : precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) else : if possible_matches_by_order [ i ] > 0 : precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) else : precisions [ i ] = 0.0 if min ( precisions ) > 0 : p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) geo_mean = math . exp ( p_log_sum ) else : geo_mean = 0 ratio = float ( translation_length ) / reference_length if ratio > 1.0 : bp = 1. else : bp = math . exp ( 1 - 1. / ratio ) bleu = geo_mean * bp return ( bleu , precisions , bp , ratio , translation_length , reference_length )", "nl": "Computes BLEU score of translated segments against one or more references ."}}
{"translation": {"code": "def _keras_wrap ( func , graph , session ) : import keras . backend as K @ wraps ( func ) def _wrapped ( * args , * * kwargs ) : with graph . as_default ( ) : K . set_session ( session ) return func ( * args , * * kwargs ) return _wrapped", "nl": "Constructs function encapsulated in the graph and the session ."}}
{"translation": {"code": "def bleu_advanced ( y_true : List [ Any ] , y_predicted : List [ Any ] , weights : Tuple = ( 1 , ) , smoothing_function = SMOOTH . method1 , auto_reweigh = False , penalty = True ) -> float : bleu_measure = sentence_bleu ( [ y_true ] , y_predicted , weights , smoothing_function , auto_reweigh ) hyp_len = len ( y_predicted ) hyp_lengths = hyp_len ref_lengths = closest_ref_length ( [ y_true ] , hyp_len ) bpenalty = brevity_penalty ( ref_lengths , hyp_lengths ) if penalty is True or bpenalty == 0 : return bleu_measure return bleu_measure / bpenalty", "nl": "Calculate BLEU score"}}
{"translation": {"code": "def round_accuracy ( y_true , y_predicted ) : predictions = [ round ( x ) for x in y_predicted ] examples_len = len ( y_true ) correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , predictions ) ] ) return correct / examples_len if examples_len else 0", "nl": "Rounds predictions and calculates accuracy in terms of absolute coincidence ."}}
{"translation": {"code": "def sk_log_loss ( y_true : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] , y_predicted : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] ) -> float : return log_loss ( y_true , y_predicted )", "nl": "Calculates log loss ."}}
{"translation": {"code": "def recall_at_k ( y_true : List [ int ] , y_pred : List [ List [ np . ndarray ] ] , k : int ) : num_examples = float ( len ( y_pred ) ) predictions = np . array ( y_pred ) predictions = np . flip ( np . argsort ( predictions , - 1 ) , - 1 ) [ : , : k ] num_correct = 0 for el in predictions : if 0 in el : num_correct += 1 return float ( num_correct ) / num_examples", "nl": "Calculates recall at k ranking metric ."}}
{"translation": {"code": "def ms_bot_framework ( self ) -> dict : card_action = { } card_action [ 'type' ] = 'postBack' card_action [ 'title' ] = self . name card_action [ 'value' ] = self . callback = self . callback return card_action", "nl": "Returns MS Bot Framework compatible state of the Button instance ."}}
{"translation": {"code": "def show_details ( item_data : Dict [ Any , Any ] ) -> str : txt = \"\" for key , value in item_data . items ( ) : txt += \"**\" + str ( key ) + \"**\" + ': ' + str ( value ) + \"  \\n\" return txt", "nl": "Format catalog item output"}}
{"translation": {"code": "def make_agent ( ) -> EcommerceAgent : config_path = find_config ( 'tfidf_retrieve' ) skill = build_model ( config_path ) agent = EcommerceAgent ( skills = [ skill ] ) return agent", "nl": "Make an agent"}}
{"translation": {"code": "def ms_bot_framework ( self ) -> list : ms_bf_controls = [ control . ms_bot_framework ( ) for control in self . controls ] return ms_bf_controls", "nl": "Returns list of MS Bot Framework compatible states of the RichMessage instance nested controls ."}}
{"translation": {"code": "def telegram ( self ) -> list : telegram_controls = [ control . telegram ( ) for control in self . controls ] return telegram_controls", "nl": "Returns list of Telegram compatible states of the RichMessage instance nested controls ."}}
{"translation": {"code": "def ms_bot_framework ( self ) -> dict : rich_card = { } buttons = [ button . ms_bot_framework ( ) for button in self . content ] rich_card [ 'buttons' ] = buttons if self . text : rich_card [ 'title' ] = self . text attachments = [ { \"contentType\" : \"application/vnd.microsoft.card.thumbnail\" , \"content\" : rich_card } ] out_activity = { } out_activity [ 'type' ] = 'message' out_activity [ 'attachments' ] = attachments return out_activity", "nl": "Returns MS Bot Framework compatible state of the ButtonsFrame instance ."}}
{"translation": {"code": "def main ( ) : args = parser . parse_args ( ) run_ms_bot_framework_server ( agent_generator = make_agent , app_id = args . ms_id , app_secret = args . ms_secret , stateful = True )", "nl": "Parse parameters and run ms bot framework"}}
{"translation": {"code": "def json ( self ) -> dict : content = { } if self . text : content [ 'text' ] = self . text content [ 'controls' ] = [ control . json ( ) for control in self . content ] self . control_json [ 'content' ] = content return self . control_json", "nl": "Returns json compatible state of the ButtonsFrame instance ."}}
{"translation": {"code": "def make_hello_bot_agent ( ) -> DefaultAgent : skill_hello = PatternMatchingSkill ( [ 'Hello world' ] , patterns = [ 'hi' , 'hello' , 'good day' ] ) skill_bye = PatternMatchingSkill ( [ 'Goodbye world' , 'See you around' ] , patterns = [ 'bye' , 'chao' , 'see you' ] ) skill_fallback = PatternMatchingSkill ( [ 'I don\\'t understand, sorry' , 'I can say \"Hello world\"' ] ) agent = DefaultAgent ( [ skill_hello , skill_bye , skill_fallback ] , skills_processor = HighestConfidenceSelector ( ) ) return agent", "nl": "Builds agent based on PatternMatchingSkill and HighestConfidenceSelector ."}}
{"translation": {"code": "def json ( self ) -> dict : content = { } content [ 'name' ] = self . name content [ 'callback' ] = self . callback self . control_json [ 'content' ] = content return self . control_json", "nl": "Returns json compatible state of the Button instance ."}}
{"translation": {"code": "def json ( self ) -> list : json_controls = [ control . json ( ) for control in self . controls ] return json_controls", "nl": "Returns list of json compatible states of the RichMessage instance nested controls ."}}
{"translation": {"code": "def get_metric_by_name ( name : str ) -> Callable [ ... , Any ] : if name not in _REGISTRY : raise ConfigError ( f'\"{name}\" is not registered as a metric' ) return fn_from_str ( _REGISTRY [ name ] )", "nl": "Returns a metric callable with a corresponding name ."}}
{"translation": {"code": "def _encode ( self , tokens : List [ str ] , mean : bool ) -> Union [ List [ np . ndarray ] , np . ndarray ] : embedded_tokens = [ ] for t in tokens : try : emb = self . tok2emb [ t ] except KeyError : try : emb = self . _get_word_vector ( t ) except KeyError : emb = np . zeros ( self . dim , dtype = np . float32 ) self . tok2emb [ t ] = emb embedded_tokens . append ( emb ) if mean is None : mean = self . mean if mean : filtered = [ et for et in embedded_tokens if np . any ( et ) ] if filtered : return np . mean ( filtered , axis = 0 ) return np . zeros ( self . dim , dtype = np . float32 ) return embedded_tokens", "nl": "Embed one text sample"}}
{"translation": {"code": "def interact_alice ( agent : Agent ) : data = request . get_json ( ) text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) payload = data [ 'request' ] . get ( 'payload' ) session_id = data [ 'session' ] [ 'session_id' ] user_id = data [ 'session' ] [ 'user_id' ] message_id = data [ 'session' ] [ 'message_id' ] dialog_id = DialogID ( user_id , session_id ) response = { 'response' : { 'end_session' : True , 'text' : '' } , \"session\" : { 'session_id' : session_id , 'message_id' : message_id , 'user_id' : user_id } , 'version' : '1.0' } agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ 0 ] if isinstance ( agent_response , RichMessage ) : response [ 'response' ] [ 'text' ] = '\\n' . join ( [ j [ 'content' ] for j in agent_response . json ( ) if j [ 'type' ] == 'plain_text' ] ) else : response [ 'response' ] [ 'text' ] = str ( agent_response ) return jsonify ( response ) , 200", "nl": "Exchange messages between basic pipelines and the Yandex . Dialogs service . If the pipeline returns multiple values only the first one is forwarded to Yandex ."}}
{"translation": {"code": "def round_f1_macro ( y_true , y_predicted ) : try : predictions = [ np . round ( x ) for x in y_predicted ] except TypeError : predictions = y_predicted return f1_score ( np . array ( y_true ) , np . array ( predictions ) , average = \"macro\" )", "nl": "Calculates F1 macro measure ."}}
{"translation": {"code": "def load ( self ) -> None : # Checks presence of the model files if self . load_path . exists ( ) : path = str ( self . load_path . resolve ( ) ) log . info ( '[loading model from {}]' . format ( path ) ) self . _net . load ( path )", "nl": "Checks existence of the model file loads the model if the file exists"}}
{"translation": {"code": "def main ( ) : args = parser . parse_args ( ) path = get_settings_path ( ) if args . default : if populate_settings_dir ( force = True ) : print ( f'Populated {path} with default settings files' ) else : print ( f'{path} is already a default settings directory' ) else : print ( f'Current DeepPavlov settings path: {path}' )", "nl": "DeepPavlov console configuration utility ."}}
{"translation": {"code": "def populate_settings_dir ( force : bool = False ) -> bool : res = False if _default_settings_path == _settings_path : return res for src in list ( _default_settings_path . glob ( '**/*.json' ) ) : dest = _settings_path / src . relative_to ( _default_settings_path ) if not force and dest . exists ( ) : continue res = True dest . parent . mkdir ( parents = True , exist_ok = True ) shutil . copy ( src , dest ) return res", "nl": "Populate settings directory with default settings files"}}
{"translation": {"code": "def _parse_config_property ( item : _T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> _T : if isinstance ( item , str ) : return item . format ( * * variables ) elif isinstance ( item , list ) : return [ _parse_config_property ( item , variables ) for item in item ] elif isinstance ( item , dict ) : return { k : _parse_config_property ( v , variables ) for k , v in item . items ( ) } else : return item", "nl": "Recursively apply config s variables values to its property"}}
{"translation": {"code": "def _repr_pretty_ ( self , p , cycle ) : if cycle : p . text ( 'Struct(...)' ) else : with p . group ( 7 , 'Struct(' , ')' ) : p . pretty ( self . _asdict ( ) )", "nl": "method that defines Struct s pretty printing rules for iPython"}}
{"translation": {"code": "def dump_weights ( tf_save_dir , outfile , options ) : def _get_outname ( tf_name ) : outname = re . sub ( ':0$' , '' , tf_name ) outname = outname . lstrip ( 'lm/' ) outname = re . sub ( '/rnn/' , '/RNN/' , outname ) outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) outname = re . sub ( '/cell_' , '/Cell' , outname ) outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) if '/RNN/' in outname : if 'projection' in outname : outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) else : outname = re . sub ( '/kernel' , '/W_0' , outname ) outname = re . sub ( '/bias' , '/B' , outname ) return outname ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) config = tf . ConfigProto ( allow_soft_placement = True ) with tf . Graph ( ) . as_default ( ) : with tf . Session ( config = config ) as sess : with tf . variable_scope ( 'lm' ) : LanguageModel ( options , False ) # Create graph # we use the \"Saver\" class to load the variables loader = tf . train . Saver ( ) loader . restore ( sess , ckpt_file ) with h5py . File ( outfile , 'w' ) as fout : for v in tf . trainable_variables ( ) : if v . name . find ( 'softmax' ) >= 0 : # don't dump these continue outname = _get_outname ( v . name ) # print(\"Saving variable {0} with name {1}\".format( #     v.name, outname)) shape = v . get_shape ( ) . as_list ( ) dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) values = sess . run ( [ v ] ) [ 0 ] dset [ ... ] = values", "nl": "Dump the trained weights from a model to a HDF5 file ."}}
{"translation": {"code": "def elmo_loss2ppl ( losses : List [ np . ndarray ] ) -> float : avg_loss = np . mean ( losses ) return float ( np . exp ( avg_loss ) )", "nl": "Calculates perplexity by loss"}}
{"translation": {"code": "def summary_gradient_updates ( grads , opt , lr ) : # strategy: # make a dict of variable name -> [variable, grad, adagrad slot] vars_grads = { } for v in tf . trainable_variables ( ) : vars_grads [ v . name ] = [ v , None , None ] for g , v in grads : vars_grads [ v . name ] [ 1 ] = g vars_grads [ v . name ] [ 2 ] = opt . get_slot ( v , 'accumulator' ) # now make summaries ret = [ ] for vname , ( v , g , a ) in vars_grads . items ( ) : if g is None : continue if isinstance ( g , tf . IndexedSlices ) : # a sparse gradient - only take norm of params that are updated updates = lr * g . values if a is not None : updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) else : updates = lr * g if a is not None : updates /= tf . sqrt ( a ) values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( \":\" , \"_\" ) , updates_norm / values_norm ) ) return ret", "nl": "get summary ops for the magnitude of gradient updates"}}
{"translation": {"code": "def export2hub ( weight_file , hub_dir , options ) : spec = make_module_spec ( options , str ( weight_file ) ) try : with tf . Graph ( ) . as_default ( ) : module = hub . Module ( spec ) with tf . Session ( ) as sess : sess . run ( tf . global_variables_initializer ( ) ) if hub_dir . exists ( ) : shutil . rmtree ( hub_dir ) module . export ( str ( hub_dir ) , sess ) finally : pass", "nl": "Exports a TF - Hub module"}}
{"translation": {"code": "def _get_log_file ( self ) : log_dir : Path = Path ( self . config [ 'log_path' ] ) . expanduser ( ) . resolve ( ) / self . agent_name log_dir . mkdir ( parents = True , exist_ok = True ) log_file_path = Path ( log_dir , f'{self._get_timestamp_utc_str()}_{self.agent_name}.log' ) log_file = open ( log_file_path , 'a' , buffering = 1 , encoding = 'utf8' ) return log_file", "nl": "Returns opened file object for writing dialog logs ."}}
{"translation": {"code": "def _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : if isinstance ( utterance , str ) : pass elif isinstance ( utterance , RichMessage ) : utterance = utterance . json ( ) elif isinstance ( utterance , ( list , dict ) ) : utterance = jsonify_data ( utterance ) else : utterance = str ( utterance ) dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id if self . log_file . tell ( ) >= self . log_max_size * 1024 : self . log_file . close ( ) self . log_file = self . _get_log_file ( ) else : try : log_msg = { } log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) log_msg [ 'dialog_id' ] = dialog_id log_msg [ 'direction' ] = direction log_msg [ 'message' ] = utterance log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) self . log_file . write ( f'{log_str}\\n' ) except IOError : log . error ( 'Failed to write dialog log.' )", "nl": "Logs single dialog utterance to current dialog log file ."}}
{"translation": {"code": "def _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : weight_name_map = { } for i in range ( 2 ) : for j in range ( 8 ) : # if we decide to add more layers root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' # convert the graph name to that in the checkpoint varname_in_file = varname [ 5 : ] if varname_in_file . startswith ( 'RNN' ) : varname_in_file = weight_name_map [ varname_in_file ] if varname_in_file == 'embedding' : with h5py . File ( embedding_weight_file , 'r' ) as fin : # Have added a special 0 index for padding not present # in the original model. embed_weights = fin [ varname_in_file ] [ ... ] weights = np . zeros ( ( embed_weights . shape [ 0 ] + 1 , embed_weights . shape [ 1 ] ) , dtype = DTYPE ) weights [ 1 : , : ] = embed_weights else : with h5py . File ( weight_file , 'r' ) as fin : if varname_in_file == 'char_embed' : # Have added a special 0 index for padding not present # in the original model. char_embed_weights = fin [ varname_in_file ] [ ... ] weights = np . zeros ( ( char_embed_weights . shape [ 0 ] + 1 , char_embed_weights . shape [ 1 ] ) , dtype = DTYPE ) weights [ 1 : , : ] = char_embed_weights else : weights = fin [ varname_in_file ] [ ... ] # Tensorflow initializers are callables that accept a shape parameter # and some optional kwargs def ret ( shape , * * kwargs ) : if list ( shape ) != list ( weights . shape ) : raise ValueError ( \"Invalid shape initializing {0}, got {1}, expected {2}\" . format ( varname_in_file , shape , weights . shape ) ) return weights return ret", "nl": "We ll stub out all the initializers in the pretrained LM with a function that loads the weights from the file"}}