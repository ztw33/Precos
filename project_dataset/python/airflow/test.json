{"translation": {"code": "def delete_database ( self , instance_id , database_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : self . log . info ( \"The database {} is already deleted from instance {}. \" \"Exiting.\" . format ( database_id , instance_id ) ) return try : operation = database . drop ( ) # type: Operation except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result ) return", "nl": "Drops a database in Cloud Spanner ."}}
{"translation": {"code": "def update_database ( self , instance_id , database_id , ddl_statements , project_id = None , operation_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) try : operation = database . update_ddl ( ddl_statements = ddl_statements , operation_id = operation_id ) if operation : result = operation . result ( ) self . log . info ( result ) return except AlreadyExists as e : if e . code == 409 and operation_id in e . message : self . log . info ( \"Replayed update_ddl message - the operation id %s \" \"was already done before.\" , operation_id ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e", "nl": "Updates DDL of a database in Cloud Spanner ."}}
{"translation": {"code": "def is_valid_plugin ( plugin_obj , existing_plugins ) : if ( inspect . isclass ( plugin_obj ) and issubclass ( plugin_obj , AirflowPlugin ) and ( plugin_obj is not AirflowPlugin ) ) : plugin_obj . validate ( ) return plugin_obj not in existing_plugins return False", "nl": "Check whether a potential object is a subclass of the AirflowPlugin class ."}}
{"translation": {"code": "def get_hook ( self ) : if not hasattr ( self , 'hook' ) : from airflow . contrib . hooks . aws_glue_catalog_hook import AwsGlueCatalogHook self . hook = AwsGlueCatalogHook ( aws_conn_id = self . aws_conn_id , region_name = self . region_name ) return self . hook", "nl": "Gets the AwsGlueCatalogHook"}}
{"translation": {"code": "def retrieve_connection ( self , session = None ) : self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : return connections [ 0 ] return None", "nl": "Retrieves the dynamically created connection from the Connection table ."}}
{"translation": {"code": "def create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : if format == 'AM_PM' : axis [ 'tickFormat' ] = \"function(d) { return get_am_pm(parseInt(d)); }\" else : axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format if label : axis [ 'axisLabel' ] = \"'\" + label + \"'\" # date format : see https://github.com/mbostock/d3/wiki/Time-Formatting if date : self . dateformat = format axis [ 'tickFormat' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parseInt(d))) }\\n\" \"\" % self . dateformat ) # flag is the x Axis is a date if name [ 0 ] == 'x' : self . x_axis_date = True # Add new axis to list of axis self . axislist [ name ] = axis # Create x2Axis if focus_enable if name == \"xAxis\" and self . focus_enable : self . axislist [ 'x2Axis' ] = axis", "nl": "Create X - axis"}}
{"translation": {"code": "def buildjschart ( self ) : self . jschart = '' # add custom tooltip string in jschart # default condition (if build_custom_tooltip is not called explicitly with date_flag=True) if self . tooltip_condition_string == '' : self . tooltip_condition_string = 'var y = String(graph.point.y);\\n' # Include data self . series_js = json . dumps ( self . series )", "nl": "generate javascript code for the chart"}}
{"translation": {"code": "def buildcontainer ( self ) : if self . container : return # Create SVG div with style if self . width : if self . width [ - 1 ] != '%' : self . style += 'width:%spx;' % self . width else : self . style += 'width:%s;' % self . width if self . height : if self . height [ - 1 ] != '%' : self . style += 'height:%spx;' % self . height else : self . style += 'height:%s;' % self . height if self . style : self . style = 'style=\"%s\"' % self . style self . container = self . containerheader + '<div id=\"%s\"><svg %s></svg></div>\\n' % ( self . name , self . style )", "nl": "generate HTML div"}}
{"translation": {"code": "def _main ( ) : # Parse arguments usage = \"usage: nvd3.py [options]\" parser = OptionParser ( usage = usage , version = ( \"python-nvd3 - Charts generator with \" \"nvd3.js and d3.js\" ) ) parser . add_option ( \"-q\" , \"--quiet\" , action = \"store_false\" , dest = \"verbose\" , default = True , help = \"don't print messages to stdout\" ) ( options , args ) = parser . parse_args ( )", "nl": "Parse options and process commands"}}
{"translation": {"code": "def buildhtmlheader ( self ) : self . htmlheader = '' # If the JavaScript assets have already been injected, don't bother re-sourcing them. global _js_initialized if '_js_initialized' not in globals ( ) or not _js_initialized : for css in self . header_css : self . htmlheader += css for js in self . header_js : self . htmlheader += js", "nl": "generate HTML header content"}}
{"translation": {"code": "def create_perm_vm_for_all_dag ( self ) : # create perm for global logical dag for dag_vm in self . DAG_VMS : for perm in self . DAG_PERMS : self . _merge_perm ( permission_name = perm , view_menu_name = dag_vm )", "nl": "Create perm - vm if not exist and insert into FAB security model for all - dags ."}}
{"translation": {"code": "def delete_role ( self , role_name ) : session = self . get_session role = session . query ( sqla_models . Role ) . filter ( sqla_models . Role . name == role_name ) . first ( ) if role : self . log . info ( \"Deleting role '%s'\" , role_name ) session . delete ( role ) session . commit ( ) else : raise AirflowException ( \"Role named '{}' does not exist\" . format ( role_name ) )", "nl": "Delete the given Role"}}
{"translation": {"code": "def skip ( self , dag_run , execution_date , tasks , session = None ) : if not tasks : return task_ids = [ d . task_id for d in tasks ] now = timezone . utcnow ( ) if dag_run : session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == dag_run . dag_id , TaskInstance . execution_date == dag_run . execution_date , TaskInstance . task_id . in_ ( task_ids ) ) . update ( { TaskInstance . state : State . SKIPPED , TaskInstance . start_date : now , TaskInstance . end_date : now } , synchronize_session = False ) session . commit ( ) else : assert execution_date is not None , \"Execution date is None and no dag run\" self . log . warning ( \"No DAG RUN present this should not happen\" ) # this is defensive against dag runs that are not complete for task in tasks : ti = TaskInstance ( task , execution_date = execution_date ) ti . state = State . SKIPPED ti . start_date = now ti . end_date = now session . merge ( ti ) session . commit ( )", "nl": "Sets tasks instances to skipped from the same dag run ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _client : self . _client = ProductSearchClient ( credentials = self . _get_credentials ( ) ) return self . _client", "nl": "Retrieves connection to Cloud Vision ."}}
{"translation": {"code": "def _configure_csv_file ( self , file_handle , schema ) : csv_writer = csv . writer ( file_handle , encoding = 'utf-8' , delimiter = self . field_delimiter ) csv_writer . writerow ( schema ) return csv_writer", "nl": "Configure a csv writer with the file_handle and write schema as headers for the new file ."}}
{"translation": {"code": "def translate ( self , values , target_language , format_ = None , source_language = None , model = None ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ = format_ , source_language = source_language , model = model , )", "nl": "Translate a string or list of strings ."}}
{"translation": {"code": "def catch_http_exception ( func ) : @ functools . wraps ( func ) def wrapper_decorator ( self , * args , * * kwargs ) : try : return func ( self , * args , * * kwargs ) except GoogleAPICallError as e : if isinstance ( e , AlreadyExists ) : raise e else : self . log . error ( 'The request failed:\\n%s' , str ( e ) ) raise AirflowException ( e ) except RetryError as e : self . log . error ( 'The request failed due to a retryable error and retry attempts failed.' ) raise AirflowException ( e ) except ValueError as e : self . log . error ( 'The request failed, the parameters are invalid.' ) raise AirflowException ( e ) except HttpError as e : self . log . error ( 'The request failed:\\n%s' , str ( e ) ) raise AirflowException ( e ) return wrapper_decorator", "nl": "Function decorator that intercepts HTTP Errors and raises AirflowException with more informative message ."}}
{"translation": {"code": "def delete_transfer_job ( self , job_name , project_id ) : return ( self . get_conn ( ) . transferJobs ( ) . patch ( jobName = job_name , body = { PROJECT_ID : project_id , TRANSFER_JOB : { STATUS1 : GcpTransferJobsStatus . DELETED } , TRANSFER_JOB_FIELD_MASK : STATUS1 , } , ) . execute ( num_retries = self . num_retries ) )", "nl": "Deletes a transfer job . This is a soft delete . After a transfer job is deleted the job and all the transfer executions are subject to garbage collection . Transfer jobs become eligible for garbage collection 30 days after soft delete ."}}
{"translation": {"code": "def cancel_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . cancel ( name = operation_name ) . execute ( num_retries = self . num_retries )", "nl": "Cancels an transfer operation in Google Storage Transfer Service ."}}
{"translation": {"code": "def pause_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . pause ( name = operation_name ) . execute ( num_retries = self . num_retries )", "nl": "Pauses an transfer operation in Google Storage Transfer Service ."}}
{"translation": {"code": "def get_transfer_job ( self , job_name , project_id = None ) : return ( self . get_conn ( ) . transferJobs ( ) . get ( jobName = job_name , projectId = project_id ) . execute ( num_retries = self . num_retries ) )", "nl": "Gets the latest state of a long - running operation in Google Storage Transfer Service ."}}
{"translation": {"code": "def prepare_classpath ( ) : if DAGS_FOLDER not in sys . path : sys . path . append ( DAGS_FOLDER ) # Add ./config/ for loading custom log parsers etc, or # airflow_local_settings etc. config_path = os . path . join ( AIRFLOW_HOME , 'config' ) if config_path not in sys . path : sys . path . append ( config_path ) if PLUGINS_FOLDER not in sys . path : sys . path . append ( PLUGINS_FOLDER )", "nl": "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath"}}
{"translation": {"code": "def send ( self ) : support_type = [ 'text' , 'link' , 'markdown' , 'actionCard' , 'feedCard' ] if self . message_type not in support_type : raise ValueError ( 'DingdingWebhookHook only support {} ' 'so far, but receive {}' . format ( support_type , self . message_type ) ) data = self . _build_message ( ) self . log . info ( 'Sending Dingding type %s message %s' , self . message_type , data ) resp = self . run ( endpoint = self . _get_endpoint ( ) , data = data , headers = { 'Content-Type' : 'application/json' } ) # Dingding success send message will with errcode equal to 0 if int ( resp . json ( ) . get ( 'errcode' ) ) != 0 : raise AirflowException ( 'Send Dingding message failed, receive error ' 'message %s' , resp . text ) self . log . info ( 'Success Send Dingding message' )", "nl": "Send Dingding message"}}
{"translation": {"code": "def _get_endpoint ( self ) : conn = self . get_connection ( self . http_conn_id ) token = conn . password if not token : raise AirflowException ( 'Dingding token is requests but get nothing, ' 'check you conn_id configuration.' ) return 'robot/send?access_token={}' . format ( token )", "nl": "Get Dingding endpoint for sending message ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _client : self . _client = SpeechClient ( credentials = self . _get_credentials ( ) ) return self . _client", "nl": "Retrieves connection to Cloud Speech ."}}
{"translation": {"code": "def recognize_speech ( self , config , audio , retry = None , timeout = None ) : client = self . get_conn ( ) response = client . recognize ( config = config , audio = audio , retry = retry , timeout = timeout ) self . log . info ( \"Recognised speech: %s\" % response ) return response", "nl": "Recognizes audio input"}}
{"translation": {"code": "def get_conn ( self , headers = None ) : conn = self . get_connection ( self . http_conn_id ) self . base_url = conn . host if conn . host else 'https://api.opsgenie.com' session = requests . Session ( ) if headers : session . headers . update ( headers ) return session", "nl": "Overwrite HttpHook get_conn because this hook just needs base_url and headers and does not need generic params"}}
{"translation": {"code": "def execute ( self , payload = { } ) : api_key = self . _get_api_key ( ) return self . run ( endpoint = 'v2/alerts' , data = json . dumps ( payload ) , headers = { 'Content-Type' : 'application/json' , 'Authorization' : 'GenieKey %s' % api_key } )", "nl": "Execute the Opsgenie Alert call"}}
{"translation": {"code": "def annotate_text ( self , document , features , encoding_type = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . annotate_text ( document = document , features = features , encoding_type = encoding_type , retry = retry , timeout = timeout , metadata = metadata , )", "nl": "A convenience method that provides all the features that analyzeSentiment analyzeEntities and analyzeSyntax provide in one call ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _conn : self . _conn = LanguageServiceClient ( credentials = self . _get_credentials ( ) ) return self . _conn", "nl": "Retrieves connection to Cloud Natural Language service ."}}
{"translation": {"code": "def get_fernet ( ) : global _fernet log = LoggingMixin ( ) . log if _fernet : return _fernet try : from cryptography . fernet import Fernet , MultiFernet , InvalidToken global InvalidFernetToken InvalidFernetToken = InvalidToken except BuiltinImportError : log . warning ( \"cryptography not found - values will not be stored encrypted.\" ) _fernet = NullFernet ( ) return _fernet try : fernet_key = configuration . conf . get ( 'core' , 'FERNET_KEY' ) if not fernet_key : log . warning ( \"empty cryptography key - values will not be stored encrypted.\" ) _fernet = NullFernet ( ) else : _fernet = MultiFernet ( [ Fernet ( fernet_part . encode ( 'utf-8' ) ) for fernet_part in fernet_key . split ( ',' ) ] ) _fernet . is_encrypted = True except ( ValueError , TypeError ) as ve : raise AirflowException ( \"Could not create Fernet object: {}\" . format ( ve ) ) return _fernet", "nl": "Deferred load of Fernet key ."}}
{"translation": {"code": "def xcom_push ( self , key , value , execution_date = None ) : if execution_date and execution_date < self . execution_date : raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date )", "nl": "Make an XCom available for tasks to pull ."}}
{"translation": {"code": "def setdefault ( cls , key , default , deserialize_json = False ) : obj = Variable . get ( key , default_var = None , deserialize_json = deserialize_json ) if obj is None : if default is not None : Variable . set ( key , default , serialize_json = deserialize_json ) return default else : raise ValueError ( 'Default Value must be set' ) else : return obj", "nl": "Like a Python builtin dict object setdefault returns the current value for a key and if it isn t there stores the default value and returns it ."}}
{"translation": {"code": "def clear_task_instances ( tis , session , activate_dag_runs = True , dag = None , ) : job_ids = [ ] for ti in tis : if ti . state == State . RUNNING : if ti . job_id : ti . state = State . SHUTDOWN job_ids . append ( ti . job_id ) else : task_id = ti . task_id if dag and dag . has_task ( task_id ) : task = dag . get_task ( task_id ) task_retries = task . retries ti . max_tries = ti . try_number + task_retries - 1 else : # Ignore errors when updating max_tries if dag is None or # task not found in dag since database records could be # outdated. We make max_tries the maximum value of its # original max_tries or the current task try number. ti . max_tries = max ( ti . max_tries , ti . try_number - 1 ) ti . state = State . NONE session . merge ( ti ) if job_ids : from airflow . jobs import BaseJob as BJ for job in session . query ( BJ ) . filter ( BJ . id . in_ ( job_ids ) ) . all ( ) : job . state = State . SHUTDOWN if activate_dag_runs and tis : from airflow . models . dagrun import DagRun # Avoid circular import drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , DagRun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) for dr in drs : dr . state = State . RUNNING dr . start_date = timezone . utcnow ( )", "nl": "Clears a set of task instances but makes sure the running ones get killed ."}}
{"translation": {"code": "def try_number ( self ) : # This is designed so that task logs end up in the right file. if self . state == State . RUNNING : return self . _try_number return self . _try_number + 1", "nl": "Return the try number that this task number will be when it is actually run ."}}
{"translation": {"code": "def generate_command ( dag_id , task_id , execution_date , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , local = False , pickle_id = None , file_path = None , raw = False , job_id = None , pool = None , cfg_path = None ) : iso = execution_date . isoformat ( ) cmd = [ \"airflow\" , \"run\" , str ( dag_id ) , str ( task_id ) , str ( iso ) ] cmd . extend ( [ \"--mark_success\" ] ) if mark_success else None cmd . extend ( [ \"--pickle\" , str ( pickle_id ) ] ) if pickle_id else None cmd . extend ( [ \"--job_id\" , str ( job_id ) ] ) if job_id else None cmd . extend ( [ \"-A\" ] ) if ignore_all_deps else None cmd . extend ( [ \"-i\" ] ) if ignore_task_deps else None cmd . extend ( [ \"-I\" ] ) if ignore_depends_on_past else None cmd . extend ( [ \"--force\" ] ) if ignore_ti_state else None cmd . extend ( [ \"--local\" ] ) if local else None cmd . extend ( [ \"--pool\" , pool ] ) if pool else None cmd . extend ( [ \"--raw\" ] ) if raw else None cmd . extend ( [ \"-sd\" , file_path ] ) if file_path else None cmd . extend ( [ \"--cfg_path\" , cfg_path ] ) if cfg_path else None return cmd", "nl": "Generates the shell command required to execute this task instance ."}}
{"translation": {"code": "def current_state ( self , session = None ) : TI = TaskInstance ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date , ) . all ( ) if ti : state = ti [ 0 ] . state else : state = None return state", "nl": "Get the very latest state from the database if a session is passed we use and looking up the state becomes part of the session otherwise a new session is used ."}}
{"translation": {"code": "def clear_xcom_data ( self , session = None ) : session . query ( XCom ) . filter ( XCom . dag_id == self . dag_id , XCom . task_id == self . task_id , XCom . execution_date == self . execution_date ) . delete ( ) session . commit ( )", "nl": "Clears all XCom data from the database for the task instance"}}
{"translation": {"code": "def key ( self ) : return self . dag_id , self . task_id , self . execution_date , self . try_number", "nl": "Returns a tuple that identifies the task instance uniquely"}}
{"translation": {"code": "def ready_for_retry ( self ) : return ( self . state == State . UP_FOR_RETRY and self . next_retry_datetime ( ) < timezone . utcnow ( ) )", "nl": "Checks on whether the task instance is in the right state and timeframe to be retried ."}}
{"translation": {"code": "def pool_full ( self , session ) : if not self . task . pool : return False pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) if not pool : return False open_slots = pool . open_slots ( session = session ) return open_slots <= 0", "nl": "Returns a boolean as to whether the slot pool has room for this task to run"}}
{"translation": {"code": "def init_run_context ( self , raw = False ) : self . raw = raw self . _set_context ( self )", "nl": "Sets the log context ."}}
{"translation": {"code": "def xcom_pull ( self , task_ids = None , dag_id = None , key = XCOM_RETURN_KEY , include_prior_dates = False ) : if dag_id is None : dag_id = self . dag_id pull_fn = functools . partial ( XCom . get_one , execution_date = self . execution_date , key = key , dag_id = dag_id , include_prior_dates = include_prior_dates ) if is_container ( task_ids ) : return tuple ( pull_fn ( task_id = t ) for t in task_ids ) else : return pull_fn ( task_id = task_ids )", "nl": "Pull XComs that optionally meet certain criteria ."}}
{"translation": {"code": "def bag_dag ( self , dag , parent_dag , root_dag ) : dag . test_cycle ( ) # throws if a task cycle is found dag . resolve_template_files ( ) dag . last_loaded = timezone . utcnow ( ) for task in dag . tasks : settings . policy ( task ) subdags = dag . subdags try : for subdag in subdags : subdag . full_filepath = dag . full_filepath subdag . parent_dag = dag subdag . is_subdag = True self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) self . dags [ dag . dag_id ] = dag self . log . debug ( 'Loaded DAG %s' , dag ) except AirflowDagCycleException as cycle_exception : # There was an error in bagging the dag. Remove it from the list of dags self . log . exception ( 'Exception bagging dag: %s' , dag . dag_id ) # Only necessary at the root level since DAG.subdags automatically # performs DFS to search through all subdags if dag == root_dag : for subdag in subdags : if subdag . dag_id in self . dags : del self . dags [ subdag . dag_id ] raise cycle_exception", "nl": "Adds the DAG into the bag recurses into sub dags . Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags"}}
{"translation": {"code": "def find ( dag_id = None , run_id = None , execution_date = None , state = None , external_trigger = None , no_backfills = False , session = None ) : DR = DagRun qry = session . query ( DR ) if dag_id : qry = qry . filter ( DR . dag_id == dag_id ) if run_id : qry = qry . filter ( DR . run_id == run_id ) if execution_date : if isinstance ( execution_date , list ) : qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) else : qry = qry . filter ( DR . execution_date == execution_date ) if state : qry = qry . filter ( DR . state == state ) if external_trigger is not None : qry = qry . filter ( DR . external_trigger == external_trigger ) if no_backfills : # in order to prevent a circular dependency from airflow . jobs import BackfillJob qry = qry . filter ( DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) ) dr = qry . order_by ( DR . execution_date ) . all ( ) return dr", "nl": "Returns a set of dag runs for the given search criteria ."}}
{"translation": {"code": "def get_dag ( self , dag_id ) : from airflow . models . dag import DagModel # Avoid circular import # If asking for a known subdag, we want to refresh the parent root_dag_id = dag_id if dag_id in self . dags : dag = self . dags [ dag_id ] if dag . is_subdag : root_dag_id = dag . parent_dag . dag_id # If the dag corresponding to root_dag_id is absent or expired orm_dag = DagModel . get_current ( root_dag_id ) if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : # Reprocess source file found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = False ) # If the source file no longer exports `dag_id`, delete it from self.dags if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : return self . dags [ dag_id ] elif dag_id in self . dags : del self . dags [ dag_id ] return self . dags . get ( dag_id )", "nl": "Gets the DAG out of the dictionary and refreshes it if expired"}}
{"translation": {"code": "def get_dagrun ( self , session ) : from airflow . models . dagrun import DagRun # Avoid circular import dr = session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == self . execution_date ) . first ( ) return dr", "nl": "Returns the DagRun for this TaskInstance"}}
{"translation": {"code": "def verify_integrity ( self , session = None ) : from airflow . models . taskinstance import TaskInstance # Avoid circular import dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) # check for removed or restored tasks task_ids = [ ] for ti in tis : task_ids . append ( ti . task_id ) task = None try : task = dag . get_task ( ti . task_id ) except AirflowException : if ti . state == State . REMOVED : pass # ti has already been removed, just ignore it elif self . state is not State . RUNNING and not dag . partial : self . log . warning ( \"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\" . format ( ti , dag ) ) Stats . incr ( \"task_removed_from_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . REMOVED is_task_in_dag = task is not None should_restore_task = is_task_in_dag and ti . state == State . REMOVED if should_restore_task : self . log . info ( \"Restoring task '{}' which was previously \" \"removed from DAG '{}'\" . format ( ti , dag ) ) Stats . incr ( \"task_restored_to_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . NONE # check for missing tasks for task in six . itervalues ( dag . task_dict ) : if task . start_date > self . execution_date and not self . is_backfill : continue if task . task_id not in task_ids : Stats . incr ( \"task_instance_created-{}\" . format ( task . __class__ . __name__ ) , 1 , 1 ) ti = TaskInstance ( task , self . execution_date ) session . add ( ti ) session . commit ( )", "nl": "Verifies the DagRun by checking for removed tasks or tasks that are not in the database yet . It will set state to removed or add the task if required ."}}
{"translation": {"code": "def extra_links ( self ) : dag_id = request . args . get ( 'dag_id' ) task_id = request . args . get ( 'task_id' ) execution_date = request . args . get ( 'execution_date' ) link_name = request . args . get ( 'link_name' ) dttm = airflow . utils . timezone . parse ( execution_date ) dag = dagbag . get_dag ( dag_id ) if not dag or task_id not in dag . task_ids : response = jsonify ( { 'url' : None , 'error' : \"can't find dag {dag} or task_id {task_id}\" . format ( dag = dag , task_id = task_id ) } ) response . status_code = 404 return response task = dag . get_task ( task_id ) try : url = task . get_extra_links ( dttm , link_name ) except ValueError as err : response = jsonify ( { 'url' : None , 'error' : str ( err ) } ) response . status_code = 404 return response if url : response = jsonify ( { 'error' : None , 'url' : url } ) response . status_code = 200 return response else : response = jsonify ( { 'url' : None , 'error' : 'No URL found for {dest}' . format ( dest = link_name ) } ) response . status_code = 404 return response", "nl": "A restful endpoint that returns external links for a given Operator"}}
{"translation": {"code": "def poke ( self , context ) : sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) sqs_conn = sqs_hook . get_conn ( ) self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) self . log . info ( \"reveived message %s\" , str ( messages ) ) if 'Messages' in messages and len ( messages [ 'Messages' ] ) > 0 : entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) if 'Successful' in result : context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) return True else : raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) return False", "nl": "Check for message on subscribed queue and write to xcom the message with key messages"}}
{"translation": {"code": "def send_message ( self , queue_url , message_body , delay_seconds = 0 , message_attributes = None ) : return self . get_conn ( ) . send_message ( QueueUrl = queue_url , MessageBody = message_body , DelaySeconds = delay_seconds , MessageAttributes = message_attributes or { } )", "nl": "Send message to the queue"}}
{"translation": {"code": "def execute ( self , context ) : hook = SQSHook ( aws_conn_id = self . aws_conn_id ) result = hook . send_message ( queue_url = self . sqs_queue , message_body = self . message_content , delay_seconds = self . delay_seconds , message_attributes = self . message_attributes ) self . log . info ( 'result is send_message is %s' , result ) return result", "nl": "Publish the message to SQS queue"}}
{"translation": {"code": "def get_conn ( self ) : if not self . _conn : self . _conn = VideoIntelligenceServiceClient ( credentials = self . _get_credentials ( ) ) return self . _conn", "nl": "Returns Gcp Video Intelligence Service client"}}
{"translation": {"code": "def correct_maybe_zipped ( fileloc ) : _ , archive , filename = re . search ( r'((.*\\.zip){})?(.*)' . format ( re . escape ( os . sep ) ) , fileloc ) . groups ( ) if archive and zipfile . is_zipfile ( archive ) : return archive else : return fileloc", "nl": "If the path contains a folder with a . zip suffix then the folder is treated as a zip archive and path to zip is returned ."}}