{"translation": {"code": "def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events", "nl": "Returns and flush the event buffer . In case dag_ids is specified it will only return and flush events for the given dag_ids . Otherwise it returns and flushes all"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn_config [ \"port\" ] = 3306 else : conn_config [ \"port\" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn_config [ \"use_unicode\" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : # SSL parameter for MySQL has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # URL parameters dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ \"local_infile\" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn", "nl": "Returns a mysql connection object"}}
{"translation": {"code": "def max_partition ( table , schema = \"default\" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )", "nl": "Gets the max partition for a table ."}}
{"translation": {"code": "def get_metastore_client ( self ) : import hmsclient from thrift . transport import TSocket , TTransport from thrift . protocol import TBinaryProtocol ms = self . metastore_conn auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'GSSAPI' ) kerberos_service_name = ms . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) socket = TSocket . TSocket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth_mechanism == 'GSSAPI' : try : import saslwrapper as sasl except ImportError : import sasl def sasl_factory ( ) : sasl_client = sasl . Client ( ) sasl_client . setAttr ( \"host\" , ms . host ) sasl_client . setAttr ( \"service\" , kerberos_service_name ) sasl_client . init ( ) return sasl_client from thrift_sasl import TSaslClientTransport transport = TSaslClientTransport ( sasl_factory , \"GSSAPI\" , socket ) else : transport = TTransport . TBufferedTransport ( socket ) protocol = TBinaryProtocol . TBinaryProtocol ( transport ) return hmsclient . HMSClient ( iprot = protocol )", "nl": "Returns a Hive thrift client ."}}
{"translation": {"code": "def _task_instances_for_dag_run ( self , dag_run , session = None ) : tasks_to_run = { } if dag_run is None : return tasks_to_run # check if we have orphaned tasks self . reset_state_for_orphaned_tasks ( filter_by_dag_run = dag_run , session = session ) # for some reason if we don't refresh the reference to run is lost dag_run . refresh_from_db ( ) make_transient ( dag_run ) # TODO(edgarRd): AIRFLOW-1464 change to batch query to improve perf for ti in dag_run . get_task_instances ( ) : # all tasks part of the backfill are scheduled to run if ti . state == State . NONE : ti . set_state ( State . SCHEDULED , session = session ) if ti . state != State . REMOVED : tasks_to_run [ ti . key ] = ti return tasks_to_run", "nl": "Returns a map of task instance key to task instance object for the tasks to run in the given dag run ."}}
{"translation": {"code": "def _get_dag_run ( self , run_date , session = None ) : run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) # consider max_active_runs but ignore when running subdags respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) # check if we are scheduling on top of a already existing dag_run # we could find a \"scheduled\" run instead of a \"backfill\" run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) if run is not None and len ( run ) > 0 : run = run [ 0 ] if run . state == State . RUNNING : respect_dag_max_active_limit = False else : run = None # enforce max_active_runs limit for dag, special cases already # handled by respect_dag_max_active_limit if ( respect_dag_max_active_limit and current_active_dag_count >= self . dag . max_active_runs ) : return None run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self . conf , ) # set required transient field run . dag = self . dag # explicitly mark as backfill and running run . state = State . RUNNING run . run_id = run_id run . verify_integrity ( session = session ) return run", "nl": "Returns a dag run for the given run date which will be matched to an existing dag run if available or create a new dag run otherwise . If the max_active_runs limit is reached this function will return None ."}}
{"translation": {"code": "def heartbeat ( self ) : try : with create_session ( ) as session : job = session . query ( BaseJob ) . filter_by ( id = self . id ) . one ( ) make_transient ( job ) session . commit ( ) if job . state == State . SHUTDOWN : self . kill ( ) is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) if not is_unit_test : # Figure out how long to sleep for sleep_for = 0 if job . latest_heartbeat : seconds_remaining = self . heartrate - ( timezone . utcnow ( ) - job . latest_heartbeat ) . total_seconds ( ) sleep_for = max ( 0 , seconds_remaining ) sleep ( sleep_for ) # Update last heartbeat time with create_session ( ) as session : job = session . query ( BaseJob ) . filter ( BaseJob . id == self . id ) . first ( ) job . latest_heartbeat = timezone . utcnow ( ) session . merge ( job ) session . commit ( ) self . heartbeat_callback ( session = session ) self . log . debug ( '[heartbeat]' ) except OperationalError as e : self . log . error ( \"Scheduler heartbeat got an exception: %s\" , str ( e ) )", "nl": "Heartbeats update the job s entry in the database with a timestamp for the latest_heartbeat and allows for the job to be killed externally . This allows at the system level to monitor what is actually active ."}}
{"translation": {"code": "def ds_add ( ds , days ) : ds = datetime . strptime ( ds , '%Y-%m-%d' ) if days : ds = ds + timedelta ( days ) return ds . isoformat ( ) [ : 10 ]", "nl": "Add or subtract days from a YYYY - MM - DD"}}
{"translation": {"code": "def make_cache_key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )", "nl": "Used by cache to get a unique key per URL"}}
{"translation": {"code": "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )", "nl": "Execute the statement against Presto . Can be used to create views ."}}
{"translation": {"code": "def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df", "nl": "Get a pandas dataframe from a sql query ."}}
{"translation": {"code": "def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) )", "nl": "Get a set of records from Presto"}}
{"translation": {"code": "def check_for_bucket ( self , bucket_name ) : try : self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) return True except ClientError as e : self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) return False", "nl": "Check if bucket_name exists ."}}
{"translation": {"code": "def check_for_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) try : self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) return True except ClientError as e : self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) return False", "nl": "Checks if a key exists in a bucket"}}
{"translation": {"code": "def list_prefixes ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) has_results = False prefixes = [ ] for page in response : if 'CommonPrefixes' in page : has_results = True for p in page [ 'CommonPrefixes' ] : prefixes . append ( p [ 'Prefix' ] ) if has_results : return prefixes", "nl": "Lists prefixes in a bucket under prefix"}}
{"translation": {"code": "def list_keys ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) has_results = False keys = [ ] for page in response : if 'Contents' in page : has_results = True for k in page [ 'Contents' ] : keys . append ( k [ 'Key' ] ) if has_results : return keys", "nl": "Lists keys in a bucket under prefix and not containing delimiter"}}
{"translation": {"code": "def get_records ( self , hql , schema = 'default' , hive_conf = None ) : return self . get_results ( hql , schema = schema , hive_conf = hive_conf ) [ 'data' ]", "nl": "Get a set of records from a Hive query ."}}
{"translation": {"code": "def get_pandas_df ( self , hql , schema = 'default' ) : import pandas as pd res = self . get_results ( hql , schema = schema ) df = pd . DataFrame ( res [ 'data' ] ) df . columns = [ c [ 0 ] for c in res [ 'header' ] ] return df", "nl": "Get a pandas dataframe from a Hive query"}}
{"translation": {"code": "def load_file ( self , filepath , table , delimiter = \",\" , field_dict = None , create = True , overwrite = True , partition = None , recreate = False , tblproperties = None ) : hql = '' if recreate : hql += \"DROP TABLE IF EXISTS {table};\\n\" . format ( table = table ) if create or recreate : if field_dict is None : raise ValueError ( \"Must provide a field dict when creating a table\" ) fields = \",\\n    \" . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) hql += \"CREATE TABLE IF NOT EXISTS {table} (\\n{fields})\\n\" . format ( table = table , fields = fields ) if partition : pfields = \",\\n    \" . join ( [ p + \" STRING\" for p in partition ] ) hql += \"PARTITIONED BY ({pfields})\\n\" . format ( pfields = pfields ) hql += \"ROW FORMAT DELIMITED\\n\" hql += \"FIELDS TERMINATED BY '{delimiter}'\\n\" . format ( delimiter = delimiter ) hql += \"STORED AS textfile\\n\" if tblproperties is not None : tprops = \", \" . join ( [ \"'{0}'='{1}'\" . format ( k , v ) for k , v in tblproperties . items ( ) ] ) hql += \"TBLPROPERTIES({tprops})\\n\" . format ( tprops = tprops ) hql += \";\" self . log . info ( hql ) self . run_cli ( hql ) hql = \"LOAD DATA LOCAL INPATH '{filepath}' \" . format ( filepath = filepath ) if overwrite : hql += \"OVERWRITE \" hql += \"INTO TABLE {table} \" . format ( table = table ) if partition : pvals = \", \" . join ( [ \"{0}='{1}'\" . format ( k , v ) for k , v in partition . items ( ) ] ) hql += \"PARTITION ({pvals})\" . format ( pvals = pvals ) # As a workaround for HIVE-10541, add a newline character # at the end of hql (AIRFLOW-2412). hql += ';\\n' self . log . info ( hql ) self . run_cli ( hql )", "nl": "Loads a local file into Hive"}}
{"translation": {"code": "def load_file ( self , filename , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = \"AES256\" client = self . get_conn ( ) client . upload_file ( filename , bucket_name , key , ExtraArgs = extra_args )", "nl": "Loads a local file to S3"}}
{"translation": {"code": "def get_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) obj = self . get_resource_type ( 's3' ) . Object ( bucket_name , key ) obj . load ( ) return obj", "nl": "Returns a boto3 . s3 . Object"}}
{"translation": {"code": "def ds_format ( ds , input_format , output_format ) : return datetime . strptime ( ds , input_format ) . strftime ( output_format )", "nl": "Takes an input string and outputs another string as specified in the output format"}}
{"translation": {"code": "def _process_task_instances ( self , dag , queue , session = None ) : # update the state of the previously active dag runs dag_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) active_dag_runs = [ ] for run in dag_runs : self . log . info ( \"Examining DAG run %s\" , run ) # don't consider runs that are executed in the future if run . execution_date > timezone . utcnow ( ) : self . log . error ( \"Execution date is in future: %s\" , run . execution_date ) continue if len ( active_dag_runs ) >= dag . max_active_runs : self . log . info ( \"Number of active dag runs reached max_active_run.\" ) break # skip backfill dagruns for now as long as they are not really scheduled if run . is_backfill : continue # todo: run.dag is transient but needs to be set run . dag = dag # todo: preferably the integrity check happens at dag collection time run . verify_integrity ( session = session ) run . update_state ( session = session ) if run . state == State . RUNNING : make_transient ( run ) active_dag_runs . append ( run ) for run in active_dag_runs : self . log . debug ( \"Examining active DAG run: %s\" , run ) # this needs a fresh session sometimes tis get detached tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) # this loop is quite slow as it uses are_dependencies_met for # every task (in ti.is_runnable). This is also called in # update_state above which has already checked these tasks for ti in tis : task = dag . get_task ( ti . task_id ) # fixme: ti.task is transient but needs to be set ti . task = task if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True ) , session = session ) : self . log . debug ( 'Queuing task: %s' , ti ) queue . append ( ti . key )", "nl": "This method schedules the tasks for a single DAG by looking at the active DAG runs and adding task instances that should run to the queue ."}}
{"translation": {"code": "def get_conn ( self ) : # When using HAClient, proxy_user must be the same, so is ok to always # take the first. effective_user = self . proxy_user autoconfig = self . autoconfig use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get_connections ( self . hdfs_conn_id ) if not effective_user : effective_user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) except AirflowException : if not autoconfig : raise if autoconfig : # will read config info from $HADOOP_HOME conf files client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) else : raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) return client", "nl": "Returns a snakebite HDFSClient object ."}}
{"translation": {"code": "def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args . execution_date ) print ( ti . current_state ( ) )", "nl": "Returns the state of a TaskInstance at the command line . >>> airflow task_state tutorial sleep 2015 - 01 - 01 success"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . sqlite_conn_id ) conn = sqlite3 . connect ( conn . host ) return conn", "nl": "Returns a sqlite connection object"}}
{"translation": {"code": "def check_for_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : return self . get_wildcard_key ( wildcard_key = wildcard_key , bucket_name = bucket_name , delimiter = delimiter ) is not None", "nl": "Checks that a key matching a wildcard expression exists in a bucket"}}
{"translation": {"code": "def get_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : if not bucket_name : ( bucket_name , wildcard_key ) = self . parse_s3_url ( wildcard_key ) prefix = re . split ( r'[*]' , wildcard_key , 1 ) [ 0 ] klist = self . list_keys ( bucket_name , prefix = prefix , delimiter = delimiter ) if klist : key_matches = [ k for k in klist if fnmatch . fnmatch ( k , wildcard_key ) ] if key_matches : return self . get_key ( key_matches [ 0 ] , bucket_name )", "nl": "Returns a boto3 . s3 . Object object matching the wildcard expression"}}
{"translation": {"code": "def check_for_prefix ( self , bucket_name , prefix , delimiter ) : prefix = prefix + delimiter if prefix [ - 1 ] != delimiter else prefix prefix_split = re . split ( r'(\\w+[{d}])$' . format ( d = delimiter ) , prefix , 1 ) previous_level = prefix_split [ 0 ] plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) return False if plist is None else prefix in plist", "nl": "Checks that a prefix exists in a bucket"}}
{"translation": {"code": "def _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : tis_changed = 0 query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) if self . using_sqlite : tis_to_change = query . with_for_update ( ) . all ( ) for ti in tis_to_change : ti . set_state ( new_state , session = session ) tis_changed += 1 else : subq = query . subquery ( ) tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) session . commit ( ) if tis_changed > 0 : self . log . warning ( \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state )", "nl": "For all DAG IDs in the SimpleDagBag look for task instances in the old_states and set them to new_state if the corresponding DagRun does not exist or exists but is not in the running state . This normally should not happen but it can if the state of DagRuns are changed manually ."}}
{"translation": {"code": "def _get_executor ( executor_name ) : if executor_name == Executors . LocalExecutor : return LocalExecutor ( ) elif executor_name == Executors . SequentialExecutor : return SequentialExecutor ( ) elif executor_name == Executors . CeleryExecutor : from airflow . executors . celery_executor import CeleryExecutor return CeleryExecutor ( ) elif executor_name == Executors . DaskExecutor : from airflow . executors . dask_executor import DaskExecutor return DaskExecutor ( ) elif executor_name == Executors . KubernetesExecutor : from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor return KubernetesExecutor ( ) else : # Loading plugins _integrate_plugins ( ) executor_path = executor_name . split ( '.' ) if len ( executor_path ) != 2 : raise AirflowException ( \"Executor {0} not supported: \" \"please specify in format plugin_module.executor\" . format ( executor_name ) ) if executor_path [ 0 ] in globals ( ) : return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) else : raise AirflowException ( \"Executor {0} not supported.\" . format ( executor_name ) )", "nl": "Creates a new instance of the named executor . In case the executor name is not know in airflow look for it in the plugins"}}
{"translation": {"code": "def get_conn ( self , schema = None ) : db = self . get_connection ( self . hiveserver2_conn_id ) auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'NONE' ) if auth_mechanism == 'NONE' and db . login is None : # we need to give a username username = 'airflow' kerberos_service_name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'KERBEROS' ) kerberos_service_name = db . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) # pyhive uses GSSAPI instead of KERBEROS as a auth_mechanism identifier if auth_mechanism == 'GSSAPI' : self . log . warning ( \"Detected deprecated 'GSSAPI' for authMechanism \" \"for %s. Please use 'KERBEROS' instead\" , self . hiveserver2_conn_id ) auth_mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth_mechanism , kerberos_service_name = kerberos_service_name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' )", "nl": "Returns a Hive connection object ."}}
{"translation": {"code": "def get_conn ( self , headers = None ) : session = requests . Session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if \"://\" in conn . host : self . base_url = conn . host else : # schema defaults to HTTP schema = conn . schema if conn . schema else \"http\" self . base_url = schema + \"://\" + conn . host if conn . port : self . base_url = self . base_url + \":\" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session", "nl": "Returns http session for use with requests"}}
{"translation": {"code": "def run ( self , endpoint , data = None , headers = None , extra_options = None ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = None if self . method == 'GET' : # GET uses params req = requests . Request ( self . method , url , params = data , headers = headers ) elif self . method == 'HEAD' : # HEAD doesn't use params req = requests . Request ( self . method , url , headers = headers ) else : # Others use data req = requests . Request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( \"Sending '%s' to url: %s\" , self . method , url ) return self . run_and_check ( session , prepped_request , extra_options )", "nl": "Performs the request"}}
{"translation": {"code": "def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( \"stream\" , False ) , verify = extra_options . get ( \"verify\" , True ) , proxies = extra_options . get ( \"proxies\" , { } ) , cert = extra_options . get ( \"cert\" ) , timeout = extra_options . get ( \"timeout\" ) , allow_redirects = extra_options . get ( \"allow_redirects\" , True ) ) if extra_options . get ( 'check_response' , True ) : self . check_response ( response ) return response except requests . exceptions . ConnectionError as ex : self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) raise ex", "nl": "Grabs extra options like timeout and actually runs the request checking for the result"}}
{"translation": {"code": "def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( ValueError , TypeError ) : ret = s return ret", "nl": "A small helper function to convert a string to a numeric value if appropriate"}}
{"translation": {"code": "def get_pandas_df ( self , sql , parameters = None ) : import pandas . io . sql as psql with closing ( self . get_conn ( ) ) as conn : return psql . read_sql ( sql , con = conn , params = parameters )", "nl": "Executes the sql and returns a pandas dataframe"}}
{"translation": {"code": "def run ( self , sql , autocommit = False , parameters = None ) : if isinstance ( sql , basestring ) : sql = [ sql ] with closing ( self . get_conn ( ) ) as conn : if self . supports_autocommit : self . set_autocommit ( conn , autocommit ) with closing ( conn . cursor ( ) ) as cur : for s in sql : if parameters is not None : self . log . info ( \"{} with parameters {}\" . format ( s , parameters ) ) cur . execute ( s , parameters ) else : self . log . info ( s ) cur . execute ( s ) # If autocommit was set to False for db that supports autocommit, # or if db does not supports autocommit, we do a manual commit. if not self . get_autocommit ( conn ) : conn . commit ( )", "nl": "Runs a command or a list of commands . Pass a list of sql statements to the sql parameter to get them to execute sequentially"}}
{"translation": {"code": "def insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 , replace = False ) : if target_fields : target_fields = \", \" . join ( target_fields ) target_fields = \"({})\" . format ( target_fields ) else : target_fields = '' i = 0 with closing ( self . get_conn ( ) ) as conn : if self . supports_autocommit : self . set_autocommit ( conn , False ) conn . commit ( ) with closing ( conn . cursor ( ) ) as cur : for i , row in enumerate ( rows , 1 ) : lst = [ ] for cell in row : lst . append ( self . _serialize_cell ( cell , conn ) ) values = tuple ( lst ) placeholders = [ \"%s\" , ] * len ( values ) if not replace : sql = \"INSERT INTO \" else : sql = \"REPLACE INTO \" sql += \"{0} {1} VALUES ({2})\" . format ( table , target_fields , \",\" . join ( placeholders ) ) cur . execute ( sql , values ) if commit_every and i % commit_every == 0 : conn . commit ( ) self . log . info ( \"Loaded %s into %s rows so far\" , i , table ) conn . commit ( ) self . log . info ( \"Done loading. Loaded a total of %s rows\" , i )", "nl": "A generic way to insert a set of tuples into a table a new transaction is created every commit_every rows"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn", "nl": "Returns a mssql connection object"}}
{"translation": {"code": "def set_autocommit ( self , conn , autocommit ) : if not self . supports_autocommit and autocommit : self . log . warn ( ( \"%s connection doesn't support \" \"autocommit but autocommit activated.\" ) , getattr ( self , self . conn_name_attr ) ) conn . autocommit = autocommit", "nl": "Sets the autocommit flag on the connection"}}
{"translation": {"code": "def execute ( self , * * kwargs ) : if not self . api_params : self . construct_api_call_params ( ) slack = SlackHook ( token = self . token , slack_conn_id = self . slack_conn_id ) slack . call ( self . method , self . api_params )", "nl": "SlackAPIOperator calls will not fail even if the call is not unsuccessful . It should not prevent a DAG from completing in success"}}
{"translation": {"code": "def load_string ( self , string_data , key , bucket_name = None , replace = False , encrypt = False , encoding = 'utf-8' ) : self . load_bytes ( string_data . encode ( encoding ) , key = key , bucket_name = bucket_name , replace = replace , encrypt = encrypt )", "nl": "Loads a string to S3"}}
{"translation": {"code": "def load_bytes ( self , bytes_data , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = \"AES256\" filelike_buffer = BytesIO ( bytes_data ) client = self . get_conn ( ) client . upload_fileobj ( filelike_buffer , bucket_name , key , ExtraArgs = extra_args )", "nl": "Loads bytes to S3"}}
{"translation": {"code": "def run_migrations_offline ( ) : context . configure ( url = settings . SQL_ALCHEMY_CONN , target_metadata = target_metadata , literal_binds = True , compare_type = COMPARE_TYPE ) with context . begin_transaction ( ) : context . run_migrations ( )", "nl": "Run migrations in offline mode ."}}
{"translation": {"code": "def run_migrations_online ( ) : connectable = settings . engine with connectable . connect ( ) as connection : context . configure ( connection = connection , transaction_per_migration = True , target_metadata = target_metadata , compare_type = COMPARE_TYPE , ) with context . begin_transaction ( ) : context . run_migrations ( )", "nl": "Run migrations in online mode ."}}
{"translation": {"code": "def detect_conf_var ( ) : ticket_cache = configuration . conf . get ( 'kerberos' , 'ccache' ) with open ( ticket_cache , 'rb' ) as f : # Note: this file is binary, so we check against a bytearray. return b'X-CACHECONF:' in f . read ( )", "nl": "Return true if the ticket cache contains conf information as is found in ticket caches of Kerberos 1 . 8 . 1 or later . This is incompatible with the Sun Java Krb5LoginModule in Java6 so we need to take an action to work around it ."}}
{"translation": {"code": "def get_mod_time ( self , path ) : conn = self . get_conn ( ) ftp_mdtm = conn . sendcmd ( 'MDTM ' + path ) time_val = ftp_mdtm [ 4 : ] # time_val optionally has microseconds try : return datetime . datetime . strptime ( time_val , \"%Y%m%d%H%M%S.%f\" ) except ValueError : return datetime . datetime . strptime ( time_val , '%Y%m%d%H%M%S' )", "nl": "Returns a datetime object representing the last time the file was modified"}}
{"translation": {"code": "def store_file ( self , remote_full_path , local_full_path_or_buffer ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if is_path : input_handle = open ( local_full_path_or_buffer , 'rb' ) else : input_handle = local_full_path_or_buffer remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) conn . storbinary ( 'STOR %s' % remote_file_name , input_handle ) if is_path : input_handle . close ( )", "nl": "Transfers a local file to the remote location ."}}
{"translation": {"code": "def retrieve_file ( self , remote_full_path , local_full_path_or_buffer , callback = None ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) # without a callback, default to writing to a user-provided file or # file-like buffer if not callback : if is_path : output_handle = open ( local_full_path_or_buffer , 'wb' ) else : output_handle = local_full_path_or_buffer callback = output_handle . write else : output_handle = None remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) self . log . info ( 'Retrieving file from FTP: %s' , remote_full_path ) conn . retrbinary ( 'RETR %s' % remote_file_name , callback ) self . log . info ( 'Finished retrieving file from FTP: %s' , remote_full_path ) if is_path and output_handle : output_handle . close ( )", "nl": "Transfers the remote file to a local location ."}}
{"translation": {"code": "def list_directory ( self , path , nlst = False ) : conn = self . get_conn ( ) conn . cwd ( path ) files = conn . nlst ( ) return files", "nl": "Returns a list of files on the remote system ."}}
{"translation": {"code": "def get_conn ( self ) : if self . conn is None : params = self . get_connection ( self . ftp_conn_id ) pasv = params . extra_dejson . get ( \"passive\" , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set_pasv ( pasv ) return self . conn", "nl": "Returns a FTP connection object"}}
{"translation": {"code": "def mlsd ( conn , path = \"\" , facts = None ) : facts = facts or [ ] if facts : conn . sendcmd ( \"OPTS MLST \" + \";\" . join ( facts ) + \";\" ) if path : cmd = \"MLSD %s\" % path else : cmd = \"MLSD\" lines = [ ] conn . retrlines ( cmd , lines . append ) for line in lines : facts_found , _ , name = line . rstrip ( ftplib . CRLF ) . partition ( ' ' ) entry = { } for fact in facts_found [ : - 1 ] . split ( \";\" ) : key , _ , value = fact . partition ( \"=\" ) entry [ key . lower ( ) ] = value yield ( name , entry )", "nl": "BACKPORT FROM PYTHON3 FTPLIB ."}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . vertica_conn_id ) conn_config = { \"user\" : conn . login , \"password\" : conn . password or '' , \"database\" : conn . schema , \"host\" : conn . host or 'localhost' } if not conn . port : conn_config [ \"port\" ] = 5433 else : conn_config [ \"port\" ] = int ( conn . port ) conn = connect ( * * conn_config ) return conn", "nl": "Returns verticaql connection object"}}
{"translation": {"code": "def to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = True , fetch_size = 1000 , hive_conf = None ) : results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) message = None i = 0 with open ( csv_filepath , 'wb' ) as f : writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) try : if output_header : self . log . debug ( 'Cursor description is %s' , header ) writer . writerow ( [ c [ 0 ] for c in header ] ) for i , row in enumerate ( results_iter , 1 ) : writer . writerow ( row ) if i % fetch_size == 0 : self . log . info ( \"Written %s rows so far.\" , i ) except ValueError as exception : message = str ( exception ) if message : # need to clean up the file first os . remove ( csv_filepath ) raise ValueError ( message ) self . log . info ( \"Done. Loaded a total of %s rows.\" , i )", "nl": "Execute hql in target schema and write results to a csv file ."}}
{"translation": {"code": "def table_exists ( self , table_name , db = 'default' ) : try : self . get_table ( table_name , db ) return True except Exception : return False", "nl": "Check if table exists"}}
{"translation": {"code": "def health ( self , session = None ) : BJ = jobs . BaseJob payload = { } scheduler_health_check_threshold = timedelta ( seconds = conf . getint ( 'scheduler' , 'scheduler_health_check_threshold' ) ) latest_scheduler_heartbeat = None payload [ 'metadatabase' ] = { 'status' : 'healthy' } try : latest_scheduler_heartbeat = session . query ( func . max ( BJ . latest_heartbeat ) ) . filter ( BJ . state == 'running' , BJ . job_type == 'SchedulerJob' ) . scalar ( ) except Exception : payload [ 'metadatabase' ] [ 'status' ] = 'unhealthy' if not latest_scheduler_heartbeat : scheduler_status = 'unhealthy' else : if timezone . utcnow ( ) - latest_scheduler_heartbeat <= scheduler_health_check_threshold : scheduler_status = 'healthy' else : scheduler_status = 'unhealthy' payload [ 'scheduler' ] = { 'status' : scheduler_status , 'latest_scheduler_heartbeat' : str ( latest_scheduler_heartbeat ) } return wwwutils . json_response ( payload )", "nl": "An endpoint helping check the health status of the Airflow instance including metadatabase and scheduler ."}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . cloudant_conn_id ) self . _validate_connection ( conn ) cloudant_session = cloudant ( user = conn . login , passwd = conn . password , account = conn . host ) return cloudant_session", "nl": "Opens a connection to the cloudant service and closes it automatically if used as context manager ."}}
{"translation": {"code": "def check_for_path ( self , hdfs_path ) : conn = self . get_conn ( ) status = conn . status ( hdfs_path , strict = False ) return bool ( status )", "nl": "Check for the existence of a path in HDFS by querying FileStatus ."}}
{"translation": {"code": "def get_conn ( self ) : connections = self . get_connections ( self . webhdfs_conn_id ) for connection in connections : try : self . log . debug ( 'Trying namenode %s' , connection . host ) client = self . _get_client ( connection ) client . status ( '/' ) self . log . debug ( 'Using namenode %s for hook' , connection . host ) return client except HdfsError as hdfs_error : self . log . debug ( 'Read operation on namenode %s failed with error: %s' , connection . host , hdfs_error ) hosts = [ connection . host for connection in connections ] error_message = 'Read operations failed on the namenodes below:\\n{hosts}' . format ( hosts = '\\n' . join ( hosts ) ) raise AirflowWebHDFSHookException ( error_message )", "nl": "Establishes a connection depending on the security mode set via config or environment variable ."}}
{"translation": {"code": "def load_file ( self , source , destination , overwrite = True , parallelism = 1 , * * kwargs ) : conn = self . get_conn ( ) conn . upload ( hdfs_path = destination , local_path = source , overwrite = overwrite , n_threads = parallelism , * * kwargs ) self . log . debug ( \"Uploaded file %s to %s\" , source , destination )", "nl": "r Uploads a file to HDFS ."}}
{"translation": {"code": "def json_response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = AirflowJsonEncoder ) , status = 200 , mimetype = \"application/json\" )", "nl": "returns a json response from a json serializable python object"}}
{"translation": {"code": "def get_conn ( self ) : self . log . debug ( 'Creating SSH client for conn_id: %s' , self . ssh_conn_id ) client = paramiko . SSHClient ( ) if not self . allow_host_key_change : self . log . warning ( 'Remote Identification Change is not verified. ' 'This wont protect against Man-In-The-Middle attacks' ) client . load_system_host_keys ( ) if self . no_host_key_check : self . log . warning ( 'No Host Key Verification. This wont protect ' 'against Man-In-The-Middle attacks' ) # Default is RejectPolicy client . set_missing_host_key_policy ( paramiko . AutoAddPolicy ( ) ) if self . password and self . password . strip ( ) : client . connect ( hostname = self . remote_host , username = self . username , password = self . password , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) else : client . connect ( hostname = self . remote_host , username = self . username , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) if self . keepalive_interval : client . get_transport ( ) . set_keepalive ( self . keepalive_interval ) self . client = client return client", "nl": "Opens a ssh connection to the remote host ."}}
{"translation": {"code": "def get_service ( self ) : http_authorized = self . _authorize ( ) return build ( 'bigquery' , 'v2' , http = http_authorized , cache_discovery = False )", "nl": "Returns a BigQuery service object ."}}
{"translation": {"code": "def get_conn ( self ) : service = self . get_service ( ) project = self . _get_field ( 'project' ) return BigQueryConnection ( service = service , project_id = project , use_legacy_sql = self . use_legacy_sql , location = self . location , num_retries = self . num_retries )", "nl": "Returns a BigQuery PEP 249 connection object ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _conn : self . _conn = storage . Client ( credentials = self . _get_credentials ( ) ) return self . _conn", "nl": "Returns a Google Cloud Storage service object ."}}
{"translation": {"code": "def download ( self , bucket_name , object_name , filename = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name ) blob = bucket . blob ( blob_name = object_name ) if filename : blob . download_to_filename ( filename ) self . log . info ( 'File downloaded to %s' , filename ) return blob . download_as_string ( )", "nl": "Get a file from Google Cloud Storage ."}}
{"translation": {"code": "def heartbeat_callback ( self , session = None ) : if self . terminating : # ensure termination if processes are created later self . task_runner . terminate ( ) return self . task_instance . refresh_from_db ( ) ti = self . task_instance fqdn = get_hostname ( ) same_hostname = fqdn == ti . hostname same_process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same_hostname : self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) raise AirflowException ( \"Hostname of job runner does not match\" ) elif not same_process : current_pid = os . getpid ( ) self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current_pid ) raise AirflowException ( \"PID of job runner does not match\" ) elif ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) self . task_runner . terminate ( ) self . terminating = True", "nl": "Self destruct task if state has been moved away from running externally"}}
{"translation": {"code": "def _authorize ( self ) : credentials = self . _get_credentials ( ) http = httplib2 . Http ( ) authed_http = google_auth_httplib2 . AuthorizedHttp ( credentials , http = http ) return authed_http", "nl": "Returns an authorized HTTP object to be used to build a Google cloud service hook connection ."}}
{"translation": {"code": "def _get_credentials ( self ) : key_path = self . _get_field ( 'key_path' , False ) keyfile_dict = self . _get_field ( 'keyfile_dict' , False ) scope = self . _get_field ( 'scope' , None ) if scope : scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] else : scopes = _DEFAULT_SCOPES if not key_path and not keyfile_dict : self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) credentials , _ = google . auth . default ( scopes = scopes ) elif key_path : # Get credentials from a JSON file. if key_path . endswith ( '.json' ) : self . log . debug ( 'Getting connection using JSON key file %s' % key_path ) credentials = ( google . oauth2 . service_account . Credentials . from_service_account_file ( key_path , scopes = scopes ) ) elif key_path . endswith ( '.p12' ) : raise AirflowException ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) else : raise AirflowException ( 'Unrecognised extension for key file.' ) else : # Get credentials from JSON data provided in the UI. try : keyfile_dict = json . loads ( keyfile_dict ) # Depending on how the JSON was formatted, it may contain # escaped newlines. Convert those to actual newlines. keyfile_dict [ 'private_key' ] = keyfile_dict [ 'private_key' ] . replace ( '\\\\n' , '\\n' ) credentials = ( google . oauth2 . service_account . Credentials . from_service_account_info ( keyfile_dict , scopes = scopes ) ) except json . decoder . JSONDecodeError : raise AirflowException ( 'Invalid key JSON.' ) return credentials . with_subject ( self . delegate_to ) if self . delegate_to else credentials", "nl": "Returns the Credentials object for Google API"}}
{"translation": {"code": "def run_command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise AirflowConfigException ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) return output", "nl": "Runs command and returns stdout"}}
{"translation": {"code": "def executemany ( self , operation , seq_of_parameters ) : for parameters in seq_of_parameters : self . execute ( operation , parameters )", "nl": "Execute a BigQuery query multiple times with different parameters ."}}
{"translation": {"code": "def _escape ( s ) : e = s e = e . replace ( '\\\\' , '\\\\\\\\' ) e = e . replace ( '\\n' , '\\\\n' ) e = e . replace ( '\\r' , '\\\\r' ) e = e . replace ( \"'\" , \"\\\\'\" ) e = e . replace ( '\"' , '\\\\\"' ) return e", "nl": "Helper method that escapes parameters to a SQL query ."}}
{"translation": {"code": "def _bind_parameters ( operation , parameters ) : # inspired by MySQL Python Connector (conversion.py) string_parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string_parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string_parameters [ name ] = \"'\" + _escape ( value ) + \"'\" else : string_parameters [ name ] = str ( value ) return operation % string_parameters", "nl": "Helper method that binds parameters to a SQL query ."}}
{"translation": {"code": "def execute ( self , operation , parameters = None ) : sql = _bind_parameters ( operation , parameters ) if parameters else operation self . job_id = self . run_query ( sql )", "nl": "Executes a BigQuery query and returns the job ID ."}}
{"translation": {"code": "def next ( self ) : if not self . job_id : return None if len ( self . buffer ) == 0 : if self . all_pages_loaded : return None query_results = ( self . service . jobs ( ) . getQueryResults ( projectId = self . project_id , jobId = self . job_id , pageToken = self . page_token ) . execute ( num_retries = self . num_retries ) ) if 'rows' in query_results and query_results [ 'rows' ] : self . page_token = query_results . get ( 'pageToken' ) fields = query_results [ 'schema' ] [ 'fields' ] col_types = [ field [ 'type' ] for field in fields ] rows = query_results [ 'rows' ] for dict_row in rows : typed_row = ( [ _bq_cast ( vs [ 'v' ] , col_types [ idx ] ) for idx , vs in enumerate ( dict_row [ 'f' ] ) ] ) self . buffer . append ( typed_row ) if not self . page_token : self . all_pages_loaded = True else : # Reset all state since we've exhausted the results. self . page_token = None self . job_id = None self . page_token = None return None return self . buffer . pop ( 0 )", "nl": "Helper method for fetchone which returns the next row from a buffer . If the buffer is empty attempts to paginate through the result set for the next page and load it into the buffer ."}}
{"translation": {"code": "def _bq_cast ( string_field , bq_type ) : if string_field is None : return None elif bq_type == 'INTEGER' : return int ( string_field ) elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP' : return float ( string_field ) elif bq_type == 'BOOLEAN' : if string_field not in [ 'true' , 'false' ] : raise ValueError ( \"{} must have value 'true' or 'false'\" . format ( string_field ) ) return string_field == 'true' else : return string_field", "nl": "Helper method that casts a BigQuery row to the appropriate data types . This is useful because BigQuery returns all fields as strings ."}}
{"translation": {"code": "def conditionally_trigger ( context , dag_run_obj ) : c_p = context [ 'params' ] [ 'condition_param' ] print ( \"Controller DAG : conditionally_trigger = {}\" . format ( c_p ) ) if context [ 'params' ] [ 'condition_param' ] : dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag_run_obj . payload ) return dag_run_obj", "nl": "This function decides whether or not to Trigger the remote DAG"}}
{"translation": {"code": "def _query_mysql ( self ) : mysql = MySqlHook ( mysql_conn_id = self . mysql_conn_id ) conn = mysql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor", "nl": "Queries mysql and returns a cursor to the results ."}}
{"translation": {"code": "def _write_local_schema_file ( self , cursor ) : schema_str = None schema_file_mime_type = 'application/json' tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) if self . schema is not None and isinstance ( self . schema , string_types ) : schema_str = self . schema . encode ( 'utf-8' ) elif self . schema is not None and isinstance ( self . schema , list ) : schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) else : schema = [ ] for field in cursor . description : # See PEP 249 for details about the description tuple. field_name = field [ 0 ] field_type = self . type_map ( field [ 1 ] ) # Always allow TIMESTAMP to be nullable. MySQLdb returns None types # for required fields because some MySQL timestamps can't be # represented by Python's datetime (e.g. 0000-00-00 00:00:00). if field [ 6 ] or field_type == 'TIMESTAMP' : field_mode = 'NULLABLE' else : field_mode = 'REQUIRED' schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( schema_str ) self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } return schema_file_to_upload", "nl": "Takes a cursor and writes the BigQuery schema in . json format for the results to a local file system ."}}
{"translation": {"code": "def type_map ( cls , mysql_type ) : d = { FIELD_TYPE . INT24 : 'INTEGER' , FIELD_TYPE . TINY : 'INTEGER' , FIELD_TYPE . BIT : 'INTEGER' , FIELD_TYPE . DATETIME : 'TIMESTAMP' , FIELD_TYPE . DATE : 'TIMESTAMP' , FIELD_TYPE . DECIMAL : 'FLOAT' , FIELD_TYPE . NEWDECIMAL : 'FLOAT' , FIELD_TYPE . DOUBLE : 'FLOAT' , FIELD_TYPE . FLOAT : 'FLOAT' , FIELD_TYPE . LONG : 'INTEGER' , FIELD_TYPE . LONGLONG : 'INTEGER' , FIELD_TYPE . SHORT : 'INTEGER' , FIELD_TYPE . TIMESTAMP : 'TIMESTAMP' , FIELD_TYPE . YEAR : 'INTEGER' , } return d [ mysql_type ] if mysql_type in d else 'STRING'", "nl": "Helper function that maps from MySQL fields to BigQuery fields . Used when a schema_filename is set ."}}
{"translation": {"code": "def upload ( self , bucket_name , object_name , filename , mime_type = 'application/octet-stream' , gzip = False ) : if gzip : filename_gz = filename + '.gz' with open ( filename , 'rb' ) as f_in : with gz . open ( filename_gz , 'wb' ) as f_out : shutil . copyfileobj ( f_in , f_out ) filename = filename_gz client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . upload_from_filename ( filename = filename , content_type = mime_type ) if gzip : os . remove ( filename ) self . log . info ( 'File %s uploaded to %s in %s bucket' , filename , object_name , bucket_name )", "nl": "Uploads a local file to Google Cloud Storage ."}}
{"translation": {"code": "def allocate_ids ( self , partial_keys ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . allocateIds ( projectId = self . project_id , body = { 'keys' : partial_keys } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'keys' ]", "nl": "Allocate IDs for incomplete keys ."}}
{"translation": {"code": "def commit ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . commit ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp", "nl": "Commit a transaction optionally creating deleting or modifying some entities ."}}
{"translation": {"code": "def lookup ( self , keys , read_consistency = None , transaction = None ) : conn = self . get_conn ( ) body = { 'keys' : keys } if read_consistency : body [ 'readConsistency' ] = read_consistency if transaction : body [ 'transaction' ] = transaction resp = ( conn . projects ( ) . lookup ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp", "nl": "Lookup some entities by key ."}}
{"translation": {"code": "def rollback ( self , transaction ) : conn = self . get_conn ( ) conn . projects ( ) . rollback ( projectId = self . project_id , body = { 'transaction' : transaction } ) . execute ( num_retries = self . num_retries )", "nl": "Roll back a transaction ."}}
{"translation": {"code": "def run_query ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . runQuery ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp [ 'batch' ]", "nl": "Run a query for entities ."}}
{"translation": {"code": "def begin_transaction ( self ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . beginTransaction ( projectId = self . project_id , body = { } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'transaction' ]", "nl": "Begins a new transaction ."}}
{"translation": {"code": "def run_cli ( self , pig , verbose = True ) : with TemporaryDirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir ) as f : f . write ( pig . encode ( 'utf-8' ) ) f . flush ( ) fname = f . name pig_bin = 'pig' cmd_extra = [ ] pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra if self . pig_properties : pig_properties_list = self . pig_properties . split ( ) pig_cmd . extend ( pig_properties_list ) if verbose : self . log . info ( \"%s\" , \" \" . join ( pig_cmd ) ) sp = subprocess . Popen ( pig_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = True ) self . sp = sp stdout = '' for line in iter ( sp . stdout . readline , b'' ) : stdout += line . decode ( 'utf-8' ) if verbose : self . log . info ( line . strip ( ) ) sp . wait ( ) if sp . returncode : raise AirflowException ( stdout ) return stdout", "nl": "Run an pig script using the pig cli"}}
{"translation": {"code": "def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import_module ( path ) backend = getattr ( module , attr ) to = get_email_address_list ( to ) to = \", \" . join ( to ) return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , * * kwargs )", "nl": "Send email using backend specified in EMAIL_BACKEND ."}}
{"translation": {"code": "def send_email_smtp ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : smtp_mail_from = configuration . conf . get ( 'smtp' , 'SMTP_MAIL_FROM' ) to = get_email_address_list ( to ) msg = MIMEMultipart ( mime_subtype ) msg [ 'Subject' ] = subject msg [ 'From' ] = smtp_mail_from msg [ 'To' ] = \", \" . join ( to ) recipients = to if cc : cc = get_email_address_list ( cc ) msg [ 'CC' ] = \", \" . join ( cc ) recipients = recipients + cc if bcc : # don't add bcc in header bcc = get_email_address_list ( bcc ) recipients = recipients + bcc msg [ 'Date' ] = formatdate ( localtime = True ) mime_text = MIMEText ( html_content , 'html' , mime_charset ) msg . attach ( mime_text ) for fname in files or [ ] : basename = os . path . basename ( fname ) with open ( fname , \"rb\" ) as f : part = MIMEApplication ( f . read ( ) , Name = basename ) part [ 'Content-Disposition' ] = 'attachment; filename=\"%s\"' % basename part [ 'Content-ID' ] = '<%s>' % basename msg . attach ( part ) send_MIME_email ( smtp_mail_from , recipients , msg , dryrun )", "nl": "Send an email with html content"}}
{"translation": {"code": "def alchemy_to_dict ( obj ) : if not obj : return None d = { } for c in obj . __table__ . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d", "nl": "Transforms a SQLAlchemy model instance into a dictionary"}}
{"translation": {"code": "def resetdb ( ) : from airflow import models # alembic adds significant import time, so we import it lazily from alembic . migration import MigrationContext log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )", "nl": "Clear out the database"}}
{"translation": {"code": "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , * * kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , * * kwargs ) return wrapper", "nl": "Function decorator that provides a session if it isn t provided . If you want to reuse a session or run the function as part of a database transaction you pass it to the function if not this wrapper will create one and close it for you ."}}
{"translation": {"code": "def apply_defaults ( func ) : # Cache inspect.signature for the wrapper closure to avoid calling it # at every decorated invocation. This is separate sig_cache created # per decoration, i.e. each function decorated using apply_defaults will # have a different sig_cache. sig_cache = signature ( func ) non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if param . default == param . empty and param . name != 'self' and param . kind not in ( param . VAR_POSITIONAL , param . VAR_KEYWORD ) } @ wraps ( func ) def wrapper ( * args , * * kwargs ) : if len ( args ) > 1 : raise AirflowException ( \"Use keyword arguments when initializing operators\" ) dag_args = { } dag_params = { } dag = kwargs . get ( 'dag' , None ) or settings . CONTEXT_MANAGER_DAG if dag : dag_args = copy ( dag . default_args ) or { } dag_params = copy ( dag . params ) or { } params = { } if 'params' in kwargs : params = kwargs [ 'params' ] dag_params . update ( params ) default_args = { } if 'default_args' in kwargs : default_args = kwargs [ 'default_args' ] if 'params' in default_args : dag_params . update ( default_args [ 'params' ] ) del default_args [ 'params' ] dag_args . update ( default_args ) default_args = dag_args for arg in sig_cache . parameters : if arg not in kwargs and arg in default_args : kwargs [ arg ] = default_args [ arg ] missing_args = list ( non_optional_args - set ( kwargs ) ) if missing_args : msg = \"Argument {0} is required\" . format ( missing_args ) raise AirflowException ( msg ) kwargs [ 'params' ] = dag_params result = func ( * args , * * kwargs ) return result return wrapper", "nl": "Function decorator that Looks for an argument named default_args and fills the unspecified arguments from it ."}}
{"translation": {"code": "def date_range ( start_date , end_date = None , num = None , delta = None ) : if not delta : return [ ] if end_date and start_date > end_date : raise Exception ( \"Wait. start_date needs to be before end_date\" ) if end_date and num : raise Exception ( \"Wait. Either specify end_date OR num\" ) if not end_date and not num : end_date = timezone . utcnow ( ) delta_iscron = False tz = start_date . tzinfo if isinstance ( delta , six . string_types ) : delta_iscron = True start_date = timezone . make_naive ( start_date , tz ) cron = croniter ( delta , start_date ) elif isinstance ( delta , timedelta ) : delta = abs ( delta ) dates = [ ] if end_date : if timezone . is_naive ( start_date ) : end_date = timezone . make_naive ( end_date , tz ) while start_date <= end_date : if timezone . is_naive ( start_date ) : dates . append ( timezone . make_aware ( start_date , tz ) ) else : dates . append ( start_date ) if delta_iscron : start_date = cron . get_next ( datetime ) else : start_date += delta else : for _ in range ( abs ( num ) ) : if timezone . is_naive ( start_date ) : dates . append ( timezone . make_aware ( start_date , tz ) ) else : dates . append ( start_date ) if delta_iscron : if num > 0 : start_date = cron . get_next ( datetime ) else : start_date = cron . get_prev ( datetime ) else : if num > 0 : start_date += delta else : start_date -= delta return sorted ( dates )", "nl": "Get a set of dates as a list based on a start end and delta delta can be something that can be added to datetime . datetime or a cron expression as a str"}}
{"translation": {"code": "def run_grant_dataset_view_access ( self , source_dataset , view_dataset , view_table , source_project = None , view_project = None ) : # Apply default values to projects source_project = source_project if source_project else self . project_id view_project = view_project if view_project else self . project_id # we don't want to clobber any existing accesses, so we have to get # info on the dataset before we can add view access source_dataset_resource = self . service . datasets ( ) . get ( projectId = source_project , datasetId = source_dataset ) . execute ( num_retries = self . num_retries ) access = source_dataset_resource [ 'access' ] if 'access' in source_dataset_resource else [ ] view_access = { 'view' : { 'projectId' : view_project , 'datasetId' : view_dataset , 'tableId' : view_table } } # check to see if the view we want to add already exists. if view_access not in access : self . log . info ( 'Granting table %s:%s.%s authorized view access to %s:%s dataset.' , view_project , view_dataset , view_table , source_project , source_dataset ) access . append ( view_access ) return self . service . datasets ( ) . patch ( projectId = source_project , datasetId = source_dataset , body = { 'access' : access } ) . execute ( num_retries = self . num_retries ) else : # if view is already in access, do nothing. self . log . info ( 'Table %s:%s.%s already has authorized view access to %s:%s dataset.' , view_project , view_dataset , view_table , source_project , source_dataset ) return source_dataset_resource", "nl": "Grant authorized view access of a dataset to a view table . If this view has already been granted access to the dataset do nothing . This method is not atomic . Running it may clobber a simultaneous update ."}}
{"translation": {"code": "def run_table_upsert ( self , dataset_id , table_resource , project_id = None ) : # check to see if the table exists table_id = table_resource [ 'tableReference' ] [ 'tableId' ] project_id = project_id if project_id is not None else self . project_id tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id ) . execute ( num_retries = self . num_retries ) while True : for table in tables_list_resp . get ( 'tables' , [ ] ) : if table [ 'tableReference' ] [ 'tableId' ] == table_id : # found the table, do update self . log . info ( 'Table %s:%s.%s exists, updating.' , project_id , dataset_id , table_id ) return self . service . tables ( ) . update ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) # If there is a next page, we need to check the next page. if 'nextPageToken' in tables_list_resp : tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id , pageToken = tables_list_resp [ 'nextPageToken' ] ) . execute ( num_retries = self . num_retries ) # If there is no next page, then the table doesn't exist. else : # do insert self . log . info ( 'Table %s:%s.%s does not exist. creating.' , project_id , dataset_id , table_id ) return self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = self . num_retries )", "nl": "creates a new empty table in the dataset ; If the table already exists update the existing table . Since BigQuery does not natively allow table upserts this is not an atomic operation ."}}
{"translation": {"code": "def unfinished ( cls ) : return [ cls . NONE , cls . SCHEDULED , cls . QUEUED , cls . RUNNING , cls . SHUTDOWN , cls . UP_FOR_RETRY , cls . UP_FOR_RESCHEDULE ]", "nl": "A list of states indicating that a task either has not completed a run or has not even started ."}}
{"translation": {"code": "def _execute ( self , session = None ) : ti_status = BackfillJob . _DagRunTaskStatus ( ) start_date = self . bf_start_date # Get intervals between the start/end dates, which will turn into dag runs run_dates = self . dag . get_run_dates ( start_date = start_date , end_date = self . bf_end_date ) if self . run_backwards : tasks_that_depend_on_past = [ t . task_id for t in self . dag . task_dict . values ( ) if t . depends_on_past ] if tasks_that_depend_on_past : raise AirflowException ( 'You cannot backfill backwards because one or more tasks depend_on_past: {}' . format ( \",\" . join ( tasks_that_depend_on_past ) ) ) run_dates = run_dates [ : : - 1 ] if len ( run_dates ) == 0 : self . log . info ( \"No run dates were found for the given dates and dag interval.\" ) return # picklin' pickle_id = None if not self . donot_pickle and self . executor . __class__ not in ( executors . LocalExecutor , executors . SequentialExecutor ) : pickle = DagPickle ( self . dag ) session . add ( pickle ) session . commit ( ) pickle_id = pickle . id executor = self . executor executor . start ( ) ti_status . total_runs = len ( run_dates ) # total dag runs in backfill try : remaining_dates = ti_status . total_runs while remaining_dates > 0 : dates_to_process = [ run_date for run_date in run_dates if run_date not in ti_status . executed_dag_run_dates ] self . _execute_for_run_dates ( run_dates = dates_to_process , ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) remaining_dates = ( ti_status . total_runs - len ( ti_status . executed_dag_run_dates ) ) err = self . _collect_errors ( ti_status = ti_status , session = session ) if err : raise AirflowException ( err ) if remaining_dates > 0 : self . log . info ( \"max_active_runs limit for dag %s has been reached \" \" - waiting for other dag runs to finish\" , self . dag_id ) time . sleep ( self . delay_on_limit_secs ) except ( KeyboardInterrupt , SystemExit ) : self . log . warning ( \"Backfill terminated by user.\" ) # TODO: we will need to terminate running task instances and set the # state to failed. self . _set_unfinished_dag_runs_to_failed ( ti_status . active_runs ) finally : session . commit ( ) executor . end ( ) self . log . info ( \"Backfill done. Exiting.\" )", "nl": "Initializes all components required to run a dag for a specified date range and calls helper method to execute the tasks ."}}
{"translation": {"code": "def _change_state_for_executable_task_instances ( self , task_instances , acceptable_states , session = None ) : if len ( task_instances ) == 0 : session . commit ( ) return [ ] TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == ti . dag_id , TI . task_id == ti . task_id , TI . execution_date == ti . execution_date ) for ti in task_instances ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) if None in acceptable_states : ti_query = ti_query . filter ( or_ ( TI . state == None , TI . state . in_ ( acceptable_states ) ) # noqa: E711 ) else : ti_query = ti_query . filter ( TI . state . in_ ( acceptable_states ) ) tis_to_set_to_queued = ( ti_query . with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_queued ) == 0 : self . log . info ( \"No tasks were able to have their state changed to queued.\" ) session . commit ( ) return [ ] # set TIs to queued state for task_instance in tis_to_set_to_queued : task_instance . state = State . QUEUED task_instance . queued_dttm = ( timezone . utcnow ( ) if not task_instance . queued_dttm else task_instance . queued_dttm ) session . merge ( task_instance ) # Generate a list of SimpleTaskInstance for the use of queuing # them in the executor. simple_task_instances = [ SimpleTaskInstance ( ti ) for ti in tis_to_set_to_queued ] task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_queued ] ) session . commit ( ) self . log . info ( \"Setting the following %s tasks to queued state:\\n\\t%s\" , len ( tis_to_set_to_queued ) , task_instance_str ) return simple_task_instances", "nl": "Changes the state of task instances in the list with one of the given states to QUEUED atomically and returns the TIs changed in SimpleTaskInstance format ."}}
{"translation": {"code": "def pprinttable ( rows ) : if not rows : return if hasattr ( rows [ 0 ] , '_fields' ) : # if namedtuple headers = rows [ 0 ] . _fields else : headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] lens = [ len ( s ) for s in headers ] for row in rows : for i in range ( len ( rows [ 0 ] ) ) : slenght = len ( \"{}\" . format ( row [ i ] ) ) if slenght > lens [ i ] : lens [ i ] = slenght formats = [ ] hformats = [ ] for i in range ( len ( rows [ 0 ] ) ) : if isinstance ( rows [ 0 ] [ i ] , int ) : formats . append ( \"%%%dd\" % lens [ i ] ) else : formats . append ( \"%%-%ds\" % lens [ i ] ) hformats . append ( \"%%-%ds\" % lens [ i ] ) pattern = \" | \" . join ( formats ) hpattern = \" | \" . join ( hformats ) separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) s = \"\" s += separator + '\\n' s += ( hpattern % tuple ( headers ) ) + '\\n' s += separator + '\\n' def f ( t ) : return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t for line in rows : s += pattern % tuple ( f ( t ) for t in line ) + '\\n' s += separator + '\\n' return s", "nl": "Returns a pretty ascii table from tuples"}}
{"translation": {"code": "def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )", "nl": "Uploads the file to Google cloud storage"}}
{"translation": {"code": "def get_conn ( self ) : http_authorized = self . _authorize ( ) return build ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = False )", "nl": "Returns a Google Cloud Dataproc service object ."}}
{"translation": {"code": "def execute ( self , context ) : self . hook = SqoopHook ( conn_id = self . conn_id , verbose = self . verbose , num_mappers = self . num_mappers , hcatalog_database = self . hcatalog_database , hcatalog_table = self . hcatalog_table , properties = self . properties ) if self . cmd_type == 'export' : self . hook . export_table ( table = self . table , export_dir = self . export_dir , input_null_string = self . input_null_string , input_null_non_string = self . input_null_non_string , staging_table = self . staging_table , clear_staging_table = self . clear_staging_table , enclosed_by = self . enclosed_by , escaped_by = self . escaped_by , input_fields_terminated_by = self . input_fields_terminated_by , input_lines_terminated_by = self . input_lines_terminated_by , input_optionally_enclosed_by = self . input_optionally_enclosed_by , batch = self . batch , relaxed_isolation = self . relaxed_isolation , extra_export_options = self . extra_export_options ) elif self . cmd_type == 'import' : # add create hcatalog table to extra import options if option passed # if new params are added to constructor can pass them in here # so don't modify sqoop_hook for each param if self . create_hcatalog_table : self . extra_import_options [ 'create-hcatalog-table' ] = '' if self . table and self . query : raise AirflowException ( 'Cannot specify query and table together. Need to specify either or.' ) if self . table : self . hook . import_table ( table = self . table , target_dir = self . target_dir , append = self . append , file_type = self . file_type , columns = self . columns , split_by = self . split_by , where = self . where , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) elif self . query : self . hook . import_query ( query = self . query , target_dir = self . target_dir , append = self . append , file_type = self . file_type , split_by = self . split_by , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) else : raise AirflowException ( \"Provide query or table parameter to import using Sqoop\" ) else : raise AirflowException ( \"cmd_type should be 'import' or 'export'\" )", "nl": "Execute sqoop job"}}
{"translation": {"code": "def import_query ( self , query , target_dir , append = False , file_type = \"text\" , split_by = None , direct = None , driver = None , extra_import_options = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ \"--query\" , query ] self . Popen ( cmd )", "nl": "Imports a specific query from the rdbms to hdfs"}}
{"translation": {"code": "def import_table ( self , table , target_dir = None , append = False , file_type = \"text\" , columns = None , split_by = None , where = None , direct = False , driver = None , extra_import_options = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ \"--table\" , table ] if columns : cmd += [ \"--columns\" , columns ] if where : cmd += [ \"--where\" , where ] self . Popen ( cmd )", "nl": "Imports table from remote location to target dir . Arguments are copies of direct sqoop command line arguments"}}
{"translation": {"code": "def context_to_airflow_vars ( context , in_env_var_format = False ) : params = dict ( ) if in_env_var_format : name_format = 'env_var_format' else : name_format = 'default' task_instance = context . get ( 'task_instance' ) if task_instance and task_instance . dag_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_ID' ] [ name_format ] ] = task_instance . dag_id if task_instance and task_instance . task_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_TASK_ID' ] [ name_format ] ] = task_instance . task_id if task_instance and task_instance . execution_date : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_EXECUTION_DATE' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) dag_run = context . get ( 'dag_run' ) if dag_run and dag_run . run_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_RUN_ID' ] [ name_format ] ] = dag_run . run_id return params", "nl": "Given a context this function provides a dictionary of values that can be used to externally reconstruct relations between dags dag_runs tasks and task_instances . Default to abc . def . ghi format and can be made to ABC_DEF_GHI format if in_env_var_format is set to True ."}}
{"translation": {"code": "def check_for_named_partition ( self , schema , table , partition_name ) : with self . metastore as client : return client . check_for_named_partition ( schema , table , partition_name )", "nl": "Checks whether a partition with a given name exists"}}
{"translation": {"code": "def create_job_flow ( self , job_flow_overrides ) : if not self . emr_conn_id : raise AirflowException ( 'emr_conn_id must be present to use create_job_flow' ) emr_conn = self . get_connection ( self . emr_conn_id ) config = emr_conn . extra_dejson . copy ( ) config . update ( job_flow_overrides ) response = self . get_conn ( ) . run_job_flow ( * * config ) return response", "nl": "Creates a job flow using the config from the EMR connection . Keys of the json extra hash may have the arguments of the boto3 run_job_flow method . Overrides for this config may be passed as the job_flow_overrides ."}}
{"translation": {"code": "def _prepare_cli_cmd ( self ) : conn = self . conn hive_bin = 'hive' cmd_extra = [ ] if self . use_beeline : hive_bin = 'beeline' jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra_dejson . get ( 'principal' , \"hive/_HOST@EXAMPLE.COM\" ) if \"_HOST\" in template : template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) proxy_user = \"\" # noqa if conn . extra_dejson . get ( 'proxy_user' ) == \"login\" and conn . login : proxy_user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) elif conn . extra_dejson . get ( 'proxy_user' ) == \"owner\" and self . run_as : proxy_user = \"hive.server2.proxy.user={0}\" . format ( self . run_as ) jdbc_url += \";principal={template};{proxy_user}\" . format ( template = template , proxy_user = proxy_user ) elif self . auth : jdbc_url += \";auth=\" + self . auth jdbc_url = '\"{}\"' . format ( jdbc_url ) cmd_extra += [ '-u' , jdbc_url ] if conn . login : cmd_extra += [ '-n' , conn . login ] if conn . password : cmd_extra += [ '-p' , conn . password ] hive_params_list = self . hive_cli_params . split ( ) return [ hive_bin ] + cmd_extra + hive_params_list", "nl": "This function creates the command list from available information"}}
{"translation": {"code": "def _prepare_hiveconf ( d ) : if not d : return [ ] return as_flattened_list ( zip ( [ \"-hiveconf\" ] * len ( d ) , [ \"{}={}\" . format ( k , v ) for k , v in d . items ( ) ] ) )", "nl": "This function prepares a list of hiveconf params from a dictionary of key value pairs ."}}
{"translation": {"code": "def _prepare_command ( self , cmd ) : connection_cmd = [ \"spark-sql\" ] if self . _conf : for conf_el in self . _conf . split ( \",\" ) : connection_cmd += [ \"--conf\" , conf_el ] if self . _total_executor_cores : connection_cmd += [ \"--total-executor-cores\" , str ( self . _total_executor_cores ) ] if self . _executor_cores : connection_cmd += [ \"--executor-cores\" , str ( self . _executor_cores ) ] if self . _executor_memory : connection_cmd += [ \"--executor-memory\" , self . _executor_memory ] if self . _keytab : connection_cmd += [ \"--keytab\" , self . _keytab ] if self . _principal : connection_cmd += [ \"--principal\" , self . _principal ] if self . _num_executors : connection_cmd += [ \"--num-executors\" , str ( self . _num_executors ) ] if self . _sql : sql = self . _sql . strip ( ) if sql . endswith ( \".sql\" ) or sql . endswith ( \".hql\" ) : connection_cmd += [ \"-f\" , sql ] else : connection_cmd += [ \"-e\" , sql ] if self . _master : connection_cmd += [ \"--master\" , self . _master ] if self . _name : connection_cmd += [ \"--name\" , self . _name ] if self . _verbose : connection_cmd += [ \"--verbose\" ] if self . _yarn_queue : connection_cmd += [ \"--queue\" , self . _yarn_queue ] connection_cmd += cmd self . log . debug ( \"Spark-Sql cmd: %s\" , connection_cmd ) return connection_cmd", "nl": "Construct the spark - sql command to execute . Verbose output is enabled as default ."}}
{"translation": {"code": "def execute ( self , context ) : self . _hook = SparkSqlHook ( sql = self . _sql , conf = self . _conf , conn_id = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )", "nl": "Call the SparkSqlHook to run the provided sql query"}}
{"translation": {"code": "def _launch_process ( result_queue , file_path , pickle_dags , dag_id_white_list , thread_name , zombies ) : def helper ( ) : # This helper runs in the newly created process log = logging . getLogger ( \"airflow.processor\" ) stdout = StreamLogWriter ( log , logging . INFO ) stderr = StreamLogWriter ( log , logging . WARN ) set_context ( log , file_path ) try : # redirect stdout/stderr to log sys . stdout = stdout sys . stderr = stderr # Re-configure the ORM engine as there are issues with multiple processes settings . configure_orm ( ) # Change the thread name to differentiate log lines. This is # really a separate process, but changing the name of the # process doesn't work, so changing the thread name instead. threading . current_thread ( ) . name = thread_name start_time = time . time ( ) log . info ( \"Started process (PID=%s) to work on %s\" , os . getpid ( ) , file_path ) scheduler_job = SchedulerJob ( dag_ids = dag_id_white_list , log = log ) result = scheduler_job . process_file ( file_path , zombies , pickle_dags ) result_queue . put ( result ) end_time = time . time ( ) log . info ( \"Processing %s took %.3f seconds\" , file_path , end_time - start_time ) except Exception : # Log exceptions through the logging framework. log . exception ( \"Got an exception! Propagating...\" ) raise finally : sys . stdout = sys . __stdout__ sys . stderr = sys . __stderr__ # We re-initialized the ORM within this Process above so we need to # tear it down manually here settings . dispose_orm ( ) p = multiprocessing . Process ( target = helper , args = ( ) , name = \"{}-Process\" . format ( thread_name ) ) p . start ( ) return p", "nl": "Launch a process to process the given file ."}}
{"translation": {"code": "def update_import_errors ( session , dagbag ) : # Clear the errors of the processed files for dagbag_file in dagbag . file_last_changed : session . query ( errors . ImportError ) . filter ( errors . ImportError . filename == dagbag_file ) . delete ( ) # Add the errors of the processed files for filename , stacktrace in six . iteritems ( dagbag . import_errors ) : session . add ( errors . ImportError ( filename = filename , stacktrace = stacktrace ) ) session . commit ( )", "nl": "For the DAGs in the given DagBag record any associated import errors and clears errors for files that no longer have them . These are usually displayed through the Airflow UI so that users know that there are issues parsing DAGs ."}}
{"translation": {"code": "def done ( self ) : if self . _process is None : raise AirflowException ( \"Tried to see if it's done before starting!\" ) if self . _done : return True # In case result queue is corrupted. if self . _result_queue and not self . _result_queue . empty ( ) : self . _result = self . _result_queue . get_nowait ( ) self . _done = True self . log . debug ( \"Waiting for %s\" , self . _process ) self . _process . join ( ) return True # Potential error case when process dies if self . _result_queue and not self . _process . is_alive ( ) : self . _done = True # Get the object from the queue or else join() can hang. if not self . _result_queue . empty ( ) : self . _result = self . _result_queue . get_nowait ( ) self . log . debug ( \"Waiting for %s\" , self . _process ) self . _process . join ( ) return True return False", "nl": "Check if the process launched to process this file is done ."}}
{"translation": {"code": "def start ( self ) : self . _process = DagFileProcessor . _launch_process ( self . _result_queue , self . file_path , self . _pickle_dags , self . _dag_id_white_list , \"DagFileProcessor{}\" . format ( self . _instance_id ) , self . _zombies ) self . _start_time = timezone . utcnow ( )", "nl": "Launch the process and start processing the DAG ."}}
{"translation": {"code": "def _process_executor_events ( self , simple_dag_bag , session = None ) : # TODO: this shares quite a lot of code with _manage_executor_state TI = models . TaskInstance for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : dag_id , task_id , execution_date , try_number = key self . log . info ( \"Executor reports execution of %s.%s execution_date=%s \" \"exited with status %s for try_number %s\" , dag_id , task_id , execution_date , state , try_number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date ) ti = qry . first ( ) if not ti : self . log . warning ( \"TaskInstance %s went missing from the database\" , ti ) continue # TODO: should we fail RUNNING as well, as we do in Backfills? if ti . try_number == try_number and ti . state == State . QUEUED : msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple_dag = simple_dag_bag . get_dag ( dag_id ) dagbag = models . DagBag ( simple_dag . full_filepath ) dag = dagbag . get_dag ( dag_id ) ti . task = dag . get_task ( task_id ) ti . handle_failure ( msg ) except Exception : self . log . error ( \"Cannot load the dag bag to handle failure for %s\" \". Setting task to FAILED without callbacks or \" \"retries. Do you have enough resources?\" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )", "nl": "Respond to executor events ."}}
{"translation": {"code": "def list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : if include_examples is None : include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) file_paths = [ ] if directory is None : return [ ] elif os . path . isfile ( directory ) : return [ directory ] elif os . path . isdir ( directory ) : patterns_by_dir = { } for root , dirs , files in os . walk ( directory , followlinks = True ) : patterns = patterns_by_dir . get ( root , [ ] ) ignore_file = os . path . join ( root , '.airflowignore' ) if os . path . isfile ( ignore_file ) : with open ( ignore_file , 'r' ) as f : # If we have new patterns create a copy so we don't change # the previous list (which would affect other subdirs) patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] # If we can ignore any subdirs entirely we should - fewer paths # to walk is better. We have to modify the ``dirs`` array in # place for this to affect os.walk dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] # We want patterns defined in a parent folder's .airflowignore to # apply to subdirs too for d in dirs : patterns_by_dir [ os . path . join ( root , d ) ] = patterns for f in files : try : file_path = os . path . join ( root , f ) if not os . path . isfile ( file_path ) : continue mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : continue if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : continue # Heuristic that guesses whether a Python file contains an # Airflow DAG definition. might_contain_dag = True if safe_mode and not zipfile . is_zipfile ( file_path ) : with open ( file_path , 'rb' ) as fp : content = fp . read ( ) might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) if not might_contain_dag : continue file_paths . append ( file_path ) except Exception : log = LoggingMixin ( ) . log log . exception ( \"Error while examining %s\" , f ) if include_examples : import airflow . example_dags example_dag_folder = airflow . example_dags . __path__ [ 0 ] file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) return file_paths", "nl": "Traverse a directory and look for Python files ."}}
{"translation": {"code": "def set_file_paths ( self , new_file_paths ) : self . _file_paths = new_file_paths self . _file_path_queue = [ x for x in self . _file_path_queue if x in new_file_paths ] # Stop processors that are working on deleted files filtered_processors = { } for file_path , processor in self . _processors . items ( ) : if file_path in new_file_paths : filtered_processors [ file_path ] = processor else : self . log . warning ( \"Stopping processor for %s\" , file_path ) processor . terminate ( ) self . _processors = filtered_processors", "nl": "Update this with a new set of paths to DAG definition files ."}}
{"translation": {"code": "def wait_until_finished ( self ) : for file_path , processor in self . _processors . items ( ) : while not processor . done : time . sleep ( 0.1 )", "nl": "Sleeps until all the processors are done ."}}
{"translation": {"code": "def heartbeat ( self ) : finished_processors = { } \"\"\":type : dict[unicode, AbstractDagFileProcessor]\"\"\" running_processors = { } \"\"\":type : dict[unicode, AbstractDagFileProcessor]\"\"\" for file_path , processor in self . _processors . items ( ) : if processor . done : self . log . debug ( \"Processor for %s finished\" , file_path ) now = timezone . utcnow ( ) finished_processors [ file_path ] = processor self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) self . _last_finish_time [ file_path ] = now self . _run_count [ file_path ] += 1 else : running_processors [ file_path ] = processor self . _processors = running_processors self . log . debug ( \"%s/%s DAG parsing processes running\" , len ( self . _processors ) , self . _parallelism ) self . log . debug ( \"%s file paths queued for processing\" , len ( self . _file_path_queue ) ) # Collect all the DAGs that were found in the processed files simple_dags = [ ] for file_path , processor in finished_processors . items ( ) : if processor . result is None : self . log . warning ( \"Processor for %s exited with return code %s.\" , processor . file_path , processor . exit_code ) else : for simple_dag in processor . result : simple_dags . append ( simple_dag ) # Generate more file paths to process if we processed all the files # already. if len ( self . _file_path_queue ) == 0 : # If the file path is already being processed, or if a file was # processed recently, wait until the next batch file_paths_in_progress = self . _processors . keys ( ) now = timezone . utcnow ( ) file_paths_recently_processed = [ ] for file_path in self . _file_paths : last_finish_time = self . get_last_finish_time ( file_path ) if ( last_finish_time is not None and ( now - last_finish_time ) . total_seconds ( ) < self . _file_process_interval ) : file_paths_recently_processed . append ( file_path ) files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) for file_path , processor in self . _processors . items ( ) : self . log . debug ( \"File path %s is still being processed (started: %s)\" , processor . file_path , processor . start_time . isoformat ( ) ) self . log . debug ( \"Queuing the following files for processing:\\n\\t%s\" , \"\\n\\t\" . join ( files_paths_to_queue ) ) self . _file_path_queue . extend ( files_paths_to_queue ) zombies = self . _find_zombies ( ) # Start more processors if we have enough slots and files to process while ( self . _parallelism - len ( self . _processors ) > 0 and len ( self . _file_path_queue ) > 0 ) : file_path = self . _file_path_queue . pop ( 0 ) processor = self . _processor_factory ( file_path , zombies ) processor . start ( ) self . log . debug ( \"Started a process (PID: %s) to generate tasks for %s\" , processor . pid , file_path ) self . _processors [ file_path ] = processor # Update heartbeat count. self . _run_count [ self . _heart_beat_key ] += 1 return simple_dags", "nl": "This should be periodically called by the manager loop . This method will kick off new processes to process DAG definition files and read the results from the finished processors ."}}
{"translation": {"code": "def process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) # As DAGs are parsed from this file, they will be converted into SimpleDags simple_dags = [ ] try : dagbag = models . DagBag ( file_path , include_examples = False ) except Exception : self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) return [ ] if len ( dagbag . dags ) > 0 : self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) else : self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) self . update_import_errors ( session , dagbag ) return [ ] # Save individual DAGs in the ORM and update DagModel.last_scheduled_time for dag in dagbag . dags . values ( ) : dag . sync_to_db ( ) paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] # Pickle the DAGs (if necessary) and put them into a SimpleDag for dag_id in dagbag . dags : # Only return DAGs that are not paused if dag_id not in paused_dag_ids : dag = dagbag . get_dag ( dag_id ) pickle_id = None if pickle_dags : pickle_id = dag . pickle ( session ) . id simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) if len ( self . dag_ids ) > 0 : dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] else : dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] # Not using multiprocessing.Queue() since it's no longer a separate # process and due to some unusual behavior. (empty() incorrectly # returns true?) ti_keys_to_schedule = [ ] self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) for ti_key in ti_keys_to_schedule : dag = dagbag . dags [ ti_key [ 0 ] ] task = dag . get_task ( ti_key [ 1 ] ) ti = models . TaskInstance ( task , ti_key [ 2 ] ) ti . refresh_from_db ( session = session , lock_for_update = True ) # We can defer checking the task dependency checks to the worker themselves # since they can be expensive to run in the scheduler. dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) # Only schedule tasks that have their dependencies met, e.g. to avoid # a task that recently got its state changed to RUNNING from somewhere # other than the scheduler from getting its state overwritten. # TODO(aoen): It's not great that we have to check all the task instance # dependencies twice; once to get the task scheduled, and again to actually # run the task. We should try to come up with a way to only check them once. if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : # Task starts out in the scheduled state. All tasks in the # scheduled state will be sent to the executor ti . state = State . SCHEDULED # Also save this task instance to the DB. self . log . info ( \"Creating / updating %s in ORM\" , ti ) session . merge ( ti ) # commit batch session . commit ( ) # Record import errors into the ORM try : self . update_import_errors ( session , dagbag ) except Exception : self . log . exception ( \"Error logging import errors!\" ) try : dagbag . kill_zombies ( zombies ) except Exception : self . log . exception ( \"Error killing zombies!\" ) return simple_dags", "nl": "Process a Python file containing Airflow DAGs ."}}
{"translation": {"code": "def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : \"\"\"\n        Sleeps until fn is true\n        \"\"\" t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . SIGTTIN ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while True : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) # Whenever some workers are not ready, wait until all workers are ready if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) # Kill a worker gracefully by asking gunicorn to reduce number of workers elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . SIGTTOU ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) # Start a new worker by asking gunicorn to increase number of workers elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : # num_ready_workers_running == num_workers_running < num_workers_expected log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) time . sleep ( 10 ) if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( AirflowWebServerTimeout , OSError ) as err : log . error ( err ) log . error ( \"Shutting down webserver\" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )", "nl": "Runs forever monitoring the child processes of"}}
{"translation": {"code": "def is_met ( self , ti , session , dep_context = None ) : return all ( status . passed for status in self . get_dep_statuses ( ti , session , dep_context ) )", "nl": "Returns whether or not this dependency is met for a given task instance . A dependency is considered met if all of the dependency statuses it reports are passing ."}}
{"translation": {"code": "def get_dep_statuses ( self , ti , session , dep_context = None ) : # this avoids a circular dependency from airflow . ti_deps . dep_context import DepContext if dep_context is None : dep_context = DepContext ( ) if self . IGNOREABLE and dep_context . ignore_all_deps : yield self . _passing_status ( reason = \"Context specified all dependencies should be ignored.\" ) return if self . IS_TASK_DEP and dep_context . ignore_task_deps : yield self . _passing_status ( reason = \"Context specified all task dependencies should be ignored.\" ) return for dep_status in self . _get_dep_statuses ( ti , session , dep_context ) : yield dep_status", "nl": "Wrapper around the private _get_dep_statuses method that contains some global checks for all dependencies ."}}
{"translation": {"code": "def get_failure_reasons ( self , ti , session , dep_context = None ) : for dep_status in self . get_dep_statuses ( ti , session , dep_context ) : if not dep_status . passed : yield dep_status . reason", "nl": "Returns an iterable of strings that explain why this dependency wasn t met ."}}
{"translation": {"code": "def run_table_delete ( self , deletion_dataset_table , ignore_if_missing = False ) : deletion_project , deletion_dataset , deletion_table = _split_tablename ( table_input = deletion_dataset_table , default_project_id = self . project_id ) try : self . service . tables ( ) . delete ( projectId = deletion_project , datasetId = deletion_dataset , tableId = deletion_table ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Deleted table %s:%s.%s.' , deletion_project , deletion_dataset , deletion_table ) except HttpError : if not ignore_if_missing : raise Exception ( 'Table deletion failed. Table does not exist.' ) else : self . log . info ( 'Table does not exist. Skipping.' )", "nl": "Delete an existing table from the dataset ; If the table does not exist return an error unless ignore_if_missing is set to True ."}}
{"translation": {"code": "def _integrate_plugins ( ) : from airflow . plugins_manager import executors_modules for executors_module in executors_modules : sys . modules [ executors_module . __name__ ] = executors_module globals ( ) [ executors_module . _name ] = executors_module", "nl": "Integrate plugins to the context ."}}
{"translation": {"code": "def exists ( self , bucket_name , object_name ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) return blob . exists ( )", "nl": "Checks for the existence of a file in Google Cloud Storage ."}}
{"translation": {"code": "def _serialize_cell ( cell , conn = None ) : if cell is None : return None if isinstance ( cell , datetime ) : return cell . isoformat ( ) return str ( cell )", "nl": "Returns the SQL literal of the cell as a string ."}}
{"translation": {"code": "def task_info ( dag_id , task_id ) : try : info = get_task ( dag_id , task_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response # JSONify and return. fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )", "nl": "Returns a JSON with a task s public instance variables ."}}
{"translation": {"code": "def send_metric ( self , metric_name , datapoint , tags = None , type_ = None , interval = None ) : response = api . Metric . send ( metric = metric_name , points = datapoint , host = self . host , tags = tags , type = type_ , interval = interval ) self . validate_response ( response ) return response", "nl": "Sends a single datapoint metric to DataDog"}}
{"translation": {"code": "def query_metric ( self , query , from_seconds_ago , to_seconds_ago ) : now = int ( time . time ( ) ) response = api . Metric . query ( start = now - from_seconds_ago , end = now - to_seconds_ago , query = query ) self . validate_response ( response ) return response", "nl": "Queries datadog for a specific metric potentially with some function applied to it and returns the results ."}}
{"translation": {"code": "def load_df ( self , df , table , field_dict = None , delimiter = ',' , encoding = 'utf8' , pandas_kwargs = None , * * kwargs ) : def _infer_field_types_from_df ( df ) : DTYPE_KIND_HIVE_TYPE = { 'b' : 'BOOLEAN' , # boolean 'i' : 'BIGINT' , # signed integer 'u' : 'BIGINT' , # unsigned integer 'f' : 'DOUBLE' , # floating-point 'c' : 'STRING' , # complex floating-point 'M' : 'TIMESTAMP' , # datetime 'O' : 'STRING' , # object 'S' : 'STRING' , # (byte-)string 'U' : 'STRING' , # Unicode 'V' : 'STRING' # void } d = OrderedDict ( ) for col , dtype in df . dtypes . iteritems ( ) : d [ col ] = DTYPE_KIND_HIVE_TYPE [ dtype . kind ] return d if pandas_kwargs is None : pandas_kwargs = { } with TemporaryDirectory ( prefix = 'airflow_hiveop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir , mode = \"w\" ) as f : if field_dict is None : field_dict = _infer_field_types_from_df ( df ) df . to_csv ( path_or_buf = f , sep = delimiter , header = False , index = False , encoding = encoding , date_format = \"%Y-%m-%d %H:%M:%S\" , * * pandas_kwargs ) f . flush ( ) return self . load_file ( filepath = f . name , table = table , delimiter = delimiter , field_dict = field_dict , * * kwargs )", "nl": "Loads a pandas DataFrame into hive ."}}
{"translation": {"code": "def _to_timestamp ( cls , column ) : # try and convert the column to datetimes # the column MUST have a four digit year somewhere in the string # there should be a better way to do this, # but just letting pandas try and convert every column without a format # caused it to convert floats as well # For example, a column of integers # between 0 and 10 are turned into timestamps # if the column cannot be converted, # just return the original column untouched try : column = pd . to_datetime ( column ) except ValueError : log = LoggingMixin ( ) . log log . warning ( \"Could not convert field to timestamps: %s\" , column . name ) return column # now convert the newly created datetimes into timestamps # we have to be careful here # because NaT cannot be converted to a timestamp # so we have to return NaN converted = [ ] for value in column : try : converted . append ( value . timestamp ( ) ) except ( ValueError , AttributeError ) : converted . append ( pd . np . NaN ) return pd . Series ( converted , index = column . index )", "nl": "Convert a column of a dataframe to UNIX timestamps if applicable"}}
{"translation": {"code": "def get_conn ( self ) : if not self . conn : connection = self . get_connection ( self . conn_id ) extras = connection . extra_dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security_token = extras [ 'security_token' ] , instance_url = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn", "nl": "Sign into Salesforce only if we are not already signed in ."}}
{"translation": {"code": "def make_query ( self , query ) : conn = self . get_conn ( ) self . log . info ( \"Querying for all objects\" ) query_results = conn . query_all ( query ) self . log . info ( \"Received results: Total size: %s; Done: %s\" , query_results [ 'totalSize' ] , query_results [ 'done' ] ) return query_results", "nl": "Make a query to Salesforce ."}}
{"translation": {"code": "def write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = False , record_time_added = False ) : fmt = fmt . lower ( ) if fmt not in [ 'csv' , 'json' , 'ndjson' ] : raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) # this line right here will convert all integers to floats # if there are any None/np.nan values in the column # that's because None/np.nan cannot exist in an integer column # we should write all of our timestamps as FLOATS in our final schema df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) df . columns = [ column . lower ( ) for column in df . columns ] # convert columns with datetime strings to datetimes # not all strings will be datetimes, so we ignore any errors that occur # we get the object's definition at this point and only consider # features that are DATE or DATETIME if coerce_to_timestamp and df . shape [ 0 ] > 0 : # get the object name out of the query results # it's stored in the \"attributes\" dictionary # for each returned record object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] self . log . info ( \"Coercing timestamps for: %s\" , object_name ) schema = self . describe_object ( object_name ) # possible columns that can be converted to timestamps # are the ones that are either date or datetime types # strings are too general and we risk unintentional conversion possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) if record_time_added : fetched_time = time . time ( ) df [ \"time_fetched_from_salesforce\" ] = fetched_time # write the CSV or JSON file depending on the option # NOTE: #   datetimes here are an issue. #   There is no good way to manage the difference #   for to_json, the options are an epoch or a ISO string #   but for to_csv, it will be a string output by datetime #   For JSON we decided to output the epoch timestamp in seconds #   (as is fairly standard for JavaScript) #   And for csv, we do a string if fmt == \"csv\" : # there are also a ton of newline objects that mess up our ability to write to csv # we remove these newlines so that the output is a valid CSV format self . log . info ( \"Cleaning data and writing to CSV\" ) possible_strings = df . columns [ df . dtypes == \"object\" ] df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) # write the dataframe df . to_csv ( filename , index = False ) elif fmt == \"json\" : df . to_json ( filename , \"records\" , date_unit = \"s\" ) elif fmt == \"ndjson\" : df . to_json ( filename , \"records\" , lines = True , date_unit = \"s\" ) return df", "nl": "Write query results to file ."}}
{"translation": {"code": "def get_available_fields ( self , obj ) : self . get_conn ( ) obj_description = self . describe_object ( obj ) return [ field [ 'name' ] for field in obj_description [ 'fields' ] ]", "nl": "Get a list of all available fields for an object ."}}
{"translation": {"code": "def get_object_from_salesforce ( self , obj , fields ) : query = \"SELECT {} FROM {}\" . format ( \",\" . join ( fields ) , obj ) self . log . info ( \"Making query to Salesforce: %s\" , query if len ( query ) < 30 else \" ... \" . join ( [ query [ : 15 ] , query [ - 15 : ] ] ) ) return self . make_query ( query )", "nl": "Get all instances of the object from Salesforce . For each model only get the fields specified in fields ."}}
{"translation": {"code": "def describe_object ( self , obj ) : conn = self . get_conn ( ) return conn . __getattr__ ( obj ) . describe ( )", "nl": "Get the description of an object from Salesforce . This description is the object s schema and some extra metadata that Salesforce stores for each object ."}}
{"translation": {"code": "def trigger_dag ( dag_id ) : data = request . get_json ( force = True ) run_id = None if 'run_id' in data : run_id = data [ 'run_id' ] conf = None if 'conf' in data : conf = data [ 'conf' ] execution_date = None if 'execution_date' in data and data [ 'execution_date' ] is not None : execution_date = data [ 'execution_date' ] # Convert string datetime into actual datetime try : execution_date = timezone . parse ( execution_date ) except ValueError : error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : dr = trigger . trigger_dag ( dag_id , run_id , conf , execution_date ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response if getattr ( g , 'user' , None ) : _log . info ( \"User %s created %s\" , g . user , dr ) response = jsonify ( message = \"Created {}\" . format ( dr ) ) return response", "nl": "Trigger a new dag run for a Dag with an execution date of now unless specified in the data ."}}
{"translation": {"code": "def has_task ( self , task_instance ) : if task_instance . key in self . queued_tasks or task_instance . key in self . running : return True", "nl": "Checks if a task is either queued or running in this executor"}}
{"translation": {"code": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = True ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) if self . be_empty : self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) return len ( result ) == 1 and result [ 0 ] [ 'path' ] == self . filepath else : self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) result . pop ( 0 ) return bool ( result ) and result [ 0 ] [ 'file_type' ] == 'f'", "nl": "poke for a non empty directory"}}
{"translation": {"code": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = False ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) return bool ( result )", "nl": "poke matching files in a directory with self . regex"}}
{"translation": {"code": "def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session . delete ( dr )", "nl": "Remove any existing DAG runs for the perf test DAGs ."}}
{"translation": {"code": "def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) # Get all the relevant task instances TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval. num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )", "nl": "Override the scheduler heartbeat to determine when the test is complete"}}
{"translation": {"code": "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )", "nl": "Print operational metrics for the scheduler test ."}}
{"translation": {"code": "def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )", "nl": "Toggle the pause state of the DAGs in the test ."}}
{"translation": {"code": "def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )", "nl": "Remove any existing task instances for the perf test DAGs ."}}
{"translation": {"code": "def is_updated_after ( self , bucket_name , object_name , ts ) : client = self . get_conn ( ) bucket = storage . Bucket ( client = client , name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_update_time = blob . updated if blob_update_time is not None : import dateutil . tz if not ts . tzinfo : ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) self . log . info ( \"Verify object date: %s > %s\" , blob_update_time , ts ) if blob_update_time > ts : return True return False", "nl": "Checks if an blob_name is updated in Google Cloud Storage ."}}
{"translation": {"code": "def scale_time_units ( time_seconds_arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time_seconds_arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time_seconds_arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time_seconds_arr ) ) return time_seconds_arr", "nl": "Convert an array of time durations in seconds to the specified time unit ."}}
{"translation": {"code": "def table_exists ( self , project_id , dataset_id , table_id ) : service = self . get_service ( ) try : service . tables ( ) . get ( projectId = project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) return True except HttpError as e : if e . resp [ 'status' ] == '404' : return False raise", "nl": "Checks for the existence of a table in Google BigQuery ."}}
{"translation": {"code": "def run_command ( self , run_with = None , join_args = False ) : run_with = run_with or [ ] cmd = [ \" \" . join ( self . _command ) ] if join_args else self . _command full_cmd = run_with + cmd self . log . info ( 'Running: %s' , full_cmd ) proc = subprocess . Popen ( full_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , close_fds = True , env = os . environ . copy ( ) , preexec_fn = os . setsid ) # Start daemon thread to read subprocess logging output log_reader = threading . Thread ( target = self . _read_task_logs , args = ( proc . stdout , ) , ) log_reader . daemon = True log_reader . start ( ) return proc", "nl": "Run the task command ."}}
{"translation": {"code": "def get_task_runner ( local_task_job ) : if _TASK_RUNNER == \"StandardTaskRunner\" : return StandardTaskRunner ( local_task_job ) elif _TASK_RUNNER == \"CgroupTaskRunner\" : from airflow . contrib . task_runner . cgroup_task_runner import CgroupTaskRunner return CgroupTaskRunner ( local_task_job ) else : raise AirflowException ( \"Unknown task runner type {}\" . format ( _TASK_RUNNER ) )", "nl": "Get the task runner that can be used to run the given job ."}}
{"translation": {"code": "def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except OSError : if not os . path . isdir ( path ) : raise finally : os . umask ( o_umask )", "nl": "Creates the directory specified by path creating intermediate directories as necessary . If directory already exists this is a no - op ."}}
{"translation": {"code": "def _create_cgroup ( self , path ) : node = trees . Tree ( ) . root path_split = path . split ( os . sep ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . debug ( \"Creating cgroup %s in %s\" , path_element , node . path ) node = node . create_cgroup ( path_element ) else : self . log . debug ( \"Not creating cgroup %s in %s since it already exists\" , path_element , node . path ) node = name_to_node [ path_element ] return node", "nl": "Create the specified cgroup ."}}
{"translation": {"code": "def on_finish ( self ) : if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : if self . run_as_user : subprocess . call ( [ 'sudo' , 'rm' , self . _cfg_path ] , close_fds = True ) else : os . remove ( self . _cfg_path )", "nl": "A callback that should be called when this is done running ."}}
{"translation": {"code": "def _delete_cgroup ( self , path ) : node = trees . Tree ( ) . root path_split = path . split ( \"/\" ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . warning ( \"Cgroup does not exist: %s\" , path ) return else : node = name_to_node [ path_element ] # node is now the leaf node parent = node . parent self . log . debug ( \"Deleting cgroup %s/%s\" , parent , node . name ) parent . delete_cgroup ( node . name )", "nl": "Delete the specified cgroup ."}}
{"translation": {"code": "def execute ( self , context ) : bucket_helper = GoogleCloudBucketHelper ( self . gcp_conn_id , self . delegate_to ) self . py_file = bucket_helper . google_cloud_to_local ( self . py_file ) hook = DataFlowHook ( gcp_conn_id = self . gcp_conn_id , delegate_to = self . delegate_to , poll_sleep = self . poll_sleep ) dataflow_options = self . dataflow_default_options . copy ( ) dataflow_options . update ( self . options ) # Convert argument names from lowerCamelCase to snake case. camel_to_snake = lambda name : re . sub ( r'[A-Z]' , lambda x : '_' + x . group ( 0 ) . lower ( ) , name ) formatted_options = { camel_to_snake ( key ) : dataflow_options [ key ] for key in dataflow_options } hook . start_python_dataflow ( self . job_name , formatted_options , self . py_file , self . py_options )", "nl": "Execute the python dataflow job ."}}
{"translation": {"code": "def uncompress_file ( input_file_name , file_extension , dest_dir ) : if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise NotImplementedError ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file_extension ) ) if file_extension . lower ( ) == '.gz' : fmodule = gzip . GzipFile elif file_extension . lower ( ) == '.bz2' : fmodule = bz2 . BZ2File with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = False ) as f_uncompressed : shutil . copyfileobj ( f_compressed , f_uncompressed ) return f_uncompressed . name", "nl": "Uncompress gz and bz2 files"}}
{"translation": {"code": "def days_ago ( n , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) : today = timezone . utcnow ( ) . replace ( hour = hour , minute = minute , second = second , microsecond = microsecond ) return today - timedelta ( days = n )", "nl": "Get a datetime object representing n days ago . By default the time is set to midnight ."}}
{"translation": {"code": "def _query_postgres ( self ) : postgres = PostgresHook ( postgres_conn_id = self . postgres_conn_id ) conn = postgres . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor", "nl": "Queries Postgres and returns a cursor to the results ."}}
{"translation": {"code": "def call ( self , path , query = None , get_all_pages = True , side_loading = False ) : zendesk = self . get_conn ( ) first_request_successful = False while not first_request_successful : try : results = zendesk . call ( path , query ) first_request_successful = True except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) # Find the key with the results keys = [ path . split ( \"/\" ) [ - 1 ] . split ( \".json\" ) [ 0 ] ] next_page = results [ 'next_page' ] if side_loading : keys += query [ 'include' ] . split ( ',' ) results = { key : results [ key ] for key in keys } if get_all_pages : while next_page is not None : try : # Need to split because the next page URL has # `github.zendesk...` # in it, but the call function needs it removed. next_url = next_page . split ( self . __url ) [ 1 ] self . log . info ( \"Calling %s\" , next_url ) more_res = zendesk . call ( next_url ) for key in results : results [ key ] . extend ( more_res [ key ] ) if next_page == more_res [ 'next_page' ] : # Unfortunately zdesk doesn't always throw ZendeskError # when we are done getting all the data. Sometimes the # next just refers to the current set of results. # Hence, need to deal with this special case break else : next_page = more_res [ 'next_page' ] except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) except ZendeskError as ze : if b\"Use a start_time older than 5 minutes\" in ze . msg : # We have pretty up to date data break else : raise ze return results", "nl": "Call Zendesk API and return results"}}
{"translation": {"code": "def __handle_rate_limit_exception ( self , rate_limit_exception ) : retry_after = int ( rate_limit_exception . response . headers . get ( 'Retry-After' , 60 ) ) self . log . info ( \"Hit Zendesk API rate limit. Pausing for %s seconds\" , retry_after ) time . sleep ( retry_after )", "nl": "Sleep for the time specified in the exception . If not specified wait for 60 seconds ."}}
{"translation": {"code": "def _process_spark_submit_log ( self , itr ) : # Consume the iterator for line in itr : line = line . strip ( ) # If we run yarn cluster mode, we want to extract the application id from # the logs so we can kill the application when we stop it unexpectedly if self . _is_yarn and self . _connection [ 'deploy_mode' ] == 'cluster' : match = re . search ( '(application[0-9_]+)' , line ) if match : self . _yarn_application_id = match . groups ( ) [ 0 ] self . log . info ( \"Identified spark driver id: %s\" , self . _yarn_application_id ) # If we run Kubernetes cluster mode, we want to extract the driver pod id # from the logs so we can kill the application when we stop it unexpectedly elif self . _is_kubernetes : match = re . search ( r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)' , line ) if match : self . _kubernetes_driver_pod = match . groups ( ) [ 0 ] self . log . info ( \"Identified spark driver pod: %s\" , self . _kubernetes_driver_pod ) # Store the Spark Exit code match_exit_code = re . search ( r'\\s*Exit code: (\\d+)' , line ) if match_exit_code : self . _spark_exit_code = int ( match_exit_code . groups ( ) [ 0 ] ) # if we run in standalone cluster mode and we want to track the driver status # we need to extract the driver id from the logs. This allows us to poll for # the status using the driver id. Also, we can kill the driver when needed. elif self . _should_track_driver_status and not self . _driver_id : match_driver_id = re . search ( r'(driver-[0-9\\-]+)' , line ) if match_driver_id : self . _driver_id = match_driver_id . groups ( ) [ 0 ] self . log . info ( \"identified spark driver id: {}\" . format ( self . _driver_id ) ) else : self . log . info ( line ) self . log . debug ( \"spark submit log: {}\" . format ( line ) )", "nl": "Processes the log files and extracts useful information out of it ."}}
{"translation": {"code": "def _build_track_driver_status_command ( self ) : connection_cmd = self . _get_spark_binary_path ( ) # The url ot the spark master connection_cmd += [ \"--master\" , self . _connection [ 'master' ] ] # The driver id so we can poll for its status if self . _driver_id : connection_cmd += [ \"--status\" , self . _driver_id ] else : raise AirflowException ( \"Invalid status: attempted to poll driver \" + \"status but no driver id is known. Giving up.\" ) self . log . debug ( \"Poll driver status cmd: %s\" , connection_cmd ) return connection_cmd", "nl": "Construct the command to poll the driver status ."}}
{"translation": {"code": "def submit ( self , application = \"\" , * * kwargs ) : spark_submit_cmd = self . _build_spark_submit_command ( application ) if hasattr ( self , '_env' ) : env = os . environ . copy ( ) env . update ( self . _env ) kwargs [ \"env\" ] = env self . _submit_sp = subprocess . Popen ( spark_submit_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - 1 , universal_newlines = True , * * kwargs ) self . _process_spark_submit_log ( iter ( self . _submit_sp . stdout . readline , '' ) ) returncode = self . _submit_sp . wait ( ) # Check spark-submit return code. In Kubernetes mode, also check the value # of exit code in the log, as it may differ. if returncode or ( self . _is_kubernetes and self . _spark_exit_code != 0 ) : raise AirflowException ( \"Cannot execute: {}. Error code is: {}.\" . format ( spark_submit_cmd , returncode ) ) self . log . debug ( \"Should track driver: {}\" . format ( self . _should_track_driver_status ) ) # We want the Airflow job to wait until the Spark driver is finished if self . _should_track_driver_status : if self . _driver_id is None : raise AirflowException ( \"No driver id is known: something went wrong when executing \" + \"the spark submit command\" ) # We start with the SUBMITTED status as initial status self . _driver_status = \"SUBMITTED\" # Start tracking the driver status (blocking function) self . _start_driver_status_tracking ( ) if self . _driver_status != \"FINISHED\" : raise AirflowException ( \"ERROR : Driver {} badly exited with status {}\" . format ( self . _driver_id , self . _driver_status ) )", "nl": "Remote Popen to execute the spark - submit job"}}
{"translation": {"code": "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )", "nl": "Parses some DatabaseError to provide a better error message"}}
{"translation": {"code": "def _update_counters ( self , ti_status ) : for key , ti in list ( ti_status . running . items ( ) ) : ti . refresh_from_db ( ) if ti . state == State . SUCCESS : ti_status . succeeded . add ( key ) self . log . debug ( \"Task instance %s succeeded. Don't rerun.\" , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . SKIPPED : ti_status . skipped . add ( key ) self . log . debug ( \"Task instance %s skipped. Don't rerun.\" , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . FAILED : self . log . error ( \"Task instance %s failed\" , ti ) ti_status . failed . add ( key ) ti_status . running . pop ( key ) continue # special case: if the task needs to run again put it back elif ti . state == State . UP_FOR_RETRY : self . log . warning ( \"Task instance %s is up for retry\" , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti # special case: if the task needs to be rescheduled put it back elif ti . state == State . UP_FOR_RESCHEDULE : self . log . warning ( \"Task instance %s is up for reschedule\" , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti # special case: The state of the task can be set to NONE by the task itself # when it reaches concurrency limits. It could also happen when the state # is changed externally, e.g. by clearing tasks from the ui. We need to cover # for that as otherwise those tasks would fall outside of the scope of # the backfill suddenly. elif ti . state == State . NONE : self . log . warning ( \"FIXME: task instance %s state was set to none externally or \" \"reaching concurrency limits. Re-adding task to queue.\" , ti ) ti . set_state ( State . SCHEDULED ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti", "nl": "Updates the counters per state of the tasks that were running . Can re - add to tasks to run in case required ."}}
{"translation": {"code": "def _manage_executor_state ( self , running ) : executor = self . executor for key , state in list ( executor . get_event_buffer ( ) . items ( ) ) : if key not in running : self . log . warning ( \"%s state %s not in running=%s\" , key , state , running . values ( ) ) continue ti = running [ key ] ti . refresh_from_db ( ) self . log . debug ( \"Executor state: %s task %s\" , state , ti ) if state == State . FAILED or state == State . SUCCESS : if ti . state == State . RUNNING or ti . state == State . QUEUED : msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) self . log . error ( msg ) ti . handle_failure ( msg )", "nl": "Checks if the executor agrees with the state of task instances that are running"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . redis_conn_id ) self . host = conn . host self . port = conn . port self . password = None if str ( conn . password ) . lower ( ) in [ 'none' , 'false' , '' ] else conn . password self . db = conn . extra_dejson . get ( 'db' , None ) if not self . redis : self . log . debug ( 'Initializing redis object for conn_id \"%s\" on %s:%s:%s' , self . redis_conn_id , self . host , self . port , self . db ) self . redis = Redis ( host = self . host , port = self . port , password = self . password , db = self . db ) return self . redis", "nl": "Returns a Redis connection ."}}
{"translation": {"code": "def export_table ( self , table , export_dir , input_null_string , input_null_non_string , staging_table , clear_staging_table , enclosed_by , escaped_by , input_fields_terminated_by , input_lines_terminated_by , input_optionally_enclosed_by , batch , relaxed_isolation , extra_export_options = None ) : cmd = self . _export_cmd ( table , export_dir , input_null_string , input_null_non_string , staging_table , clear_staging_table , enclosed_by , escaped_by , input_fields_terminated_by , input_lines_terminated_by , input_optionally_enclosed_by , batch , relaxed_isolation , extra_export_options ) self . Popen ( cmd )", "nl": "Exports Hive table to remote location . Arguments are copies of direct sqoop command line Arguments"}}
{"translation": {"code": "def check_for_prefix ( self , container_name , prefix , * * kwargs ) : matches = self . connection . list_blobs ( container_name , prefix , num_results = 1 , * * kwargs ) return len ( list ( matches ) ) > 0", "nl": "Check if a prefix exists on Azure Blob storage ."}}
{"translation": {"code": "def check_for_blob ( self , container_name , blob_name , * * kwargs ) : return self . connection . exists ( container_name , blob_name , * * kwargs )", "nl": "Check if a blob exists on Azure Blob Storage ."}}
{"translation": {"code": "def _do_api_call ( self , endpoint_info , json ) : method , endpoint = endpoint_info url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) if 'token' in self . databricks_conn . extra_dejson : self . log . info ( 'Using token auth.' ) auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) else : self . log . info ( 'Using basic auth.' ) auth = ( self . databricks_conn . login , self . databricks_conn . password ) if method == 'GET' : request_func = requests . get elif method == 'POST' : request_func = requests . post else : raise AirflowException ( 'Unexpected HTTP Method: ' + method ) attempt_num = 1 while True : try : response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) response . raise_for_status ( ) return response . json ( ) except requests_exceptions . RequestException as e : if not _retryable_error ( e ) : # In this case, the user probably made a mistake. # Don't retry. raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) self . _log_request_error ( attempt_num , e ) if attempt_num == self . retry_limit : raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) attempt_num += 1 sleep ( self . retry_delay )", "nl": "Utility function to perform an API call with retries"}}
{"translation": {"code": "def _parse_host ( host ) : urlparse_host = urlparse . urlparse ( host ) . hostname if urlparse_host : # In this case, host = https://xx.cloud.databricks.com return urlparse_host else : # In this case, host = xx.cloud.databricks.com return host", "nl": "The purpose of this function is to be robust to improper connections settings provided by users specifically in the host field ."}}
{"translation": {"code": "def list ( self , bucket_name , versions = None , max_results = None , prefix = None , delimiter = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) ids = [ ] pageToken = None while True : blobs = bucket . list_blobs ( max_results = max_results , page_token = pageToken , prefix = prefix , delimiter = delimiter , versions = versions ) blob_names = [ ] for blob in blobs : blob_names . append ( blob . name ) prefixes = blobs . prefixes if prefixes : ids += list ( prefixes ) else : ids += blob_names pageToken = blobs . next_page_token if pageToken is None : # empty next page token break return ids", "nl": "List all objects from the bucket with the give string prefix in name"}}
{"translation": {"code": "def delete ( self , bucket_name , object_name ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . delete ( ) self . log . info ( 'Blob %s deleted.' , object_name )", "nl": "Deletes an object from the bucket ."}}
{"translation": {"code": "def get_default_executor ( ) : global DEFAULT_EXECUTOR if DEFAULT_EXECUTOR is not None : return DEFAULT_EXECUTOR executor_name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT_EXECUTOR = _get_executor ( executor_name ) log = LoggingMixin ( ) . log log . info ( \"Using executor %s\" , executor_name ) return DEFAULT_EXECUTOR", "nl": "Creates a new instance of the configured executor if none exists and returns it"}}
{"translation": {"code": "def get_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( \"Pool '%s' doesn't exist\" % name ) return pool", "nl": "Get pool by a given name ."}}
{"translation": {"code": "def create_pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) try : slots = int ( slots ) except ValueError : raise AirflowBadRequest ( \"Bad value for `slots`: %s\" % slots ) session . expire_on_commit = False pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool", "nl": "Create a pool with a given parameters ."}}
{"translation": {"code": "def create_pool ( ) : params = request . get_json ( force = True ) try : pool = pool_api . create_pool ( * * params ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )", "nl": "Create a pool ."}}
{"translation": {"code": "def delete_pool ( name ) : try : pool = pool_api . delete_pool ( name = name ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )", "nl": "Delete pool ."}}
{"translation": {"code": "def get_pools ( ) : try : pools = pool_api . get_pools ( ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( [ p . to_json ( ) for p in pools ] )", "nl": "Get all pools ."}}
{"translation": {"code": "def delete_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( \"Pool '%s' doesn't exist\" % name ) session . delete ( pool ) session . commit ( ) return pool", "nl": "Delete pool by a given name ."}}
{"translation": {"code": "def copy ( self , source_bucket , source_object , destination_bucket = None , destination_object = None ) : destination_bucket = destination_bucket or source_bucket destination_object = destination_object or source_object if source_bucket == destination_bucket and source_object == destination_object : raise ValueError ( 'Either source/destination bucket or source/destination object ' 'must be different, not both the same: bucket=%s, object=%s' % ( source_bucket , source_object ) ) if not source_bucket or not source_object : raise ValueError ( 'source_bucket and source_object cannot be empty.' ) client = self . get_conn ( ) source_bucket = client . get_bucket ( source_bucket ) source_object = source_bucket . blob ( source_object ) destination_bucket = client . get_bucket ( destination_bucket ) destination_object = source_bucket . copy_blob ( blob = source_object , destination_bucket = destination_bucket , new_name = destination_object ) self . log . info ( 'Object %s in bucket %s copied to object %s in bucket %s' , source_object . name , source_bucket . name , destination_object . name , destination_bucket . name )", "nl": "Copies an object from a bucket to another with renaming if requested ."}}
{"translation": {"code": "def get_conn ( self ) : authed_http = self . _authorize ( ) return build ( 'ml' , 'v1' , http = authed_http , cache_discovery = False )", "nl": "Returns a Google MLEngine service object ."}}
{"translation": {"code": "def create_version ( self , project_id , model_name , version_spec ) : parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) create_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . create ( parent = parent_name , body = version_spec ) response = create_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )", "nl": "Creates the Version on Google Cloud ML Engine ."}}
{"translation": {"code": "def set_default_version ( self , project_id , model_name , version_name ) : full_version_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . setDefault ( name = full_version_name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except HttpError as e : self . log . error ( 'Something went wrong: %s' , e ) raise", "nl": "Sets a version to be the default . Blocks until finished ."}}
{"translation": {"code": "def list_versions ( self , project_id , model_name ) : result = [ ] full_parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageSize = 100 ) response = request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next_page_token is not None : next_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageToken = next_page_token , pageSize = 100 ) response = next_request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result", "nl": "Lists all available versions of a model . Blocks until finished ."}}
{"translation": {"code": "def delete_version ( self , project_id , model_name , version_name ) : full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) response = delete_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )", "nl": "Deletes the given version of a model . Blocks until finished ."}}
{"translation": {"code": "def create_model ( self , project_id , model ) : if not model [ 'name' ] : raise ValueError ( \"Model name must be provided and \" \"could not be an empty string\" ) project = 'projects/{}' . format ( project_id ) request = self . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )", "nl": "Create a Model . Blocks until finished ."}}
{"translation": {"code": "def get_model ( self , project_id , model_name ) : if not model_name : raise ValueError ( \"Model name must be provided and \" \"it could not be an empty string\" ) full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name ) try : return request . execute ( ) except HttpError as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise", "nl": "Gets a Model . Blocks until finished ."}}
{"translation": {"code": "def _normalize_mlengine_job_id ( job_id ) : # Add a prefix when a job_id starts with a digit or a template match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id # Clean up 'bad' characters except templates tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) # Clean up last substring or the full string if no templates cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id", "nl": "Replaces invalid MLEngine job_id characters with _ ."}}
{"translation": {"code": "def _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : if interval <= 0 : raise ValueError ( \"Interval must be > 0\" ) while True : job = self . _get_job ( project_id , job_id ) if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : return job time . sleep ( interval )", "nl": "Waits for the Job to reach a terminal state ."}}
{"translation": {"code": "def _get_job ( self , project_id , job_id ) : job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) while True : try : return request . execute ( ) except HttpError as e : if e . resp . status == 429 : # polling after 30 seconds when quota failure occurs time . sleep ( 30 ) else : self . log . error ( 'Failed to get MLEngine job: {}' . format ( e ) ) raise", "nl": "Gets a MLEngine job based on the job name ."}}
{"translation": {"code": "def create_job ( self , project_id , job , use_existing_job_fn = None ) : request = self . _mlengine . projects ( ) . jobs ( ) . create ( parent = 'projects/{}' . format ( project_id ) , body = job ) job_id = job [ 'jobId' ] try : request . execute ( ) except HttpError as e : # 409 means there is an existing job with the same job ID. if e . resp . status == 409 : if use_existing_job_fn is not None : existing_job = self . _get_job ( project_id , job_id ) if not use_existing_job_fn ( existing_job ) : self . log . error ( 'Job with job_id %s already exist, but it does ' 'not match our expectation: %s' , job_id , existing_job ) raise self . log . info ( 'Job with job_id %s already exist. Will waiting for it to finish' , job_id ) else : self . log . error ( 'Failed to create MLEngine job: {}' . format ( e ) ) raise return self . _wait_for_job_done ( project_id , job_id )", "nl": "Launches a MLEngine job and wait for it to reach a terminal state ."}}
{"translation": {"code": "def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : # Verify that task_prefix doesn't have any special characters except hyphen # '-', which is the only allowed non-alphanumeric character by Dataflow. if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise AirflowException ( \"`metric_fn` param must be callable.\" ) if not callable ( validate_fn ) : raise AirflowException ( \"`validate_fn` param must be callable.\" ) if dag is not None and dag . default_args is not None : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , * * kwargs ) : prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != \"gs\" or not bucket or not obj : raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) gcs_hook = GoogleCloudStorageHook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation", "nl": "Creates Operators needed for model evaluation and returns ."}}
{"translation": {"code": "def _execute_task_instances ( self , simple_dag_bag , states , session = None ) : executable_tis = self . _find_executable_task_instances ( simple_dag_bag , states , session = session ) def query ( result , items ) : simple_tis_with_state_changed = self . _change_state_for_executable_task_instances ( items , states , session = session ) self . _enqueue_task_instances_with_queued_state ( simple_dag_bag , simple_tis_with_state_changed ) session . commit ( ) return result + len ( simple_tis_with_state_changed ) return helpers . reduce_in_chunks ( query , executable_tis , 0 , self . max_tis_per_query )", "nl": "Attempts to execute TaskInstances that should be executed by the scheduler ."}}
{"translation": {"code": "def _enqueue_task_instances_with_queued_state ( self , simple_dag_bag , simple_task_instances ) : TI = models . TaskInstance # actually enqueue them for simple_task_instance in simple_task_instances : simple_dag = simple_dag_bag . get_dag ( simple_task_instance . dag_id ) command = TI . generate_command ( simple_task_instance . dag_id , simple_task_instance . task_id , simple_task_instance . execution_date , local = True , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , pool = simple_task_instance . pool , file_path = simple_dag . full_filepath , pickle_id = simple_dag . pickle_id ) priority = simple_task_instance . priority_weight queue = simple_task_instance . queue self . log . info ( \"Sending %s to executor with priority %s and queue %s\" , simple_task_instance . key , priority , queue ) self . executor . queue_command ( simple_task_instance , command , priority = priority , queue = queue )", "nl": "Takes task_instances which should have been set to queued and enqueues them with the executor ."}}
{"translation": {"code": "def _execute_for_run_dates ( self , run_dates , ti_status , executor , pickle_id , start_date , session = None ) : for next_run_date in run_dates : dag_run = self . _get_dag_run ( next_run_date , session = session ) tis_map = self . _task_instances_for_dag_run ( dag_run , session = session ) if dag_run is None : continue ti_status . active_runs . append ( dag_run ) ti_status . to_run . update ( tis_map or { } ) processed_dag_run_dates = self . _process_backfill_task_instances ( ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) ti_status . executed_dag_run_dates . update ( processed_dag_run_dates )", "nl": "Computes the dag runs and their respective task instances for the given run dates and executes the task instances . Returns a list of execution dates of the dag runs that were executed ."}}
{"translation": {"code": "def close ( self ) : # When application exit, system shuts down all handlers by # calling close method. Here we check if logger is already # closed to prevent uploading the log to remote storage multiple # times when `logging.shutdown` is called. if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : # read log and remove old logs to get just the latest additions with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . s3_write ( log , remote_loc ) # Mark closed so we don't double write if close is called twice self . closed = True", "nl": "Close and upload local log file to remote storage S3 ."}}
{"translation": {"code": "def construct_ingest_query ( self , static_path , columns ) : # backward compatibility for num_shards, # but target_partition_size is the default setting # and overwrites the num_shards num_shards = self . num_shards target_partition_size = self . target_partition_size if self . target_partition_size == - 1 : if self . num_shards == - 1 : target_partition_size = DEFAULT_TARGET_PARTITION_SIZE else : num_shards = - 1 metric_names = [ m [ 'fieldName' ] for m in self . metric_spec if m [ 'type' ] != 'count' ] # Take all the columns, which are not the time dimension # or a metric, as the dimension columns dimensions = [ c for c in columns if c not in metric_names and c != self . ts_dim ] ingest_query_dict = { \"type\" : \"index_hadoop\" , \"spec\" : { \"dataSchema\" : { \"metricsSpec\" : self . metric_spec , \"granularitySpec\" : { \"queryGranularity\" : self . query_granularity , \"intervals\" : self . intervals , \"type\" : \"uniform\" , \"segmentGranularity\" : self . segment_granularity , } , \"parser\" : { \"type\" : \"string\" , \"parseSpec\" : { \"columns\" : columns , \"dimensionsSpec\" : { \"dimensionExclusions\" : [ ] , \"dimensions\" : dimensions , # list of names \"spatialDimensions\" : [ ] } , \"timestampSpec\" : { \"column\" : self . ts_dim , \"format\" : \"auto\" } , \"format\" : \"tsv\" } } , \"dataSource\" : self . druid_datasource } , \"tuningConfig\" : { \"type\" : \"hadoop\" , \"jobProperties\" : { \"mapreduce.job.user.classpath.first\" : \"false\" , \"mapreduce.map.output.compress\" : \"false\" , \"mapreduce.output.fileoutputformat.compress\" : \"false\" , } , \"partitionsSpec\" : { \"type\" : \"hashed\" , \"targetPartitionSize\" : target_partition_size , \"numShards\" : num_shards , } , } , \"ioConfig\" : { \"inputSpec\" : { \"paths\" : static_path , \"type\" : \"static\" } , \"type\" : \"hadoop\" } } } if self . job_properties : ingest_query_dict [ 'spec' ] [ 'tuningConfig' ] [ 'jobProperties' ] . update ( self . job_properties ) if self . hadoop_dependency_coordinates : ingest_query_dict [ 'hadoopDependencyCoordinates' ] = self . hadoop_dependency_coordinates return ingest_query_dict", "nl": "Builds an ingest query for an HDFS TSV load ."}}
{"translation": {"code": "def export_to_storage_bucket ( self , bucket , namespace = None , entity_filter = None , labels = None ) : admin_conn = self . get_conn ( ) output_uri_prefix = 'gs://' + '/' . join ( filter ( None , [ bucket , namespace ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'outputUrlPrefix' : output_uri_prefix , 'entityFilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . export ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp", "nl": "Export entities from Cloud Datastore to Cloud Storage for backup ."}}
{"translation": {"code": "def get_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . get ( name = name ) . execute ( num_retries = self . num_retries ) ) return resp", "nl": "Gets the latest state of a long - running operation ."}}
{"translation": {"code": "def delete_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) ) return resp", "nl": "Deletes the long - running operation ."}}
{"translation": {"code": "def poll_operation_until_done ( self , name , polling_interval_in_seconds ) : while True : result = self . get_operation ( name ) state = result [ 'metadata' ] [ 'common' ] [ 'state' ] if state == 'PROCESSING' : self . log . info ( 'Operation is processing. Re-polling state in {} seconds' . format ( polling_interval_in_seconds ) ) time . sleep ( polling_interval_in_seconds ) else : return result", "nl": "Poll backup operation state until it s completed ."}}
{"translation": {"code": "def import_from_storage_bucket ( self , bucket , file , namespace = None , entity_filter = None , labels = None ) : admin_conn = self . get_conn ( ) input_url = 'gs://' + '/' . join ( filter ( None , [ bucket , namespace , file ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'inputUrl' : input_url , 'entityFilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . import_ ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp", "nl": "Import a backup from Cloud Storage to Cloud Datastore ."}}
{"translation": {"code": "def load_string ( self , string_data , container_name , blob_name , * * kwargs ) : # Reorder the argument order from airflow.hooks.S3_hook.load_string. self . connection . create_blob_from_text ( container_name , blob_name , string_data , * * kwargs )", "nl": "Upload a string to Azure Blob Storage ."}}
{"translation": {"code": "def write_batch_data ( self , items ) : dynamodb_conn = self . get_conn ( ) try : table = dynamodb_conn . Table ( self . table_name ) with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : for item in items : batch . put_item ( Item = item ) return True except Exception as general_error : raise AirflowException ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) )", "nl": "Write batch items to dynamodb table with provisioned throughout capacity ."}}
{"translation": {"code": "def __get_concurrency_maps ( self , states , session = None ) : TI = models . TaskInstance ti_concurrency_query = ( session . query ( TI . task_id , TI . dag_id , func . count ( '*' ) ) . filter ( TI . state . in_ ( states ) ) . group_by ( TI . task_id , TI . dag_id ) ) . all ( ) dag_map = defaultdict ( int ) task_map = defaultdict ( int ) for result in ti_concurrency_query : task_id , dag_id , count = result dag_map [ dag_id ] += count task_map [ ( dag_id , task_id ) ] = count return dag_map , task_map", "nl": "Get the concurrency maps ."}}
{"translation": {"code": "def cancel_query ( self ) : jobs = self . service . jobs ( ) if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) if self . location : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) else : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'No running BigQuery jobs to cancel.' ) return # Wait for all the calls to cancel to finish max_polling_attempts = 12 polling_attempts = 0 job_complete = False while polling_attempts < max_polling_attempts and not job_complete : polling_attempts = polling_attempts + 1 job_complete = self . poll_job_complete ( self . running_job_id ) if job_complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) elif polling_attempts == max_polling_attempts : self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) time . sleep ( 5 )", "nl": "Cancel all started queries that have not yet completed"}}
{"translation": {"code": "def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur . copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )", "nl": "Executes SQL using psycopg2 copy_expert method . Necessary to execute COPY command without access to a superuser ."}}
{"translation": {"code": "def read_key ( self , key , bucket_name = None ) : obj = self . get_key ( key , bucket_name ) return obj . get ( ) [ 'Body' ] . read ( ) . decode ( 'utf-8' )", "nl": "Reads a key from S3"}}
{"translation": {"code": "def _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : config = configparser . ConfigParser ( ) if config . read ( config_file_name ) : # pragma: no cover sections = config . sections ( ) else : raise AirflowException ( \"Couldn't read {0}\" . format ( config_file_name ) ) # Setting option names depending on file format if config_format is None : config_format = 'boto' conf_format = config_format . lower ( ) if conf_format == 'boto' : # pragma: no cover if profile is not None and 'profile ' + profile in sections : cred_section = 'profile ' + profile else : cred_section = 'Credentials' elif conf_format == 'aws' and profile is not None : cred_section = profile else : cred_section = 'default' # Option names if conf_format in ( 'boto' , 'aws' ) : # pragma: no cover key_id_option = 'aws_access_key_id' secret_key_option = 'aws_secret_access_key' # security_token_option = 'aws_security_token' else : key_id_option = 'access_key' secret_key_option = 'secret_key' # Actual Parsing if cred_section not in sections : raise AirflowException ( \"This config file format is not recognized\" ) else : try : access_key = config . get ( cred_section , key_id_option ) secret_key = config . get ( cred_section , secret_key_option ) except Exception : logging . warning ( \"Option Error in parsing s3 config file\" ) raise return access_key , secret_key", "nl": "Parses a config file for s3 credentials . Can currently parse boto s3cmd . conf and AWS SDK config formats"}}
{"translation": {"code": "def _get_environment ( self ) : env = { } for env_var_name , env_var_val in six . iteritems ( self . kube_config . kube_env_vars ) : env [ env_var_name ] = env_var_val env [ \"AIRFLOW__CORE__EXECUTOR\" ] = \"LocalExecutor\" if self . kube_config . airflow_configmap : env [ 'AIRFLOW_HOME' ] = self . worker_airflow_home env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = self . worker_airflow_dags if ( not self . kube_config . airflow_configmap and 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self . kube_config . kube_secrets ) : env [ 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' ] = conf . get ( \"core\" , \"SQL_ALCHEMY_CONN\" ) if self . kube_config . git_dags_folder_mount_point : # /root/airflow/dags/repo/dags dag_volume_mount_path = os . path . join ( self . kube_config . git_dags_folder_mount_point , self . kube_config . git_sync_dest , # repo self . kube_config . git_subpath # dags ) env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = dag_volume_mount_path return env", "nl": "Defines any necessary environment variables for the pod executor"}}
{"translation": {"code": "def _get_init_containers ( self ) : # If we're using volume claims to mount the dags, no init container is needed if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : return [ ] # Otherwise, define a git-sync init container init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] if self . kube_config . git_user : init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) if self . kube_config . git_password : init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : False } ] if self . kube_config . git_ssh_key_secret_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) if self . kube_config . git_ssh_known_hosts_configmap_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) else : init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'false' } ) return [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533 } , # git-sync user 'env' : init_environment , 'volumeMounts' : volume_mounts } ]", "nl": "When using git to retrieve the DAGs use the GitSync Init Container"}}
{"translation": {"code": "def _get_secrets ( self ) : worker_secrets = [ ] for env_var_name , obj_key_pair in six . iteritems ( self . kube_config . kube_secrets ) : k8s_secret_obj , k8s_secret_key = obj_key_pair . split ( '=' ) worker_secrets . append ( Secret ( 'env' , env_var_name , k8s_secret_obj , k8s_secret_key ) ) if self . kube_config . env_from_secret_ref : for secret_ref in self . kube_config . env_from_secret_ref . split ( ',' ) : worker_secrets . append ( Secret ( 'env' , None , secret_ref ) ) return worker_secrets", "nl": "Defines any necessary secrets for the pod executor"}}
{"translation": {"code": "def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , sandbox_mode = False , * * kwargs ) : if files is None : files = [ ] mail = Mail ( ) from_email = kwargs . get ( 'from_email' ) or os . environ . get ( 'SENDGRID_MAIL_FROM' ) from_name = kwargs . get ( 'from_name' ) or os . environ . get ( 'SENDGRID_MAIL_SENDER' ) mail . from_email = Email ( from_email , from_name ) mail . subject = subject mail . mail_settings = MailSettings ( ) if sandbox_mode : mail . mail_settings . sandbox_mode = SandBoxMode ( enable = True ) # Add the recipient list of to emails. personalization = Personalization ( ) to = get_email_address_list ( to ) for to_address in to : personalization . add_to ( Email ( to_address ) ) if cc : cc = get_email_address_list ( cc ) for cc_address in cc : personalization . add_cc ( Email ( cc_address ) ) if bcc : bcc = get_email_address_list ( bcc ) for bcc_address in bcc : personalization . add_bcc ( Email ( bcc_address ) ) # Add custom_args to personalization if present pers_custom_args = kwargs . get ( 'personalization_custom_args' , None ) if isinstance ( pers_custom_args , dict ) : for key in pers_custom_args . keys ( ) : personalization . add_custom_arg ( CustomArg ( key , pers_custom_args [ key ] ) ) mail . add_personalization ( personalization ) mail . add_content ( Content ( 'text/html' , html_content ) ) categories = kwargs . get ( 'categories' , [ ] ) for cat in categories : mail . add_category ( Category ( cat ) ) # Add email attachment. for fname in files : basename = os . path . basename ( fname ) attachment = Attachment ( ) attachment . type = mimetypes . guess_type ( basename ) [ 0 ] attachment . filename = basename attachment . disposition = \"attachment\" attachment . content_id = '<{0}>' . format ( basename ) with open ( fname , \"rb\" ) as f : attachment . content = base64 . b64encode ( f . read ( ) ) . decode ( 'utf-8' ) mail . add_attachment ( attachment ) _post_sendgrid_mail ( mail . get ( ) )", "nl": "Send an email with html content using sendgrid ."}}
{"translation": {"code": "def create_cluster_snapshot ( self , snapshot_identifier , cluster_identifier ) : response = self . get_conn ( ) . create_cluster_snapshot ( SnapshotIdentifier = snapshot_identifier , ClusterIdentifier = cluster_identifier , ) return response [ 'Snapshot' ] if response [ 'Snapshot' ] else None", "nl": "Creates a snapshot of a cluster"}}
{"translation": {"code": "def restore_from_cluster_snapshot ( self , cluster_identifier , snapshot_identifier ) : response = self . get_conn ( ) . restore_from_cluster_snapshot ( ClusterIdentifier = cluster_identifier , SnapshotIdentifier = snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None", "nl": "Restores a cluster from its snapshot"}}
{"translation": {"code": "def describe_cluster_snapshots ( self , cluster_identifier ) : response = self . get_conn ( ) . describe_cluster_snapshots ( ClusterIdentifier = cluster_identifier ) if 'Snapshots' not in response : return None snapshots = response [ 'Snapshots' ] snapshots = filter ( lambda x : x [ 'Status' ] , snapshots ) snapshots . sort ( key = lambda x : x [ 'SnapshotCreateTime' ] , reverse = True ) return snapshots", "nl": "Gets a list of snapshots for a cluster"}}
{"translation": {"code": "def delete_cluster ( self , cluster_identifier , skip_final_cluster_snapshot = True , final_cluster_snapshot_identifier = '' ) : response = self . get_conn ( ) . delete_cluster ( ClusterIdentifier = cluster_identifier , SkipFinalClusterSnapshot = skip_final_cluster_snapshot , FinalClusterSnapshotIdentifier = final_cluster_snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None", "nl": "Delete a cluster and optionally create a snapshot"}}
{"translation": {"code": "def cluster_status ( self , cluster_identifier ) : conn = self . get_conn ( ) try : response = conn . describe_clusters ( ClusterIdentifier = cluster_identifier ) [ 'Clusters' ] return response [ 0 ] [ 'ClusterStatus' ] if response else None except conn . exceptions . ClusterNotFoundFault : return 'cluster_not_found'", "nl": "Return status of a cluster"}}
{"translation": {"code": "def flush ( self ) : if len ( self . _buffer ) > 0 : self . logger . log ( self . level , self . _buffer ) self . _buffer = str ( )", "nl": "Ensure all logging output has been flushed"}}
{"translation": {"code": "def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( FunctionName = self . function_name , InvocationType = self . invocation_type , LogType = self . log_type , Payload = payload , Qualifier = self . qualifier ) return response", "nl": "Invoke Lambda Function"}}
{"translation": {"code": "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( )", "nl": "Contextmanager that will create and teardown a session ."}}
{"translation": {"code": "def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Check that we won't overwrite the timezone of an aware datetime. if is_localized ( value ) : raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) if hasattr ( value , 'fold' ) : # In case of python 3.6 we want to do the same that pendulum does for python3.5 # i.e in case we move clock back we want to schedule the run at the time of the second # instance of the same clock time rather than the first one. # Fold parameter has no impact in other cases so we can safely set it to 1 here value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : # This method is available for pytz time zones. return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : # For pendulum return timezone . convert ( value ) else : # This may be wrong around DST changes! return value . replace ( tzinfo = timezone )", "nl": "Make a naive datetime . datetime in a given time zone aware ."}}
{"translation": {"code": "def make_naive ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE # Emulate the behavior of astimezone() on Python < 3.6. if is_naive ( value ) : raise ValueError ( \"make_naive() cannot be applied to a naive datetime\" ) o = value . astimezone ( timezone ) # cross library compatibility naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return naive", "nl": "Make an aware datetime . datetime naive in a given time zone ."}}
{"translation": {"code": "def datetime ( * args , * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt . datetime ( * args , * * kwargs )", "nl": "Wrapper around datetime . datetime that adds settings . TIMEZONE if tzinfo not specified"}}
{"translation": {"code": "def dispose_orm ( ) : log . debug ( \"Disposing DB connection pool (PID %s)\" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose ( ) engine = None", "nl": "Properly close pooled database connections"}}
{"translation": {"code": "def _wait_for_task_ended ( self ) : try : waiter = self . client . get_waiter ( 'job_execution_complete' ) waiter . config . max_attempts = sys . maxsize # timeout is managed by airflow waiter . wait ( jobs = [ self . jobId ] ) except ValueError : # If waiter not available use expo retry = True retries = 0 while retries < self . max_retries and retry : self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) response = self . client . describe_jobs ( jobs = [ self . jobId ] ) if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : retry = False sleep ( 1 + pow ( retries * 0.1 , 2 ) ) retries += 1", "nl": "Try to use a waiter from the below pull request"}}
{"translation": {"code": "def get_size ( self , bucket_name , object_name ) : self . log . info ( 'Checking the file size of object: %s in bucket_name: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_size = blob . size self . log . info ( 'The file size of %s is %s bytes.' , object_name , blob_size ) return blob_size", "nl": "Gets the size of a file in Google Cloud Storage ."}}
{"translation": {"code": "def getsection ( self , section ) : if ( section not in self . _sections and section not in self . airflow_defaults . _sections ) : return None _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) if section in self . _sections : _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) for env_var in sorted ( os . environ . keys ( ) ) : if env_var . startswith ( section_prefix ) : key = env_var . replace ( section_prefix , '' ) . lower ( ) _section [ key ] = self . _get_env_var_option ( section , key ) for key , val in iteritems ( _section ) : try : val = int ( val ) except ValueError : try : val = float ( val ) except ValueError : if val . lower ( ) in ( 't' , 'true' ) : val = True elif val . lower ( ) in ( 'f' , 'false' ) : val = False _section [ key ] = val return _section", "nl": "Returns the section as a dict . Values are converted to int float bool as required ."}}
{"translation": {"code": "def _process_spark_status_log ( self , itr ) : # Consume the iterator for line in itr : line = line . strip ( ) # Check if the log line is about the driver status and extract the status. if \"driverState\" in line : self . _driver_status = line . split ( ' : ' ) [ 1 ] . replace ( ',' , '' ) . replace ( '\\\"' , '' ) . strip ( ) self . log . debug ( \"spark driver status log: {}\" . format ( line ) )", "nl": "parses the logs of the spark driver status query process"}}
{"translation": {"code": "def filter_for_filesize ( result , size = None ) : if size : log = LoggingMixin ( ) . log log . debug ( 'Filtering for file size >= %s in files: %s' , size , map ( lambda x : x [ 'path' ] , result ) ) size *= settings . MEGABYTE result = [ x for x in result if x [ 'length' ] >= size ] log . debug ( 'HdfsSensor.poke: after size filter result is %s' , result ) return result", "nl": "Will test the filepath result and test if its size is at least self . filesize"}}
{"translation": {"code": "def filter_for_ignored_ext ( result , ignored_ext , ignore_copying ) : if ignore_copying : log = LoggingMixin ( ) . log regex_builder = r\"^.*\\.(%s$)$\" % '$|' . join ( ignored_ext ) ignored_extensions_regex = re . compile ( regex_builder ) log . debug ( 'Filtering result for ignored extensions: %s in files %s' , ignored_extensions_regex . pattern , map ( lambda x : x [ 'path' ] , result ) ) result = [ x for x in result if not ignored_extensions_regex . match ( x [ 'path' ] ) ] log . debug ( 'HdfsSensor.poke: after ext filter result is %s' , result ) return result", "nl": "Will filter if instructed to do so the result to remove matching criteria"}}
{"translation": {"code": "def wait ( self , operation ) : submitted = _DataProcOperation ( self . get_conn ( ) , operation , self . num_retries ) submitted . wait_for_done ( )", "nl": "Awaits for Google Cloud Dataproc Operation to complete ."}}
{"translation": {"code": "def patch_table ( self , dataset_id , table_id , project_id = None , description = None , expiration_time = None , external_data_configuration = None , friendly_name = None , labels = None , schema = None , time_partitioning = None , view = None , require_partition_filter = None ) : project_id = project_id if project_id is not None else self . project_id table_resource = { } if description is not None : table_resource [ 'description' ] = description if expiration_time is not None : table_resource [ 'expirationTime' ] = expiration_time if external_data_configuration : table_resource [ 'externalDataConfiguration' ] = external_data_configuration if friendly_name is not None : table_resource [ 'friendlyName' ] = friendly_name if labels : table_resource [ 'labels' ] = labels if schema : table_resource [ 'schema' ] = { 'fields' : schema } if time_partitioning : table_resource [ 'timePartitioning' ] = time_partitioning if view : table_resource [ 'view' ] = view if require_partition_filter is not None : table_resource [ 'requirePartitionFilter' ] = require_partition_filter self . log . info ( 'Patching Table %s:%s.%s' , project_id , dataset_id , table_id ) try : self . service . tables ( ) . patch ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Table patched successfully: %s:%s.%s' , project_id , dataset_id , table_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )", "nl": "Patch information in an existing table . It only updates fileds that are provided in the request object ."}}
{"translation": {"code": "def get_crc32c ( self , bucket_name , object_name ) : self . log . info ( 'Retrieving the crc32c checksum of ' 'object_name: %s in bucket_name: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_crc32c = blob . crc32c self . log . info ( 'The crc32c checksum of %s is %s' , object_name , blob_crc32c ) return blob_crc32c", "nl": "Gets the CRC32c checksum of an object in Google Cloud Storage ."}}
{"translation": {"code": "def get_md5hash ( self , bucket_name , object_name ) : self . log . info ( 'Retrieving the MD5 hash of ' 'object: %s in bucket: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_md5hash = blob . md5_hash self . log . info ( 'The md5Hash of %s is %s' , object_name , blob_md5hash ) return blob_md5hash", "nl": "Gets the MD5 hash of an object in Google Cloud Storage ."}}
{"translation": {"code": "def create_empty_table ( self , project_id , dataset_id , table_id , schema_fields = None , time_partitioning = None , cluster_fields = None , labels = None , view = None , num_retries = None ) : project_id = project_id if project_id is not None else self . project_id table_resource = { 'tableReference' : { 'tableId' : table_id } } if schema_fields : table_resource [ 'schema' ] = { 'fields' : schema_fields } if time_partitioning : table_resource [ 'timePartitioning' ] = time_partitioning if cluster_fields : table_resource [ 'clustering' ] = { 'fields' : cluster_fields } if labels : table_resource [ 'labels' ] = labels if view : table_resource [ 'view' ] = view num_retries = num_retries if num_retries else self . num_retries self . log . info ( 'Creating Table %s:%s.%s' , project_id , dataset_id , table_id ) try : self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = num_retries ) self . log . info ( 'Table created successfully: %s:%s.%s' , project_id , dataset_id , table_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )", "nl": "Creates a new empty table in the dataset . To create a view which is defined by a SQL query parse a dictionary to view kwarg"}}
{"translation": {"code": "def delete_dag ( dag_id ) : try : count = delete . delete_dag ( dag_id ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response return jsonify ( message = \"Removed {} record(s)\" . format ( count ) , count = count )", "nl": "Delete all DB records related to the specified Dag ."}}
{"translation": {"code": "def create_bucket ( self , bucket_name , resource = None , storage_class = 'MULTI_REGIONAL' , location = 'US' , project_id = None , labels = None ) : self . log . info ( 'Creating Bucket: %s; Location: %s; Storage Class: %s' , bucket_name , location , storage_class ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) bucket_resource = resource or { } for item in bucket_resource : if item != \"name\" : bucket . _patch_property ( name = item , value = resource [ item ] ) bucket . storage_class = storage_class bucket . labels = labels or { } bucket . create ( project = project_id , location = location ) return bucket . id", "nl": "Creates a new bucket . Google Cloud Storage uses a flat namespace so you can t create a bucket with a name that is already in use ."}}
{"translation": {"code": "def get_hostname ( ) : # First we attempt to fetch the callable path from the config. try : callable_path = conf . get ( 'core' , 'hostname_callable' ) except AirflowConfigException : callable_path = None # Then we handle the case when the config is missing or empty. This is the # default behavior. if not callable_path : return socket . getfqdn ( ) # Since we have a callable path, we try to import and run it next. module_path , attr_name = callable_path . split ( ':' ) module = importlib . import_module ( module_path ) callable = getattr ( module , attr_name ) return callable ( )", "nl": "Fetch the hostname using the callable from the config or using socket . getfqdn as a fallback ."}}
{"translation": {"code": "def jenkins_request_with_headers ( jenkins_server , req ) : try : response = jenkins_server . jenkins_request ( req ) response_body = response . content response_headers = response . headers if response_body is None : raise jenkins . EmptyResponseException ( \"Error communicating with server[%s]: \" \"empty response\" % jenkins_server . server ) return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } except HTTPError as e : # Jenkins's funky authentication means its nigh impossible to # distinguish errors. if e . code in [ 401 , 403 , 500 ] : # six.moves.urllib.error.HTTPError provides a 'reason' # attribute for all python version except for ver 2.6 # Falling back to HTTPError.msg since it contains the # same info as reason raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) elif e . code == 404 : raise jenkins . NotFoundException ( 'Requested item could not be found' ) else : raise except socket . timeout as e : raise jenkins . TimeoutException ( 'Error in request: %s' % e ) except URLError as e : # python 2.6 compatibility to ensure same exception raised # since URLError wraps a socket timeout on python 2.6. if str ( e . reason ) == \"timed out\" : raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) raise JenkinsException ( 'Error in request: %s' % e . reason )", "nl": "We need to get the headers in addition to the body answer to get the location from them This function uses jenkins_request method from python - jenkins library with just the return call changed"}}
{"translation": {"code": "def get_credentials ( self , region_name = None ) : session , _ = self . _get_credentials ( region_name ) # Credentials are refreshable, so accessing your access key and # secret key separately can lead to a race condition. # See https://stackoverflow.com/a/36291428/8283373 return session . get_credentials ( ) . get_frozen_credentials ( )", "nl": "Get the underlying botocore . Credentials object ."}}
{"translation": {"code": "def read_file ( self , container_name , blob_name , * * kwargs ) : return self . connection . get_blob_to_text ( container_name , blob_name , * * kwargs ) . content", "nl": "Read a file from Azure Blob Storage and return as a string ."}}
{"translation": {"code": "def get_conn ( self ) : if self . conn is None : cnopts = pysftp . CnOpts ( ) if self . no_host_key_check : cnopts . hostkeys = None cnopts . compression = self . compress conn_params = { 'host' : self . remote_host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn_params [ 'password' ] = self . password if self . key_file : conn_params [ 'private_key' ] = self . key_file if self . private_key_pass : conn_params [ 'private_key_pass' ] = self . private_key_pass self . conn = pysftp . Connection ( * * conn_params ) return self . conn", "nl": "Returns an SFTP connection object"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn", "nl": "Establish a connection to druid broker ."}}
{"translation": {"code": "def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : with create_session ( ) as session : if g . user . is_anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . __name__ , task_instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task_id = request . args . get ( 'task_id' ) , dag_id = request . args . get ( 'dag_id' ) ) if 'execution_date' in request . args : log . execution_date = pendulum . parse ( request . args . get ( 'execution_date' ) ) session . add ( log ) return f ( * args , * * kwargs ) return wrapper", "nl": "Decorator to log user actions"}}
{"translation": {"code": "def gzipped ( f ) : @ functools . wraps ( f ) def view_func ( * args , * * kwargs ) : @ after_this_request def zipper ( response ) : accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept_encoding . lower ( ) : return response response . direct_passthrough = False if ( response . status_code < 200 or response . status_code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip_buffer = IO ( ) gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) gzip_file . write ( response . data ) gzip_file . close ( ) response . data = gzip_buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , * * kwargs ) return view_func", "nl": "Decorator to make a view compressed"}}
{"translation": {"code": "def execute ( self , context ) : self . hook = SlackWebhookHook ( self . http_conn_id , self . webhook_token , self . message , self . attachments , self . channel , self . username , self . icon_emoji , self . link_names , self . proxy ) self . hook . execute ( )", "nl": "Call the SlackWebhookHook to post the provided Slack message"}}
{"translation": {"code": "def _build_discord_payload ( self ) : payload = { } if self . username : payload [ 'username' ] = self . username if self . avatar_url : payload [ 'avatar_url' ] = self . avatar_url payload [ 'tts' ] = self . tts if len ( self . message ) <= 2000 : payload [ 'content' ] = self . message else : raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) return json . dumps ( payload )", "nl": "Construct the Discord JSON payload . All relevant parameters are combined here to a valid Discord JSON payload ."}}
{"translation": {"code": "def execute ( self ) : proxies = { } if self . proxy : # we only need https proxy for Discord proxies = { 'https' : self . proxy } discord_payload = self . _build_discord_payload ( ) self . run ( endpoint = self . webhook_endpoint , data = discord_payload , headers = { 'Content-type' : 'application/json' } , extra_options = { 'proxies' : proxies } )", "nl": "Execute the Discord webhook call"}}
{"translation": {"code": "def execute ( self , context ) : self . hook = DiscordWebhookHook ( self . http_conn_id , self . webhook_endpoint , self . message , self . username , self . avatar_url , self . tts , self . proxy ) self . hook . execute ( )", "nl": "Call the DiscordWebhookHook to post message"}}
{"translation": {"code": "def _get_webhook_endpoint ( self , http_conn_id , webhook_endpoint ) : if webhook_endpoint : endpoint = webhook_endpoint elif http_conn_id : conn = self . get_connection ( http_conn_id ) extra = conn . extra_dejson endpoint = extra . get ( 'webhook_endpoint' , '' ) else : raise AirflowException ( 'Cannot get webhook endpoint: No valid Discord ' 'webhook endpoint or http_conn_id supplied.' ) # make sure endpoint matches the expected Discord webhook format if not re . match ( '^webhooks/[0-9]+/[a-zA-Z0-9_-]+$' , endpoint ) : raise AirflowException ( 'Expected Discord webhook endpoint in the form ' 'of \"webhooks/{webhook.id}/{webhook.token}\".' ) return endpoint", "nl": "Given a Discord http_conn_id return the default webhook endpoint or override if a webhook_endpoint is manually supplied ."}}
{"translation": {"code": "def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : \"\"\"\n        An wrapper for cli functions. It assumes to have Namespace instance\n        at 1st positional argument\n        :param args: Positional argument. It assumes to have Namespace instance\n        at 1st positional argument\n        :param kwargs: A passthrough keyword argument\n        \"\"\" assert args assert isinstance ( args [ 0 ] , Namespace ) , \"1st positional argument should be argparse.Namespace instance, \" \"but {}\" . format ( args [ 0 ] ) metrics = _build_metrics ( f . __name__ , args [ 0 ] ) cli_action_loggers . on_pre_execution ( * * metrics ) try : return f ( * args , * * kwargs ) except Exception as e : metrics [ 'error' ] = e raise finally : metrics [ 'end_datetime' ] = datetime . utcnow ( ) cli_action_loggers . on_post_execution ( * * metrics ) return wrapper", "nl": "Decorates function to execute function at the same time submitting action_logging but in CLI context . It will call action logger callbacks twice one for pre - execution and the other one for post - execution ."}}
{"translation": {"code": "def _build_metrics ( func_name , namespace ) : metrics = { 'sub_command' : func_name , 'start_datetime' : datetime . utcnow ( ) , 'full_command' : '{}' . format ( list ( sys . argv ) ) , 'user' : getpass . getuser ( ) } assert isinstance ( namespace , Namespace ) tmp_dic = vars ( namespace ) metrics [ 'dag_id' ] = tmp_dic . get ( 'dag_id' ) metrics [ 'task_id' ] = tmp_dic . get ( 'task_id' ) metrics [ 'execution_date' ] = tmp_dic . get ( 'execution_date' ) metrics [ 'host_name' ] = socket . gethostname ( ) extra = json . dumps ( dict ( ( k , metrics [ k ] ) for k in ( 'host_name' , 'full_command' ) ) ) log = Log ( event = 'cli_{}' . format ( func_name ) , task_instance = None , owner = metrics [ 'user' ] , extra = extra , task_id = metrics . get ( 'task_id' ) , dag_id = metrics . get ( 'dag_id' ) , execution_date = metrics . get ( 'execution_date' ) ) metrics [ 'log' ] = log return metrics", "nl": "Builds metrics dict from function args It assumes that function arguments is from airflow . bin . cli module s function and has Namespace instance where it optionally contains dag_id task_id and execution_date ."}}
{"translation": {"code": "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key", "nl": "returns aws_access_key_id aws_secret_access_key from extra"}}
{"translation": {"code": "def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn", "nl": "Returns a snowflake . connection object"}}
{"translation": {"code": "def close ( self ) : # When application exit, system shuts down all handlers by # calling close method. Here we check if logger is already # closed to prevent uploading the log to remote storage multiple # times when `logging.shutdown` is called. if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : # read log and remove old logs to get just the latest additions with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb_write ( log , remote_loc , append = True ) if self . delete_local_copy : shutil . rmtree ( os . path . dirname ( local_loc ) ) # Mark closed so we don't double write if close is called twice self . closed = True", "nl": "Close and upload local log file to remote storage Wasb ."}}
{"translation": {"code": "def remove_option ( self , section , option , remove_default = True ) : if super ( ) . has_option ( section , option ) : super ( ) . remove_option ( section , option ) if self . airflow_defaults . has_option ( section , option ) and remove_default : self . airflow_defaults . remove_option ( section , option )", "nl": "Remove an option if it exists in config from a file or default config . If both of config have the same option this removes the option in both configs unless remove_default = False ."}}
{"translation": {"code": "def select_key ( self , key , bucket_name = None , expression = 'SELECT * FROM S3Object' , expression_type = 'SQL' , input_serialization = None , output_serialization = None ) : if input_serialization is None : input_serialization = { 'CSV' : { } } if output_serialization is None : output_serialization = { 'CSV' : { } } if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) response = self . get_conn ( ) . select_object_content ( Bucket = bucket_name , Key = key , Expression = expression , ExpressionType = expression_type , InputSerialization = input_serialization , OutputSerialization = output_serialization ) return '' . join ( event [ 'Records' ] [ 'Payload' ] . decode ( 'utf-8' ) for event in response [ 'Payload' ] if 'Records' in event )", "nl": "Reads a key with S3 Select ."}}
{"translation": {"code": "def insert_rows ( self , table , rows , target_fields = None ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )", "nl": "A generic way to insert a set of tuples into a table ."}}
{"translation": {"code": "def put_records ( self , records ) : firehose_conn = self . get_conn ( ) response = firehose_conn . put_record_batch ( DeliveryStreamName = self . delivery_stream , Records = records ) return response", "nl": "Write batch records to Kinesis Firehose"}}
{"translation": {"code": "def get_uri ( self ) : conn = self . get_connection ( getattr ( self , self . conn_name_attr ) ) host = conn . host if conn . port is not None : host += ':{port}' . format ( port = conn . port ) conn_type = 'http' if not conn . conn_type else conn . conn_type endpoint = conn . extra_dejson . get ( 'endpoint' , 'pql' ) return '{conn_type}://{host}/{endpoint}' . format ( conn_type = conn_type , host = host , endpoint = endpoint )", "nl": "Get the connection uri for pinot broker ."}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . pinot_broker_conn_id ) pinot_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot_broker_conn", "nl": "Establish a connection to pinot broker through pinot dbqpi ."}}
{"translation": {"code": "def on_error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment_error} with ' 'items: {with_items}' . format ( segment_error = error , with_items = items ) ) raise AirflowException ( 'Segment error: {}' . format ( error ) )", "nl": "Handles error callbacks when using Segment with segment_debug_mode set to True"}}
{"translation": {"code": "def _get_col_type_dict ( self ) : schema = [ ] if isinstance ( self . schema , string_types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col_type_dict = { } try : col_type_dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except KeyError : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying_a_json_schema_file' ) return col_type_dict", "nl": "Return a dict of column name and column type based on self . schema if not None ."}}
{"translation": {"code": "def chunks ( items , chunk_size ) : if chunk_size <= 0 : raise ValueError ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk_size ) : yield items [ i : i + chunk_size ]", "nl": "Yield successive chunks of a given size from a list of items"}}
{"translation": {"code": "def reduce_in_chunks ( fn , iterable , initializer , chunk_size = 0 ) : if len ( iterable ) == 0 : return initializer if chunk_size == 0 : chunk_size = len ( iterable ) return reduce ( fn , chunks ( iterable , chunk_size ) , initializer )", "nl": "Reduce the given list of items by splitting it into chunks of the given size and passing each chunk through the reducer"}}
{"translation": {"code": "def apply_lineage ( func ) : backend = _get_backend ( ) @ wraps ( func ) def wrapper ( self , context , * args , * * kwargs ) : self . log . debug ( \"Backend: %s, Lineage called with inlets: %s, outlets: %s\" , backend , self . inlets , self . outlets ) ret_val = func ( self , context , * args , * * kwargs ) outlets = [ x . as_dict ( ) for x in self . outlets ] inlets = [ x . as_dict ( ) for x in self . inlets ] if len ( self . outlets ) > 0 : self . xcom_push ( context , key = PIPELINE_OUTLETS , value = outlets , execution_date = context [ 'ti' ] . execution_date ) if len ( self . inlets ) > 0 : self . xcom_push ( context , key = PIPELINE_INLETS , value = inlets , execution_date = context [ 'ti' ] . execution_date ) if backend : backend . send_lineage ( operator = self , inlets = self . inlets , outlets = self . outlets , context = context ) return ret_val return wrapper", "nl": "Saves the lineage to XCom and if configured to do so sends it to the backend ."}}
{"translation": {"code": "def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . HTTPError : self . log . error ( \"HTTP error: %s\" , response . reason ) if self . method not in [ 'GET' , 'HEAD' ] : self . log . error ( response . text ) raise AirflowException ( str ( response . status_code ) + \":\" + response . reason )", "nl": "Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson self . account_name = service_options . get ( 'account_name' ) adlCreds = lib . auth ( tenant_id = service_options . get ( 'tenant' ) , client_secret = conn . password , client_id = conn . login ) adlsFileSystemClient = core . AzureDLFileSystem ( adlCreds , store_name = self . account_name ) adlsFileSystemClient . connect ( ) return adlsFileSystemClient", "nl": "Return a AzureDLFileSystem object ."}}
{"translation": {"code": "def upload_file ( self , local_path , remote_path , nthreads = 64 , overwrite = True , buffersize = 4194304 , blocksize = 4194304 ) : multithread . ADLUploader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize )", "nl": "Upload a file to Azure Data Lake ."}}
{"translation": {"code": "def check_for_file ( self , file_path ) : try : files = self . connection . glob ( file_path , details = False , invalidate_cache = True ) return len ( files ) == 1 except FileNotFoundError : return False", "nl": "Check if a file exists on Azure Data Lake ."}}
{"translation": {"code": "def _query_cassandra ( self ) : self . hook = CassandraHook ( cassandra_conn_id = self . cassandra_conn_id ) session = self . hook . get_conn ( ) cursor = session . execute ( self . cql ) return cursor", "nl": "Queries cassandra and returns a cursor to the results ."}}
{"translation": {"code": "def convert_user_type ( cls , name , value ) : names = value . _fields values = [ cls . convert_value ( name , getattr ( value , name ) ) for name in names ] return cls . generate_data_dict ( names , values )", "nl": "Converts a user type to RECORD that contains n fields where n is the number of attributes . Each element in the user type class will be converted to its corresponding data type in BQ ."}}
{"translation": {"code": "def get_conn ( self ) : if self . session and not self . session . is_shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session", "nl": "Returns a cassandra Session object"}}
{"translation": {"code": "def get_conn ( self ) : if self . client is not None : return self . client # Mongo Connection Options dict that is unpacked when passed to MongoClient options = self . extras # If we are using SSL disable requiring certs from specific hostname if options . get ( 'ssl' , False ) : options . update ( { 'ssl_cert_reqs' : CERT_NONE } ) self . client = MongoClient ( self . uri , * * options ) return self . client", "nl": "Fetches PyMongo Client"}}
{"translation": {"code": "def get_collection ( self , mongo_collection , mongo_db = None ) : mongo_db = mongo_db if mongo_db is not None else self . connection . schema mongo_conn = self . get_conn ( ) return mongo_conn . get_database ( mongo_db ) . get_collection ( mongo_collection )", "nl": "Fetches a mongo collection object for querying ."}}
{"translation": {"code": "def execute ( self , context ) : s3_conn = S3Hook ( self . s3_conn_id ) # Grab collection and execute query according to whether or not it is a pipeline if self . is_pipeline : results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , mongo_db = self . mongo_db ) else : results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) # Performs transform then stringifies the docs results into json format docs_str = self . _stringify ( self . transform ( results ) ) # Load Into S3 s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) return True", "nl": "Executed by task_instance at runtime"}}
{"translation": {"code": "def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( \"COPY {table} TO STDOUT\" . format ( table = table ) , tmp_file )", "nl": "Dumps a database table into a tab - delimited file"}}
{"translation": {"code": "def get_results ( self , hql , schema = 'default' , fetch_size = None , hive_conf = None ) : results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) results = { 'data' : list ( results_iter ) , 'header' : header } return results", "nl": "Get results of the provided hql in target schema ."}}
{"translation": {"code": "def get_operation ( self , operation_name , project_id = None ) : return self . get_client ( ) . get_operation ( project_id = project_id or self . project_id , zone = self . location , operation_id = operation_name )", "nl": "Fetches the operation from Google Cloud"}}
{"translation": {"code": "def wait_for_operation ( self , operation , project_id = None ) : self . log . info ( \"Waiting for OPERATION_NAME %s\" , operation . name ) time . sleep ( OPERATIONAL_POLL_INTERVAL ) while operation . status != Operation . Status . DONE : if operation . status == Operation . Status . RUNNING or operation . status == Operation . Status . PENDING : time . sleep ( OPERATIONAL_POLL_INTERVAL ) else : raise exceptions . GoogleCloudError ( \"Operation has failed with status: %s\" % operation . status ) # To update status of operation operation = self . get_operation ( operation . name , project_id = project_id or self . project_id ) return operation", "nl": "Given an operation continuously fetches the status from Google Cloud until either completion or an error occurring"}}
{"translation": {"code": "def _dict_to_proto ( py_dict , proto ) : dict_json_str = json . dumps ( py_dict ) return json_format . Parse ( dict_json_str , proto )", "nl": "Converts a python dictionary to the proto supplied"}}
{"translation": {"code": "def get_cluster ( self , name , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : self . log . info ( \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\" , project_id or self . project_id , self . location , name ) return self . get_client ( ) . get_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster_id = name , retry = retry , timeout = timeout ) . self_link", "nl": "Gets details of specified cluster"}}
{"translation": {"code": "def create_cluster ( self , cluster , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : if isinstance ( cluster , dict ) : cluster_proto = Cluster ( ) cluster = self . _dict_to_proto ( py_dict = cluster , proto = cluster_proto ) elif not isinstance ( cluster , Cluster ) : raise AirflowException ( \"cluster is not instance of Cluster proto or python dict\" ) self . _append_label ( cluster , 'airflow-version' , 'v' + version . version ) self . log . info ( \"Creating (project_id=%s, zone=%s, cluster_name=%s)\" , self . project_id , self . location , cluster . name ) try : op = self . get_client ( ) . create_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster = cluster , retry = retry , timeout = timeout ) op = self . wait_for_operation ( op ) return op . target_link except AlreadyExists as error : self . log . info ( 'Assuming Success: %s' , error . message ) return self . get_cluster ( name = cluster . name ) . self_link", "nl": "Creates a cluster consisting of the specified number and type of Google Compute Engine instances ."}}
{"translation": {"code": "def _append_label ( cluster_proto , key , val ) : val = val . replace ( '.' , '-' ) . replace ( '+' , '-' ) cluster_proto . resource_labels . update ( { key : val } ) return cluster_proto", "nl": "Append labels to provided Cluster Protobuf"}}
{"translation": {"code": "def record_exists ( self , table , keys ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) ks = \" AND \" . join ( \"{}=%({})s\" . format ( key , key ) for key in keys . keys ( ) ) cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\" . format ( keyspace = keyspace , table = table , keys = ks ) try : rs = self . get_conn ( ) . execute ( cql , keys ) return rs . one ( ) is not None except Exception : return False", "nl": "Checks if a record exists in Cassandra"}}
{"translation": {"code": "def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return FileService ( account_name = conn . login , account_key = conn . password , * * service_options )", "nl": "Return the FileService object ."}}
{"translation": {"code": "def check_for_directory ( self , share_name , directory_name , * * kwargs ) : return self . connection . exists ( share_name , directory_name , * * kwargs )", "nl": "Check if a directory exists on Azure File Share ."}}
{"translation": {"code": "def check_for_file ( self , share_name , directory_name , file_name , * * kwargs ) : return self . connection . exists ( share_name , directory_name , file_name , * * kwargs )", "nl": "Check if a file exists on Azure File Share ."}}
{"translation": {"code": "def list_directories_and_files ( self , share_name , directory_name = None , * * kwargs ) : return self . connection . list_directories_and_files ( share_name , directory_name , * * kwargs )", "nl": "Return the list of directories and files stored on a Azure File Share ."}}
{"translation": {"code": "def create_directory ( self , share_name , directory_name , * * kwargs ) : return self . connection . create_directory ( share_name , directory_name , * * kwargs )", "nl": "Create a new directory on a Azure File Share ."}}
{"translation": {"code": "def load_file ( self , file_path , share_name , directory_name , file_name , * * kwargs ) : self . connection . create_file_from_path ( share_name , directory_name , file_name , file_path , * * kwargs )", "nl": "Upload a file to Azure File Share ."}}
{"translation": {"code": "def load_string ( self , string_data , share_name , directory_name , file_name , * * kwargs ) : self . connection . create_file_from_text ( share_name , directory_name , file_name , string_data , * * kwargs )", "nl": "Upload a string to Azure File Share ."}}
{"translation": {"code": "def load_stream ( self , stream , share_name , directory_name , file_name , count , * * kwargs ) : self . connection . create_file_from_stream ( share_name , directory_name , file_name , stream , count , * * kwargs )", "nl": "Upload a stream to Azure File Share ."}}
{"translation": {"code": "def table_exists ( self , table ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) cluster_metadata = self . get_conn ( ) . cluster . metadata return ( keyspace in cluster_metadata . keyspaces and table in cluster_metadata . keyspaces [ keyspace ] . tables )", "nl": "Checks if a table exists in Cassandra"}}
{"translation": {"code": "def _has_perm ( self , permission_name , view_menu_name ) : if hasattr ( self , 'perms' ) : if ( permission_name , view_menu_name ) in self . perms : return True # rebuild the permissions set self . _get_and_cache_perms ( ) return ( permission_name , view_menu_name ) in self . perms", "nl": "Whether the user has this perm"}}
{"translation": {"code": "def clean_perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get_session pvms = ( sesh . query ( sqla_models . PermissionView ) . filter ( or_ ( sqla_models . PermissionView . permission == None , # NOQA sqla_models . PermissionView . view_menu == None , # NOQA ) ) ) deleted_count = pvms . delete ( ) sesh . commit ( ) if deleted_count : self . log . info ( 'Deleted %s faulty permissions' , deleted_count )", "nl": "FAB leaves faulty permissions that need to be cleaned up"}}
{"translation": {"code": "def get_user_roles ( self , user = None ) : if user is None : user = g . user if user . is_anonymous : public_role = appbuilder . config . get ( 'AUTH_ROLE_PUBLIC' ) return [ appbuilder . security_manager . find_role ( public_role ) ] if public_role else [ ] return user . roles", "nl": "Get all the roles associated with the user ."}}
{"translation": {"code": "def _merge_perm ( self , permission_name , view_menu_name ) : permission = self . find_permission ( permission_name ) view_menu = self . find_view_menu ( view_menu_name ) pv = None if permission and view_menu : pv = self . get_session . query ( self . permissionview_model ) . filter_by ( permission = permission , view_menu = view_menu ) . first ( ) if not pv and permission_name and view_menu_name : self . add_permission_view_menu ( permission_name , view_menu_name )", "nl": "Add the new permission view_menu to ab_permission_view_role if not exists . It will add the related entry to ab_permission and ab_view_menu two meta tables as well ."}}
{"translation": {"code": "def get_all_permissions_views ( self ) : perms_views = set ( ) for role in self . get_user_roles ( ) : perms_views . update ( { ( perm_view . permission . name , perm_view . view_menu . name ) for perm_view in role . permissions } ) return perms_views", "nl": "Returns a set of tuples with the perm name and view menu name"}}
{"translation": {"code": "def _has_role ( self , role_name_or_list ) : if not isinstance ( role_name_or_list , list ) : role_name_or_list = [ role_name_or_list ] return any ( [ r . name in role_name_or_list for r in self . get_user_roles ( ) ] )", "nl": "Whether the user has this role name"}}
{"translation": {"code": "def update_admin_perm_view ( self ) : pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) pvms = [ p for p in pvms if p . permission and p . view_menu ] admin = self . find_role ( 'Admin' ) admin . permissions = list ( set ( admin . permissions ) | set ( pvms ) ) self . get_session . commit ( )", "nl": "Admin should have all the permission - views . Add the missing ones to the table for admin ."}}
{"translation": {"code": "def init_role ( self , role_name , role_vms , role_perms ) : pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) pvms = [ p for p in pvms if p . permission and p . view_menu ] role = self . find_role ( role_name ) if not role : role = self . add_role ( role_name ) if len ( role . permissions ) == 0 : self . log . info ( 'Initializing permissions for role:%s in the database.' , role_name ) role_pvms = set ( ) for pvm in pvms : if pvm . view_menu . name in role_vms and pvm . permission . name in role_perms : role_pvms . add ( pvm ) role . permissions = list ( role_pvms ) self . get_session . merge ( role ) self . get_session . commit ( ) else : self . log . debug ( 'Existing permissions for the role:%s ' 'within the database will persist.' , role_name )", "nl": "Initialize the role with the permissions and related view - menus ."}}
{"translation": {"code": "def encrypt ( self , key_name , plaintext , authenticated_data = None ) : keys = self . get_conn ( ) . projects ( ) . locations ( ) . keyRings ( ) . cryptoKeys ( ) body = { 'plaintext' : _b64encode ( plaintext ) } if authenticated_data : body [ 'additionalAuthenticatedData' ] = _b64encode ( authenticated_data ) request = keys . encrypt ( name = key_name , body = body ) response = request . execute ( num_retries = self . num_retries ) ciphertext = response [ 'ciphertext' ] return ciphertext", "nl": "Encrypts a plaintext message using Google Cloud KMS ."}}
{"translation": {"code": "def process_result_value ( self , value , dialect ) : if value is not None : if value . tzinfo is None : value = value . replace ( tzinfo = utc ) else : value = value . astimezone ( utc ) return value", "nl": "Processes DateTimes from the DB making sure it is always returning UTC . Not using timezone . convert_to_utc as that converts to configured TIMEZONE while the DB might be running with some other setting . We assume UTC datetimes in the database ."}}
{"translation": {"code": "def get_log_conn ( self ) : config = botocore . config . Config ( retries = { 'max_attempts' : 15 } ) return self . get_client_type ( 'logs' , config = config )", "nl": "Establish an AWS connection for retrieving logs during training"}}
{"translation": {"code": "def create_tuning_job ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : self . check_tuning_config ( config ) response = self . get_conn ( ) . create_hyper_parameter_tuning_job ( * * config ) if wait_for_completion : self . check_status ( config [ 'HyperParameterTuningJobName' ] , 'HyperParameterTuningJobStatus' , self . describe_tuning_job , check_interval , max_ingestion_time ) return response", "nl": "Create a tuning job"}}
{"translation": {"code": "def create_training_job ( self , config , wait_for_completion = True , print_log = True , check_interval = 30 , max_ingestion_time = None ) : self . check_training_config ( config ) response = self . get_conn ( ) . create_training_job ( * * config ) if print_log : self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) elif wait_for_completion : describe_response = self . check_status ( config [ 'TrainingJobName' ] , 'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) return response", "nl": "Create a training job"}}
{"translation": {"code": "def check_s3_url ( self , s3url ) : bucket , key = S3Hook . parse_s3_url ( s3url ) if not self . s3_hook . check_for_bucket ( bucket_name = bucket ) : raise AirflowException ( \"The input S3 Bucket {} does not exist \" . format ( bucket ) ) if key and not self . s3_hook . check_for_key ( key = key , bucket_name = bucket ) and not self . s3_hook . check_for_prefix ( prefix = key , bucket_name = bucket , delimiter = '/' ) : # check if s3 key exists in the case user provides a single file # or if s3 prefix exists in the case user provides multiple files in # a prefix raise AirflowException ( \"The input S3 Key \" \"or Prefix {} does not exist in the Bucket {}\" . format ( s3url , bucket ) ) return True", "nl": "Check if an S3 URL exists"}}
{"translation": {"code": "def configure_s3_resources ( self , config ) : s3_operations = config . pop ( 'S3Operations' , None ) if s3_operations is not None : create_bucket_ops = s3_operations . get ( 'S3CreateBucket' , [ ] ) upload_ops = s3_operations . get ( 'S3Upload' , [ ] ) for op in create_bucket_ops : self . s3_hook . create_bucket ( bucket_name = op [ 'Bucket' ] ) for op in upload_ops : if op [ 'Tar' ] : self . tar_and_s3_upload ( op [ 'Path' ] , op [ 'Key' ] , op [ 'Bucket' ] ) else : self . s3_hook . load_file ( op [ 'Path' ] , op [ 'Key' ] , op [ 'Bucket' ] )", "nl": "Extract the S3 operations from the configuration and execute them ."}}
{"translation": {"code": "def open_maybe_zipped ( f , mode = 'r' ) : _ , archive , filename = ZIP_REGEX . search ( f ) . groups ( ) if archive and zipfile . is_zipfile ( archive ) : return zipfile . ZipFile ( archive , mode = mode ) . open ( filename ) else : return io . open ( f , mode = mode )", "nl": "Opens the given file . If the path contains a folder with a . zip suffix then the folder is treated as a zip archive opening the file inside the archive ."}}
{"translation": {"code": "def _validate_value ( key , value , expected_type ) : if not isinstance ( value , expected_type ) : raise TypeError ( \"{} argument must have a type {} not {}\" . format ( key , expected_type , type ( value ) ) )", "nl": "function to check expected type and raise error if type is not correct"}}
{"translation": {"code": "def _deep_string_coerce ( content , json_path = 'json' ) : c = _deep_string_coerce if isinstance ( content , six . string_types ) : return content elif isinstance ( content , six . integer_types + ( float , ) ) : # Databricks can tolerate either numeric or string types in the API backend. return str ( content ) elif isinstance ( content , ( list , tuple ) ) : return [ c ( e , '{0}[{1}]' . format ( json_path , i ) ) for i , e in enumerate ( content ) ] elif isinstance ( content , dict ) : return { k : c ( v , '{0}[{1}]' . format ( json_path , k ) ) for k , v in list ( content . items ( ) ) } else : param_type = type ( content ) msg = 'Type {0} used for parameter {1} is not a number or a string' . format ( param_type , json_path ) raise AirflowException ( msg )", "nl": "Coerces content or all values of content if it is a dict to a string . The function will throw if content contains non - string or non - numeric types ."}}
{"translation": {"code": "def _handle_databricks_operator_execution ( operator , hook , log , context ) : if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = XCOM_RUN_ID_KEY , value = operator . run_id ) log . info ( 'Run submitted with run_id: %s' , operator . run_id ) run_page_url = hook . get_run_page_url ( operator . run_id ) if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = XCOM_RUN_PAGE_URL_KEY , value = run_page_url ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) while True : run_state = hook . get_run_state ( operator . run_id ) if run_state . is_terminal : if run_state . is_successful : log . info ( '%s completed successfully.' , operator . task_id ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) return else : error_message = '{t} failed with terminal state: {s}' . format ( t = operator . task_id , s = run_state ) raise AirflowException ( error_message ) else : log . info ( '%s in run state: %s' , operator . task_id , run_state ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) log . info ( 'Sleeping for %s seconds.' , operator . polling_period_seconds ) time . sleep ( operator . polling_period_seconds )", "nl": "Handles the Airflow + Databricks lifecycle logic for a Databricks operator"}}
{"translation": {"code": "def copy_object ( self , source_bucket_key , dest_bucket_key , source_bucket_name = None , dest_bucket_name = None , source_version_id = None ) : if dest_bucket_name is None : dest_bucket_name , dest_bucket_key = self . parse_s3_url ( dest_bucket_key ) else : parsed_url = urlparse ( dest_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise AirflowException ( 'If dest_bucket_name is provided, ' + 'dest_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) if source_bucket_name is None : source_bucket_name , source_bucket_key = self . parse_s3_url ( source_bucket_key ) else : parsed_url = urlparse ( source_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise AirflowException ( 'If source_bucket_name is provided, ' + 'source_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) CopySource = { 'Bucket' : source_bucket_name , 'Key' : source_bucket_key , 'VersionId' : source_version_id } response = self . get_conn ( ) . copy_object ( Bucket = dest_bucket_name , Key = dest_bucket_key , CopySource = CopySource ) return response", "nl": "Creates a copy of an object that is already stored in S3 ."}}
{"translation": {"code": "def _num_tasks_per_fetch_process ( self ) : return max ( 1 , int ( math . ceil ( 1.0 * len ( self . tasks ) / self . _sync_parallelism ) ) )", "nl": "How many Celery tasks should be sent to each worker process ."}}
{"translation": {"code": "def fetch_celery_task_state ( celery_task ) : try : with timeout ( seconds = 2 ) : # Accessing state property of celery task will make actual network request # to get the current state of the task. res = ( celery_task [ 0 ] , celery_task [ 1 ] . state ) except Exception as e : exception_traceback = \"Celery Task ID: {}\\n{}\" . format ( celery_task [ 0 ] , traceback . format_exc ( ) ) res = ExceptionWithTraceback ( e , exception_traceback ) return res", "nl": "Fetch and return the state of the given celery task . The scope of this function is global so that it can be called by subprocesses in the pool ."}}
{"translation": {"code": "def create_transform_job ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : self . check_s3_url ( config [ 'TransformInput' ] [ 'DataSource' ] [ 'S3DataSource' ] [ 'S3Uri' ] ) response = self . get_conn ( ) . create_transform_job ( * * config ) if wait_for_completion : self . check_status ( config [ 'TransformJobName' ] , 'TransformJobStatus' , self . describe_transform_job , check_interval , max_ingestion_time ) return response", "nl": "Create a transform job"}}
{"translation": {"code": "def _get_dep_statuses ( self , ti , session , dep_context ) : if dep_context . ignore_in_reschedule_period : yield self . _passing_status ( reason = \"The context specified that being in a reschedule period was \" \"permitted.\" ) return if ti . state not in self . RESCHEDULEABLE_STATES : yield self . _passing_status ( reason = \"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\" ) return task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) if not task_reschedules : yield self . _passing_status ( reason = \"There is no reschedule request for this task instance.\" ) return now = timezone . utcnow ( ) next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date if now >= next_reschedule_date : yield self . _passing_status ( reason = \"Task instance id ready for reschedule.\" ) return yield self . _failing_status ( reason = \"Task is not ready for reschedule yet but will be rescheduled \" \"automatically. Current date is {0} and task will be rescheduled \" \"at {1}.\" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) )", "nl": "Determines whether a task is ready to be rescheduled . Only tasks in NONE state with at least one row in task_reschedule table are handled by this dependency class otherwise this dependency is considered as passed . This dependency fails if the latest reschedule request s reschedule date is still in future ."}}
{"translation": {"code": "def delete_function ( self , name ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )", "nl": "Deletes the specified Cloud Function ."}}
{"translation": {"code": "def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , # Those two headers needs to be specified according to: # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl # nopep8 headers = { 'Content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url", "nl": "Uploads zip file with sources ."}}
{"translation": {"code": "def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updateMask = \",\" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )", "nl": "Updates Cloud Functions according to the specified update mask ."}}
{"translation": {"code": "def create_new_function ( self , location , body , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )", "nl": "Creates a new function in Cloud Function in the location specified in the body ."}}
{"translation": {"code": "def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . get ( name = name ) . execute ( num_retries = self . num_retries )", "nl": "Returns the Cloud Function with the given name ."}}
{"translation": {"code": "def get_dataset ( self , dataset_id , project_id = None ) : if not dataset_id or not isinstance ( dataset_id , str ) : raise ValueError ( \"dataset_id argument must be provided and has \" \"a type 'str'. You provided: {}\" . format ( dataset_id ) ) dataset_project_id = project_id if project_id else self . project_id try : dataset_resource = self . service . datasets ( ) . get ( datasetId = dataset_id , projectId = dataset_project_id ) . execute ( num_retries = self . num_retries ) self . log . info ( \"Dataset Resource: %s\" , dataset_resource ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) return dataset_resource", "nl": "Method returns dataset_resource if dataset exist and raised 404 error if dataset does not exist"}}
{"translation": {"code": "def get_datasets_list ( self , project_id = None ) : dataset_project_id = project_id if project_id else self . project_id try : datasets_list = self . service . datasets ( ) . list ( projectId = dataset_project_id ) . execute ( num_retries = self . num_retries ) [ 'datasets' ] self . log . info ( \"Datasets List: %s\" , datasets_list ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) return datasets_list", "nl": "Method returns full list of BigQuery datasets in the current project"}}
{"translation": {"code": "def _wait_for_operation_to_complete ( self , project_id , operation_name , zone = None ) : service = self . get_conn ( ) while True : if zone is None : # noinspection PyTypeChecker operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) else : # noinspection PyTypeChecker operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) if operation_response . get ( \"status\" ) == GceOperationStatus . DONE : error = operation_response . get ( \"error\" ) if error : code = operation_response . get ( \"httpErrorStatusCode\" ) msg = operation_response . get ( \"httpErrorMessage\" ) # Extracting the errors list as string and trimming square braces error_msg = str ( error . get ( \"errors\" ) ) [ 1 : - 1 ] raise AirflowException ( \"{} {}: \" . format ( code , msg ) + error_msg ) # No meaningful info to return from the response in case of success return time . sleep ( TIME_TO_SLEEP_IN_SECONDS )", "nl": "Waits for the named operation to complete - checks status of the async call ."}}
{"translation": {"code": "def get_instance_template ( self , resource_id , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . get ( project = project_id , instanceTemplate = resource_id ) . execute ( num_retries = self . num_retries ) return response", "nl": "Retrieves instance template by project_id and resource_id . Must be called with keyword arguments rather than positional ."}}
{"translation": {"code": "def set_machine_type ( self , zone , resource_id , body , project_id = None ) : response = self . _execute_set_machine_type ( zone , resource_id , body , project_id ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )", "nl": "Sets machine type of an instance defined by project_id zone and resource_id . Must be called with keyword arguments rather than positional ."}}
{"translation": {"code": "def start_instance ( self , zone , resource_id , project_id = None ) : response = self . get_conn ( ) . instances ( ) . start ( project = project_id , zone = zone , instance = resource_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )", "nl": "Starts an existing instance defined by project_id zone and resource_id . Must be called with keyword arguments rather than positional ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _conn : http_authorized = self . _authorize ( ) self . _conn = build ( 'compute' , self . api_version , http = http_authorized , cache_discovery = False ) return self . _conn", "nl": "Retrieves connection to Google Compute Engine ."}}
{"translation": {"code": "def delete_file ( self , container_name , blob_name , is_prefix = False , ignore_if_missing = False , * * kwargs ) : if is_prefix : blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , * * kwargs ) ] elif self . check_for_blob ( container_name , blob_name ) : blobs_to_delete = [ blob_name ] else : blobs_to_delete = [ ] if not ignore_if_missing and len ( blobs_to_delete ) == 0 : raise AirflowException ( 'Blob(s) not found: {}' . format ( blob_name ) ) for blob_uri in blobs_to_delete : self . log . info ( \"Deleting blob: \" + blob_uri ) self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , * * kwargs )", "nl": "Delete a file from Azure Blob Storage ."}}
{"translation": {"code": "def list ( self , path ) : if \"*\" in path : return self . connection . glob ( path ) else : return self . connection . walk ( path )", "nl": "List files in Azure Data Lake Storage"}}
{"translation": {"code": "def start ( self ) : self . _process = self . _launch_process ( self . _dag_directory , self . _file_paths , self . _max_runs , self . _processor_factory , self . _child_signal_conn , self . _stat_queue , self . _result_queue , self . _async_mode ) self . log . info ( \"Launched DagFileProcessorManager with pid: %s\" , self . _process . pid )", "nl": "Launch DagFileProcessorManager processor and start DAG parsing loop in manager ."}}
{"translation": {"code": "def clear_nonexistent_import_errors ( self , session ) : query = session . query ( errors . ImportError ) if self . _file_paths : query = query . filter ( ~ errors . ImportError . filename . in_ ( self . _file_paths ) ) query . delete ( synchronize_session = 'fetch' ) session . commit ( )", "nl": "Clears import errors for files that no longer exist ."}}
{"translation": {"code": "def end ( self ) : pids_to_kill = self . get_all_pids ( ) if len ( pids_to_kill ) > 0 : # First try SIGTERM this_process = psutil . Process ( os . getpid ( ) ) # Only check child processes to ensure that we don't have a case # where we kill the wrong process because a child process died # but the PID got reused. child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] for child in child_processes : self . log . info ( \"Terminating child PID: %s\" , child . pid ) child . terminate ( ) # TODO: Remove magic number timeout = 5 self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) try : psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) except psutil . TimeoutExpired : self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) # Then SIGKILL child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] if len ( child_processes ) > 0 : self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) for child in child_processes : self . log . info ( \"Killing child PID: %s\" , child . pid ) child . kill ( ) child . wait ( )", "nl": "Kill all child processes on exit since we don t want to leave them as orphaned ."}}
{"translation": {"code": "def _exit_gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) if self . processor_agent : self . processor_agent . end ( ) sys . exit ( os . EX_OK )", "nl": "Helper method to clean up processor_agent to avoid leaving orphan processes ."}}
{"translation": {"code": "def _log_file_processing_stats ( self , known_file_paths ) : # File Path: Path to the file containing the DAG definition # PID: PID associated with the process that's processing the file. May # be empty. # Runtime: If the process is currently running, how long it's been # running for in seconds. # Last Runtime: If the process ran before, how long did it take to # finish in seconds # Last Run: When the file finished processing in the previous run. headers = [ \"File Path\" , \"PID\" , \"Runtime\" , \"Last Runtime\" , \"Last Run\" ] rows = [ ] for file_path in known_file_paths : last_runtime = self . get_last_runtime ( file_path ) file_name = os . path . basename ( file_path ) file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) if last_runtime : Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) processor_pid = self . get_pid ( file_path ) processor_start_time = self . get_start_time ( file_path ) runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) last_run = self . get_last_finish_time ( file_path ) if last_run : seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) # Sort by longest last runtime. (Can't sort None values in python3) rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) formatted_rows = [ ] for file_path , pid , runtime , last_runtime , last_run in rows : formatted_rows . append ( ( file_path , pid , \"{:.2f}s\" . format ( runtime ) if runtime else None , \"{:.2f}s\" . format ( last_runtime ) if last_runtime else None , last_run . strftime ( \"%Y-%m-%dT%H:%M:%S\" ) if last_run else None ) ) log_str = ( \"\\n\" + \"=\" * 80 + \"\\n\" + \"DAG File Processing Stats\\n\\n\" + tabulate ( formatted_rows , headers = headers ) + \"\\n\" + \"=\" * 80 ) self . log . info ( log_str )", "nl": "Print out stats about how files are getting processed ."}}
{"translation": {"code": "def _print_stat ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0 : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( )", "nl": "Occasionally print out stats about how fast the files are getting processed"}}
{"translation": {"code": "def _refresh_dag_dir ( self ) : elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) if elapsed_time_since_refresh > self . dag_dir_list_interval : # Build up a list of Python files that could contain DAGs self . log . info ( \"Searching for files in %s\" , self . _dag_directory ) self . _file_paths = list_py_file_paths ( self . _dag_directory ) self . last_dag_dir_refresh_time = timezone . utcnow ( ) self . log . info ( \"There are %s files in %s\" , len ( self . _file_paths ) , self . _dag_directory ) self . set_file_paths ( self . _file_paths ) try : self . log . debug ( \"Removing old import errors\" ) self . clear_nonexistent_import_errors ( ) except Exception : self . log . exception ( \"Error removing old import errors\" )", "nl": "Refresh file paths from dag dir if we haven t done it for too long ."}}
{"translation": {"code": "def start_in_sync ( self ) : while True : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) elif agent_signal == DagParsingSignal . AGENT_HEARTBEAT : self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) self . wait_until_finished ( ) self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) if max_runs_reached : self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) break", "nl": "Parse DAG files in a loop controlled by DagParsingSignal . Actual DAG parsing loop will run once upon receiving one agent heartbeat message and will report done when finished the loop ."}}
{"translation": {"code": "def start ( self ) : self . log . info ( \"Processing files using up to %s processes at a time \" , self . _parallelism ) self . log . info ( \"Process each file at most once every %s seconds\" , self . _file_process_interval ) self . log . info ( \"Checking for new files in %s every %s seconds\" , self . _dag_directory , self . dag_dir_list_interval ) if self . _async_mode : self . log . debug ( \"Starting DagFileProcessorManager in async mode\" ) self . start_in_async ( ) else : self . log . debug ( \"Starting DagFileProcessorManager in sync mode\" ) self . start_in_sync ( )", "nl": "Use multiple processes to parse and generate tasks for the DAGs in parallel . By processing them in separate processes we can get parallelism and isolation from potentially harmful user code ."}}
{"translation": {"code": "def _exit_gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( \"Finished terminating DAG processors.\" ) sys . exit ( os . EX_OK )", "nl": "Helper method to clean up DAG file processors to avoid leaving orphan processes ."}}
{"translation": {"code": "def terminate ( self ) : self . log . info ( \"Sending termination message to manager.\" ) self . _child_signal_conn . send ( DagParsingSignal . TERMINATE_MANAGER )", "nl": "Send termination signal to DAG parsing processor manager and expect it to terminate all DAG file processors ."}}
{"translation": {"code": "def start_in_async ( self ) : while True : loop_start_time = time . time ( ) if self . _signal_conn . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( \"Sleeping for %.2f seconds to prevent excessive logging\" , sleep_length ) time . sleep ( sleep_length )", "nl": "Parse DAG files repeatedly in a standalone loop ."}}
{"translation": {"code": "def replace_many ( self , mongo_collection , docs , filter_docs = None , mongo_db = None , upsert = False , collation = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_docs : filter_docs = [ { '_id' : doc [ '_id' ] } for doc in docs ] requests = [ ReplaceOne ( filter_docs [ i ] , docs [ i ] , upsert = upsert , collation = collation ) for i in range ( len ( docs ) ) ] return collection . bulk_write ( requests , * * kwargs )", "nl": "Replaces many documents in a mongo collection ."}}
{"translation": {"code": "def delete_instance ( self , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Deletes a Cloud SQL instance ."}}
{"translation": {"code": "def patch_instance ( self , body , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Updates settings of a Cloud SQL instance ."}}
{"translation": {"code": "def create_instance ( self , body , project_id = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Creates a new Cloud SQL instance ."}}
{"translation": {"code": "def get_instance ( self , instance , project_id = None ) : return self . get_conn ( ) . instances ( ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )", "nl": "Retrieves a resource containing information about a Cloud SQL instance ."}}
{"translation": {"code": "def load_file_obj ( self , file_obj , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = \"AES256\" client = self . get_conn ( ) client . upload_fileobj ( file_obj , bucket_name , key , ExtraArgs = extra_args )", "nl": "Loads a file object to S3"}}
{"translation": {"code": "def check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : sec = 0 description = self . describe_training_job ( job_name ) self . log . info ( secondary_training_status_message ( description , None ) ) instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] status = description [ 'TrainingJobStatus' ] stream_names = [ ] # The list of log streams positions = { } # The current position in each stream, map of stream name -> position job_already_completed = status not in non_terminal_states state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE # The loop below implements a state machine that alternates between checking the job status and # reading whatever is available in the logs at this point. Note, that if we were called with # wait_for_completion == False, we never check the job status. # # If wait_for_completion == TRUE and job is not completed, the initial state is TAILING # If wait_for_completion == FALSE, the initial state is COMPLETE # (doesn't matter if the job really is complete). # # The state table: # # STATE               ACTIONS                        CONDITION             NEW STATE # ----------------    ----------------               -----------------     ---------------- # TAILING             Read logs, Pause, Get status   Job complete          JOB_COMPLETE #                                                    Else                  TAILING # JOB_COMPLETE        Read logs, Pause               Any                   COMPLETE # COMPLETE            Read logs, Exit                                      N/A # # Notes: # - The JOB_COMPLETE state forces us to do an extra pause and read any items that # got to Cloudwatch after the job was marked complete. last_describe_job_call = time . time ( ) last_description = description while True : time . sleep ( check_interval ) sec = sec + check_interval state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) if state == LogState . COMPLETE : break if max_ingestion_time and sec > max_ingestion_time : # ensure that the job gets killed if the max ingestion time is exceeded raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) if wait_for_completion : status = last_description [ 'TrainingJobStatus' ] if status in failed_states : reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) )", "nl": "Display the logs for a given training job optionally tailing them until the job is complete ."}}
{"translation": {"code": "def describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : log_group = '/aws/sagemaker/TrainingJobs' if len ( stream_names ) < instance_count : # Log streams are created whenever a container starts writing to stdout/err, so this list # may be dynamic until we have a stream for every instance. logs_conn = self . get_log_conn ( ) try : streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) except logs_conn . exceptions . ResourceNotFoundException : # On the very first training job run on an account, there's no log group until # the container starts logging, so ignore any errors thrown about that pass if len ( stream_names ) > 0 : for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream_names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == LogState . COMPLETE : return state , last_description , last_describe_job_call if state == LogState . JOB_COMPLETE : state = LogState . COMPLETE elif time . time ( ) - last_describe_job_call >= 30 : description = self . describe_training_job ( job_name ) last_describe_job_call = time . time ( ) if secondary_training_status_changed ( description , last_description ) : self . log . info ( secondary_training_status_message ( description , last_description ) ) last_description = description status = description [ 'TrainingJobStatus' ] if status not in self . non_terminal_states : state = LogState . JOB_COMPLETE return state , last_description , last_describe_job_call", "nl": "Return the training job info associated with job_name and print CloudWatch logs"}}
{"translation": {"code": "def create_endpoint ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : response = self . get_conn ( ) . create_endpoint ( * * config ) if wait_for_completion : self . check_status ( config [ 'EndpointName' ] , 'EndpointStatus' , self . describe_endpoint , check_interval , max_ingestion_time , non_terminal_states = self . endpoint_non_terminal_states ) return response", "nl": "Create an endpoint"}}
{"translation": {"code": "def check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : if not non_terminal_states : non_terminal_states = self . non_terminal_states sec = 0 running = True while running : time . sleep ( check_interval ) sec = sec + check_interval try : response = describe_function ( job_name ) status = response [ key ] self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) except KeyError : raise AirflowException ( 'Could not get status of the SageMaker job' ) except ClientError : raise AirflowException ( 'AWS request failed, check logs for more info' ) if status in non_terminal_states : running = True elif status in self . failed_states : raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) else : running = False if max_ingestion_time and sec > max_ingestion_time : # ensure that the job gets killed if the max ingestion time is exceeded raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) self . log . info ( 'SageMaker Job Compeleted' ) response = describe_function ( job_name ) return response", "nl": "Check status of a SageMaker job"}}
{"translation": {"code": "def secondary_training_status_message ( job_description , prev_description ) : if job_description is None or job_description . get ( 'SecondaryStatusTransitions' ) is None or len ( job_description . get ( 'SecondaryStatusTransitions' ) ) == 0 : return '' prev_description_secondary_transitions = prev_description . get ( 'SecondaryStatusTransitions' ) if prev_description is not None else None prev_transitions_num = len ( prev_description [ 'SecondaryStatusTransitions' ] ) if prev_description_secondary_transitions is not None else 0 current_transitions = job_description [ 'SecondaryStatusTransitions' ] transitions_to_print = current_transitions [ - 1 : ] if len ( current_transitions ) == prev_transitions_num else current_transitions [ prev_transitions_num - len ( current_transitions ) : ] status_strs = [ ] for transition in transitions_to_print : message = transition [ 'StatusMessage' ] time_str = timezone . convert_to_utc ( job_description [ 'LastModifiedTime' ] ) . strftime ( '%Y-%m-%d %H:%M:%S' ) status_strs . append ( '{} {} - {}' . format ( time_str , transition [ 'Status' ] , message ) ) return '\\n' . join ( status_strs )", "nl": "Returns a string contains start time and the secondary training job status message ."}}
{"translation": {"code": "def secondary_training_status_changed ( current_job_description , prev_job_description ) : current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == 0 : return False prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None last_message = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and len ( prev_job_secondary_status_transitions ) > 0 else '' message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] return message != last_message", "nl": "Returns true if training job s secondary status message has changed ."}}
{"translation": {"code": "def create_bucket ( self , bucket_name , region_name = None ) : s3_conn = self . get_conn ( ) if not region_name : region_name = s3_conn . meta . region_name if region_name == 'us-east-1' : self . get_conn ( ) . create_bucket ( Bucket = bucket_name ) else : self . get_conn ( ) . create_bucket ( Bucket = bucket_name , CreateBucketConfiguration = { 'LocationConstraint' : region_name } )", "nl": "Creates an Amazon S3 bucket ."}}
{"translation": {"code": "def tar_and_s3_upload ( self , path , key , bucket ) : with tempfile . TemporaryFile ( ) as temp_file : if os . path . isdir ( path ) : files = [ os . path . join ( path , name ) for name in os . listdir ( path ) ] else : files = [ path ] with tarfile . open ( mode = 'w:gz' , fileobj = temp_file ) as tar_file : for f in files : tar_file . add ( f , arcname = os . path . basename ( f ) ) temp_file . seek ( 0 ) self . s3_hook . load_file_obj ( temp_file , key , bucket , replace = True )", "nl": "Tar the local file or directory and upload to s3"}}
{"translation": {"code": "def patch_database ( self , instance , database , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Updates a database resource inside a Cloud SQL instance ."}}
{"translation": {"code": "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries )", "nl": "Retrieves a database resource from a Cloud SQL instance ."}}
{"translation": {"code": "def delete_database ( self , instance , database , project_id = None ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Deletes a database from a Cloud SQL instance ."}}
{"translation": {"code": "def create_database ( self , instance , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Creates a new database inside a Cloud SQL instance ."}}
{"translation": {"code": "def publish_to_target ( self , target_arn , message ) : conn = self . get_conn ( ) messages = { 'default' : message } return conn . publish ( TargetArn = target_arn , Message = json . dumps ( messages ) , MessageStructure = 'json' )", "nl": "Publish a message to a topic or an endpoint ."}}
{"translation": {"code": "def has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = False ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) return len ( mail_attachments ) > 0", "nl": "Checks the mail folder for mails containing attachments with the given name ."}}
{"translation": {"code": "def retrieve_mail_attachments ( self , name , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) return mail_attachments", "nl": "Retrieves mail s attachments in the mail folder by its name ."}}
{"translation": {"code": "def get_attachments_by_name ( self , name , check_regex , find_first = False ) : attachments = [ ] for part in self . mail . walk ( ) : mail_part = MailPart ( part ) if mail_part . is_attachment ( ) : found_attachment = mail_part . has_matching_name ( name ) if check_regex else mail_part . has_equal_name ( name ) if found_attachment : file_name , file_payload = mail_part . get_file ( ) self . log . info ( 'Found attachment: {}' . format ( file_name ) ) attachments . append ( ( file_name , file_payload ) ) if find_first : break return attachments", "nl": "Gets all attachments by name for the mail ."}}
{"translation": {"code": "def get_file ( self ) : return self . part . get_filename ( ) , self . part . get_payload ( decode = True )", "nl": "Gets the file including name and payload ."}}
{"translation": {"code": "def download_mail_attachments ( self , name , local_output_directory , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) self . _create_files ( mail_attachments , local_output_directory )", "nl": "Downloads mail s attachments in the mail folder by its name to the local directory ."}}
{"translation": {"code": "def poke ( self , context ) : self . log . info ( 'Poking for %s' , self . attachment_name ) with ImapHook ( imap_conn_id = self . conn_id ) as imap_hook : return imap_hook . has_mail_attachment ( name = self . attachment_name , mail_folder = self . mail_folder , check_regex = self . check_regex )", "nl": "Pokes for a mail attachment on the mail server ."}}
{"translation": {"code": "def patch_instance_group_manager ( self , zone , resource_id , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceGroupManagers ( ) . patch ( project = project_id , zone = zone , instanceGroupManager = resource_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )", "nl": "Patches Instance Group Manager with the specified body . Must be called with keyword arguments rather than positional ."}}
{"translation": {"code": "def insert_instance_template ( self , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . insert ( project = project_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )", "nl": "Inserts instance template using body specified Must be called with keyword arguments rather than positional ."}}
{"translation": {"code": "def get_instance_group_manager ( self , zone , resource_id , project_id = None ) : response = self . get_conn ( ) . instanceGroupManagers ( ) . get ( project = project_id , zone = zone , instanceGroupManager = resource_id ) . execute ( num_retries = self . num_retries ) return response", "nl": "Retrieves Instance Group Manager by project_id zone and resource_id . Must be called with keyword arguments rather than positional ."}}
{"translation": {"code": "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None", "nl": "Returns version of the Cloud SQL Proxy ."}}
{"translation": {"code": "def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]", "nl": "Reserve free TCP port to be used by Cloud SQL Proxy"}}
{"translation": {"code": "def delete_connection ( self , session = None ) : self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( \"Connection was already deleted!\" )", "nl": "Delete the dynamically created connection from the Connection table ."}}
{"translation": {"code": "def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( \"Proxy runner can only be retrieved in case of use_proxy = True\" ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )", "nl": "Retrieve Cloud SQL Proxy runner . It is used to manage the proxy lifecycle per task ."}}
{"translation": {"code": "def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook", "nl": "Retrieve database hook . This is the actual Postgres or MySQL database hook that uses proxy or connects directly to the Google Cloud SQL database ."}}
{"translation": {"code": "def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : # Needed for python 2 compatibility (exists_ok missing) pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self . log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) if line != '' : self . log . info ( line ) if \"googleapi: Error\" in line or \"invalid instance name:\" in line : self . stop_proxy ( ) raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) if \"Ready for new connections\" in line : return", "nl": "Starts Cloud SQL Proxy ."}}
{"translation": {"code": "def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )", "nl": "Clean up database hook after it was used ."}}
{"translation": {"code": "def create_connection ( self , session = None ) : connection = Connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( \"Creating connection %s\" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add ( connection ) session . commit ( )", "nl": "Create connection in the Connection table according to whether it uses proxy TCP UNIX sockets SSL . Connection ID will be randomly generated ."}}
{"translation": {"code": "def stop_proxy ( self ) : if not self . sql_proxy_process : raise AirflowException ( \"The sql proxy is not started yet\" ) else : self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = None # Cleanup! self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) if self . sql_proxy_was_downloaded : self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) # Silently ignore if the file has already been removed (concurrency) try : os . remove ( self . sql_proxy_path ) except OSError as e : if not e . errno == errno . ENOENT : raise else : self . log . info ( \"Skipped removing proxy - it was not downloaded: %s\" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) # Here file cannot be delete by concurrent task (each task has its own copy) os . remove ( self . credentials_path )", "nl": "Stops running proxy ."}}
{"translation": {"code": "def poll_query_status ( self , query_execution_id , max_tries = None ) : try_number = 1 final_query_state = None # Query state when query reaches final state or max_tries reached while True : query_state = self . check_query_status ( query_execution_id ) if query_state is None : self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) elif query_state in self . INTERMEDIATE_STATES : self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) else : self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) final_query_state = query_state break if max_tries and try_number >= max_tries : # Break loop if max_tries reached final_query_state = query_state break try_number += 1 sleep ( self . sleep_time ) return final_query_state", "nl": "Poll the status of submitted athena query until query state reaches final state . Returns one of the final states"}}
{"translation": {"code": "def check_query_status ( self , query_execution_id ) : response = self . conn . get_query_execution ( QueryExecutionId = query_execution_id ) state = None try : state = response [ 'QueryExecution' ] [ 'Status' ] [ 'State' ] except Exception as ex : self . log . error ( 'Exception while getting query state' , ex ) finally : return state", "nl": "Fetch the status of submitted athena query . Returns None or one of valid query states ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . conn : self . conn = self . get_client_type ( 'athena' ) return self . conn", "nl": "check if aws conn exists already or create one and return it"}}
{"translation": {"code": "def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . query_execution_context [ 'Database' ] = self . database self . result_configuration [ 'OutputLocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in AWSAthenaHook . FAILURE_STATES : raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )", "nl": "Run Presto Query on Athena"}}
{"translation": {"code": "def run_query ( self , query , query_context , result_configuration , client_request_token = None ) : response = self . conn . start_query_execution ( QueryString = query , ClientRequestToken = client_request_token , QueryExecutionContext = query_context , ResultConfiguration = result_configuration ) query_execution_id = response [ 'QueryExecutionId' ] return query_execution_id", "nl": "Run Presto query on athena with provided config and return submitted query_execution_id"}}
{"translation": {"code": "def wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : while timeout > 0 : operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : return time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) timeout -= TIME_TO_SLEEP_IN_SECONDS raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" )", "nl": "Waits until the job reaches the expected state ."}}
{"translation": {"code": "def insert_all ( self , project_id , dataset_id , table_id , rows , ignore_unknown_values = False , skip_invalid_rows = False , fail_on_error = False ) : dataset_project_id = project_id if project_id else self . project_id body = { \"rows\" : rows , \"ignoreUnknownValues\" : ignore_unknown_values , \"kind\" : \"bigquery#tableDataInsertAllRequest\" , \"skipInvalidRows\" : skip_invalid_rows , } try : self . log . info ( 'Inserting %s row(s) into Table %s:%s.%s' , len ( rows ) , dataset_project_id , dataset_id , table_id ) resp = self . service . tabledata ( ) . insertAll ( projectId = dataset_project_id , datasetId = dataset_id , tableId = table_id , body = body ) . execute ( num_retries = self . num_retries ) if 'insertErrors' not in resp : self . log . info ( 'All row(s) inserted successfully: %s:%s.%s' , dataset_project_id , dataset_id , table_id ) else : error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}' . format ( len ( resp [ 'insertErrors' ] ) , dataset_project_id , dataset_id , table_id , resp [ 'insertErrors' ] ) if fail_on_error : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( error_msg ) ) self . log . info ( error_msg ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )", "nl": "Method to stream data into BigQuery one record at a time without needing to run a load job"}}
{"translation": {"code": "def _num_tasks_per_send_process ( self , to_send_count ) : return max ( 1 , int ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) )", "nl": "How many Celery tasks should each worker process send ."}}
{"translation": {"code": "def _change_state_for_tasks_failed_to_execute ( self , session ) : if self . executor . queued_tasks : TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date , # The TI.try_number will return raw try_number+1 since the # ti is not running. And we need to -1 to match the DB record. TI . _try_number == try_number - 1 , TI . state == State . QUEUED ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . keys ( ) ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) tis_to_set_to_scheduled = ( ti_query . with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_scheduled ) == 0 : session . commit ( ) return # set TIs to queued state for task_instance in tis_to_set_to_scheduled : task_instance . state = State . SCHEDULED task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) session . commit ( ) self . log . info ( \"Set the following tasks to scheduled state:\\n\\t%s\" , task_instance_str )", "nl": "If there are tasks left over in the executor we set them back to SCHEDULED to avoid creating hanging tasks ."}}
{"translation": {"code": "def construct_task_instance ( self , session = None , lock_for_update = False ) : TI = airflow . models . TaskInstance qry = session . query ( TI ) . filter ( TI . dag_id == self . _dag_id , TI . task_id == self . _task_id , TI . execution_date == self . _execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) return ti", "nl": "Construct a TaskInstance from the database based on the primary key"}}
{"translation": {"code": "def _make_intermediate_dirs ( sftp_client , remote_directory ) : if remote_directory == '/' : sftp_client . chdir ( '/' ) return if remote_directory == '' : return try : sftp_client . chdir ( remote_directory ) except IOError : dirname , basename = os . path . split ( remote_directory . rstrip ( '/' ) ) _make_intermediate_dirs ( sftp_client , dirname ) sftp_client . mkdir ( basename ) sftp_client . chdir ( basename ) return", "nl": "Create all the intermediate directories in a remote host"}}
{"translation": {"code": "def export_instance ( self , instance , body , project_id = None ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name ) except HttpError as ex : raise AirflowException ( 'Exporting instance {} failed: {}' . format ( instance , ex . content ) )", "nl": "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file ."}}
{"translation": {"code": "def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) # We need to check to see if this database already exists so we don't try # to create it twice existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } )", "nl": "Creates a new database in CosmosDB ."}}
{"translation": {"code": "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True", "nl": "Checks if a collection exists in CosmosDB ."}}
{"translation": {"code": "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) self . get_conn ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )", "nl": "Deletes an existing collection in the CosmosDB database ."}}
{"translation": {"code": "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot get a document without an id\" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None", "nl": "Get a document from an existing collection in the CosmosDB database ."}}
{"translation": {"code": "def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) # Query them in SQL query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None", "nl": "Get a list of documents from an existing collection in the CosmosDB database via SQL query ."}}
{"translation": {"code": "def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) # We need to check to see if this container already exists so we don't try # to create it twice existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : collection_name } ] } ) ) # Only create if we did not find it already existing if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } )", "nl": "Creates a new collection in the CosmosDB database ."}}
{"translation": {"code": "def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client # Initialize the Python Azure Cosmos DB client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client", "nl": "Return a cosmos db client ."}}
{"translation": {"code": "def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )", "nl": "Deletes an existing database in CosmosDB ."}}
{"translation": {"code": "def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True", "nl": "Checks if a database exists in CosmosDB ."}}
{"translation": {"code": "def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot delete a document without an id\" ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )", "nl": "Delete an existing document out of a collection in the CosmosDB database ."}}
{"translation": {"code": "def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( \"You cannot insert empty documents\" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents", "nl": "Insert a list of new documents into an existing collection in the CosmosDB database ."}}
{"translation": {"code": "def get_instance ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : return None return instance", "nl": "Gets information about a particular instance ."}}
{"translation": {"code": "def _apply_to_instance ( self , project_id , instance_id , configuration_name , node_count , display_name , func ) : # noinspection PyUnresolvedReferences instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id , configuration_name = configuration_name , node_count = node_count , display_name = display_name ) try : operation = func ( instance ) # type: Operation except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result )", "nl": "Invokes a method on a given instance by applying a specified Callable ."}}
{"translation": {"code": "def create_instance ( self , instance_id , configuration_name , node_count , display_name , project_id = None ) : self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . create ( ) )", "nl": "Creates a new Cloud Spanner instance ."}}
{"translation": {"code": "def update_instance ( self , instance_id , configuration_name , node_count , display_name , project_id = None ) : return self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . update ( ) )", "nl": "Updates an existing Cloud Spanner instance ."}}
{"translation": {"code": "def delete_instance ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id ) try : instance . delete ( ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e", "nl": "Deletes an existing Cloud Spanner instance ."}}
{"translation": {"code": "def create_database ( self , instance_id , database_id , ddl_statements , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id , ddl_statements = ddl_statements ) try : operation = database . create ( ) # type: Operation except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result ) return", "nl": "Creates a new database in Cloud Spanner ."}}
{"translation": {"code": "def _get_client ( self , project_id ) : if not self . _client : self . _client = Client ( project = project_id , credentials = self . _get_credentials ( ) ) return self . _client", "nl": "Provides a client for interacting with the Cloud Spanner API ."}}
{"translation": {"code": "def extra_dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( \"Failed parsing the json for conn_id %s\" , self . conn_id ) return obj", "nl": "Returns the extra property by deserializing json ."}}
{"translation": {"code": "def create_or_update ( self , resource_group , name , container_group ) : self . connection . container_groups . create_or_update ( resource_group , name , container_group )", "nl": "Create a new container group"}}
{"translation": {"code": "def delete ( self , resource_group , name ) : self . connection . container_groups . delete ( resource_group , name )", "nl": "Delete a container group"}}
{"translation": {"code": "def get_messages ( self , resource_group , name ) : instance_view = self . _get_instance_view ( resource_group , name ) return [ event . message for event in instance_view . events ]", "nl": "Get the messages of a container group"}}
{"translation": {"code": "def get_state_exitcode_details ( self , resource_group , name ) : current_state = self . _get_instance_view ( resource_group , name ) . current_state return ( current_state . state , current_state . exit_code , current_state . detail_status )", "nl": "Get the state and exitcode of a container group"}}
{"translation": {"code": "def exists ( self , resource_group , name ) : for container in self . connection . container_groups . list_by_resource_group ( resource_group ) : if container . name == name : return True return False", "nl": "Test if a container group exists"}}
{"translation": {"code": "def get_logs ( self , resource_group , name , tail = 1000 ) : logs = self . connection . container . list_logs ( resource_group , name , name , tail = tail ) return logs . content . splitlines ( True )", "nl": "Get the tail from logs of a container group"}}
{"translation": {"code": "def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except ValueError : return e", "nl": "Extract error code from ftp exception"}}
{"translation": {"code": "def render_log_filename ( ti , try_number , filename_template ) : filename_template , filename_jinja_template = parse_template_string ( filename_template ) if filename_jinja_template : jinja_context = ti . get_template_context ( ) jinja_context [ 'try_number' ] = try_number return filename_jinja_template . render ( * * jinja_context ) return filename_template . format ( dag_id = ti . dag_id , task_id = ti . task_id , execution_date = ti . execution_date . isoformat ( ) , try_number = try_number )", "nl": "Given task instance try_number filename_template return the rendered log filename"}}
{"translation": {"code": "def delete_instance ( self , instance_id , project_id = None ) : instance = self . get_instance ( instance_id = instance_id , project_id = project_id ) if instance : instance . delete ( ) else : self . log . info ( \"The instance '%s' does not exist in project '%s'. Exiting\" , instance_id , project_id )", "nl": "Deletes the specified Cloud Bigtable instance . Raises google . api_core . exceptions . NotFound if the Cloud Bigtable instance does not exist ."}}
{"translation": {"code": "def create_instance ( self , instance_id , main_cluster_id , main_cluster_zone , project_id = None , replica_cluster_id = None , replica_cluster_zone = None , instance_display_name = None , instance_type = enums . Instance . Type . TYPE_UNSPECIFIED , instance_labels = None , cluster_nodes = None , cluster_storage_type = enums . StorageType . STORAGE_TYPE_UNSPECIFIED , timeout = None ) : cluster_storage_type = enums . StorageType ( cluster_storage_type ) instance_type = enums . Instance . Type ( instance_type ) instance = Instance ( instance_id , self . _get_client ( project_id = project_id ) , instance_display_name , instance_type , instance_labels , ) clusters = [ instance . cluster ( main_cluster_id , main_cluster_zone , cluster_nodes , cluster_storage_type ) ] if replica_cluster_id and replica_cluster_zone : clusters . append ( instance . cluster ( replica_cluster_id , replica_cluster_zone , cluster_nodes , cluster_storage_type ) ) operation = instance . create ( clusters = clusters ) operation . result ( timeout ) return instance", "nl": "Creates new instance ."}}
{"translation": {"code": "def create_table ( instance , table_id , initial_split_keys = None , column_families = None ) : if column_families is None : column_families = { } if initial_split_keys is None : initial_split_keys = [ ] table = Table ( table_id , instance ) table . create ( initial_split_keys , column_families )", "nl": "Creates the specified Cloud Bigtable table . Raises google . api_core . exceptions . AlreadyExists if the table exists ."}}
{"translation": {"code": "def update_cluster ( instance , cluster_id , nodes ) : cluster = Cluster ( cluster_id , instance ) cluster . serve_nodes = nodes cluster . update ( )", "nl": "Updates number of nodes in the specified Cloud Bigtable cluster . Raises google . api_core . exceptions . NotFound if the cluster does not exist ."}}
{"translation": {"code": "def delete_table ( self , instance_id , table_id , project_id = None ) : table = self . get_instance ( instance_id = instance_id , project_id = project_id ) . table ( table_id = table_id ) table . delete ( )", "nl": "Deletes the specified table in Cloud Bigtable . Raises google . api_core . exceptions . NotFound if the table does not exist ."}}