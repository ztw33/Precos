{"translation": {"code": "def get_database ( self , instance_id , database_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : return None else : return database", "nl": "Retrieves a database in Cloud Spanner . If the database does not exist in the specified instance it returns None ."}}
{"translation": {"code": "def load_entrypoint_plugins ( entry_points , airflow_plugins ) : for entry_point in entry_points : log . debug ( 'Importing entry_point plugin %s' , entry_point . name ) plugin_obj = entry_point . load ( ) if is_valid_plugin ( plugin_obj , airflow_plugins ) : if callable ( getattr ( plugin_obj , 'on_load' , None ) ) : plugin_obj . on_load ( ) airflow_plugins . append ( plugin_obj ) return airflow_plugins", "nl": "Load AirflowPlugin subclasses from the entrypoints provided . The entry_point group should be airflow . plugins ."}}
{"translation": {"code": "def poke ( self , context ) : if '.' in self . table_name : self . database_name , self . table_name = self . table_name . split ( '.' ) self . log . info ( 'Poking for table %s. %s, expression %s' , self . database_name , self . table_name , self . expression ) return self . get_hook ( ) . check_for_partition ( self . database_name , self . table_name , self . expression )", "nl": "Checks for existence of the partition in the AWS Glue Catalog table"}}
{"translation": {"code": "def get_partitions ( self , database_name , table_name , expression = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'get_partitions' ) response = paginator . paginate ( DatabaseName = database_name , TableName = table_name , Expression = expression , PaginationConfig = config ) partitions = set ( ) for page in response : for p in page [ 'Partitions' ] : partitions . add ( tuple ( p [ 'Values' ] ) ) return partitions", "nl": "Retrieves the partition values for a table ."}}
{"translation": {"code": "def _check_task_id ( self , context ) : ti = context [ 'ti' ] celery_result = ti . xcom_pull ( task_ids = self . target_task_id ) return celery_result . ready ( )", "nl": "Gets the returned Celery result from the Airflow task ID provided to the sensor and returns True if the celery result has been finished execution ."}}
{"translation": {"code": "def fallback_to_default_project_id ( func ) : @ functools . wraps ( func ) def inner_wrapper ( self , * args , * * kwargs ) : if len ( args ) > 0 : raise AirflowException ( \"You must use keyword arguments in this methods rather than\" \" positional\" ) if 'project_id' in kwargs : kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) else : kwargs [ 'project_id' ] = self . _get_project_id ( None ) if not kwargs [ 'project_id' ] : raise AirflowException ( \"The project id must be passed either as \" \"keyword project_id parameter or as project_id extra \" \"in GCP connection definition. Both are not set!\" ) return func ( self , * args , * * kwargs ) return inner_wrapper", "nl": "Decorator that provides fallback for Google Cloud Platform project id . If the project is None it will be replaced with the project_id from the service account the Hook is authenticated with . Project id can be specified either via project_id kwarg or via first parameter in positional args ."}}
{"translation": {"code": "def create_y_axis ( self , name , label = None , format = None , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format if label : axis [ 'axisLabel' ] = \"'\" + label + \"'\" # Add new axis to list of axis self . axislist [ name ] = axis", "nl": "Create Y - axis"}}
{"translation": {"code": "def find_for_task_instance ( task_instance , session ) : TR = TaskReschedule return ( session . query ( TR ) . filter ( TR . dag_id == task_instance . dag_id , TR . task_id == task_instance . task_id , TR . execution_date == task_instance . execution_date , TR . try_number == task_instance . try_number ) . order_by ( asc ( TR . id ) ) . all ( ) )", "nl": "Returns all task reschedules for the task instance and try number in ascending order ."}}
{"translation": {"code": "def _sync_dag_view_permissions ( self , dag_id , access_control ) : def _get_or_create_dag_permission ( perm_name ) : dag_perm = self . find_permission_view_menu ( perm_name , dag_id ) if not dag_perm : self . log . info ( \"Creating new permission '%s' on view '%s'\" , perm_name , dag_id ) dag_perm = self . add_permission_view_menu ( perm_name , dag_id ) return dag_perm def _revoke_stale_permissions ( dag_view ) : existing_dag_perms = self . find_permissions_view_menu ( dag_view ) for perm in existing_dag_perms : non_admin_roles = [ role for role in perm . role if role . name != 'Admin' ] for role in non_admin_roles : target_perms_for_role = access_control . get ( role . name , { } ) if perm . permission . name not in target_perms_for_role : self . log . info ( \"Revoking '%s' on DAG '%s' for role '%s'\" , perm . permission , dag_id , role . name ) self . del_permission_role ( role , perm ) dag_view = self . find_view_menu ( dag_id ) if dag_view : _revoke_stale_permissions ( dag_view ) for rolename , perms in access_control . items ( ) : role = self . find_role ( rolename ) if not role : raise AirflowException ( \"The access_control mapping for DAG '{}' includes a role \" \"named '{}', but that role does not exist\" . format ( dag_id , rolename ) ) perms = set ( perms ) invalid_perms = perms - self . DAG_PERMS if invalid_perms : raise AirflowException ( \"The access_control map for DAG '{}' includes the following \" \"invalid permissions: {}; The set of valid permissions \" \"is: {}\" . format ( dag_id , ( perms - self . DAG_PERMS ) , self . DAG_PERMS ) ) for perm_name in perms : dag_perm = _get_or_create_dag_permission ( perm_name ) self . add_permission_role ( role , dag_perm )", "nl": "Set the access policy on the given DAG s ViewModel ."}}
{"translation": {"code": "def compose ( self , bucket_name , source_objects , destination_object ) : if not source_objects or not len ( source_objects ) : raise ValueError ( 'source_objects cannot be empty.' ) if not bucket_name or not destination_object : raise ValueError ( 'bucket_name and destination_object cannot be empty.' ) self . log . info ( \"Composing %s to %s in the bucket %s\" , source_objects , destination_object , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name ) destination_blob = bucket . blob ( destination_object ) destination_blob . compose ( sources = [ bucket . blob ( blob_name = source_object ) for source_object in source_objects ] ) self . log . info ( \"Completed successfully.\" )", "nl": "Composes a list of existing object into a new object in the same storage bucket_name"}}
{"translation": {"code": "def _get_security_context ( self ) : security_context = { } if self . kube_config . worker_run_as_user : security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user if self . kube_config . worker_fs_group : security_context [ 'fsGroup' ] = self . kube_config . worker_fs_group # set fs_group to 65533 if not explicitly specified and using git ssh keypair auth if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : security_context [ 'fsGroup' ] = 65533 return security_context", "nl": "Defines the security context"}}
{"translation": {"code": "def poke ( self , context ) : self . log . info ( 'RedisPubSubSensor checking for message on channels: %s' , self . channels ) message = self . pubsub . get_message ( ) self . log . info ( 'Message %s from channel %s' , message , self . channels ) # Process only message types if message and message [ 'type' ] == 'message' : context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) self . pubsub . unsubscribe ( self . channels ) return True return False", "nl": "Check for message on subscribed channels and write to xcom the message with key message"}}
{"translation": {"code": "def _get_field ( self , field_name , default = None ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default", "nl": "Fetches a field from extras and returns it . This is some Airflow magic . The grpc hook type adds custom UI elements to the hook page which allow admins to specify scopes credential pem files etc . They get formatted as shown below ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _client : self . _client = Client ( credentials = self . _get_credentials ( ) ) return self . _client", "nl": "Retrieves connection to Cloud Translate"}}
{"translation": {"code": "def get_table_location ( self , database_name , table_name ) : table = self . get_table ( database_name , table_name ) return table [ 'StorageDescriptor' ] [ 'Location' ]", "nl": "Get the physical location of the table"}}
{"translation": {"code": "def get_table ( self , database_name , table_name ) : result = self . get_conn ( ) . get_table ( DatabaseName = database_name , Name = table_name ) return result [ 'Table' ]", "nl": "Get the information of the table"}}
{"translation": {"code": "def _convert_date_to_dict ( field_date ) : return { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year }", "nl": "Convert native python datetime . date object to a format supported by the API"}}
{"translation": {"code": "def _convert_time_to_dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second }", "nl": "Convert native python datetime . time object to a format supported by the API"}}
{"translation": {"code": "def list_transfer_job ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , FILTER , FILTER_PROJECT_ID ) request = conn . transferJobs ( ) . list ( filter = json . dumps ( filter ) ) jobs = [ ] while request is not None : response = request . execute ( num_retries = self . num_retries ) jobs . extend ( response [ TRANSFER_JOBS ] ) request = conn . transferJobs ( ) . list_next ( previous_request = request , previous_response = response ) return jobs", "nl": "Lists long - running operations in Google Storage Transfer Service that match the specified filter ."}}
{"translation": {"code": "def resume_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . resume ( name = operation_name ) . execute ( num_retries = self . num_retries )", "nl": "Resumes an transfer operation in Google Storage Transfer Service ."}}
{"translation": {"code": "def create_transfer_job ( self , body ) : body = self . _inject_project_id ( body , BODY , PROJECT_ID ) return self . get_conn ( ) . transferJobs ( ) . create ( body = body ) . execute ( num_retries = self . num_retries )", "nl": "Creates a transfer job that runs periodically ."}}
{"translation": {"code": "def update_transfer_job ( self , job_name , body ) : body = self . _inject_project_id ( body , BODY , PROJECT_ID ) return ( self . get_conn ( ) . transferJobs ( ) . patch ( jobName = job_name , body = body ) . execute ( num_retries = self . num_retries ) )", "nl": "Updates a transfer job that runs periodically ."}}
{"translation": {"code": "def get_conn ( self ) : if not self . _client : self . _client = TextToSpeechClient ( credentials = self . _get_credentials ( ) ) return self . _client", "nl": "Retrieves connection to Cloud Text to Speech ."}}
{"translation": {"code": "def synthesize_speech ( self , input_data , voice , audio_config , retry = None , timeout = None ) : client = self . get_conn ( ) self . log . info ( \"Synthesizing input: %s\" % input_data ) return client . synthesize_speech ( input_ = input_data , voice = voice , audio_config = audio_config , retry = retry , timeout = timeout )", "nl": "Synthesizes text input"}}
{"translation": {"code": "def _set_unfinished_dag_runs_to_failed ( self , dag_runs , session = None ) : for dag_run in dag_runs : dag_run . update_state ( ) if dag_run . state not in State . finished ( ) : dag_run . set_state ( State . FAILED ) session . merge ( dag_run )", "nl": "Go through the dag_runs and update the state based on the task_instance state . Then set DAG runs that are not finished to failed ."}}
{"translation": {"code": "def _get_api_key ( self ) : conn = self . get_connection ( self . http_conn_id ) api_key = conn . password if not api_key : raise AirflowException ( 'Opsgenie API Key is required for this hook, ' 'please check your conn_id configuration.' ) return api_key", "nl": "Get Opsgenie api_key for creating alert"}}
{"translation": {"code": "def execute ( self , context ) : self . hook = OpsgenieAlertHook ( self . opsgenie_conn_id ) self . hook . execute ( self . _build_opsgenie_payload ( ) )", "nl": "Call the OpsgenieAlertHook to post message"}}
{"translation": {"code": "def _build_opsgenie_payload ( self ) : payload = { } for key in [ \"message\" , \"alias\" , \"description\" , \"responders\" , \"visibleTo\" , \"actions\" , \"tags\" , \"details\" , \"entity\" , \"source\" , \"priority\" , \"user\" , \"note\" ] : val = getattr ( self , key ) if val : payload [ key ] = val return payload", "nl": "Construct the Opsgenie JSON payload . All relevant parameters are combined here to a valid Opsgenie JSON payload ."}}
{"translation": {"code": "def chain ( * tasks ) : for up_task , down_task in zip ( tasks [ : - 1 ] , tasks [ 1 : ] ) : up_task . set_downstream ( down_task )", "nl": "Given a number of tasks builds a dependency chain ."}}
{"translation": {"code": "def classify_text ( self , document , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . classify_text ( document = document , retry = retry , timeout = timeout , metadata = metadata )", "nl": "Classifies a document into categories ."}}
{"translation": {"code": "def get_template_field ( env , fullname ) : modname , classname = fullname . rsplit ( \".\" , 1 ) try : with mock ( env . config . autodoc_mock_imports ) : mod = import_module ( modname ) except ImportError : raise RoleException ( \"Error loading %s module.\" % ( modname , ) ) clazz = getattr ( mod , classname ) if not clazz : raise RoleException ( \"Error finding %s class in %s module.\" % ( classname , modname ) ) template_fields = getattr ( clazz , \"template_fields\" ) if not template_fields : raise RoleException ( \"Could not find the template fields for %s class in %s module.\" % ( classname , modname ) ) return list ( template_fields )", "nl": "Gets template fields for specific operator class ."}}
{"translation": {"code": "def template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : text = utils . unescape ( text ) try : template_fields = get_template_field ( app . env , text ) except RoleException as e : msg = inliner . reporter . error ( \"invalid class name %s \\n%s\" % ( text , e , ) , line = lineno ) prb = inliner . problematic ( rawtext , rawtext , msg ) return [ prb ] , [ msg ] node = nodes . inline ( rawtext = rawtext ) for i , field in enumerate ( template_fields ) : if i != 0 : node += nodes . Text ( \", \" ) node += nodes . literal ( field , \"\" , nodes . Text ( field ) ) return [ node ] , [ ]", "nl": "A role that allows you to include a list of template fields in the middle of the text . This is especially useful when writing guides describing how to use the operator . The result is a list of fields where each field is shorted in the literal block ."}}
{"translation": {"code": "def analyze_entities ( self , document , encoding_type = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . analyze_entities ( document = document , encoding_type = encoding_type , retry = retry , timeout = timeout , metadata = metadata )", "nl": "Finds named entities in the text along with entity types salience mentions for each entity and other properties ."}}
{"translation": {"code": "def error ( self , session = None ) : self . log . error ( \"Recording the task instance as FAILED\" ) self . state = State . FAILED session . merge ( self ) session . commit ( )", "nl": "Forces the task instance s state to FAILED in the database ."}}
{"translation": {"code": "def refresh_from_db ( self , session = None , lock_for_update = False ) : TI = TaskInstance qry = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) if ti : self . state = ti . state self . start_date = ti . start_date self . end_date = ti . end_date # Get the raw value of try_number column, don't read through the # accessor here otherwise it will be incremeneted by one already. self . try_number = ti . _try_number self . max_tries = ti . max_tries self . hostname = ti . hostname self . pid = ti . pid self . executor_config = ti . executor_config else : self . state = None", "nl": "Refreshes the task instance from the database based on the primary key"}}
{"translation": {"code": "def open_slots ( self , session ) : from airflow . models . taskinstance import TaskInstance as TI # Avoid circular import used_slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in_ ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used_slots", "nl": "Returns the number of slots open at the moment"}}
{"translation": {"code": "def are_dependents_done ( self , session = None ) : task = self . task if not task . downstream_task_ids : return True ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance . execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) count = ti [ 0 ] [ 0 ] return count == len ( task . downstream_task_ids )", "nl": "Checks whether the dependents of this task instance have all succeeded . This is meant to be used by wait_for_downstream ."}}
{"translation": {"code": "def next_retry_datetime ( self ) : delay = self . task . retry_delay if self . task . retry_exponential_backoff : min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) # deterministic per task instance hash = int ( hashlib . sha1 ( \"{}#{}#{}#{}\" . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number) modded_hash = min_backoff + hash % min_backoff # timedelta has a maximum representable value. The exponentiation # here means this value can be exceeded after a certain number # of tries (around 50 if the initial delay is 1s, even fewer if # the delay is larger). Cap the value here before creating a # timedelta object so the operation doesn't fail. delay_backoff_in_seconds = min ( modded_hash , timedelta . max . total_seconds ( ) - 1 ) delay = timedelta ( seconds = delay_backoff_in_seconds ) if self . task . max_retry_delay : delay = min ( self . task . max_retry_delay , delay ) return self . end_date + delay", "nl": "Get datetime of the next retry if the task instance fails . For exponential backoff retry_delay is used as base and will be converted to seconds ."}}
{"translation": {"code": "def dagbag_report ( self ) : report = textwrap . dedent ( \"\"\"\\n\n        -------------------------------------------------------------------\n        DagBag loading stats for {dag_folder}\n        -------------------------------------------------------------------\n        Number of DAGs: {dag_num}\n        Total task number: {task_num}\n        DagBag parsing time: {duration}\n        {table}\n        \"\"\" ) stats = self . dagbag_stats return report . format ( dag_folder = self . dag_folder , duration = sum ( [ o . duration for o in stats ] ) , dag_num = sum ( [ o . dag_num for o in stats ] ) , task_num = sum ( [ o . task_num for o in stats ] ) , table = pprinttable ( stats ) , )", "nl": "Prints a report around DagBag loading stats"}}
{"translation": {"code": "def collect_dags ( self , dag_folder = None , only_if_updated = True , include_examples = configuration . conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'DAG_DISCOVERY_SAFE_MODE' ) ) : start_dttm = timezone . utcnow ( ) dag_folder = dag_folder or self . dag_folder # Used to store stats around DagBag processing stats = [ ] FileLoadStat = namedtuple ( 'FileLoadStat' , \"file duration dag_num task_num dags\" ) dag_folder = correct_maybe_zipped ( dag_folder ) for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : try : ts = timezone . utcnow ( ) found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) td = timezone . utcnow ( ) - ts td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000 ) stats . append ( FileLoadStat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] ) , ) ) except Exception as e : self . log . exception ( e ) Stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , 1 ) Stats . gauge ( 'dagbag_size' , len ( self . dags ) , 1 ) Stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , 1 ) self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = True )", "nl": "Given a file path or a folder this method looks for python modules imports them and adds them to the dagbag collection ."}}
{"translation": {"code": "def get_task_instances ( self , state = None , session = None ) : from airflow . models . taskinstance import TaskInstance # Avoid circular import tis = session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . execution_date == self . execution_date , ) if state : if isinstance ( state , six . string_types ) : tis = tis . filter ( TaskInstance . state == state ) else : # this is required to deal with NULL values if None in state : tis = tis . filter ( or_ ( TaskInstance . state . in_ ( state ) , TaskInstance . state . is_ ( None ) ) ) else : tis = tis . filter ( TaskInstance . state . in_ ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( TaskInstance . task_id . in_ ( self . dag . task_ids ) ) return tis . all ( )", "nl": "Returns the task instances for this dag run"}}
{"translation": {"code": "def get_last_dagrun ( dag_id , session , include_externally_triggered = False ) : DR = DagRun query = session . query ( DR ) . filter ( DR . dag_id == dag_id ) if not include_externally_triggered : query = query . filter ( DR . external_trigger == False ) # noqa query = query . order_by ( DR . execution_date . desc ( ) ) return query . first ( )", "nl": "Returns the last dag run for a dag None if there was none . Last dag run can be any type of run eg . scheduled or backfilled . Overridden DagRuns are ignored ."}}
{"translation": {"code": "def create_dagrun ( self , run_id , state , execution_date , start_date = None , external_trigger = False , conf = None , session = None ) : return self . get_dag ( ) . create_dagrun ( run_id = run_id , state = state , execution_date = execution_date , start_date = start_date , external_trigger = external_trigger , conf = conf , session = session )", "nl": "Creates a dag run from this dag including the tasks associated with this dag . Returns the dag run ."}}
{"translation": {"code": "def get_task_instance ( self , task_id , session = None ) : from airflow . models . taskinstance import TaskInstance # Avoid circular import TI = TaskInstance ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . execution_date == self . execution_date , TI . task_id == task_id ) . first ( ) return ti", "nl": "Returns the task instance specified by task_id for this dag run"}}
{"translation": {"code": "def get_previous_dagrun ( self , session = None ) : return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date < self . execution_date ) . order_by ( DagRun . execution_date . desc ( ) ) . first ( )", "nl": "The previous DagRun if there is one"}}
{"translation": {"code": "def kill_zombies ( self , zombies , session = None ) : from airflow . models . taskinstance import TaskInstance # Avoid circular import for zombie in zombies : if zombie . dag_id in self . dags : dag = self . dags [ zombie . dag_id ] if zombie . task_id in dag . task_ids : task = dag . get_task ( zombie . task_id ) ti = TaskInstance ( task , zombie . execution_date ) # Get properties needed for failure handling from SimpleTaskInstance. ti . start_date = zombie . start_date ti . end_date = zombie . end_date ti . try_number = zombie . try_number ti . state = zombie . state ti . test_mode = configuration . getboolean ( 'core' , 'unit_test_mode' ) ti . handle_failure ( \"{} detected as zombie\" . format ( ti ) , ti . test_mode , ti . get_template_context ( ) ) self . log . info ( 'Marked zombie job %s as %s' , ti , ti . state ) Stats . incr ( 'zombies_killed' ) session . commit ( )", "nl": "Fail given zombie tasks which are tasks that haven t had a heartbeat for too long in the current DagBag ."}}
{"translation": {"code": "def get_previous_scheduled_dagrun ( self , session = None ) : dag = self . get_dag ( ) return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == dag . previous_schedule ( self . execution_date ) ) . first ( )", "nl": "The previous SCHEDULED DagRun if there is one"}}
{"translation": {"code": "def update_state ( self , session = None ) : dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) self . log . debug ( \"Updating state for %s considering %s task(s)\" , self , len ( tis ) ) for ti in list ( tis ) : # skip in db? if ti . state == State . REMOVED : tis . remove ( ti ) else : ti . task = dag . get_task ( ti . task_id ) # pre-calculate # db is faster start_dttm = timezone . utcnow ( ) unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) # small speed up if unfinished_tasks and none_depends_on_past and none_task_concurrency : # todo: this can actually get pretty slow: one task costs between 0.01-015s no_dependencies_met = True for ut in unfinished_tasks : # We need to flag upstream and check for changes because upstream # failures/re-schedules can result in deadlock false positives old_state = ut . state deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True , ignore_in_retry_period = True , ignore_in_reschedule_period = True ) , session = session ) if deps_met or old_state != ut . current_state ( session = session ) : no_dependencies_met = False break duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 Stats . timing ( \"dagrun.dependency-check.{}\" . format ( self . dag_id ) , duration ) root_ids = [ t . task_id for t in dag . roots ] roots = [ t for t in tis if t . task_id in root_ids ] # if all roots finished and at least one failed, the run failed if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : self . log . info ( 'Marking run %s failed' , self ) self . set_state ( State . FAILED ) dag . handle_callback ( self , success = False , reason = 'task_failure' , session = session ) # if all roots succeeded and no unfinished tasks, the run succeeded elif not unfinished_tasks and all ( r . state in ( State . SUCCESS , State . SKIPPED ) for r in roots ) : self . log . info ( 'Marking run %s successful' , self ) self . set_state ( State . SUCCESS ) dag . handle_callback ( self , success = True , reason = 'success' , session = session ) # if *all tasks* are deadlocked, the run failed elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : self . log . info ( 'Deadlock; marking run %s failed' , self ) self . set_state ( State . FAILED ) dag . handle_callback ( self , success = False , reason = 'all_tasks_deadlocked' , session = session ) # finally, if the roots aren't done, the dag is still running else : self . set_state ( State . RUNNING ) self . _emit_duration_stats_for_finished_state ( ) # todo: determine we want to use with_for_update to make sure to lock the run session . merge ( self ) session . commit ( ) return self . state", "nl": "Determines the overall state of the DagRun based on the state of its TaskInstances ."}}
{"translation": {"code": "def get_extra_links ( self , operator , dttm ) : conn = BaseHook . get_connection ( operator . kwargs [ 'qubole_conn_id' ] ) if conn and conn . host : host = re . sub ( r'api$' , 'v2/analyze?command_id=' , conn . host ) else : host = 'https://api.qubole.com/v2/analyze?command_id=' ti = TaskInstance ( task = operator , execution_date = dttm ) qds_command_id = ti . xcom_pull ( task_ids = operator . task_id , key = 'qbol_cmd_id' ) url = host + str ( qds_command_id ) if qds_command_id else '' return url", "nl": "Get link to qubole command result page ."}}
{"translation": {"code": "def create_queue ( self , queue_name , attributes = None ) : return self . get_conn ( ) . create_queue ( QueueName = queue_name , Attributes = attributes or { } )", "nl": "Create queue using connection object"}}
{"translation": {"code": "def prepare_additional_parameters ( additional_properties , language_hints , web_detection_params ) : if language_hints is None and web_detection_params is None : return additional_properties if additional_properties is None : return { } merged_additional_parameters = deepcopy ( additional_properties ) if 'image_context' not in merged_additional_parameters : merged_additional_parameters [ 'image_context' ] = { } merged_additional_parameters [ 'image_context' ] [ 'language_hints' ] = merged_additional_parameters [ 'image_context' ] . get ( 'language_hints' , language_hints ) merged_additional_parameters [ 'image_context' ] [ 'web_detection_params' ] = merged_additional_parameters [ 'image_context' ] . get ( 'web_detection_params' , web_detection_params ) return merged_additional_parameters", "nl": "Creates additional_properties parameter based on language_hints web_detection_params and additional_properties parameters specified by the user"}}
{"translation": {"code": "def _query_mssql ( self ) : mssql = MsSqlHook ( mssql_conn_id = self . mssql_conn_id ) conn = mssql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor", "nl": "Queries MSSQL and returns a cursor of results ."}}
{"translation": {"code": "def annotate_video ( self , input_uri = None , input_content = None , features = None , video_context = None , output_uri = None , location = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) return client . annotate_video ( input_uri = input_uri , input_content = input_content , features = features , video_context = video_context , output_uri = output_uri , location_id = location , retry = retry , timeout = timeout , metadata = metadata , )", "nl": "Performs video annotation ."}}