{"translation": {"code": "def _get_report_id ( self ) : if self . watch : # Searches for the first occurence of the nextflow pipeline # file name in the .nextflow.log file pipeline_path = get_nextflow_filepath ( self . log_file ) # Get hash from the entire pipeline file pipeline_hash = hashlib . md5 ( ) with open ( pipeline_path , \"rb\" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : pipeline_hash . update ( chunk ) # Get hash from the current working dir and hostname workdir = os . getcwd ( ) . encode ( \"utf8\" ) hostname = socket . gethostname ( ) . encode ( \"utf8\" ) hardware_addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( ) else : with open ( self . report_file ) as fh : report_json = json . loads ( fh . read ( ) ) metadata = report_json [ \"data\" ] [ \"results\" ] [ 0 ] [ \"nfMetadata\" ] try : report_id = metadata [ \"scriptId\" ] + metadata [ \"sessionId\" ] except KeyError : raise eh . ReportError ( \"Incomplete or corrupt report JSON file \" \"missing the 'scriptId' and/or 'sessionId' \" \"metadata information\" ) return report_id", "nl": "Returns a hash of the reports JSON file"}}
{"translation": {"code": "def _parse_assembly ( self , assembly_file ) : # Temporary storage of sequence data seq_temp = [ ] # Id counter for contig that will serve as key in self.contigs contig_id = 0 # Initialize kmer coverage and header cov , header = None , None with open ( assembly_file ) as fh : logger . debug ( \"Starting iteration of assembly file: {}\" . format ( assembly_file ) ) for line in fh : # Skip empty lines if not line . strip ( ) : continue else : # Remove whitespace surrounding line for further processing line = line . strip ( ) if line . startswith ( \">\" ) : # If a sequence has already been populated, save the # previous contig information if seq_temp : # Use join() to convert string list into the full # contig string. This is generally much more efficient # than successively concatenating strings. seq = \"\" . join ( seq_temp ) logger . debug ( \"Populating contig with contig_id '{}', \" \"header '{}' and cov '{}'\" . format ( contig_id , header , cov ) ) self . _populate_contigs ( contig_id , header , cov , seq ) # Reset temporary sequence storage seq_temp = [ ] contig_id += 1 header = line [ 1 : ] cov = self . _parse_coverage ( line ) else : seq_temp . append ( line ) # Populate last contig entry logger . debug ( \"Populating contig with contig_id '{}', \" \"header '{}' and cov '{}'\" . format ( contig_id , header , cov ) ) seq = \"\" . join ( seq_temp ) self . _populate_contigs ( contig_id , header , cov , seq )", "nl": "Parse an assembly fasta file ."}}
{"translation": {"code": "def _get_gc_content ( sequence , length ) : # Get AT/GC/N counts at = sum ( map ( sequence . count , [ \"A\" , \"T\" ] ) ) gc = sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) n = length - ( at + gc ) # Get AT/GC/N proportions at_prop = at / length gc_prop = gc / length n_prop = n / length return { \"at\" : at , \"gc\" : gc , \"n\" : n , \"at_prop\" : at_prop , \"gc_prop\" : gc_prop , \"n_prop\" : n_prop }", "nl": "Get GC content and proportions ."}}
{"translation": {"code": "def write_report ( self , output_file ) : logger . debug ( \"Writing the assembly report into: {}\" . format ( output_file ) ) with open ( output_file , \"w\" ) as fh : for contig_id , vals in self . report . items ( ) : fh . write ( \"{}, {}\\\\n\" . format ( contig_id , vals ) )", "nl": "Writes a report with the test results for the current assembly"}}
{"translation": {"code": "def filter_contigs ( self , * comparisons ) : # Reset list of filtered ids self . filtered_ids = [ ] self . report = { } gc_filters = [ [ \"gc_prop\" , \">=\" , self . min_gc ] , [ \"gc_prop\" , \"<=\" , 1 - self . min_gc ] ] self . filters = list ( comparisons ) + gc_filters logger . debug ( \"Filtering contigs using filters: {}\" . format ( self . filters ) ) for contig_id , contig in self . contigs . items ( ) : for key , op , value in list ( comparisons ) + gc_filters : if not self . _test_truth ( contig [ key ] , op , value ) : self . filtered_ids . append ( contig_id ) self . report [ contig_id ] = \"{}/{}/{}\" . format ( key , contig [ key ] , value ) break else : self . report [ contig_id ] = \"pass\"", "nl": "Filters the contigs of the assembly according to user provided \\ comparisons ."}}
{"translation": {"code": "def _send_live_report ( self , report_id ) : # Determines the maximum number of reports sent at the same time in # the same payload buffer_size = 100 logger . debug ( \"Report buffer size set to: {}\" . format ( buffer_size ) ) for i in range ( 0 , len ( self . report_queue ) , buffer_size ) : # Reset the report compilation batch reports_compilation = [ ] # Iterate over report JSON batches determined by buffer_size for report in self . report_queue [ i : i + buffer_size ] : try : report_file = [ x for x in os . listdir ( report ) if x . endswith ( \".json\" ) ] [ 0 ] except IndexError : continue with open ( join ( report , report_file ) ) as fh : reports_compilation . append ( json . loads ( fh . read ( ) ) ) logger . debug ( \"Payload sent with size: {}\" . format ( asizeof ( json . dumps ( reports_compilation ) ) ) ) logger . debug ( \"status: {}\" . format ( self . status_info ) ) try : requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : reports_compilation , \"status\" : self . status_info } ) except requests . exceptions . ConnectionError : logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) sys . exit ( 1 ) # When there is no change in the report queue, but there is a change # in the run status of the pipeline if not self . report_queue : logger . debug ( \"status: {}\" . format ( self . status_info ) ) try : requests . put ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : [ ] , \"status\" : self . status_info } ) except requests . exceptions . ConnectionError : logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The\" \" server may be down or there is a problem with your \" \"internet connection.\" , \"red_bold\" ) ) sys . exit ( 1 ) # Reset the report queue after sending the request self . report_queue = [ ]", "nl": "Sends a PUT request with the report JSON files currently in the report_queue attribute ."}}
{"translation": {"code": "def update_trace_watch ( self ) : # Check the size stamp of the tracefile. Only proceed with the parsing # if it changed from the previous size. size_stamp = os . path . getsize ( self . trace_file ) self . trace_retry = 0 if size_stamp and size_stamp == self . trace_sizestamp : return else : logger . debug ( \"Updating trace size stamp to: {}\" . format ( size_stamp ) ) self . trace_sizestamp = size_stamp with open ( self . trace_file ) as fh : # Skip potential empty lines at the start of file header = next ( fh ) . strip ( ) while not header : header = next ( fh ) . strip ( ) # Get header mappings before parsing the file hm = self . _header_mapping ( header ) for line in fh : # Skip empty lines if line . strip ( ) == \"\" : continue fields = line . strip ( ) . split ( \"\\t\" ) # Skip if task ID was already processes if fields [ hm [ \"task_id\" ] ] in self . stored_ids : continue if fields [ hm [ \"process\" ] ] == \"report\" : self . report_queue . append ( self . _expand_path ( fields [ hm [ \"hash\" ] ] ) ) self . send = True # Add the processed trace line to the stored ids. It will be # skipped in future parsers self . stored_ids . append ( fields [ hm [ \"task_id\" ] ] )", "nl": "Parses the nextflow trace file and retrieves the path of report JSON files that have not been sent to the service yet ."}}
{"translation": {"code": "def _init_live_reports ( self , report_id ) : logger . debug ( \"Sending initial POST request to {} to start report live\" \" update\" . format ( self . broadcast_address ) ) try : with open ( \".metadata.json\" ) as fh : metadata = [ json . load ( fh ) ] except : metadata = [ ] start_json = { \"data\" : { \"results\" : metadata } } try : requests . post ( self . broadcast_address , json = { \"run_id\" : report_id , \"report_json\" : start_json , \"status\" : self . status_info } ) except requests . exceptions . ConnectionError : logger . error ( colored_print ( \"ERROR: Could not establish connection with server. The server\" \" may be down or there is a problem with your internet \" \"connection.\" , \"red_bold\" ) ) sys . exit ( 1 )", "nl": "Sends a POST request to initialize the live reports"}}
{"translation": {"code": "def main ( newick ) : logger . info ( \"Starting newick file processing\" ) print ( newick ) tree = dendropy . Tree . get ( file = open ( newick , 'r' ) , schema = \"newick\" ) tree . reroot_at_midpoint ( ) to_write = tree . as_string ( \"newick\" ) . strip ( ) . replace ( \"[&R] \" , '' ) . replace ( ' ' , '_' ) . replace ( \"'\" , \"\" ) with open ( \".report.json\" , \"w\" ) as json_report : json_dic = { \"treeData\" : [ { \"trees\" : [ to_write ] } ] , } json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) with open ( \".status\" , \"w\" ) as status_fh : status_fh . write ( \"pass\" )", "nl": "Main executor of the process_newick template ."}}
{"translation": {"code": "def brew_recipe ( recipe_name ) : # This will iterate over all modules included in the recipes subpackage # It will return the import class and the module name, algon with the # correct prefix prefix = \"{}.\" . format ( recipes . __name__ ) for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : # Import the current module _module = importer . find_module ( modname ) . load_module ( modname ) # Fetch all available classes in module _recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) ] # Iterate over each Recipe class, and check for a match with the # provided recipe name. for cls in _recipe_classes : # Create instance of class to allow fetching the name attribute recipe_cls = cls ( ) if getattr ( recipe_cls , \"name\" , None ) == recipe_name : return recipe_cls . brew ( ) logger . error ( colored_print ( \"Recipe name '{}' does not exist.\" . format ( recipe_name ) ) ) sys . exit ( 1 )", "nl": "Returns a pipeline string from a recipe name ."}}
{"translation": {"code": "def list_recipes ( full = False ) : logger . info ( colored_print ( \"\\n===== L I S T   O F   R E C I P E S =====\\n\" , \"green_bold\" ) ) # This will iterate over all modules included in the recipes subpackage # It will return the import class and the module name, algon with the # correct prefix prefix = \"{}.\" . format ( recipes . __name__ ) for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : # Import the current module _module = importer . find_module ( modname ) . load_module ( modname ) # Fetch all available classes in module _recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if isinstance ( cls , type ) ] # Iterate over each Recipe class, and check for a match with the # provided recipe name. for cls in _recipe_classes : recipe_cls = cls ( ) if hasattr ( recipe_cls , \"name\" ) : logger . info ( colored_print ( \"=> {}\" . format ( recipe_cls . name ) , \"blue_bold\" ) ) if full : logger . info ( colored_print ( \"\\t {}\" . format ( recipe_cls . __doc__ ) , \"purple_bold\" ) ) logger . info ( colored_print ( \"Pipeline string: {}\\n\" . format ( recipe_cls . pipeline_str ) , \"yellow_bold\" ) ) sys . exit ( 0 )", "nl": "Method that iterates over all available recipes and prints their information to the standard output"}}
{"translation": {"code": "def export_directives ( self ) : directives_json = { } # Skip first init process for p in self . processes [ 1 : ] : directives_json [ p . template ] = p . directives # Flush params json to stdout sys . stdout . write ( json . dumps ( directives_json ) )", "nl": "Export pipeline directives as a JSON to stdout"}}
{"translation": {"code": "def remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) : # Replaces the unique identifiers by the original process names for index , val in enumerate ( pipeline_links ) : if val [ \"input\" ] [ \"process\" ] != \"__init__\" : val [ \"input\" ] [ \"process\" ] = identifiers_to_tags [ val [ \"input\" ] [ \"process\" ] ] if val [ \"output\" ] [ \"process\" ] != \"__init__\" : val [ \"output\" ] [ \"process\" ] = identifiers_to_tags [ val [ \"output\" ] [ \"process\" ] ] return pipeline_links", "nl": "Removes unique identifiers and add the original process names to the already parsed pipelines"}}
{"translation": {"code": "def add_unique_identifiers ( pipeline_str ) : # Add space at beginning and end of pipeline to allow regex mapping of final # process in linear pipelines pipeline_str_modified = \" {} \" . format ( pipeline_str ) # Regex to get all process names. Catch all words without spaces and that # are not fork tokens or pipes reg_find_proc = r\"[^\\s{}{}{}]+\" . format ( LANE_TOKEN , FORK_TOKEN , CLOSE_TOKEN ) process_names = re . findall ( reg_find_proc , pipeline_str_modified ) identifiers_to_tags = { } \"\"\"\n    dict: Matches new process names (identifiers) with original process \n    names\n    \"\"\" new_process_names = [ ] \"\"\"\n    list: New process names used to replace in the pipeline string\n    \"\"\" # Assigns the new process names by appending a numeric id at the end of # the process name for index , val in enumerate ( process_names ) : if \"=\" in val : parts = val . split ( \"=\" ) new_id = \"{}_{}={}\" . format ( parts [ 0 ] , index , parts [ 1 ] ) else : new_id = \"{}_{}\" . format ( val , index ) # add new process with id new_process_names . append ( new_id ) # makes a match between new process name and original process name identifiers_to_tags [ new_id ] = val # Add space between forks, pipes and the process names for the replace # regex to work match_result = lambda match : \" {} \" . format ( match . group ( ) ) # force to add a space between each token so that regex modification can # be applied find = r'[{}{}{}]+' . format ( FORK_TOKEN , LANE_TOKEN , CLOSE_TOKEN ) pipeline_str_modified = re . sub ( find , match_result , pipeline_str_modified ) # Replace original process names by the unique identifiers for index , val in enumerate ( process_names ) : # regex to replace process names with non assigned process ids # escape characters are required to match to the dict keys # (identifiers_to_tags), since python keys with escape characters # must be escaped find = r'{}[^_]' . format ( val ) . replace ( \"\\\\\" , \"\\\\\\\\\" ) pipeline_str_modified = re . sub ( find , new_process_names [ index ] + \" \" , pipeline_str_modified , 1 ) return pipeline_str_modified , identifiers_to_tags", "nl": "Returns the pipeline string with unique identifiers and a dictionary with references between the unique keys and the original values"}}
{"translation": {"code": "def fetch_docker_tags ( self ) : # dict to store the already parsed components (useful when forks are # given to the pipeline string via -t flag dict_of_parsed = { } # fetches terminal width and subtracts 3 because we always add a # new line character and we want a space at the beggining and at the end # of each line terminal_width = shutil . get_terminal_size ( ) . columns - 3 # first header center_string = \" Selected container tags \" # starts a list with the headers tags_list = [ [ \"=\" * int ( terminal_width / 4 ) , \"{0}{1}{0}\" . format ( \"=\" * int ( ( ( terminal_width / 2 - len ( center_string ) ) / 2 ) ) , center_string ) , \"{}\\n\" . format ( \"=\" * int ( terminal_width / 4 ) ) ] , [ \"component\" , \"container\" , \"tags\" ] , [ \"=\" * int ( terminal_width / 4 ) , \"=\" * int ( terminal_width / 2 ) , \"=\" * int ( terminal_width / 4 ) ] ] # Skip first init process and iterate through the others for p in self . processes [ 1 : ] : template = p . template # if component has already been printed then skip and don't print # again if template in dict_of_parsed : continue # starts a list of  containers for the current process in # dict_of_parsed, in which each containers will be added to this # list once it gets parsed dict_of_parsed [ template ] = { \"container\" : [ ] } # fetch repo name from directives of each component. for directives in p . directives . values ( ) : try : repo = directives [ \"container\" ] default_version = directives [ \"version\" ] except KeyError : # adds the default container if container key isn't present # this happens for instance in integrity_coverage repo = \"flowcraft/flowcraft_base\" default_version = \"1.0.0-1\" # checks if repo_version already exists in list of the # containers for the current component being queried repo_version = repo + default_version if repo_version not in dict_of_parsed [ template ] [ \"container\" ] : # make the request to docker hub r = requests . get ( \"https://hub.docker.com/v2/repositories/{}/tags/\" . format ( repo ) ) # checks the status code of the request, if it is 200 then # parses docker hub entry, otherwise retrieve no tags but # alerts the user if r . status_code != 404 : # parse response content to dict and fetch results key r_content = json . loads ( r . content ) [ \"results\" ] for version in r_content : printed_version = ( version [ \"name\" ] + \"*\" ) if version [ \"name\" ] == default_version else version [ \"name\" ] tags_list . append ( [ template , repo , printed_version ] ) else : tags_list . append ( [ template , repo , \"No DockerHub tags\" ] ) dict_of_parsed [ template ] [ \"container\" ] . append ( repo_version ) # iterate through each entry in tags_list and print the list of tags # for each component. Each entry (excluding the headers) contains # 3 elements (component name, container and tag version) for x , entry in enumerate ( tags_list ) : # adds different color to the header in the first list and # if row is pair add one color and if is even add another (different # background) color = \"blue_bold\" if x < 3 else ( \"white\" if x % 2 != 0 else \"0;37;40m\" ) # generates a small list with the terminal width for each column, # this will be given to string formatting as the 3, 4 and 5 element final_width = [ int ( terminal_width / 4 ) , int ( terminal_width / 2 ) , int ( terminal_width / 4 ) ] # writes the string to the stdout sys . stdout . write ( colored_print ( \"\\n {0: <{3}} {1: ^{4}} {2: >{5}}\" . format ( * entry , * final_width ) , color ) ) # assures that the entire line gets the same color sys . stdout . write ( \"\\n{0: >{1}}\\n\" . format ( \"(* = default)\" , terminal_width + 3 ) )", "nl": "Export all dockerhub tags associated with each component given by the - t flag ."}}