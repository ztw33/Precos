{"translation": {"code": "def set_compiler_channels ( self , channel_list , operator = \"mix\" ) : if not channel_list : raise eh . ProcessError ( \"At least one status channel must be \" \"provided to include this process in the \" \"pipeline\" ) if len ( channel_list ) == 1 : logger . debug ( \"Setting only one status channel: {}\" . format ( channel_list [ 0 ] ) ) self . _context = { \"compile_channels\" : channel_list [ 0 ] } else : first_status = channel_list [ 0 ] if operator == \"mix\" : lst = \",\" . join ( channel_list [ 1 : ] ) s = \"{}.mix({})\" . format ( first_status , lst ) elif operator == \"join\" : s = first_status for ch in channel_list [ 1 : ] : s += \".join({})\" . format ( ch ) s += \".map{ ot -> [ ot[0], ot[1..-1] ] }\" logger . debug ( \"Status channel string: {}\" . format ( s ) ) self . _context = { \"compile_channels\" : s }", "nl": "General method for setting the input channels for the status process"}}
{"translation": {"code": "def set_secondary_channel ( self , source , channel_list ) : logger . debug ( \"Setting secondary channel for source '{}': {}\" . format ( source , channel_list ) ) source = \"{}_{}\" . format ( source , self . pid ) # Removes possible duplicate channels, when the fork is terminal channel_list = sorted ( list ( set ( channel_list ) ) ) # When there is only one channel to fork into, use the 'set' operator # instead of 'into' op = \"set\" if len ( channel_list ) == 1 else \"into\" self . forks . append ( \"\\n{}.{}{{ {} }}\\n\" . format ( source , op , \";\" . join ( channel_list ) ) ) logger . debug ( \"Setting forks attribute to: {}\" . format ( self . forks ) ) self . _context = { * * self . _context , * * { \"forks\" : \"\\n\" . join ( self . forks ) } }", "nl": "General purpose method for setting a secondary channel"}}
{"translation": {"code": "def update_main_forks ( self , sink ) : if not self . main_forks : self . main_forks = [ self . output_channel ] self . output_channel = \"_{}\" . format ( self . output_channel ) self . main_forks . append ( sink ) # fork_lst = self.forks + self.main_forks operator = \"set\" if len ( self . main_forks ) == 1 else \"into\" self . forks = [ \"\\n{}.{}{{ {} }}\\n\" . format ( self . output_channel , operator , \";\" . join ( self . main_forks ) ) ] self . _context = { * * self . _context , * * { \"forks\" : \"\" . join ( self . forks ) , \"output_channel\" : self . output_channel } }", "nl": "Updates the forks attribute with the sink channel destination"}}
{"translation": {"code": "def set_channels ( self , * * kwargs ) : if not self . pid : self . pid = \"{}_{}\" . format ( self . lane , kwargs . get ( \"pid\" ) ) for i in self . status_channels : if i . startswith ( \"STATUS_\" ) : self . status_strs . append ( \"{}_{}\" . format ( i , self . pid ) ) else : self . status_strs . append ( \"STATUS_{}_{}\" . format ( i , self . pid ) ) if self . main_forks : logger . debug ( \"Setting main fork channels: {}\" . format ( self . main_forks ) ) operator = \"set\" if len ( self . main_forks ) == 1 else \"into\" self . forks = [ \"\\n{}.{}{{ {} }}\\n\" . format ( self . output_channel , operator , \";\" . join ( self . main_forks ) ) ] self . _context = { * * kwargs , * * { \"input_channel\" : self . input_channel , \"output_channel\" : self . output_channel , \"template\" : self . template , \"forks\" : \"\\n\" . join ( self . forks ) , \"pid\" : self . pid } }", "nl": "General purpose method that sets the main channels"}}
{"translation": {"code": "def template_str ( self ) : if not self . _context : raise eh . ProcessError ( \"Channels must be setup first using the \" \"set_channels method\" ) logger . debug ( \"Setting context for template {}: {}\" . format ( self . template , self . _context ) ) x = self . render ( self . _template_path , self . _context ) return x", "nl": "Class property that returns a populated template string"}}
{"translation": {"code": "def render ( template , context ) : path , filename = os . path . split ( template ) return jinja2 . Environment ( loader = jinja2 . FileSystemLoader ( path or './' ) ) . get_template ( filename ) . render ( context )", "nl": "Wrapper to the jinja2 render method from a template file"}}
{"translation": {"code": "def set_raw_inputs ( self , raw_input ) : logger . debug ( \"Setting raw inputs using raw input dict: {}\" . format ( raw_input ) ) primary_inputs = [ ] for input_type , el in raw_input . items ( ) : primary_inputs . append ( el [ \"channel_str\" ] ) # Update the process' parameters with the raw input raw_channel = self . RAW_MAPPING [ input_type ] self . params [ input_type ] = { \"default\" : raw_channel [ \"default_value\" ] , \"description\" : raw_channel [ \"description\" ] } op = \"set\" if len ( el [ \"raw_forks\" ] ) == 1 else \"into\" self . forks . append ( \"\\n{}.{}{{ {} }}\\n\" . format ( el [ \"channel\" ] , op , \";\" . join ( el [ \"raw_forks\" ] ) ) ) logger . debug ( \"Setting raw inputs: {}\" . format ( primary_inputs ) ) logger . debug ( \"Setting forks attribute to: {}\" . format ( self . forks ) ) self . _context = { * * self . _context , * * { \"forks\" : \"\\n\" . join ( self . forks ) , \"main_inputs\" : \"\\n\" . join ( primary_inputs ) } }", "nl": "Sets the main input channels of the pipeline and their forks ."}}
{"translation": {"code": "def _set_template ( self , template ) : # Set template directory tpl_dir = join ( dirname ( abspath ( __file__ ) ) , \"templates\" ) # Set template file path tpl_path = join ( tpl_dir , template + \".nf\" ) if not os . path . exists ( tpl_path ) : raise eh . ProcessError ( \"Template {} does not exist\" . format ( tpl_path ) ) self . _template_path = join ( tpl_dir , template + \".nf\" )", "nl": "Sets the path to the appropriate jinja template file"}}
{"translation": {"code": "def set_secondary_inputs ( self , channel_dict ) : logger . debug ( \"Setting secondary inputs: {}\" . format ( channel_dict ) ) secondary_input_str = \"\\n\" . join ( list ( channel_dict . values ( ) ) ) self . _context = { * * self . _context , * * { \"secondary_inputs\" : secondary_input_str } }", "nl": "Adds secondary inputs to the start of the pipeline ."}}
{"translation": {"code": "def build ( self ) : logger . info ( colored_print ( \"\\tSuccessfully connected {} process(es) with {} \" \"fork(s) across {} lane(s) \\u2713\" . format ( len ( self . processes [ 1 : ] ) , len ( self . _fork_tree ) , self . lanes ) ) ) # Generate regular nextflow header that sets up the shebang, imports # and all possible initial channels self . _build_header ( ) self . _set_channels ( ) self . _set_init_process ( ) self . _set_secondary_channels ( ) logger . info ( colored_print ( \"\\tSuccessfully set {} secondary channel(s) \\u2713\" . format ( len ( self . secondary_channels ) ) ) ) self . _set_compiler_channels ( ) self . _set_configurations ( ) logger . info ( colored_print ( \"\\tFinished configurations \\u2713\" ) ) for p in self . processes : self . template += \"\\n{}\" . format ( p . template_str ) self . _build_footer ( ) project_root = dirname ( self . nf_file ) # Write configs self . write_configs ( project_root ) # Write pipeline file with open ( self . nf_file , \"w\" ) as fh : fh . write ( self . template ) logger . info ( colored_print ( \"\\tPipeline written into {} \\u2713\" . format ( self . nf_file ) ) )", "nl": "Main pipeline builder"}}
{"translation": {"code": "def fork_connection ( source , sink , source_lane , lane ) : logger . debug ( \"Establishing forking of source '{}' into processes\" \" '{}'. Source lane set to '{}' and lane set to '{}'\" . format ( source , sink , source_lane , lane ) ) res = [ ] # Increase the lane counter for the first lane lane_counter = lane + 1 for p in sink : res . append ( { \"input\" : { \"process\" : source , \"lane\" : source_lane } , \"output\" : { \"process\" : p , \"lane\" : lane_counter } } ) lane_counter += 1 return res", "nl": "Makes the connection between a process and the first processes in the lanes to which it forks ."}}
{"translation": {"code": "def linear_connection ( plist , lane ) : logger . debug ( \"Establishing linear connection with processes: {}\" . format ( plist ) ) res = [ ] previous = None for p in plist : # Skip first process if not previous : previous = p continue res . append ( { \"input\" : { \"process\" : previous , \"lane\" : lane } , \"output\" : { \"process\" : p , \"lane\" : lane } } ) previous = p return res", "nl": "Connects a linear list of processes into a list of dictionaries"}}
{"translation": {"code": "def get_lanes ( lanes_str ) : logger . debug ( \"Parsing lanes from raw string: {}\" . format ( lanes_str ) ) # Temporarily stores the lanes string after removal of nested forks parsed_lanes = \"\" # Flag used to determined whether the cursor is inside or outside the # right fork infork = 0 for i in lanes_str : # Nested fork started if i == FORK_TOKEN : infork += 1 # Nested fork stopped if i == CLOSE_TOKEN : infork -= 1 if infork < 0 : break # Save only when in the right fork if infork == 0 : # Ignore forking syntax tokens if i not in [ FORK_TOKEN , CLOSE_TOKEN ] : parsed_lanes += i return [ x . split ( ) for x in parsed_lanes . split ( LANE_TOKEN ) ]", "nl": "From a raw pipeline string get a list of lanes from the start of the current fork ."}}
{"translation": {"code": "def inner_fork_insanity_checks ( pipeline_string ) : # first lets get all forks to a list. list_of_forks = [ ] # stores forks left_indexes = [ ] # stores indexes of left brackets # iterate through the string looking for '(' and ')'. for pos , char in enumerate ( pipeline_string ) : if char == FORK_TOKEN : # saves pos to left_indexes list left_indexes . append ( pos ) elif char == CLOSE_TOKEN and len ( left_indexes ) > 0 : # saves fork to list_of_forks list_of_forks . append ( pipeline_string [ left_indexes [ - 1 ] + 1 : pos ] ) # removes last bracket from left_indexes list left_indexes = left_indexes [ : - 1 ] # sort list in descending order of number of forks list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = True ) # Now, we can iterate through list_of_forks and check for errors in each # fork for fork in list_of_forks : # remove inner forks for these checks since each fork has its own entry # in list_of_forks. Note that each fork is now sorted in descending # order which enables to remove sequentially the string for the fork # potentially with more inner forks for subfork in list_of_forks : # checks if subfork is contained in fork and if they are different, # avoiding to remove itself if subfork in list_of_forks and subfork != fork : # removes inner forks. Note that string has no spaces fork_simplified = fork . replace ( \"({})\" . format ( subfork ) , \"\" ) else : fork_simplified = fork # Checks if there is no fork separator character '|' within each fork if not len ( fork_simplified . split ( LANE_TOKEN ) ) > 1 : raise SanityError ( \"One of the forks doesn't have '|' \" \"separator between the processes to fork. This is\" \" the prime suspect: '({})'\" . format ( fork ) )", "nl": "This function performs two sanity checks in the pipeline string . The first check assures that each fork contains a lane token | while the second check looks for duplicated processes within the same fork ."}}
{"translation": {"code": "def insanity_checks ( pipeline_str ) : # Gets rid of all spaces in string p_string = pipeline_str . replace ( \" \" , \"\" ) . strip ( ) # some of the check functions use the pipeline_str as the user provided but # the majority uses the parsed p_string. checks = [ [ p_string , [ empty_tasks , brackets_but_no_lanes , brackets_insanity_check , lane_char_insanity_check , final_char_insanity_check , fork_procs_insanity_check , start_proc_insanity_check , late_proc_insanity_check ] ] , [ pipeline_str , [ inner_fork_insanity_checks ] ] ] # executes sanity checks in pipeline string before parsing it. for param , func_list in checks : for func in func_list : func ( param )", "nl": "Wrapper that performs all sanity checks on the pipeline string"}}
{"translation": {"code": "def _build_header ( self ) : logger . debug ( \"===============\" ) logger . debug ( \"Building header\" ) logger . debug ( \"===============\" ) self . template += hs . header", "nl": "Adds the header template to the master template string"}}
{"translation": {"code": "def colored_print ( msg , color_label = \"white_bold\" ) : if sys . stdout . encoding != \"UTF-8\" : msg = \"\" . join ( [ i if ord ( i ) < 128 else \"\" for i in msg ] ) # try except first looks for the color in COLORS dictionary, otherwise use # color_label as the color. try : col = COLORS [ color_label ] except KeyError : col = color_label return \"\\x1b[{}{}\\x1b[0m\" . format ( col , msg )", "nl": "This function enables users to add a color to the print . It also enables to pass end_char to print allowing to print several strings in the same line in different prints ."}}
{"translation": {"code": "def procs_dict_parser ( procs_dict ) : logger . info ( colored_print ( \"\\n===== L I S T   O F   P R O C E S S E S =====\\n\" , \"green_bold\" ) ) #Sort to print alphabetically ordered list of processes to ease reading procs_dict_ordered = { k : procs_dict [ k ] for k in sorted ( procs_dict ) } for template , dict_proc_info in procs_dict_ordered . items ( ) : template_str = \"=> {}\" . format ( template ) logger . info ( colored_print ( template_str , \"blue_bold\" ) ) for info in dict_proc_info : info_str = \"{}:\" . format ( info ) if isinstance ( dict_proc_info [ info ] , list ) : if not dict_proc_info [ info ] : arg_msg = \"None\" else : arg_msg = \", \" . join ( dict_proc_info [ info ] ) elif info == \"directives\" : # this is used for the \"directives\", which is a dict if not dict_proc_info [ info ] : # if dict is empty then add None to the message arg_msg = \"None\" else : # otherwise fetch all template names within a component # and all the directives for each template to a list list_msg = [ \"\\n      {}: {}\" . format ( templt , \" , \" . join ( [ \"{}: {}\" . format ( dr , val ) for dr , val in drs . items ( ) ] ) ) for templt , drs in dict_proc_info [ info ] . items ( ) ] # write list to a str arg_msg = \"\" . join ( list_msg ) else : arg_msg = dict_proc_info [ info ] logger . info ( \"   {} {}\" . format ( colored_print ( info_str , \"white_underline\" ) , arg_msg ) )", "nl": "This function handles the dictionary of attributes of each Process class to print to stdout lists of all the components or the components which the user specifies in the - t flag ."}}
{"translation": {"code": "def _set_channels ( self ) : logger . debug ( \"=====================\" ) logger . debug ( \"Setting main channels\" ) logger . debug ( \"=====================\" ) for i , p in enumerate ( self . processes ) : # Set main channels for the process logger . debug ( \"[{}] Setting main channels with pid: {}\" . format ( p . template , i ) ) p . set_channels ( pid = i ) # If there is no parent lane, set the raw input channel from user logger . debug ( \"{} {} {}\" . format ( p . parent_lane , p . input_type , p . template ) ) if not p . parent_lane and p . input_type : self . _update_raw_input ( p ) self . _update_extra_inputs ( p ) self . _update_secondary_channels ( p ) logger . info ( colored_print ( \"\\tChannels set for {} \\u2713\" . format ( p . template ) ) )", "nl": "Sets the main channels for the pipeline"}}
{"translation": {"code": "def get_user_channel ( self , input_channel , input_type = None ) : res = { \"input_channel\" : input_channel } itype = input_type if input_type else self . input_type if itype in self . RAW_MAPPING : channel_info = self . RAW_MAPPING [ itype ] return { * * res , * * channel_info }", "nl": "Returns the main raw channel for the process"}}
{"translation": {"code": "def set_main_channel_names ( self , input_suffix , output_suffix , lane ) : self . input_channel = \"{}_in_{}\" . format ( self . template , input_suffix ) self . output_channel = \"{}_out_{}\" . format ( self . template , output_suffix ) self . lane = lane", "nl": "Sets the main channel names based on the provide input and output channel suffixes . This is performed when connecting processes ."}}
{"translation": {"code": "def _set_init_process ( self ) : logger . debug ( \"========================\" ) logger . debug ( \"Setting secondary inputs\" ) logger . debug ( \"========================\" ) # Get init process init_process = self . processes [ 0 ] logger . debug ( \"Setting main raw inputs: \" \"{}\" . format ( self . main_raw_inputs ) ) init_process . set_raw_inputs ( self . main_raw_inputs ) logger . debug ( \"Setting extra inputs: {}\" . format ( self . extra_inputs ) ) init_process . set_extra_inputs ( self . extra_inputs )", "nl": "Sets the main raw inputs and secondary inputs on the init process"}}
{"translation": {"code": "def _set_secondary_channels ( self ) : logger . debug ( \"==========================\" ) logger . debug ( \"Setting secondary channels\" ) logger . debug ( \"==========================\" ) logger . debug ( \"Setting secondary channels: {}\" . format ( self . secondary_channels ) ) for source , lanes in self . secondary_channels . items ( ) : for vals in lanes . values ( ) : if not vals [ \"end\" ] : logger . debug ( \"[{}] No secondary links to setup\" . format ( vals [ \"p\" ] . template ) ) continue logger . debug ( \"[{}] Setting secondary links for \" \"source {}: {}\" . format ( vals [ \"p\" ] . template , source , vals [ \"end\" ] ) ) vals [ \"p\" ] . set_secondary_channel ( source , vals [ \"end\" ] )", "nl": "Sets the secondary channels for the pipeline"}}
{"translation": {"code": "def _set_status_channels ( self ) : status_inst = pc . StatusCompiler ( template = \"status_compiler\" ) report_inst = pc . ReportCompiler ( template = \"report_compiler\" ) # Compile status channels from pipeline process status_channels = [ ] for p in [ p for p in self . processes ] : if not any ( [ isinstance ( p , x ) for x in self . skip_class ] ) : status_channels . extend ( p . status_strs ) if not status_channels : logger . debug ( \"No status channels found. Skipping status compiler\" \"process\" ) return logger . debug ( \"Setting status channels: {}\" . format ( status_channels ) ) # Check for duplicate channels. Raise exception if found. if len ( status_channels ) != len ( set ( status_channels ) ) : raise eh . ProcessError ( \"Duplicate status channels detected. Please ensure that \" \"the 'status_channels' attributes of each process are \" \"unique. Here are the status channels:\\n\\n{}\" . format ( \", \" . join ( status_channels ) ) ) status_inst . set_compiler_channels ( status_channels ) report_channels = [ \"REPORT_{}\" . format ( x . lstrip ( \"STATUS_\" ) ) for x in status_channels ] report_inst . set_compiler_channels ( report_channels ) self . processes . extend ( [ status_inst , report_inst ] )", "nl": "Compiles all status channels for the status compiler process"}}
{"translation": {"code": "def proc_collector ( process_map , args , pipeline_string ) : arguments_list = [ ] # prints a detailed list of the process class arguments if args . detailed_list : # list of attributes to be passed to proc_collector arguments_list += [ \"input_type\" , \"output_type\" , \"description\" , \"dependencies\" , \"conflicts\" , \"directives\" ] # prints a short list with each process and the corresponding description if args . short_list : arguments_list += [ \"description\" ] if arguments_list : # dict to store only the required entries procs_dict = { } # loops between all process_map Processes for name , cls in process_map . items ( ) : # instantiates each Process class cls_inst = cls ( template = name ) # checks if recipe is provided if pipeline_string : if name not in pipeline_string : continue d = { arg_key : vars ( cls_inst ) [ arg_key ] for arg_key in vars ( cls_inst ) if arg_key in arguments_list } procs_dict [ name ] = d procs_dict_parser ( procs_dict ) sys . exit ( 0 )", "nl": "Function that collects all processes available and stores a dictionary of the required arguments of each process class to be passed to procs_dict_parser"}}
{"translation": {"code": "def parse_pipeline ( pipeline_str ) : if os . path . exists ( pipeline_str ) : logger . debug ( \"Found pipeline file: {}\" . format ( pipeline_str ) ) with open ( pipeline_str ) as fh : pipeline_str = \"\" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) logger . info ( colored_print ( \"Resulting pipeline string:\\n\" ) ) logger . info ( colored_print ( pipeline_str + \"\\n\" ) ) # Perform pipeline insanity checks insanity_checks ( pipeline_str ) logger . debug ( \"Parsing pipeline string: {}\" . format ( pipeline_str ) ) pipeline_links = [ ] lane = 1 # Add unique identifiers to each process to allow a correct connection # between forks with same processes pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( pipeline_str ) # Get number of forks in the pipeline nforks = pipeline_str_modified . count ( FORK_TOKEN ) logger . debug ( \"Found {} fork(s)\" . format ( nforks ) ) # If there are no forks, connect the pipeline as purely linear if not nforks : logger . debug ( \"Detected linear pipeline string : {}\" . format ( pipeline_str ) ) linear_pipeline = [ \"__init__\" ] + pipeline_str_modified . split ( ) pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) # Removes unique identifiers used for correctly assign fork parents with #  a possible same process name pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) return pipeline_links for i in range ( nforks ) : logger . debug ( \"Processing fork {} in lane {}\" . format ( i , lane ) ) # Split the pipeline at each fork start position. fields[-1] will # hold the process after the fork. fields[-2] will hold the processes # before the fork. fields = pipeline_str_modified . split ( FORK_TOKEN , i + 1 ) # Get the processes before the fork. This may be empty when the # fork is at the beginning of the pipeline. previous_process = fields [ - 2 ] . split ( LANE_TOKEN ) [ - 1 ] . split ( ) logger . debug ( \"Previous processes string: {}\" . format ( fields [ - 2 ] ) ) logger . debug ( \"Previous processes list: {}\" . format ( previous_process ) ) # Get lanes after the fork next_lanes = get_lanes ( fields [ - 1 ] ) logger . debug ( \"Next lanes object: {}\" . format ( next_lanes ) ) # Get the immediate targets of the fork fork_sink = [ x [ 0 ] for x in next_lanes ] logger . debug ( \"The fork sinks into the processes: {}\" . format ( fork_sink ) ) # The first fork is a special case, where the processes before AND # after the fork (until the start of another fork) are added to # the ``pipeline_links`` variable. Otherwise, only the processes # after the fork will be added if i == 0 : # If there are no previous process, the fork is at the beginning # of the pipeline string. In this case, inject the special # \"init\" process. if not previous_process : previous_process = [ \"__init__\" ] lane = 0 else : previous_process = [ \"__init__\" ] + previous_process # Add the linear modules before the fork pipeline_links . extend ( linear_connection ( previous_process , lane ) ) fork_source = previous_process [ - 1 ] logger . debug ( \"Fork source is set to: {}\" . format ( fork_source ) ) fork_lane = get_source_lane ( previous_process , pipeline_links ) logger . debug ( \"Fork lane is set to: {}\" . format ( fork_lane ) ) # Add the forking modules pipeline_links . extend ( fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) # Add the linear connections in the subsequent lanes pipeline_links . extend ( linear_lane_connection ( next_lanes , lane ) ) lane += len ( fork_sink ) pipeline_links = remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) return pipeline_links", "nl": "Parses a pipeline string into a list of dictionaries with the connections between processes"}}
{"translation": {"code": "def get_source_lane ( fork_process , pipeline_list ) : fork_source = fork_process [ - 1 ] fork_sig = [ x for x in fork_process if x != \"__init__\" ] for position , p in enumerate ( pipeline_list [ : : - 1 ] ) : if p [ \"output\" ] [ \"process\" ] == fork_source : lane = p [ \"output\" ] [ \"lane\" ] logger . debug ( \"Possible source match found in position {} in lane\" \" {}\" . format ( position , lane ) ) lane_sequence = [ x [ \"output\" ] [ \"process\" ] for x in pipeline_list if x [ \"output\" ] [ \"lane\" ] == lane ] logger . debug ( \"Testing lane sequence '{}' against fork signature\" \" '{}'\" . format ( lane_sequence , fork_sig ) ) if lane_sequence == fork_sig : return p [ \"output\" ] [ \"lane\" ] return 0", "nl": "Returns the lane of the last process that matches fork_process"}}
{"translation": {"code": "def run_auto_pipeline ( self , tasks ) : self . forks = self . define_pipeline_string ( self . process_descriptions , tasks , True , True , self . count_forks , tasks , self . forks ) self . pipeline_string = self . build_pipeline_string ( self . forks ) return self . pipeline_string", "nl": "Main method to run the automatic pipeline creation"}}
{"translation": {"code": "def build_downstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : if task in process_descriptions : if process_descriptions [ task ] [ 2 ] is not None : if len ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) ) > 1 : local_forks = process_descriptions [ task ] [ 2 ] . split ( \"|\" ) # Adds the process to the pipeline fragment downstream # and defines a new pipeline fragment for each fork. # Those will only look for downstream processes for local_fork in local_forks : if local_fork in total_tasks : count_forks += 1 task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) return task_pipeline else : if process_descriptions [ task ] [ 2 ] in total_tasks : task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] ) # Proceeds building downstream until the output for a # process is None self . build_downstream ( process_descriptions , process_descriptions [ task ] [ 2 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) return task_pipeline else : return task_pipeline", "nl": "Builds the downstream pipeline of the current process"}}
{"translation": {"code": "def build_upstream ( self , process_descriptions , task , all_tasks , task_pipeline , count_forks , total_tasks , forks ) : if task in process_descriptions : if process_descriptions [ task ] [ 1 ] is not None : if len ( process_descriptions [ task ] [ 1 ] . split ( \"|\" ) ) > 1 : local_forks = process_descriptions [ task ] [ 1 ] . split ( \"|\" ) # Produces a new pipeline fragment for each forkable #  process for local_fork in local_forks : if local_fork in total_tasks : count_forks += 1 task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] ) self . define_pipeline_string ( process_descriptions , local_fork , False , True , count_forks , total_tasks , forks ) return task_pipeline else : # Adds the process to the pipeline fragment in case it is # provided in the task list if process_descriptions [ task ] [ 1 ] in total_tasks : task_pipeline . insert ( 0 , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] ) # Proceeds building upstream until the input for a # process is None self . build_upstream ( process_descriptions , process_descriptions [ task ] [ 1 ] . split ( \"|\" ) [ 0 ] , all_tasks , task_pipeline , count_forks , total_tasks , forks ) else : logger . error ( colored_print ( \"{} not in provided protocols as \" \"input for {}\" . format ( process_descriptions [ task ] [ 1 ] , task ) , \"red_bold\" ) ) sys . exit ( ) return task_pipeline else : return task_pipeline", "nl": "Builds the upstream pipeline of the current process"}}
{"translation": {"code": "def validate_pipeline ( pipeline_string ) : if \"(\" in pipeline_string or \")\" in pipeline_string or \"|\" in pipeline_string : logger . error ( colored_print ( \"Please provide a valid task list!\" , \"red_bold\" ) ) return False return True", "nl": "Validate pipeline string"}}
{"translation": {"code": "def define_pipeline_string ( self , process_descriptions , tasks , check_upstream , check_downstream , count_forks , total_tasks , forks ) : tasks_array = tasks . split ( ) for task_unsplit in tasks_array : task = task_unsplit . split ( \"=\" ) [ 0 ] if task not in process_descriptions . keys ( ) : logger . error ( colored_print ( \"{} not in the possible processes\" . format ( task ) , \"red_bold\" ) ) sys . exit ( ) else : process_split = task_unsplit . split ( \"=\" ) if len ( process_split ) > 1 : self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] # Only uses the process if it is not already in the possible forks if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : task_pipeline = [ ] if task in process_descriptions : if check_upstream : task_pipeline = self . build_upstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) task_pipeline . append ( task ) if check_downstream : task_pipeline = self . build_downstream ( process_descriptions , task , tasks_array , task_pipeline , count_forks , total_tasks , forks ) # Adds the pipeline fragment to the list of possible forks forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) # Checks for task in fork. Case order of input processes is reversed elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : for fork in forks : if task not in fork : try : dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) fork . insert ( dependent_index , task ) except ValueError : continue for i in range ( 0 , len ( forks ) ) : for j in range ( 0 , len ( forks [ i ] ) ) : try : if len ( forks [ i ] [ j ] . split ( \"|\" ) ) > 1 : forks [ i ] [ j ] = forks [ i ] [ j ] . split ( \"|\" ) tmp_fork = [ ] for s in forks [ i ] [ j ] : if s in total_tasks : tmp_fork . append ( s ) forks [ i ] [ j ] = tmp_fork except AttributeError as e : continue return forks", "nl": "Builds the possible forks and connections between the provided processes"}}
{"translation": {"code": "def _get_resources_string ( res_dict , pid ) : config_str = \"\" ignore_directives = [ \"container\" , \"version\" ] for p , directives in res_dict . items ( ) : for d , val in directives . items ( ) : if d in ignore_directives : continue config_str += '\\n\\t${}_{}.{} = {}' . format ( p , pid , d , val ) return config_str", "nl": "Returns the nextflow resources string from a dictionary object"}}
{"translation": {"code": "def _get_container_string ( cont_dict , pid ) : config_str = \"\" for p , directives in cont_dict . items ( ) : container = \"\" if \"container\" in directives : container += directives [ \"container\" ] if \"version\" in directives : container += \":{}\" . format ( directives [ \"version\" ] ) else : container += \":latest\" if container : config_str += '\\n\\t${}_{}.container = \"{}\"' . format ( p , pid , container ) return config_str", "nl": "Returns the nextflow containers string from a dictionary object"}}
{"translation": {"code": "def _get_merged_params_string ( self ) : params_temp = { } for p in self . processes : logger . debug ( \"[{}] Adding parameters: {}\" . format ( p . template , p . params ) ) for param , val in p . params . items ( ) : params_temp [ param ] = val [ \"default\" ] config_str = \"\\n\\t\" + \"\\n\\t\" . join ( [ \"{} = {}\" . format ( param , val ) for param , val in params_temp . items ( ) ] ) return config_str", "nl": "Returns the merged nextflow params string from a dictionary object ."}}
{"translation": {"code": "def _set_configurations ( self ) : logger . debug ( \"======================\" ) logger . debug ( \"Setting configurations\" ) logger . debug ( \"======================\" ) resources = \"\" containers = \"\" params = \"\" manifest = \"\" if self . merge_params : params += self . _get_merged_params_string ( ) help_list = self . _get_merged_params_help ( ) else : params += self . _get_params_string ( ) help_list = self . _get_params_help ( ) for p in self . processes : # Skip processes with the directives attribute populated if not p . directives : continue logger . debug ( \"[{}] Adding directives: {}\" . format ( p . template , p . directives ) ) resources += self . _get_resources_string ( p . directives , p . pid ) containers += self . _get_container_string ( p . directives , p . pid ) manifest = self . _get_manifest_string ( ) self . resources = self . _render_config ( \"resources.config\" , { \"process_info\" : resources } ) self . containers = self . _render_config ( \"containers.config\" , { \"container_info\" : containers } ) self . params = self . _render_config ( \"params.config\" , { \"params_info\" : params } ) self . manifest = self . _render_config ( \"manifest.config\" , { \"manifest_info\" : manifest } ) self . help = self . _render_config ( \"Helper.groovy\" , { \"nf_file\" : basename ( self . nf_file ) , \"help_list\" : help_list , \"version\" : __version__ , \"pipeline_name\" : \" \" . join ( [ x . upper ( ) for x in self . pipeline_name ] ) } ) self . user_config = self . _render_config ( \"user.config\" , { } )", "nl": "This method will iterate over all process in the pipeline and populate the nextflow configuration files with the directives of each process in the pipeline ."}}
{"translation": {"code": "def _build_footer ( self ) : logger . debug ( \"===============\" ) logger . debug ( \"Building header\" ) logger . debug ( \"===============\" ) self . template += fs . footer", "nl": "Adds the footer template to the master template string"}}
{"translation": {"code": "def update_attributes ( self , attr_dict ) : # Update directives # Allowed attributes to write valid_directives = [ \"pid\" , \"ignore_type\" , \"ignore_pid\" , \"extra_input\" , \"group\" , \"input_type\" ] for attribute , val in attr_dict . items ( ) : # If the attribute has a valid directive key, update that # directive if attribute in valid_directives and hasattr ( self , attribute ) : setattr ( self , attribute , val ) # The params attribute is special, in the sense that it provides # information for the self.params attribute. elif attribute == \"params\" : for name , value in val . items ( ) : if name in self . params : self . params [ name ] [ \"default\" ] = value else : raise eh . ProcessError ( \"The parameter name '{}' does not exist for \" \"component '{}'\" . format ( name , self . template ) ) else : for p in self . directives : self . directives [ p ] [ attribute ] = val", "nl": "Updates the directives attribute from a dictionary object ."}}
{"translation": {"code": "def _parse_process_name ( name_str ) : directives = None fields = name_str . split ( \"=\" ) process_name = fields [ 0 ] if len ( fields ) == 2 : _directives = fields [ 1 ] . replace ( \"'\" , '\"' ) try : directives = json . loads ( _directives ) except json . decoder . JSONDecodeError : raise eh . ProcessError ( \"Could not parse directives for process '{}'. The raw\" \" string is: {}\\n\" \"Possible causes include:\\n\" \"\\t1. Spaces inside directives\\n\" \"\\t2. Missing '=' symbol before directives\\n\" \"\\t3. Missing quotes (' or \\\") around directives\\n\" \"A valid example: process_name={{'cpus':'2'}}\" . format ( process_name , name_str ) ) return process_name , directives", "nl": "Parses the process string and returns the process name and its directives"}}
{"translation": {"code": "def render_pipeline ( self ) : dict_viz = { \"name\" : \"root\" , \"children\" : [ ] } last_of_us = { } f_tree = self . _fork_tree if self . _fork_tree else { 1 : [ 1 ] } for x , ( k , v ) in enumerate ( f_tree . items ( ) ) : for p in self . processes [ 1 : ] : if x == 0 and p . lane not in [ k ] + v : continue if x > 0 and p . lane not in v : continue if not p . parent_lane : lst = dict_viz [ \"children\" ] else : lst = last_of_us [ p . parent_lane ] tooltip = { \"name\" : \"{}_{}\" . format ( p . template , p . pid ) , \"process\" : { \"pid\" : p . pid , \"input\" : p . input_type , \"output\" : p . output_type if p . output_type else \"None\" , \"lane\" : p . lane , } , \"children\" : [ ] } dir_var = \"\" for k2 , v2 in p . directives . items ( ) : dir_var += k2 for d in v2 : try : # Remove quotes from string directives directive = v2 [ d ] . replace ( \"'\" , \"\" ) . replace ( '\"' , '' ) if isinstance ( v2 [ d ] , str ) else v2 [ d ] dir_var += \"{}: {}\" . format ( d , directive ) except KeyError : pass if dir_var : tooltip [ \"process\" ] [ \"directives\" ] = dir_var else : tooltip [ \"process\" ] [ \"directives\" ] = \"N/A\" lst . append ( tooltip ) last_of_us [ p . lane ] = lst [ - 1 ] [ \"children\" ] # write to file dict_viz self . dag_to_file ( dict_viz ) # Write tree forking information for dotfile with open ( os . path . join ( dirname ( self . nf_file ) , \".forkTree.json\" ) , \"w\" ) as fh : fh . write ( json . dumps ( self . _fork_tree ) ) # send with jinja to html resource return self . _render_config ( \"pipeline_graph.html\" , { \"data\" : dict_viz } )", "nl": "Write pipeline attributes to json"}}
{"translation": {"code": "def remove_inner_forks ( text ) : n = 1 # run at least once for one level of fork # Then this loop assures that all brackets will get removed in a nested # structure while n : # this removes non-nested brackets text , n = re . subn ( r'\\([^()]*\\)' , '' , text ) return text", "nl": "Recursively removes nested brackets"}}
{"translation": {"code": "def brew_innuendo ( args ) : # Create recipe class instance automatic_pipeline = Innuendo ( ) if not args . tasks : input_processes = \" \" . join ( automatic_pipeline . process_descriptions . keys ( ) ) else : input_processes = args . tasks # Validate the provided pipeline processes validated = automatic_pipeline . validate_pipeline ( input_processes ) if not validated : sys . exit ( 1 ) # Get the final pipeline string pipeline_string = automatic_pipeline . run_auto_pipeline ( input_processes ) return pipeline_string", "nl": "Brews a given list of processes according to the recipe"}}
{"translation": {"code": "def _get_params_string ( self ) : params_str = \"\" for p in self . processes : logger . debug ( \"[{}] Adding parameters: {}\\n\" . format ( p . template , p . params ) ) # Add an header with the template name to structure the params # configuration if p . params and p . template != \"init\" : p . set_param_id ( \"_{}\" . format ( p . pid ) ) params_str += \"\\n\\t/*\" params_str += \"\\n\\tComponent '{}_{}'\\n\" . format ( p . template , p . pid ) params_str += \"\\t{}\\n\" . format ( \"-\" * ( len ( p . template ) + len ( p . pid ) + 12 ) ) params_str += \"\\t*/\\n\" for param , val in p . params . items ( ) : if p . template == \"init\" : param_id = param else : param_id = \"{}_{}\" . format ( param , p . pid ) params_str += \"\\t{} = {}\\n\" . format ( param_id , val [ \"default\" ] ) return params_str", "nl": "Returns the nextflow params string from a dictionary object ."}}
{"translation": {"code": "def write_configs ( self , project_root ) : # Write resources config with open ( join ( project_root , \"resources.config\" ) , \"w\" ) as fh : fh . write ( self . resources ) # Write containers config with open ( join ( project_root , \"containers.config\" ) , \"w\" ) as fh : fh . write ( self . containers ) # Write containers config with open ( join ( project_root , \"params.config\" ) , \"w\" ) as fh : fh . write ( self . params ) # Write manifest config with open ( join ( project_root , \"manifest.config\" ) , \"w\" ) as fh : fh . write ( self . manifest ) # Write user config if not present in the project directory if not exists ( join ( project_root , \"user.config\" ) ) : with open ( join ( project_root , \"user.config\" ) , \"w\" ) as fh : fh . write ( self . user_config ) lib_dir = join ( project_root , \"lib\" ) if not exists ( lib_dir ) : os . makedirs ( lib_dir ) with open ( join ( lib_dir , \"Helper.groovy\" ) , \"w\" ) as fh : fh . write ( self . help ) # Generate the pipeline DAG pipeline_to_json = self . render_pipeline ( ) with open ( splitext ( self . nf_file ) [ 0 ] + \".html\" , \"w\" ) as fh : fh . write ( pipeline_to_json )", "nl": "Wrapper method that writes all configuration files to the pipeline directory"}}
{"translation": {"code": "def set_extra_inputs ( self , channel_dict ) : extra_inputs = [ ] for param , info in channel_dict . items ( ) : # Update the process' parameters with the raw input raw_channel = self . RAW_MAPPING [ info [ \"input_type\" ] ] self . params [ param ] = { \"default\" : raw_channel [ \"default_value\" ] , \"description\" : raw_channel [ \"description\" ] } channel_name = \"IN_{}_extraInput\" . format ( param ) channel_str = self . RAW_MAPPING [ info [ \"input_type\" ] ] [ \"channel_str\" ] extra_inputs . append ( \"{} = {}\" . format ( channel_name , channel_str . format ( param ) ) ) op = \"set\" if len ( info [ \"channels\" ] ) == 1 else \"into\" extra_inputs . append ( \"{}.{}{{ {} }}\" . format ( channel_name , op , \";\" . join ( info [ \"channels\" ] ) ) ) self . _context = { * * self . _context , * * { \"extra_inputs\" : \"\\n\" . join ( extra_inputs ) } }", "nl": "Sets the initial definition of the extra input channels ."}}
{"translation": {"code": "def _search_tree_backwards ( self , template , parent_lanes ) : for p in self . processes [ : : - 1 ] : # Ignore process in different lanes if p . lane not in parent_lanes : continue # template found if p . template == template : return True return False", "nl": "Searches the process tree backwards in search of a provided process"}}
{"translation": {"code": "def _add_dependency ( self , p , template , inlane , outlane , pid ) : dependency_proc = self . process_map [ template ] ( template = template ) if dependency_proc . input_type != p . input_type : logger . error ( \"Cannot automatically add dependency with different\" \" input type. Input type of process '{}' is '{}.\" \" Input type of dependency '{}' is '{}'\" . format ( p . template , p . input_type , template , dependency_proc . input_type ) ) input_suf = \"{}_{}_dep\" . format ( inlane , pid ) output_suf = \"{}_{}_dep\" . format ( outlane , pid ) dependency_proc . set_main_channel_names ( input_suf , output_suf , outlane ) # To insert the dependency process before the current process, we'll # need to move the input channel name of the later to the former, and # set a new connection between the dependency and the process. dependency_proc . input_channel = p . input_channel p . input_channel = dependency_proc . output_channel # If the current process was the first in the pipeline, change the # lanes so that the dependency becomes the first process if not p . parent_lane : p . parent_lane = outlane dependency_proc . parent_lane = None else : dependency_proc . parent_lane = inlane p . parent_lane = outlane self . processes . append ( dependency_proc )", "nl": "Automatically Adds a dependency of a process ."}}
{"translation": {"code": "def _header_mapping ( header ) : return dict ( ( x . strip ( ) , pos ) for pos , x in enumerate ( header . split ( \"\\t\" ) ) )", "nl": "Parses the trace file header and retrieves the positions of each column key ."}}
{"translation": {"code": "def display_overview ( self ) : stay_alive = True self . screen = curses . initscr ( ) self . screen . keypad ( True ) self . screen . nodelay ( - 1 ) curses . cbreak ( ) curses . noecho ( ) curses . start_color ( ) self . screen_lines = self . screen . getmaxyx ( ) [ 0 ] # self.screen_width = self.screen.getmaxyx()[1] try : while stay_alive : # Provide functionality to certain keybindings self . _curses_keybindings ( ) # Updates main inspector attributes self . update_inspection ( ) # Display curses interface self . flush_overview ( ) sleep ( self . refresh_rate ) except FileNotFoundError : sys . stderr . write ( colored_print ( \"ERROR: nextflow log and/or trace files are no longer \" \"reachable!\" , \"red_bold\" ) ) except Exception as e : sys . stderr . write ( str ( e ) ) finally : curses . nocbreak ( ) self . screen . keypad ( 0 ) curses . echo ( ) curses . endwin ( )", "nl": "Displays the default pipeline inspection overview"}}
{"translation": {"code": "def _updown ( self , direction ) : if direction == \"up\" and self . top_line != 0 : self . top_line -= 1 elif direction == \"down\" and self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3 : self . top_line += 1", "nl": "Provides curses scroll functionality ."}}
{"translation": {"code": "def _hms ( s ) : if s == \"-\" : return 0 if s . endswith ( \"ms\" ) : return float ( s . rstrip ( \"ms\" ) ) / 1000 fields = list ( map ( float , re . split ( \"[dhms]\" , s ) [ : - 1 ] ) ) if len ( fields ) == 4 : return fields [ 0 ] * 24 * 3600 + fields [ 1 ] * 3600 + fields [ 2 ] * 60 + fields [ 3 ] if len ( fields ) == 3 : return fields [ 0 ] * 3600 + fields [ 1 ] * 60 + fields [ 2 ] elif len ( fields ) == 2 : return fields [ 0 ] * 60 + fields [ 1 ] else : return fields [ 0 ]", "nl": "Converts a hms string into seconds ."}}
{"translation": {"code": "def update_inspection ( self ) : try : self . log_parser ( ) except ( FileNotFoundError , StopIteration ) as e : logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ 0 ] ) ) self . log_retry += 1 if self . log_retry == self . MAX_RETRIES : raise e try : self . trace_parser ( ) except ( FileNotFoundError , StopIteration ) as e : logger . debug ( \"ERROR: \" + str ( sys . exc_info ( ) [ 0 ] ) ) self . trace_retry += 1 if self . trace_retry == self . MAX_RETRIES : raise e", "nl": "Wrapper method that calls the appropriate main updating methods of the inspection ."}}
{"translation": {"code": "def _size_coverter ( s ) : if s . upper ( ) . endswith ( \"KB\" ) : return float ( s . rstrip ( \"KB\" ) ) / 1024 elif s . upper ( ) . endswith ( \" B\" ) : return float ( s . rstrip ( \"B\" ) ) / 1024 / 1024 elif s . upper ( ) . endswith ( \"MB\" ) : return float ( s . rstrip ( \"MB\" ) ) elif s . upper ( ) . endswith ( \"GB\" ) : return float ( s . rstrip ( \"GB\" ) ) * 1024 elif s . upper ( ) . endswith ( \"TB\" ) : return float ( s . rstrip ( \"TB\" ) ) * 1024 * 1024 else : return float ( s )", "nl": "Converts size string into megabytes"}}
{"translation": {"code": "def log_parser ( self ) : # Check the timestamp of the log file. Only proceed with the parsing # if it changed from the previous time. size_stamp = os . path . getsize ( self . log_file ) self . log_retry = 0 if size_stamp and size_stamp == self . log_sizestamp : return else : logger . debug ( \"Updating log size stamp to: {}\" . format ( size_stamp ) ) self . log_sizestamp = size_stamp # Regular expression to catch four groups: # 1. Start timestamp # 2. Work directory hash # 3. Process name # 4. Tag name r = \".* (.*) \\[.*\\].*\\[(.*)\\].*process > (.*) \\((.*)\\).*\" with open ( self . log_file ) as fh : for line in fh : if \"Submitted process >\" in line or \"Re-submitted process >\" in line or \"Cached process >\" in line : m = re . match ( r , line ) if not m : continue time_start = m . group ( 1 ) workdir = m . group ( 2 ) process = m . group ( 3 ) tag = m . group ( 4 ) # Skip if this line has already been parsed if time_start + tag not in self . stored_log_ids : self . stored_log_ids . append ( time_start + tag ) else : continue # For first time processes if process not in self . processes : continue p = self . processes [ process ] # Skip is process/tag combination has finished or is retrying if tag in list ( p [ \"finished\" ] ) + list ( p [ \"retry\" ] ) : continue # Update failed process/tags when they have been re-submitted if tag in list ( p [ \"failed\" ] ) and \"Re-submitted process >\" in line : p [ \"retry\" ] . add ( tag ) self . send = True continue # Set process barrier to running. Check for barrier status # are performed at the end of the trace parsing in the # _update_barrier_status method. p [ \"barrier\" ] = \"R\" if tag not in p [ \"submitted\" ] : p [ \"submitted\" ] . add ( tag ) # Update the process_tags attribute with the new tag. # Update only when the tag does not exist. This may rarely # occur when the tag is parsed first in the trace file if tag not in self . process_tags [ process ] : self . process_tags [ process ] [ tag ] = { \"workdir\" : self . _expand_path ( workdir ) , \"start\" : time_start } self . send = True # When the tag is filled in the trace file parsing, # the timestamp may not be present in the trace. In # those cases, fill that information here. elif not self . process_tags [ process ] [ tag ] [ \"start\" ] : self . process_tags [ process ] [ tag ] [ \"start\" ] = time_start self . send = True self . _update_pipeline_status ( )", "nl": "Method that parses the nextflow log file once and updates the submitted number of samples for each process"}}
{"translation": {"code": "def _get_pipeline_processes ( self ) : with open ( self . log_file ) as fh : for line in fh : if re . match ( \".*Creating operator.*\" , line ) : # Retrieves the process name from the string match = re . match ( \".*Creating operator > (.*) --\" , line ) process = match . group ( 1 ) if any ( [ process . startswith ( x ) for x in self . _blacklist ] ) : continue if process not in self . skip_processes : self . processes [ match . group ( 1 ) ] = { \"barrier\" : \"W\" , \"submitted\" : set ( ) , \"finished\" : set ( ) , \"failed\" : set ( ) , \"retry\" : set ( ) , \"cpus\" : None , \"memory\" : None } self . process_tags [ process ] = { } # Retrieves the pipeline name from the string if re . match ( \".*Launching `.*` \\[.*\\] \" , line ) : tag_match = re . match ( \".*Launching `.*` \\[(.*)\\] \" , line ) self . pipeline_tag = tag_match . group ( 1 ) if tag_match else \"?\" name_match = re . match ( \".*Launching `(.*)` \\[.*\\] \" , line ) self . pipeline_name = name_match . group ( 1 ) if name_match else \"?\" self . content_lines = len ( self . processes )", "nl": "Parses the . nextflow . log file and retrieves the complete list of processes"}}
{"translation": {"code": "def _update_barrier_status ( self ) : with open ( self . log_file ) as fh : for line in fh : # Exit barrier update after session abort signal if \"Session aborted\" in line : return if \"<<< barrier arrive\" in line : # Retrieve process name from string process_m = re . match ( \".*process: (.*)\\)\" , line ) if process_m : process = process_m . group ( 1 ) # Updates process channel to complete if process in self . processes : self . processes [ process ] [ \"barrier\" ] = \"C\"", "nl": "Checks whether the channels to each process have been closed ."}}
{"translation": {"code": "def check_summary_health ( summary_file , * * kwargs ) : # Store the summary categories that cannot fail. If they fail, do not # proceed with this sample fail_sensitive = kwargs . get ( \"fail_sensitive\" , [ \"Per base sequence quality\" , \"Overrepresented sequences\" , \"Sequence Length Distribution\" , \"Per sequence GC content\" ] ) logger . debug ( \"Fail sensitive categories: {}\" . format ( fail_sensitive ) ) # Store summary categories that must pass. If they do not, do not proceed # with that sample must_pass = kwargs . get ( \"must_pass\" , [ \"Per base N content\" , \"Adapter Content\" ] ) logger . debug ( \"Must pass categories: {}\" . format ( must_pass ) ) warning_fail_sensitive = kwargs . get ( \"warning_fail_sensitive\" , [ \"Per base sequence quality\" , \"Overrepresented sequences\" , ] ) warning_must_pass = kwargs . get ( \"warning_must_pass\" , [ \"Per base sequence content\" ] ) # Get summary dictionary summary_info = get_summary ( summary_file ) # This flag will change to False if one of the tests fails health = True # List of failing categories failed = [ ] # List of warning categories warning = [ ] for cat , test in summary_info . items ( ) : logger . debug ( \"Assessing category {} with result {}\" . format ( cat , test ) ) # FAILURES # Check for fail sensitive if cat in fail_sensitive and test == \"FAIL\" : health = False failed . append ( \"{}:{}\" . format ( cat , test ) ) logger . error ( \"Category {} failed a fail sensitive \" \"category\" . format ( cat ) ) # Check for must pass if cat in must_pass and test != \"PASS\" : health = False failed . append ( \"{}:{}\" . format ( cat , test ) ) logger . error ( \"Category {} failed a must pass category\" . format ( cat ) ) # WARNINGS # Check for fail sensitive if cat in warning_fail_sensitive and test == \"FAIL\" : warning . append ( \"Failed category: {}\" . format ( cat ) ) logger . warning ( \"Category {} flagged at a fail sensitive \" \"category\" . format ( cat ) ) if cat in warning_must_pass and test != \"PASS\" : warning . append ( \"Did not pass category: {}\" . format ( cat ) ) logger . warning ( \"Category {} flagged at a must pass \" \"category\" . format ( cat ) ) # Passed all tests return health , failed , warning", "nl": "Checks the health of a sample from the FastQC summary file ."}}
{"translation": {"code": "def get_trim_index ( biased_list ) : # Return index 0 if there are no biased positions if set ( biased_list ) == { False } : return 0 if set ( biased_list [ : 5 ] ) == { False } : return 0 # Iterate over the biased_list array. Keep the iteration going until # we find a biased position with the two following positions unbiased # (e.g.: True, False, False). # When this condition is verified, return the last biased position # index for subsequent trimming. for i , val in enumerate ( biased_list ) : if val and set ( biased_list [ i + 1 : i + 3 ] ) == { False } : return i + 1 # If the previous iteration could not find and index to trim, it means # that the whole list is basically biased. Return the length of the # biased_list return len ( biased_list )", "nl": "Returns the trim index from a bool list"}}
{"translation": {"code": "def get_summary ( summary_file ) : summary_info = OrderedDict ( ) logger . debug ( \"Retrieving summary information from file: {}\" . format ( summary_file ) ) with open ( summary_file ) as fh : for line in fh : # Skip empty lines if not line . strip ( ) : continue # Populate summary info fields = [ x . strip ( ) for x in line . split ( \"\\t\" ) ] summary_info [ fields [ 1 ] ] = fields [ 0 ] logger . debug ( \"Retrieved summary information from file: {}\" . format ( summary_info ) ) return summary_info", "nl": "Parses a FastQC summary report file and returns it as a dictionary ."}}
{"translation": {"code": "def write_json_report ( sample_id , data1 , data2 ) : parser_map = { \"base_sequence_quality\" : \">>Per base sequence quality\" , \"sequence_quality\" : \">>Per sequence quality scores\" , \"base_gc_content\" : \">>Per sequence GC content\" , \"base_n_content\" : \">>Per base N content\" , \"sequence_length_dist\" : \">>Sequence Length Distribution\" , \"per_base_sequence_content\" : \">>Per base sequence content\" } json_dic = { \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"base_sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_quality\" : { \"status\" : None , \"data\" : [ ] } , \"base_gc_content\" : { \"status\" : None , \"data\" : [ ] } , \"base_n_content\" : { \"status\" : None , \"data\" : [ ] } , \"sequence_length_dist\" : { \"status\" : None , \"data\" : [ ] } , \"per_base_sequence_content\" : { \"status\" : None , \"data\" : [ ] } } } ] } for cat , start_str in parser_map . items ( ) : if cat == \"per_base_sequence_content\" : fs = 1 fe = 5 else : fs = 1 fe = 2 report1 , status1 = _get_quality_stats ( data1 , start_str , field_start = fs , field_end = fe ) report2 , status2 = _get_quality_stats ( data2 , start_str , field_start = fs , field_end = fe ) status = None for i in [ \"fail\" , \"warn\" , \"pass\" ] : if i in [ status1 , status2 ] : status = i json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ cat ] [ \"data\" ] = [ report1 , report2 ] json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ cat ] [ \"status\" ] = status return json_dic", "nl": "Writes the report"}}
{"translation": {"code": "def get_sample_trim ( p1_data , p2_data ) : sample_ranges = [ trim_range ( x ) for x in [ p1_data , p2_data ] ] # Get the optimal trim position for 5' end optimal_5trim = max ( [ x [ 0 ] for x in sample_ranges ] ) # Get optimal trim position for 3' end optimal_3trim = min ( [ x [ 1 ] for x in sample_ranges ] ) return optimal_5trim , optimal_3trim", "nl": "Get the optimal read trim range from data files of paired FastQ reads ."}}
{"translation": {"code": "def main ( sample_id , fastq_pair , clear ) : logger . info ( \"Starting skesa\" ) # Determine output file if \"_trim.\" in fastq_pair [ 0 ] : sample_id += \"_trim\" version = __get_version_skesa ( ) [ \"version\" ] output_file = \"{}_skesa{}.fasta\" . format ( sample_id , version . replace ( \".\" , \"\" ) ) cli = [ \"skesa\" , \"--fastq\" , \"{},{}\" . format ( fastq_pair [ 0 ] , fastq_pair [ 1 ] ) , \"--gz\" , \"--use_paired_ends\" , \"--cores\" , \"${task.cpus}\" ] logger . debug ( \"Running Skesa subprocess with command: {}\" . format ( cli ) ) with open ( output_file , \"w\" ) as fh : p = subprocess . Popen ( cli , stdout = fh , stderr = PIPE ) stdout , stderr = p . communicate ( ) # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to # string try : stderr = stderr . decode ( \"utf8\" ) stdout = stdout . decode ( \"utf8\" ) except ( UnicodeDecodeError , AttributeError ) : stderr = str ( stderr ) stdout = str ( stdout ) logger . info ( \"Finished Skesa subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) logger . info ( \"Fished Skesa subprocess with STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) logger . info ( \"Finished Skesa with return code: {}\" . format ( p . returncode ) ) # Remove input fastq files when clear option is specified. # Only remove temporary input when the expected output exists. if clear == \"true\" and os . path . exists ( output_file ) : clean_up ( fastq_pair ) with open ( \".status\" , \"w\" ) as fh : if p . returncode != 0 : fh . write ( \"error\" ) raise SystemExit ( p . returncode ) else : fh . write ( \"pass\" )", "nl": "Main executor of the skesa template ."}}
{"translation": {"code": "def get_gc_sliding ( self , window = 2000 ) : gc_res = [ ] # Get complete sequence to calculate sliding window values complete_seq = \"\" . join ( self . contigs . values ( ) ) . lower ( ) for i in range ( 0 , len ( complete_seq ) , window ) : seq_window = complete_seq [ i : i + window ] # Get GC proportion gc_res . append ( round ( self . _gc_prop ( seq_window , len ( seq_window ) ) , 2 ) ) return gc_res", "nl": "Calculates a sliding window of the GC content for the assembly"}}
{"translation": {"code": "def trim_range ( data_file ) : logger . debug ( \"Starting trim range assessment\" ) # Target string for nucleotide bias assessment target_nuc_bias = \">>Per base sequence content\" logger . debug ( \"Target string to start nucleotide bias assessment set to \" \"{}\" . format ( target_nuc_bias ) ) # This flag will become True when gathering base proportion data # from file. gather = False # This variable will store a boolean array on the biased/unbiased # positions. Biased position will be True, while unbiased positions # will be False biased = [ ] with open ( data_file ) as fh : for line in fh : # Start assessment of nucleotide bias if line . startswith ( target_nuc_bias ) : # Skip comment line logger . debug ( \"Found target string at line: {}\" . format ( line ) ) next ( fh ) gather = True # Stop assessment when reaching end of target module elif line . startswith ( \">>END_MODULE\" ) and gather : logger . debug ( \"Stopping parsing at line: {}\" . format ( line ) ) break elif gather : # Get proportions of each nucleotide g , a , t , c = [ float ( x ) for x in line . strip ( ) . split ( ) [ 1 : ] ] # Get 'GC' and 'AT content gc = ( g + 0.1 ) / ( c + 0.1 ) at = ( a + 0.1 ) / ( t + 0.1 ) # Assess bias if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2 : biased . append ( False ) else : biased . append ( True ) logger . debug ( \"Finished bias assessment with result: {}\" . format ( biased ) ) # Split biased list in half to get the 5' and 3' ends biased_5end , biased_3end = biased [ : int ( len ( biased ) / 2 ) ] , biased [ int ( len ( biased ) / 2 ) : ] [ : : - 1 ] logger . debug ( \"Getting optimal trim range from biased list\" ) trim_nt = [ 0 , 0 ] # Assess number of nucleotides to clip at 5' end trim_nt [ 0 ] = get_trim_index ( biased_5end ) logger . debug ( \"Optimal trim range at 5' end set to: {}\" . format ( trim_nt [ 0 ] ) ) # Assess number of nucleotides to clip at 3' end trim_nt [ 1 ] = len ( biased ) - get_trim_index ( biased_3end ) logger . debug ( \"Optimal trim range at 3' end set to: {}\" . format ( trim_nt [ 1 ] ) ) return trim_nt", "nl": "Assess the optimal trim range for a given FastQC data file ."}}
{"translation": {"code": "def _get_window_labels ( self , window ) : # Get summary stats, if they have not yet been triggered if not self . summary_info : self . get_summary_stats ( ) # Get contig boundary positon c = 0 xbars = [ ] for contig , seq in self . contigs . items ( ) : contig_id = self . _get_contig_id ( contig ) self . contig_boundaries [ contig_id ] = [ c , c + len ( seq ) ] c += len ( seq ) xbars . append ( ( contig_id , c , contig ) ) return xbars", "nl": "Returns the mapping between sliding window points and their contigs and the x - axis position of contig"}}
{"translation": {"code": "def depth_file_reader ( depth_file ) : # dict to store the mean coverage for each reference depth_dic_coverage = { } for line in depth_file : tab_split = line . split ( ) # split by any white space reference = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3 ] ) # store # only the gi for the reference position = tab_split [ 1 ] num_reads_align = float ( tab_split [ 2 ] . rstrip ( ) ) if reference not in depth_dic_coverage : depth_dic_coverage [ reference ] = { } depth_dic_coverage [ reference ] [ position ] = num_reads_align logger . info ( \"Finished parsing depth file.\" ) depth_file . close ( ) logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( depth_dic_coverage ) / 1024 ) ) return depth_dic_coverage", "nl": "Function that parse samtools depth file and creates 3 dictionaries that will be useful to make the outputs of this script both the tabular file and the json file that may be imported by pATLAS"}}
{"translation": {"code": "def main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , clear ) : logger . info ( \"Starting trimmomatic\" ) # Create base CLI cli = [ \"java\" , \"-Xmx{}\" . format ( \"$task.memory\" [ : - 1 ] . lower ( ) . replace ( \" \" , \"\" ) ) , \"-jar\" , TRIM_PATH . strip ( ) , \"PE\" , \"-threads\" , \"$task.cpus\" ] # If the phred encoding was detected, provide it try : # Check if the provided PHRED can be converted to int phred = int ( phred ) phred_flag = \"-phred{}\" . format ( str ( phred ) ) cli += [ phred_flag ] # Could not detect phred encoding. Do not add explicit encoding to # trimmomatic and let it guess except ValueError : pass # Add input samples to CLI cli += fastq_pair # Add output file names output_names = [ ] for i in range ( len ( fastq_pair ) ) : output_names . append ( \"{}_{}_trim.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) output_names . append ( \"{}_{}_U.fastq.gz\" . format ( SAMPLE_ID , str ( i + 1 ) ) ) cli += output_names if trim_range != [ \"None\" ] : cli += [ \"CROP:{}\" . format ( trim_range [ 1 ] ) , \"HEADCROP:{}\" . format ( trim_range [ 0 ] ) , ] if os . path . exists ( adapters_file ) : logger . debug ( \"Using the provided adapters file '{}'\" . format ( adapters_file ) ) else : logger . debug ( \"Adapters file '{}' not provided or does not exist. Using\" \" default adapters\" . format ( adapters_file ) ) adapters_file = merge_default_adapters ( ) cli += [ \"ILLUMINACLIP:{}:3:30:10:6:true\" . format ( adapters_file ) ] #create log file im temporary dir to avoid issues when running on a docker container in macOS logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , \"{}_trimlog.txt\" . format ( sample_id ) ) # Add trimmomatic options cli += [ \"SLIDINGWINDOW:{}\" . format ( trim_opts [ 0 ] ) , \"LEADING:{}\" . format ( trim_opts [ 1 ] ) , \"TRAILING:{}\" . format ( trim_opts [ 2 ] ) , \"MINLEN:{}\" . format ( trim_opts [ 3 ] ) , \"TOPHRED33\" , \"-trimlog\" , logfile ] logger . debug ( \"Running trimmomatic subprocess with command: {}\" . format ( cli ) ) p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) stdout , stderr = p . communicate ( ) # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to # string try : stderr = stderr . decode ( \"utf8\" ) except ( UnicodeDecodeError , AttributeError ) : stderr = str ( stderr ) logger . info ( \"Finished trimmomatic subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) logger . info ( \"Finished trimmomatic subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) logger . info ( \"Finished trimmomatic with return code: {}\" . format ( p . returncode ) ) trimmomatic_log ( logfile , sample_id ) if p . returncode == 0 and os . path . exists ( \"{}_1_trim.fastq.gz\" . format ( SAMPLE_ID ) ) : clean_up ( fastq_pair , clear ) # Check if trimmomatic ran successfully. If not, write the error message # to the status channel and exit. with open ( \".status\" , \"w\" ) as status_fh : if p . returncode != 0 : status_fh . write ( \"fail\" ) return else : status_fh . write ( \"pass\" )", "nl": "Main executor of the trimmomatic template ."}}
{"translation": {"code": "def _gc_prop ( s , length ) : gc = sum ( map ( s . count , [ \"c\" , \"g\" ] ) ) return gc / length", "nl": "Get proportion of GC from a string"}}
{"translation": {"code": "def guess_file_compression ( file_path , magic_dict = None ) : if not magic_dict : magic_dict = MAGIC_DICT max_len = max ( len ( x ) for x in magic_dict ) with open ( file_path , \"rb\" ) as f : file_start = f . read ( max_len ) logger . debug ( \"Binary signature start: {}\" . format ( file_start ) ) for magic , file_type in magic_dict . items ( ) : if file_start . startswith ( magic ) : return file_type return None", "nl": "Guesses the compression of an input file ."}}
{"translation": {"code": "def merge_default_adapters ( ) : default_adapters = [ os . path . join ( ADAPTERS_PATH , x ) for x in os . listdir ( ADAPTERS_PATH ) ] filepath = os . path . join ( os . getcwd ( ) , \"default_adapters.fasta\" ) with open ( filepath , \"w\" ) as fh , fileinput . input ( default_adapters ) as in_fh : for line in in_fh : fh . write ( \"{}{}\" . format ( line , \"\\\\n\" ) ) return filepath", "nl": "Merges the default adapters file in the trimmomatic adapters directory"}}
{"translation": {"code": "def clean_up ( fastq_pairs , clear ) : # Find unpaired fastq files unpaired_fastq = [ f for f in os . listdir ( \".\" ) if f . endswith ( \"_U.fastq.gz\" ) ] # Remove unpaired fastq files, if any for fpath in unpaired_fastq : os . remove ( fpath ) # Expected output to assess whether it is safe to remove temporary input expected_out = [ f for f in os . listdir ( \".\" ) if f . endswith ( \"_trim.fastq.gz\" ) ] if clear == \"true\" and len ( expected_out ) == 2 : for fq in fastq_pairs : # Get real path of fastq files, following symlinks rp = os . path . realpath ( fq ) logger . debug ( \"Removing temporary fastq file path: {}\" . format ( rp ) ) if re . match ( \".*/work/.{2}/.{30}/.*\" , rp ) : os . remove ( rp )", "nl": "Cleans the working directory of unwanted temporary files"}}
{"translation": {"code": "def set_kmers ( kmer_opt , max_read_len ) : logger . debug ( \"Kmer option set to: {}\" . format ( kmer_opt ) ) # Check if kmer option is set to auto if kmer_opt == \"auto\" : if max_read_len >= 175 : kmers = [ 55 , 77 , 99 , 113 , 127 ] else : kmers = [ 21 , 33 , 55 , 67 , 77 ] logger . debug ( \"Kmer range automatically selected based on max read\" \"length of {}: {}\" . format ( max_read_len , kmers ) ) # Check if manual kmers were specified elif len ( kmer_opt . split ( ) ) > 1 : kmers = kmer_opt . split ( ) logger . debug ( \"Kmer range manually set to: {}\" . format ( kmers ) ) else : kmers = [ ] logger . debug ( \"Kmer range set to empty (will be automatically \" \"determined by SPAdes\" ) return kmers", "nl": "Returns a kmer list based on the provided kmer option and max read len ."}}
{"translation": {"code": "def main ( sample_id , fastq_pair , max_len , kmer , clear ) : logger . info ( \"Starting spades\" ) logger . info ( \"Setting SPAdes kmers\" ) kmers = set_kmers ( kmer , max_len ) logger . info ( \"SPAdes kmers set to: {}\" . format ( kmers ) ) cli = [ \"metaspades.py\" , \"--only-assembler\" , \"--threads\" , \"$task.cpus\" , \"-o\" , \".\" ] # Add kmers, if any were specified if kmers : cli += [ \"-k {}\" . format ( \",\" . join ( [ str ( x ) for x in kmers ] ) ) ] # Add FastQ files cli += [ \"-1\" , fastq_pair [ 0 ] , \"-2\" , fastq_pair [ 1 ] ] logger . debug ( \"Running metaSPAdes subprocess with command: {}\" . format ( cli ) ) p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) stdout , stderr = p . communicate ( ) # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to # string try : stderr = stderr . decode ( \"utf8\" ) stdout = stdout . decode ( \"utf8\" ) except ( UnicodeDecodeError , AttributeError ) : stderr = str ( stderr ) stdout = str ( stdout ) logger . info ( \"Finished metaSPAdes subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) logger . info ( \"Fished metaSPAdes subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) logger . info ( \"Finished metaSPAdes with return code: {}\" . format ( p . returncode ) ) with open ( \".status\" , \"w\" ) as fh : if p . returncode != 0 : fh . write ( \"error\" ) return else : fh . write ( \"pass\" ) # Change the default contigs.fasta assembly name to a more informative one if \"_trim.\" in fastq_pair [ 0 ] : sample_id += \"_trim\" assembly_file = \"{}_metaspades.fasta\" . format ( sample_id ) os . rename ( \"contigs.fasta\" , assembly_file ) logger . info ( \"Setting main assembly file to: {}\" . format ( assembly_file ) ) # Remove input fastq files when clear option is specified. # Only remove temporary input when the expected output exists. if clear == \"true\" and os . path . exists ( assembly_file ) : clean_up ( fastq_pair )", "nl": "Main executor of the spades template ."}}
{"translation": {"code": "def convert_adatpers ( adapter_fasta ) : adapter_out = \"fastqc_adapters.tab\" logger . debug ( \"Setting output adapters file to: {}\" . format ( adapter_out ) ) try : with open ( adapter_fasta ) as fh , open ( adapter_out , \"w\" ) as adap_fh : for line in fh : if line . startswith ( \">\" ) : head = line [ 1 : ] . strip ( ) # Get the next line with the sequence string sequence = next ( fh ) . strip ( ) adap_fh . write ( \"{}\\\\t{}\\\\n\" . format ( head , sequence ) ) logger . info ( \"Converted adapters file\" ) return adapter_out # If an invalid adapters file is provided, return None. except FileNotFoundError : logger . warning ( \"Could not find the provided adapters file: {}\" . format ( adapter_fasta ) ) return", "nl": "Generates an adapter file for FastQC from a fasta file ."}}
{"translation": {"code": "def main ( fastq_pair , adapter_file , cpus ) : logger . info ( \"Starting fastqc\" ) # If an adapter file was provided, convert it to FastQC format if os . path . exists ( adapter_file ) : logger . info ( \"Adapters file provided: {}\" . format ( adapter_file ) ) adapters = convert_adatpers ( adapter_file ) else : logger . info ( \"Adapters file '{}' not provided or does not \" \"exist\" . format ( adapter_file ) ) adapters = None # Setting command line for FastQC cli = [ \"fastqc\" , \"--extract\" , \"--nogroup\" , \"--format\" , \"fastq\" , \"--threads\" , str ( cpus ) ] # Add adapters file to command line, if it exists if adapters : cli += [ \"--adapters\" , \"{}\" . format ( adapters ) ] # Add FastQ files at the end of command line cli += fastq_pair logger . debug ( \"Running fastqc subprocess with command: {}\" . format ( cli ) ) p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE , shell = False ) stdout , stderr = p . communicate ( ) # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to # string try : stderr = stderr . decode ( \"utf8\" ) except ( UnicodeDecodeError , AttributeError ) : stderr = str ( stderr ) logger . info ( \"Finished fastqc subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) logger . info ( \"Fished fastqc subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) logger . info ( \"Finished fastqc with return code: {}\" . format ( p . returncode ) ) logger . info ( \"Checking if FastQC output was correctly generated\" ) # Check if the FastQC output was correctly generated. with open ( \".status\" , \"w\" ) as status_fh : for fastq in fastq_pair : fpath = join ( fastq . rsplit ( \".\" , 2 ) [ 0 ] + \"_fastqc\" , \"fastqc_data.txt\" ) logger . debug ( \"Checking path: {}\" . format ( fpath ) ) # If the FastQC output does not exist, pass the STDERR to # the output status channel and exit if not exists ( fpath ) : logger . warning ( \"Path does not exist: {}\" . format ( fpath ) ) status_fh . write ( \"fail\" ) return logger . debug ( \"Found path: {}\" . format ( fpath ) ) # If the output directories exist, write 'pass' to the output status # channel status_fh . write ( \"pass\" ) logger . info ( \"Retrieving relevant FastQC output files\" ) # Both FastQC have been correctly executed. Get the relevant FastQC # output files for the output channel for i , fastq in enumerate ( fastq_pair ) : # Get results for each pair fastqc_dir = fastq . rsplit ( \".\" , 2 ) [ 0 ] + \"_fastqc\" summary_file = join ( fastqc_dir , \"summary.txt\" ) logger . debug ( \"Retrieving summary file: {}\" . format ( summary_file ) ) fastqc_data_file = join ( fastqc_dir , \"fastqc_data.txt\" ) logger . debug ( \"Retrieving data file: {}\" . format ( fastqc_data_file ) ) # Rename output files to a file name that is easier to handle in the # output channel os . rename ( fastqc_data_file , \"pair_{}_data\" . format ( i + 1 ) ) os . rename ( summary_file , \"pair_{}_summary\" . format ( i + 1 ) )", "nl": "Main executor of the fastq template ."}}
{"translation": {"code": "def send_to_output ( master_dict , mash_output , sample_id , assembly_file ) : plot_dict = { } # create a new file only if master_dict is populated if master_dict : out_file = open ( \"{}.json\" . format ( \"\" . join ( mash_output . split ( \".\" ) [ 0 ] ) ) , \"w\" ) out_file . write ( json . dumps ( master_dict ) ) out_file . close ( ) # iterate through master_dict in order to make contigs the keys for k , v in master_dict . items ( ) : if not v [ 2 ] in plot_dict : plot_dict [ v [ 2 ] ] = [ k ] else : plot_dict [ v [ 2 ] ] . append ( k ) number_hits = len ( master_dict ) else : number_hits = 0 json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Dist\" , \"table\" : \"plasmids\" , \"patlas_mashdist\" : master_dict , \"value\" : number_hits } ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMashDistXrange\" : plot_dict } , \"assemblyFile\" : assembly_file } ] } with open ( \".report.json\" , \"w\" ) as json_report : json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) )", "nl": "Send dictionary to output json file This function sends master_dict dictionary to a json file if master_dict is populated with entries otherwise it won t create the file"}}
{"translation": {"code": "def main ( mash_output , hash_cutoff , sample_id , assembly_file ) : input_f = open ( mash_output , \"r\" ) master_dict = { } for line in input_f : tab_split = line . split ( \"\\t\" ) current_seq = tab_split [ 1 ] . strip ( ) ref_accession = \"_\" . join ( tab_split [ 0 ] . strip ( ) . split ( \"_\" ) [ 0 : 3 ] ) mash_dist = tab_split [ 2 ] . strip ( ) hashes_list = tab_split [ - 1 ] . strip ( ) . split ( \"/\" ) # creates a percentage of the shared hashes between the sample and the # reference perc_hashes = float ( hashes_list [ 0 ] ) / float ( hashes_list [ 1 ] ) # if ref_accession already in dict, i.e., if the same accession number # matches more than one contig. if ref_accession in master_dict . keys ( ) : current_seq += \", {}\" . format ( master_dict [ ref_accession ] [ - 1 ] ) # assures that only the hashes with a given shared percentage are # reported to json file if perc_hashes > float ( hash_cutoff ) : master_dict [ ref_accession ] = [ round ( 1 - float ( mash_dist ) , 2 ) , round ( perc_hashes , 2 ) , current_seq ] # assures that file is closed in last iteration of the loop send_to_output ( master_dict , mash_output , sample_id , assembly_file )", "nl": "Main function that allows to dump a mash dist txt file to a json file"}}
{"translation": {"code": "def build_versions ( self ) : version_storage = [ ] template_version = self . context . get ( \"__version__\" , None ) template_program = self . context . get ( \"__template__\" , None ) template_build = self . context . get ( \"__build__\" , None ) if template_version and template_program and template_build : if self . logger : self . logger . debug ( \"Adding template version: {}; {}; \" \"{}\" . format ( template_program , template_version , template_build ) ) version_storage . append ( { \"program\" : template_program , \"version\" : template_version , \"build\" : template_build } ) for var , obj in self . context . items ( ) : if var . startswith ( \"__get_version\" ) : ver = obj ( ) version_storage . append ( ver ) if self . logger : self . logger . debug ( \"Found additional software version\" \"{}\" . format ( ver ) ) with open ( \".versions\" , \"w\" ) as fh : fh . write ( json . dumps ( version_storage , separators = ( \",\" , \":\" ) ) )", "nl": "Writes versions JSON for a template file"}}
{"translation": {"code": "def main ( mash_output , sample_id ) : logger . info ( \"Reading file : {}\" . format ( mash_output ) ) read_mash_output = open ( mash_output ) dic = { } median_list = [ ] filtered_dic = { } logger . info ( \"Generating dictionary and list to pre-process the final json\" ) for line in read_mash_output : tab_split = line . split ( \"\\t\" ) identity = tab_split [ 0 ] # shared_hashes = tab_split[1] median_multiplicity = tab_split [ 2 ] # p_value = tab_split[3] query_id = tab_split [ 4 ] # query-comment should not exist here and it is irrelevant # here identity is what in fact interests to report to json but # median_multiplicity also is important since it gives an rough # estimation of the coverage depth for each plasmid. # Plasmids should have higher coverage depth due to their increased # copy number in relation to the chromosome. dic [ query_id ] = [ identity , median_multiplicity ] median_list . append ( float ( median_multiplicity ) ) output_json = open ( \" \" . join ( mash_output . split ( \".\" ) [ : - 1 ] ) + \".json\" , \"w\" ) # median cutoff is twice the median of all median_multiplicity values # reported by mash screen. In the case of plasmids, since the database # has 9k entries and reads shouldn't have that many sequences it seems ok... if len ( median_list ) > 0 : # this statement assures that median_list has indeed any entries median_cutoff = median ( median_list ) logger . info ( \"Generating final json to dump to a file\" ) for k , v in dic . items ( ) : # estimated copy number copy_number = int ( float ( v [ 1 ] ) / median_cutoff ) # assure that plasmid as at least twice the median coverage depth if float ( v [ 1 ] ) > median_cutoff : filtered_dic [ \"_\" . join ( k . split ( \"_\" ) [ 0 : 3 ] ) ] = [ round ( float ( v [ 0 ] ) , 2 ) , copy_number ] logger . info ( \"Exported dictionary has {} entries\" . format ( len ( filtered_dic ) ) ) else : # if no entries were found raise an error logger . error ( \"No matches were found using mash screen for the queried reads\" ) output_json . write ( json . dumps ( filtered_dic ) ) output_json . close ( ) json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mash Screen\" , \"table\" : \"plasmids\" , \"patlas_mashscreen\" : filtered_dic , \"value\" : len ( filtered_dic ) } ] } ] , } with open ( \".report.json\" , \"w\" ) as json_report : json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) )", "nl": "converts top results from mash screen txt output to json format"}}
{"translation": {"code": "def main ( depth_file , json_dict , cutoff , sample_id ) : # check for the appropriate value for the cutoff value for coverage results logger . debug ( \"Cutoff value: {}. Type: {}\" . format ( cutoff , type ( cutoff ) ) ) try : cutoff_val = float ( cutoff ) if cutoff_val < 0.4 : logger . warning ( \"This cutoff value will generate a high volume of \" \"plot data. Therefore '.report.json' can be too big\" ) except ValueError : logger . error ( \"Cutoff value should be a string such as: '0.6'. \" \"The outputted value: {}. Make sure to provide an \" \"appropriate value for --cov_cutoff\" . format ( cutoff ) ) sys . exit ( 1 ) # loads dict from file, this file is provided in docker image plasmid_length = json . load ( open ( json_dict ) ) if plasmid_length : logger . info ( \"Loaded dictionary of plasmid lengths\" ) else : logger . error ( \"Something went wrong and plasmid lengths dictionary\" \"could not be loaded. Check if process received this\" \"param successfully.\" ) sys . exit ( 1 ) # read depth file depth_file_in = open ( depth_file ) # first reads the depth file and generates dictionaries to handle the input # to a simpler format logger . info ( \"Reading depth file and creating dictionary to dump.\" ) depth_dic_coverage = depth_file_reader ( depth_file_in ) percentage_bases_covered , dict_cov = generate_jsons ( depth_dic_coverage , plasmid_length , cutoff_val ) if percentage_bases_covered and dict_cov : logger . info ( \"percentage_bases_covered length: {}\" . format ( str ( len ( percentage_bases_covered ) ) ) ) logger . info ( \"dict_cov length: {}\" . format ( str ( len ( dict_cov ) ) ) ) else : logger . error ( \"Both dicts that dump to JSON file or .report.json are \" \"empty.\" ) # then dump do file logger . info ( \"Dumping to {}\" . format ( \"{}_mapping.json\" . format ( depth_file ) ) ) with open ( \"{}_mapping.json\" . format ( depth_file ) , \"w\" ) as output_json : output_json . write ( json . dumps ( percentage_bases_covered ) ) json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Mapping\" , \"table\" : \"plasmids\" , \"patlas_mapping\" : percentage_bases_covered , \"value\" : len ( percentage_bases_covered ) } ] } ] , \"sample\" : sample_id , \"patlas_mapping\" : percentage_bases_covered , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"patlasMappingSliding\" : dict_cov } , } ] } logger . debug ( \"Size of dict_cov: {} kb\" . format ( asizeof ( json_dic ) / 1024 ) ) logger . info ( \"Writing to .report.json\" ) with open ( \".report.json\" , \"w\" ) as json_report : json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) )", "nl": "Function that handles the inputs required to parse depth files from bowtie and dumps a dict to a json file that can be imported into pATLAS ."}}
{"translation": {"code": "def get_qual_range ( qual_str ) : vals = [ ord ( c ) for c in qual_str ] return min ( vals ) , max ( vals )", "nl": "Get range of the Unicode encode range for a given string of characters ."}}
{"translation": {"code": "def parse_log ( log_file ) : template = OrderedDict ( [ # Total length after trimming ( \"clean_len\" , 0 ) , # Total trimmed base pairs ( \"total_trim\" , 0 ) , # Total trimmed base pairs in percentage ( \"total_trim_perc\" , 0 ) , # Total trimmed at 5' end ( \"5trim\" , 0 ) , # Total trimmed at 3' end ( \"3trim\" , 0 ) , # Bad reads (completely trimmed) ( \"bad_reads\" , 0 ) ] ) with open ( log_file ) as fh : for line in fh : # This will split the log fields into: # 0. read length after trimming # 1. amount trimmed from the start # 2. last surviving base # 3. amount trimmed from the end fields = [ int ( x ) for x in line . strip ( ) . split ( ) [ - 4 : ] ] if not fields [ 0 ] : template [ \"bad_reads\" ] += 1 template [ \"5trim\" ] += fields [ 1 ] template [ \"3trim\" ] += fields [ 3 ] template [ \"total_trim\" ] += fields [ 1 ] + fields [ 3 ] template [ \"clean_len\" ] += fields [ 0 ] total_len = template [ \"clean_len\" ] + template [ \"total_trim\" ] if total_len : template [ \"total_trim_perc\" ] = round ( ( template [ \"total_trim\" ] / total_len ) * 100 , 2 ) else : template [ \"total_trim_perc\" ] = 0 return template", "nl": "Retrieves some statistics from a single Trimmomatic log file ."}}
{"translation": {"code": "def _parse_assembly ( self , assembly_file ) : with open ( assembly_file ) as fh : header = None logger . debug ( \"Starting iteration of assembly file: {}\" . format ( assembly_file ) ) for line in fh : # Skip empty lines if not line . strip ( ) : continue if line . startswith ( \">\" ) : # Add contig header to contig dictionary header = line [ 1 : ] . strip ( ) self . contigs [ header ] = [ ] else : # Add sequence string for the current contig self . contigs [ header ] . append ( line . strip ( ) ) # After populating the contigs dictionary, convert the values # list into a string sequence self . contigs = OrderedDict ( ( header , \"\" . join ( seq ) ) for header , seq in self . contigs . items ( ) )", "nl": "Parse an assembly file in fasta format ."}}
{"translation": {"code": "def main ( sample_id , assembly_file , coverage_file , coverage_bp_file , bam_file , opts , gsize ) : min_assembly_coverage , max_contigs = opts logger . info ( \"Starting assembly mapping processing\" ) # Get coverage info, total size and total coverage from the assembly logger . info ( \"Parsing coverage table\" ) coverage_info , a_cov = parse_coverage_table ( coverage_file ) a_size , contig_size = get_assembly_size ( assembly_file ) logger . info ( \"Assembly processed with a total size of '{}' and coverage\" \" of '{}'\" . format ( a_size , a_cov ) ) # Get number of assembled bp after filters logger . info ( \"Parsing coverage per bp table\" ) coverage_bp_data = get_coverage_from_file ( coverage_bp_file ) # Assess the minimum assembly coverage min_coverage = evaluate_min_coverage ( min_assembly_coverage , a_cov , a_size ) # Check if filtering the assembly using the provided min_coverage will # reduce the final bp number to less than 80% of the estimated genome # size. # If the check below passes with True, then the filtered assembly # is above the 80% genome size threshold. filtered_assembly = \"{}_filt.fasta\" . format ( os . path . splitext ( assembly_file ) [ 0 ] ) filtered_bam = \"filtered.bam\" logger . info ( \"Checking filtered assembly\" ) if check_filtered_assembly ( coverage_info , coverage_bp_data , min_coverage , gsize , contig_size , int ( max_contigs ) , sample_id ) : # Filter assembly contigs based on the minimum coverage. logger . info ( \"Filtered assembly passed minimum size threshold\" ) logger . info ( \"Writting filtered assembly\" ) filter_assembly ( assembly_file , min_coverage , coverage_info , filtered_assembly ) logger . info ( \"Filtering BAM file according to saved contigs\" ) filter_bam ( coverage_info , bam_file , min_coverage , filtered_bam ) # Could not filter the assembly as it would drop below acceptable # length levels. Copy the original assembly to the output assembly file # for compliance with the output channel else : shutil . copy ( assembly_file , filtered_assembly ) shutil . copy ( bam_file , filtered_bam ) shutil . copy ( bam_file + \".bai\" , filtered_bam + \".bai\" ) with open ( \".status\" , \"w\" ) as status_fh : status_fh . write ( \"pass\" )", "nl": "Main executor of the process_assembly_mapping template ."}}
{"translation": {"code": "def get_assembly_size ( assembly_file ) : assembly_size = 0 contig_size = { } header = \"\" with open ( assembly_file ) as fh : for line in fh : # Skip empty lines if line . strip ( ) == \"\" : continue if line . startswith ( \">\" ) : header = line . strip ( ) [ 1 : ] contig_size [ header ] = 0 else : line_len = len ( line . strip ( ) ) assembly_size += line_len contig_size [ header ] += line_len return assembly_size , contig_size", "nl": "Returns the number of nucleotides and the size per contig for the provided assembly file path"}}
{"translation": {"code": "def get_summary_stats ( self , output_csv = None ) : contig_size_list = [ ] self . summary_info [ \"ncontigs\" ] = len ( self . contigs ) for contig_id , sequence in self . contigs . items ( ) : logger . debug ( \"Processing contig: {}\" . format ( contig_id ) ) # Get contig sequence size contig_len = len ( sequence ) # Add size for average contig size contig_size_list . append ( contig_len ) # Add to total assembly length self . summary_info [ \"total_len\" ] += contig_len # Add to average gc self . summary_info [ \"avg_gc\" ] . append ( sum ( map ( sequence . count , [ \"G\" , \"C\" ] ) ) / contig_len ) # Add to missing data self . summary_info [ \"missing_data\" ] += sequence . count ( \"N\" ) # Get average contig size logger . debug ( \"Getting average contig size\" ) self . summary_info [ \"avg_contig_size\" ] = sum ( contig_size_list ) / len ( contig_size_list ) # Get average gc content logger . debug ( \"Getting average GC content\" ) self . summary_info [ \"avg_gc\" ] = sum ( self . summary_info [ \"avg_gc\" ] ) / len ( self . summary_info [ \"avg_gc\" ] ) # Get N50 logger . debug ( \"Getting N50\" ) cum_size = 0 for l in sorted ( contig_size_list , reverse = True ) : cum_size += l if cum_size >= self . summary_info [ \"total_len\" ] / 2 : self . summary_info [ \"n50\" ] = l break if output_csv : logger . debug ( \"Writing report to csv\" ) # Write summary info to CSV with open ( output_csv , \"w\" ) as fh : summary_line = \"{}, {}\\\\n\" . format ( self . sample , \",\" . join ( [ str ( x ) for x in self . summary_info . values ( ) ] ) ) fh . write ( summary_line )", "nl": "Generates a CSV report with summary statistics about the assembly"}}
{"translation": {"code": "def main ( sample_id , trace_file , workdir ) : # Determine the path of the stored JSON for the sample_id stats_suffix = \".stats.json\" stats_path = join ( workdir , sample_id + stats_suffix ) trace_path = join ( workdir , trace_file ) logger . info ( \"Starting pipeline status routine\" ) logger . debug ( \"Checking for previous pipeline status data\" ) stats_array = get_previous_stats ( stats_path ) logger . info ( \"Stats JSON object set to : {}\" . format ( stats_array ) ) # Search for this substring in the tags field. Only lines with this # tag will be processed for the reports tag = \" getStats\" logger . debug ( \"Tag variable set to: {}\" . format ( tag ) ) logger . info ( \"Starting parsing of trace file: {}\" . format ( trace_path ) ) with open ( trace_path ) as fh : header = next ( fh ) . strip ( ) . split ( ) logger . debug ( \"Header set to: {}\" . format ( header ) ) for line in fh : fields = line . strip ( ) . split ( \"\\t\" ) # Check if tag substring is in the tag field of the nextflow trace if tag in fields [ 2 ] and fields [ 3 ] == \"COMPLETED\" : logger . debug ( \"Parsing trace line with COMPLETED status: {}\" . format ( line ) ) current_json = get_json_info ( fields , header ) stats_array [ fields [ 0 ] ] = current_json else : logger . debug ( \"Ignoring trace line without COMPLETED status\" \" or stats specific tag: {}\" . format ( line ) ) with open ( join ( stats_path ) , \"w\" ) as fh , open ( \".report.json\" , \"w\" ) as rfh : fh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) ) rfh . write ( json . dumps ( stats_array , separators = ( \",\" , \":\" ) ) )", "nl": "Parses a nextflow trace file searches for processes with a specific tag and sends a JSON report with the relevant information"}}
{"translation": {"code": "def filter_bam ( coverage_info , bam_file , min_coverage , output_bam ) : # Get list of contigs that will be kept contig_list = [ x for x , vals in coverage_info . items ( ) if vals [ \"cov\" ] >= min_coverage ] cli = [ \"samtools\" , \"view\" , \"-bh\" , \"-F\" , \"4\" , \"-o\" , output_bam , \"-@\" , \"1\" , bam_file , ] cli += contig_list logger . debug ( \"Runnig samtools view subprocess with command: {}\" . format ( cli ) ) p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) stdout , stderr = p . communicate ( ) # Attempt to decode STDERR output from bytes. If unsuccessful, coerce to # string try : stderr = stderr . decode ( \"utf8\" ) stdout = stdout . decode ( \"utf8\" ) except ( UnicodeDecodeError , AttributeError ) : stderr = str ( stderr ) stdout = str ( stdout ) logger . info ( \"Finished samtools view subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) logger . info ( \"Fished samtools view subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) logger . info ( \"Finished samtools view with return code: {}\" . format ( p . returncode ) ) if not p . returncode : # Create index cli = [ \"samtools\" , \"index\" , output_bam ] logger . debug ( \"Runnig samtools index subprocess with command: \" \"{}\" . format ( cli ) ) p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) stdout , stderr = p . communicate ( ) try : stderr = stderr . decode ( \"utf8\" ) stdout = stdout . decode ( \"utf8\" ) except ( UnicodeDecodeError , AttributeError ) : stderr = str ( stderr ) stdout = str ( stdout ) logger . info ( \"Finished samtools index subprocess with STDOUT:\\\\n\" \"======================================\\\\n{}\" . format ( stdout ) ) logger . info ( \"Fished samtools index subprocesswith STDERR:\\\\n\" \"======================================\\\\n{}\" . format ( stderr ) ) logger . info ( \"Finished samtools index with return code: {}\" . format ( p . returncode ) )", "nl": "Uses Samtools to filter a BAM file according to minimum coverage"}}
{"translation": {"code": "def filter_assembly ( assembly_file , minimum_coverage , coverage_info , output_file ) : # This flag will determine whether sequence data should be written or # ignored because the current contig did not pass the minimum # coverage threshold write_flag = False with open ( assembly_file ) as fh , open ( output_file , \"w\" ) as out_fh : for line in fh : if line . startswith ( \">\" ) : # Reset write_flag write_flag = False # Get header of contig header = line . strip ( ) [ 1 : ] # Check coverage for current contig contig_cov = coverage_info [ header ] [ \"cov\" ] # If the contig coverage is above the threshold, write to # output filtered assembly if contig_cov >= minimum_coverage : write_flag = True out_fh . write ( line ) elif write_flag : out_fh . write ( line )", "nl": "Generates a filtered assembly file ."}}
{"translation": {"code": "def parse_coverage_table ( coverage_file ) : # Stores the correspondence between a contig and the corresponding coverage # e.g.: {\"contig_1\": {\"cov\": 424} } coverage_dict = OrderedDict ( ) # Stores the total coverage total_cov = 0 with open ( coverage_file ) as fh : for line in fh : # Get contig and coverage contig , cov = line . strip ( ) . split ( ) coverage_dict [ contig ] = { \"cov\" : int ( cov ) } # Add total coverage total_cov += int ( cov ) logger . debug ( \"Processing contig '{}' with coverage '{}'\" \"\" . format ( contig , cov ) ) return coverage_dict , total_cov", "nl": "Parses a file with coverage information into objects ."}}
{"translation": {"code": "def write_report ( storage_dic , output_file , sample_id ) : with open ( output_file , \"w\" ) as fh , open ( \".report.json\" , \"w\" ) as json_rep : # Write header fh . write ( \"Sample,Total length,Total trimmed,%,5end Trim,3end Trim,\" \"bad_reads\\\\n\" ) # Write contents for sample , vals in storage_dic . items ( ) : fh . write ( \"{},{}\\\\n\" . format ( sample , \",\" . join ( [ str ( x ) for x in vals . values ( ) ] ) ) ) json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"trimmed\" , \"value\" : vals [ \"total_trim_perc\" ] , \"table\" : \"qc\" , \"columnBar\" : True } , ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"sparkline\" : vals [ \"clean_len\" ] } } ] , \"badReads\" : vals [ \"bad_reads\" ] } json_rep . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) )", "nl": "Writes a report from multiple samples ."}}
{"translation": {"code": "def main ( log_files ) : log_storage = OrderedDict ( ) for log in log_files : log_id = log . rstrip ( \"_trimlog.txt\" ) # Populate storage of current sample log_storage [ log_id ] = parse_log ( log ) # Remove temporary trim log file os . remove ( log ) write_report ( log_storage , \"trimmomatic_report.csv\" , log_id )", "nl": "Main executor of the trimmomatic_report template ."}}
{"translation": {"code": "def evaluate_min_coverage ( coverage_opt , assembly_coverage , assembly_size ) : if coverage_opt == \"auto\" : # Get the 1/3 value of the current assembly coverage min_coverage = ( assembly_coverage / assembly_size ) * .3 logger . info ( \"Minimum assembly coverage automatically set to: \" \"{}\" . format ( min_coverage ) ) # If the 1/3 coverage is lower than 10, change it to the minimum of # 10 if min_coverage < 10 : logger . info ( \"Minimum assembly coverage cannot be set to lower\" \" that 10. Setting to 10\" ) min_coverage = 10 else : min_coverage = int ( coverage_opt ) logger . info ( \"Minimum assembly coverage manually set to: {}\" . format ( min_coverage ) ) return min_coverage", "nl": "Evaluates the minimum coverage threshold from the value provided in the coverage_opt ."}}
{"translation": {"code": "def get_encodings_in_range ( rmin , rmax ) : valid_encodings = [ ] valid_phred = [ ] for encoding , ( phred , ( emin , emax ) ) in RANGES . items ( ) : if rmin >= emin and rmax <= emax : valid_encodings . append ( encoding ) valid_phred . append ( phred ) return valid_encodings , valid_phred", "nl": "Returns the valid encodings for a given encoding range ."}}
{"translation": {"code": "def main ( sample_id , assembly_file , coverage_bp_file = None ) : logger . info ( \"Starting assembly report\" ) assembly_obj = Assembly ( assembly_file , sample_id ) logger . info ( \"Retrieving summary statistics for assembly\" ) assembly_obj . get_summary_stats ( \"{}_assembly_report.csv\" . format ( sample_id ) ) size_dist = [ len ( x ) for x in assembly_obj . contigs . values ( ) ] json_dic = { \"tableRow\" : [ { \"sample\" : sample_id , \"data\" : [ { \"header\" : \"Contigs\" , \"value\" : assembly_obj . summary_info [ \"ncontigs\" ] , \"table\" : \"assembly\" , \"columnBar\" : True } , { \"header\" : \"Assembled BP\" , \"value\" : assembly_obj . summary_info [ \"total_len\" ] , \"table\" : \"assembly\" , \"columnBar\" : True } , ] } ] , \"plotData\" : [ { \"sample\" : sample_id , \"data\" : { \"size_dist\" : size_dist } } ] } if coverage_bp_file : try : window = 2000 gc_sliding_data = assembly_obj . get_gc_sliding ( window = window ) cov_sliding_data = assembly_obj . get_coverage_sliding ( coverage_bp_file , window = window ) # Get total basepairs based on the individual coverage of each # contig bpx total_bp = sum ( [ sum ( x ) for x in assembly_obj . contig_coverage . values ( ) ] ) # Add data to json report json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ \"genomeSliding\" ] = { \"gcData\" : gc_sliding_data , \"covData\" : cov_sliding_data , \"window\" : window , \"xbars\" : assembly_obj . _get_window_labels ( window ) , \"assemblyFile\" : os . path . basename ( assembly_file ) } json_dic [ \"plotData\" ] [ 0 ] [ \"data\" ] [ \"sparkline\" ] = total_bp except : logger . error ( \"Unexpected error creating sliding window data:\\\\n\" \"{}\" . format ( traceback . format_exc ( ) ) ) # Write json report with open ( \".report.json\" , \"w\" ) as json_report : json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) ) with open ( \".status\" , \"w\" ) as status_fh : status_fh . write ( \"pass\" )", "nl": "Main executor of the assembly_report template ."}}
{"translation": {"code": "def parse_files ( self , fls ) : for f in fls : # Make sure paths exists if os . path . exists ( f ) : self . _parser ( f ) else : logger . warning ( \"File {} does not exist\" . format ( f ) )", "nl": "Public method for parsing abricate output files ."}}
{"translation": {"code": "def write_report_data ( self ) : json_plot = self . get_plot_data ( ) json_table = self . get_table_data ( ) json_dic = { * * json_plot , * * json_table } with open ( \".report.json\" , \"w\" ) as json_report : json_report . write ( json . dumps ( json_dic , separators = ( \",\" , \":\" ) ) )", "nl": "Writes the JSON report to a json file"}}
{"translation": {"code": "def _parser ( self , fl ) : with open ( fl ) as fh : for line in fh : # Skip header and comment lines if line . startswith ( \"#\" ) or line . strip ( ) == \"\" : continue fields = line . strip ( ) . split ( \"\\t\" ) try : coverage = float ( fields [ 8 ] ) except ValueError : coverage = None try : identity = float ( fields [ 9 ] ) except ValueError : identity = None try : accession = fields [ 11 ] except IndexError : accession = None self . storage [ self . _key ] = { \"log_file\" : os . path . basename ( fl ) , \"infile\" : fields [ 0 ] , \"reference\" : fields [ 1 ] , \"seq_range\" : ( int ( fields [ 2 ] ) , int ( fields [ 3 ] ) ) , \"gene\" : fields [ 4 ] , \"accession\" : accession , \"database\" : fields [ 10 ] , \"coverage\" : coverage , \"identity\" : identity } self . _key += 1", "nl": "Parser for a single abricate output file ."}}
{"translation": {"code": "def iter_filter ( self , filters , databases = None , fields = None , filter_behavior = \"and\" ) : if filter_behavior not in [ \"and\" , \"or\" ] : raise ValueError ( \"Filter behavior must be either 'and' or 'or'\" ) for dic in self . storage . values ( ) : # This attribute will determine whether an entry will be yielded # or not _pass = False # Stores the flags with the test results for each filter # The results will be either True or False flag = [ ] # Filter for databases if databases : # Skip entry if not in specified database if dic [ \"database\" ] not in databases : continue # Apply filters for f in filters : # Get value of current filter val = dic [ f [ 0 ] ] if not self . _test_truth ( val , f [ 1 ] , f [ 2 ] ) : flag . append ( False ) else : flag . append ( True ) # Test whether the entry will pass based on the test results # and the filter behaviour if filter_behavior == \"and\" : if all ( flag ) : _pass = True elif filter_behavior == \"or\" : if any ( flag ) : _pass = True if _pass : if fields : yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) else : yield dic", "nl": "General purpose filter iterator ."}}
{"translation": {"code": "def _get_contig_id ( contig_str ) : contig_id = contig_str try : contig_id = re . search ( \".*NODE_([0-9]*)_.*\" , contig_str ) . group ( 1 ) except AttributeError : pass try : contig_id = re . search ( \".*Contig_([0-9]*)_.*\" , contig_str ) . group ( 1 ) except AttributeError : pass return contig_id", "nl": "Tries to retrieve contig id . Returns the original string if it is unable to retrieve the id ."}}
{"translation": {"code": "def get_plot_data ( self ) : json_dic = { \"plotData\" : [ ] } sample_dic = { } sample_assembly_map = { } for entry in self . storage . values ( ) : sample_id = re . match ( \"(.*)_abr\" , entry [ \"log_file\" ] ) . groups ( ) [ 0 ] if sample_id not in sample_dic : sample_dic [ sample_id ] = { } # Get contig ID using the same regex as in `assembly_report.py` # template contig_id = self . _get_contig_id ( entry [ \"reference\" ] ) # Get database database = entry [ \"database\" ] if database not in sample_dic [ sample_id ] : sample_dic [ sample_id ] [ database ] = [ ] # Update the sample-assembly correspondence dict if sample_id not in sample_assembly_map : sample_assembly_map [ sample_id ] = entry [ \"infile\" ] sample_dic [ sample_id ] [ database ] . append ( { \"contig\" : contig_id , \"seqRange\" : entry [ \"seq_range\" ] , \"gene\" : entry [ \"gene\" ] . replace ( \"'\" , \"\" ) , \"accession\" : entry [ \"accession\" ] , \"coverage\" : entry [ \"coverage\" ] , \"identity\" : entry [ \"identity\" ] , } , ) for sample , data in sample_dic . items ( ) : json_dic [ \"plotData\" ] . append ( { \"sample\" : sample , \"data\" : { \"abricateXrange\" : data } , \"assemblyFile\" : sample_assembly_map [ sample ] } ) return json_dic", "nl": "Generates the JSON report to plot the gene boxes"}}
{"translation": {"code": "def _check_required_files ( self ) : if not os . path . exists ( self . trace_file ) : raise eh . InspectionError ( \"The provided trace file could not be \" \"opened: {}\" . format ( self . trace_file ) ) if not os . path . exists ( self . log_file ) : raise eh . InspectionError ( \"The .nextflow.log files could not be \" \"opened. Are you sure you are in a \" \"nextflow project directory?\" )", "nl": "Checks whetner the trace and log files are available"}}
{"translation": {"code": "def _update_process_stats ( self ) : good_status = [ \"COMPLETED\" , \"CACHED\" ] for process , vals in self . trace_info . items ( ) : # Update submission status of tags for each process vals = self . _update_tag_status ( process , vals ) # Update process resources self . _update_process_resources ( process , vals ) self . process_stats [ process ] = { } inst = self . process_stats [ process ] # Get number of completed samples inst [ \"completed\" ] = \"{}\" . format ( len ( [ x for x in vals if x [ \"status\" ] in good_status ] ) ) # Get average time try : time_array = [ self . _hms ( x [ \"realtime\" ] ) for x in vals ] mean_time = round ( sum ( time_array ) / len ( time_array ) , 1 ) mean_time_str = strftime ( '%H:%M:%S' , gmtime ( mean_time ) ) inst [ \"realtime\" ] = mean_time_str # When the realtime column is not present except KeyError : inst [ \"realtime\" ] = \"-\" # Get cumulative cpu/hours try : cpu_hours = [ self . _cpu_load_parser ( x [ \"cpus\" ] , x [ \"%cpu\" ] , x [ \"realtime\" ] ) for x in vals ] inst [ \"cpuhour\" ] = round ( sum ( cpu_hours ) , 2 ) # When the realtime, cpus or %cpus column are not present except KeyError : inst [ \"cpuhour\" ] = \"-\" # Assess resource warnings inst [ \"cpu_warnings\" ] , inst [ \"mem_warnings\" ] = self . _assess_resource_warnings ( process , vals ) # Get maximum memory try : rss_values = [ self . _size_coverter ( x [ \"rss\" ] ) for x in vals if x [ \"rss\" ] != \"-\" ] if rss_values : max_rss = round ( max ( rss_values ) ) rss_str = self . _size_compress ( max_rss ) else : rss_str = \"-\" inst [ \"maxmem\" ] = rss_str except KeyError : inst [ \"maxmem\" ] = \"-\" # Get read size try : rchar_values = [ self . _size_coverter ( x [ \"rchar\" ] ) for x in vals if x [ \"rchar\" ] != \"-\" ] if rchar_values : avg_rchar = round ( sum ( rchar_values ) / len ( rchar_values ) ) rchar_str = self . _size_compress ( avg_rchar ) else : rchar_str = \"-\" except KeyError : rchar_str = \"-\" inst [ \"avgread\" ] = rchar_str # Get write size try : wchar_values = [ self . _size_coverter ( x [ \"wchar\" ] ) for x in vals if x [ \"wchar\" ] != \"-\" ] if wchar_values : avg_wchar = round ( sum ( wchar_values ) / len ( wchar_values ) ) wchar_str = self . _size_compress ( avg_wchar ) else : wchar_str = \"-\" except KeyError : wchar_str = \"-\" inst [ \"avgwrite\" ] = wchar_str", "nl": "Updates the process stats with the information from the processes"}}
{"translation": {"code": "def _rightleft ( self , direction ) : if direction == \"left\" and self . padding != 0 : self . padding -= 1 if direction == \"right\" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max_width : self . padding += 1", "nl": "Provides curses horizontal padding"}}
{"translation": {"code": "def _clear_inspect ( self ) : self . trace_info = defaultdict ( list ) self . process_tags = { } self . process_stats = { } self . samples = [ ] self . stored_ids = [ ] self . stored_log_ids = [ ] self . time_start = None self . time_stop = None self . execution_command = None self . nextflow_version = None self . abort_cause = None self . _c = 0 # Clean up of tag running status for p in self . processes . values ( ) : p [ \"barrier\" ] = \"W\" for i in [ \"submitted\" , \"finished\" , \"failed\" , \"retry\" ] : p [ i ] = set ( )", "nl": "Clears inspect attributes when re - executing a pipeline"}}
{"translation": {"code": "def _get_run_hash ( self ) : # Get name and path of the pipeline from the log file pipeline_path = get_nextflow_filepath ( self . log_file ) # Get hash from the entire pipeline file pipeline_hash = hashlib . md5 ( ) with open ( pipeline_path , \"rb\" ) as fh : for chunk in iter ( lambda : fh . read ( 4096 ) , b\"\" ) : pipeline_hash . update ( chunk ) # Get hash from the current working dir and hostname workdir = self . workdir . encode ( \"utf8\" ) hostname = socket . gethostname ( ) . encode ( \"utf8\" ) hardware_addr = str ( uuid . getnode ( ) ) . encode ( \"utf8\" ) dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( )", "nl": "Gets the hash of the nextflow file"}}
{"translation": {"code": "def _retrieve_log ( path ) : if not os . path . exists ( path ) : return None with open ( path ) as fh : return fh . readlines ( )", "nl": "Method used to retrieve the contents of a log file into a list ."}}
{"translation": {"code": "def _assess_resource_warnings ( self , process , vals ) : cpu_warnings = { } mem_warnings = { } for i in vals : try : expected_load = float ( i [ \"cpus\" ] ) * 100 cpu_load = float ( i [ \"%cpu\" ] . replace ( \",\" , \".\" ) . replace ( \"%\" , \"\" ) ) if expected_load * 0.9 > cpu_load > expected_load * 1.10 : cpu_warnings [ i [ \"tag\" ] ] = { \"expected\" : expected_load , \"value\" : cpu_load } except ( ValueError , KeyError ) : pass try : rss = self . _size_coverter ( i [ \"rss\" ] ) mem_allocated = self . _size_coverter ( i [ \"memory\" ] ) if rss > mem_allocated * 1.10 : mem_warnings [ i [ \"tag\" ] ] = { \"expected\" : mem_allocated , \"value\" : rss } except ( ValueError , KeyError ) : pass return cpu_warnings , mem_warnings", "nl": "Assess whether the cpu load or memory usage is above the allocation"}}
{"translation": {"code": "def _get_log_lines ( self , n = 300 ) : with open ( self . log_file ) as fh : last_lines = fh . readlines ( ) [ - n : ] return last_lines", "nl": "Returns a list with the last n lines of the nextflow log file"}}
{"translation": {"code": "def _prepare_static_info ( self ) : pipeline_files = { } with open ( join ( self . workdir , self . pipeline_name ) ) as fh : pipeline_files [ \"pipelineFile\" ] = fh . readlines ( ) nf_config = join ( self . workdir , \"nextflow.config\" ) if os . path . exists ( nf_config ) : with open ( nf_config ) as fh : pipeline_files [ \"configFile\" ] = fh . readlines ( ) # Check for specific flowcraft configurations files configs = { \"params.config\" : \"paramsFile\" , \"resources.config\" : \"resourcesFile\" , \"containers.config\" : \"containersFile\" , \"user.config\" : \"userFile\" , } for config , key in configs . items ( ) : cfile = join ( self . workdir , config ) if os . path . exists ( cfile ) : with open ( cfile ) as fh : pipeline_files [ key ] = fh . readlines ( ) return pipeline_files", "nl": "Prepares the first batch of information containing static information such as the pipeline file and configuration files"}}
{"translation": {"code": "def _dag_file_to_dict ( self ) : try : dag_file = open ( os . path . join ( self . workdir , \".treeDag.json\" ) ) dag_json = json . load ( dag_file ) except ( FileNotFoundError , json . decoder . JSONDecodeError ) : logger . warning ( colored_print ( \"WARNING: dotfile named .treeDag.json not found or corrupted\" , \"red_bold\" ) ) dag_json = { } return dag_json", "nl": "Function that opens the dotfile named . treeDag . json in the current working directory"}}
{"translation": {"code": "def dag_to_file ( self , dict_viz , output_file = \".treeDag.json\" ) : outfile_dag = open ( os . path . join ( dirname ( self . nf_file ) , output_file ) , \"w\" ) outfile_dag . write ( json . dumps ( dict_viz ) ) outfile_dag . close ( )", "nl": "Writes dag to output file"}}
{"translation": {"code": "def export_params ( self ) : params_json = { } # Skip first init process for p in self . processes [ 1 : ] : params_json [ p . template ] = p . params # Flush params json to stdout sys . stdout . write ( json . dumps ( params_json ) )", "nl": "Export pipeline params as a JSON to stdout"}}