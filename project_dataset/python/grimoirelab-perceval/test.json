{"translation": {"code": "def parse_reviews ( raw_data ) : # Join isolated reviews in JSON in array for parsing items_raw = \"[\" + raw_data . replace ( \"\\n\" , \",\" ) + \"]\" items_raw = items_raw . replace ( \",]\" , \"]\" ) items = json . loads ( items_raw ) reviews = [ ] for item in items : if 'project' in item . keys ( ) : reviews . append ( item ) return reviews", "nl": "Parse a Gerrit reviews list ."}}
{"translation": {"code": "def metadata_updated_on ( item ) : ts = item [ MBox . DATE_FIELD ] ts = str_to_datetime ( ts ) return ts . timestamp ( )", "nl": "Extracts the update time from a MBox item ."}}
{"translation": {"code": "def __retrieve_archives ( self , from_date ) : archives = [ ] candidates = self . __list_supybot_archives ( ) for candidate in candidates : dt = self . __parse_date_from_filepath ( candidate ) if dt . date ( ) >= from_date . date ( ) : archives . append ( ( dt , candidate ) ) else : logger . debug ( \"Archive %s stored before %s; skipped\" , candidate , str ( from_date ) ) archives . sort ( key = lambda x : x [ 0 ] ) return [ archive [ 1 ] for archive in archives ]", "nl": "Retrieve the Supybot archives after the given date"}}
{"translation": {"code": "def __list_supybot_archives ( self ) : archives = [ ] for root , _ , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) archives . append ( location ) return archives", "nl": "List the filepath of the archives stored in dirpath"}}
{"translation": {"code": "def remove_archive ( self , archive_path ) : try : Archive ( archive_path ) except ArchiveError as e : raise ArchiveManagerError ( cause = str ( e ) ) os . remove ( archive_path )", "nl": "Remove an archive ."}}
{"translation": {"code": "def create_archive ( self ) : hashcode = uuid . uuid4 ( ) . hex archive_dir = os . path . join ( self . dirpath , hashcode [ 0 : 2 ] ) archive_name = hashcode [ 2 : ] + self . STORAGE_EXT archive_path = os . path . join ( archive_dir , archive_name ) if not os . path . exists ( archive_dir ) : os . makedirs ( archive_dir ) try : archive = Archive . create ( archive_path ) except ArchiveError as e : raise ArchiveManagerError ( cause = str ( e ) ) return archive", "nl": "Create a new archive ."}}
{"translation": {"code": "def _search_archives ( self , origin , backend_name , category , archived_after ) : for archive_path in self . _search_files ( ) : try : archive = Archive ( archive_path ) except ArchiveError : continue match = archive . origin == origin and archive . backend_name == backend_name and archive . category == category and archive . created_on >= archived_after if not match : continue yield archive_path , archive . created_on", "nl": "Search archives using filters ."}}
{"translation": {"code": "def _search_files ( self ) : for root , _ , files in os . walk ( self . dirpath ) : for filename in files : location = os . path . join ( root , filename ) yield location", "nl": "Retrieve the file paths stored under the base path ."}}
{"translation": {"code": "def _set_archive_arguments ( self ) : group = self . parser . add_argument_group ( 'archive arguments' ) group . add_argument ( '--archive-path' , dest = 'archive_path' , default = None , help = \"directory path to the archives\" ) group . add_argument ( '--no-archive' , dest = 'no_archive' , action = 'store_true' , help = \"do not archive data\" ) group . add_argument ( '--fetch-archive' , dest = 'fetch_archive' , action = 'store_true' , help = \"fetch data from the archives\" ) group . add_argument ( '--archived-since' , dest = 'archived_since' , default = '1970-01-01' , help = \"retrieve items archived since the given date\" )", "nl": "Activate archive arguments parsing"}}
{"translation": {"code": "def fetch_from_archive ( backend_class , backend_args , manager , category , archived_after ) : init_args = find_signature_parameters ( backend_class . __init__ , backend_args ) backend = backend_class ( * * init_args ) filepaths = manager . search ( backend . origin , backend . __class__ . __name__ , category , archived_after ) for filepath in filepaths : backend . archive = Archive ( filepath ) items = backend . fetch_from_archive ( ) try : for item in items : yield item except ArchiveError as e : logger . warning ( \"Ignoring %s archive due to: %s\" , filepath , str ( e ) )", "nl": "Fetch items from an archive manager ."}}
{"translation": {"code": "def __execute_from_remote ( self , cmd ) : result = None # data result from the cmd execution retries = 0 while retries < self . MAX_RETRIES : try : result = subprocess . check_output ( cmd , shell = True ) break except subprocess . CalledProcessError as ex : logger . error ( \"gerrit cmd %s failed: %s\" , cmd , ex ) time . sleep ( self . RETRY_WAIT * retries ) retries += 1 if result is None : result = RuntimeError ( cmd + \" failed \" + str ( self . MAX_RETRIES ) + \" times. Giving up!\" ) if self . archive : cmd = self . sanitize_for_archive ( cmd ) self . archive . store ( cmd , None , None , result ) if isinstance ( result , RuntimeError ) : raise result return result", "nl": "Execute gerrit command with retry if it fails"}}
{"translation": {"code": "def __execute_from_archive ( self , cmd ) : cmd = self . sanitize_for_archive ( cmd ) response = self . archive . retrieve ( cmd , None , None ) if isinstance ( response , RuntimeError ) : raise response return response", "nl": "Execute gerrit command against the archive"}}
{"translation": {"code": "def _parse_posts ( self , raw_posts ) : parsed_posts = self . parse_json ( raw_posts ) # Posts are not sorted. The order is provided by # 'order' key. for post_id in parsed_posts [ 'order' ] : yield parsed_posts [ 'posts' ] [ post_id ]", "nl": "Parse posts and returns in order ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] logger . info ( \"Fetching messages of '%s' - '%s' channel from %s\" , self . url , self . channel , str ( from_date ) ) fetching = True page = 0 nposts = 0 # Convert timestamp to integer for comparing since = int ( from_date . timestamp ( ) * 1000 ) while fetching : raw_posts = self . client . posts ( self . channel , page = page ) posts_before = nposts for post in self . _parse_posts ( raw_posts ) : if post [ 'update_at' ] < since : fetching = False break # Fetch user data user_id = post [ 'user_id' ] user = self . _get_or_fetch_user ( user_id ) post [ 'user_data' ] = user yield post nposts += 1 if fetching : # If no new posts were fetched; stop the process if posts_before == nposts : fetching = False else : page += 1 logger . info ( \"Fetch process completed: %s posts fetched\" , nposts )", "nl": "Fetch the messages ."}}
{"translation": {"code": "def __get_pull_commits ( self , pr_number ) : hashes = [ ] group_pull_commits = self . client . pull_commits ( pr_number ) for raw_pull_commits in group_pull_commits : for commit in json . loads ( raw_pull_commits ) : commit_hash = commit [ 'sha' ] hashes . append ( commit_hash ) return hashes", "nl": "Get pull request commit hashes"}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True ) # Backend token is required action = parser . parser . _option_string_actions [ '--api-token' ] action . required = True # Optional arguments group = parser . parser . add_argument_group ( 'Groupsio arguments' ) group . add_argument ( '--mboxes-path' , dest = 'mboxes_path' , help = \"Path where mbox files will be stored\" ) group . add_argument ( '--no-verify' , dest = 'verify' , action = 'store_false' , help = \"Value 'True' enable SSL verification\" ) # Required arguments parser . parser . add_argument ( 'group_name' , help = \"Name of the group on Groups.io\" ) return parser", "nl": "Returns the Groupsio argument parser ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_HITS ) : kwargs = { } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch data from Google API ."}}
{"translation": {"code": "def _init_rate_limit ( self ) : url = urijoin ( self . base_url , 'projects' , self . owner + '%2F' + self . repository ) try : response = super ( ) . fetch ( url ) self . update_rate_limit ( response ) except requests . exceptions . HTTPError as error : if error . response . status_code == 401 : raise error else : logger . warning ( \"Rate limit not initialized: %s\" , error )", "nl": "Initialize rate limit information"}}
{"translation": {"code": "def emojis ( self , item_type , item_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( item_type , str ( item_id ) , GitLabClient . EMOJI ) return self . fetch_items ( path , payload )", "nl": "Get emojis from pagination"}}
{"translation": {"code": "def __get_merge_versions ( self , merge_id ) : versions = [ ] group_versions = self . client . merge_versions ( merge_id ) for raw_versions in group_versions : for version in json . loads ( raw_versions ) : version_id = version [ 'id' ] version_full_raw = self . client . merge_version ( merge_id , version_id ) version_full = json . loads ( version_full_raw ) version_full . pop ( 'diffs' , None ) versions . append ( version_full ) return versions", "nl": "Get merge versions"}}
{"translation": {"code": "def __fetch_repo_info ( self ) : raw_repo = self . client . repo ( ) repo = json . loads ( raw_repo ) fetched_on = datetime_utcnow ( ) repo [ 'fetched_on' ] = fetched_on . timestamp ( ) yield repo", "nl": "Get repo info about stars watchers and forks"}}
{"translation": {"code": "def repo ( self ) : path = urijoin ( self . base_url , 'repos' , self . owner , self . repository ) r = self . fetch ( path ) repo = r . text return repo", "nl": "Get repository data"}}
{"translation": {"code": "def filter_classified_data ( self , item ) : item_uuid = uuid ( self . origin , self . metadata_id ( item ) ) logger . debug ( \"Filtering classified data for item %s\" , item_uuid ) for cf in self . CLASSIFIED_FIELDS : try : _remove_key_from_nested_dict ( item , cf ) except KeyError : logger . debug ( \"Classified field '%s' not found for item %s; field ignored\" , '.' . join ( cf ) , item_uuid ) logger . debug ( \"Classified data filtered for item %s\" , item_uuid ) return item", "nl": "Remove classified or confidential data from an item ."}}
{"translation": {"code": "def get_issues ( self , from_date ) : url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , 'search' ) issues = self . get_items ( from_date , url ) return issues", "nl": "Retrieve all the issues from a given date ."}}