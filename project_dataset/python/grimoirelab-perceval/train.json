{"translation": {"code": "def bugs ( self , * bug_ids ) : params = { self . PBUG_ID : bug_ids , self . PCTYPE : self . CTYPE_XML , self . PEXCLUDE_FIELD : 'attachmentdata' } response = self . call ( self . CGI_BUG , params ) return response", "nl": "Get the information of a list of bugs in XML format ."}}
{"translation": {"code": "def metadata ( self ) : params = { self . PCTYPE : self . CTYPE_XML } response = self . call ( self . CGI_BUG , params ) return response", "nl": "Get metadata information in XML format ."}}
{"translation": {"code": "def bug_activity ( self , bug_id ) : params = { self . PBUG_ID : bug_id } response = self . call ( self . CGI_BUG_ACTIVITY , params ) return response", "nl": "Get the activity of a bug in HTML format ."}}
{"translation": {"code": "def buglist ( self , from_date = DEFAULT_DATETIME ) : if not self . version : self . version = self . __fetch_version ( ) if self . version in self . OLD_STYLE_VERSIONS : order = 'Last+Changed' else : order = 'changeddate' date = from_date . strftime ( \"%Y-%m-%d %H:%M:%S\" ) params = { self . PCHFIELD_FROM : date , self . PCTYPE : self . CTYPE_CSV , self . PLIMIT : self . max_bugs_csv , self . PORDER : order } response = self . call ( self . CGI_BUGLIST , params ) return response", "nl": "Get a summary of bugs in CSV format ."}}
{"translation": {"code": "def parse_buglist ( raw_csv ) : reader = csv . DictReader ( raw_csv . split ( '\\n' ) , delimiter = ',' , quotechar = '\"' ) for row in reader : yield row", "nl": "Parse a Bugzilla CSV bug list ."}}
{"translation": {"code": "def xml_to_dict ( raw_xml ) : def node_to_dict ( node ) : d = { } d . update ( node . items ( ) ) text = getattr ( node , 'text' , None ) if text is not None : d [ '__text__' ] = text childs = { } for child in node : childs . setdefault ( child . tag , [ ] ) . append ( node_to_dict ( child ) ) d . update ( childs . items ( ) ) return d purged_xml = remove_invalid_xml_chars ( raw_xml ) try : tree = xml . etree . ElementTree . fromstring ( purged_xml ) except xml . etree . ElementTree . ParseError as e : cause = \"XML stream %s\" % ( str ( e ) ) raise ParseError ( cause = cause ) d = node_to_dict ( tree ) return d", "nl": "Convert a XML stream into a dictionary ."}}
{"translation": {"code": "def parse_bugs_details ( raw_xml ) : bugs = xml_to_dict ( raw_xml ) if 'bug' not in bugs : cause = \"No bugs found. XML stream seems to be invalid.\" raise ParseError ( cause = cause ) for bug in bugs [ 'bug' ] : yield bug", "nl": "Parse a Bugilla bugs details XML stream ."}}
{"translation": {"code": "def parse_bug_activity ( raw_html ) : def is_activity_empty ( bs ) : EMPTY_ACTIVITY = \"No changes have been made to this (?:bug|issue) yet.\" tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) return tag is not None def find_activity_table ( bs ) : # The first table with 5 columns is the table of activity tables = bs . find_all ( 'table' ) for tb in tables : nheaders = len ( tb . tr . find_all ( 'th' , recursive = False ) ) if nheaders == 5 : return tb raise ParseError ( cause = \"Table of bug activity not found.\" ) def remove_tags ( bs ) : HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : tag . replaceWith ( tag . text ) def format_text ( bs ) : strings = [ s . strip ( ' \\n\\t' ) for s in bs . stripped_strings ] s = ' ' . join ( strings ) return s # Parsing starts here bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) if is_activity_empty ( bs ) : fields = [ ] else : activity_tb = find_activity_table ( bs ) remove_tags ( activity_tb ) fields = activity_tb . find_all ( 'td' ) while fields : # First two fields: 'Who' and 'When'. who = fields . pop ( 0 ) when = fields . pop ( 0 ) # The attribute 'rowspan' of 'who' field tells how many # changes were made on the same date. n = int ( who . get ( 'rowspan' ) ) # Next fields are split into chunks of three elements: # 'What', 'Removed' and 'Added'. These chunks share # 'Who' and 'When' values. for _ in range ( n ) : what = fields . pop ( 0 ) removed = fields . pop ( 0 ) added = fields . pop ( 0 ) event = { 'Who' : format_text ( who ) , 'When' : format_text ( when ) , 'What' : format_text ( what ) , 'Removed' : format_text ( removed ) , 'Added' : format_text ( added ) } yield event", "nl": "Parse a Bugzilla bug activity HTML stream ."}}
{"translation": {"code": "def _set_output_arguments ( self ) : group = self . parser . add_argument_group ( 'output arguments' ) group . add_argument ( '-o' , '--output' , type = argparse . FileType ( 'w' ) , dest = 'outfile' , default = sys . stdout , help = \"output file\" ) group . add_argument ( '--json-line' , dest = 'json_line' , action = 'store_true' , help = \"produce a JSON line for each output item\" )", "nl": "Activate output arguments parsing"}}
{"translation": {"code": "def reviews ( self , last_item , filter_ = None ) : cmd = self . _get_gerrit_cmd ( last_item , filter_ ) logger . debug ( \"Getting reviews with command: %s\" , cmd ) raw_data = self . __execute ( cmd ) raw_data = str ( raw_data , \"UTF-8\" ) return raw_data", "nl": "Get the reviews starting from last_item ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] if self . client . version [ 0 ] == 2 and self . client . version [ 1 ] == 8 : fetcher = self . _fetch_gerrit28 ( from_date ) else : fetcher = self . _fetch_gerrit ( from_date ) for review in fetcher : yield review", "nl": "Fetch the reviews"}}
{"translation": {"code": "def version ( self ) : if self . _version : return self . _version cmd = self . gerrit_cmd + \" %s \" % ( GerritClient . CMD_VERSION ) logger . debug ( \"Getting version: %s\" % ( cmd ) ) raw_data = self . __execute ( cmd ) raw_data = str ( raw_data , \"UTF-8\" ) logger . debug ( \"Gerrit version: %s\" % ( raw_data ) ) # output: gerrit version 2.10-rc1-988-g333a9dd m = re . match ( GerritClient . VERSION_REGEX , raw_data ) if not m : cause = \"Invalid gerrit version %s\" % raw_data raise BackendError ( cause = cause ) try : mayor = int ( m . group ( 1 ) ) minor = int ( m . group ( 2 ) ) except Exception : cause = \"Gerrit client could not determine the server version.\" raise BackendError ( cause = cause ) self . _version = [ mayor , minor ] return self . _version", "nl": "Return the Gerrit server version ."}}
{"translation": {"code": "def next_retrieve_group_item ( self , last_item = None , entry = None ) : next_item = None gerrit_version = self . version if gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] > 9 : if last_item is None : next_item = 0 else : next_item = last_item elif gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] == 9 : # https://groups.google.com/forum/#!topic/repo-discuss/yQgRR5hlS3E cause = \"Gerrit 2.9.0 does not support pagination\" raise BackendError ( cause = cause ) else : if entry is not None : next_item = entry [ 'sortKey' ] return next_item", "nl": "Return the item to start from in next reviews group ."}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , archive = True ) # Gerrit options group = parser . parser . add_argument_group ( 'Gerrit arguments' ) group . add_argument ( '--user' , dest = 'user' , help = \"Gerrit ssh user\" ) group . add_argument ( '--max-reviews' , dest = 'max_reviews' , type = int , default = MAX_REVIEWS , help = \"Max number of reviews per ssh query.\" ) group . add_argument ( '--blacklist-reviews' , dest = 'blacklist_reviews' , nargs = '*' , help = \"Wrong reviews that must not be retrieved.\" ) group . add_argument ( '--disable-host-key-check' , dest = 'disable_host_key_check' , action = 'store_true' , help = \"Don't check remote host identity\" ) group . add_argument ( '--ssh-port' , dest = 'port' , default = PORT , type = int , help = \"Set SSH port of the Gerrit server\" ) # Required arguments parser . parser . add_argument ( 'hostname' , help = \"Hostname of the Gerrit server\" ) return parser", "nl": "Returns the Gerrit argument parser ."}}
{"translation": {"code": "def parse ( self ) : for line in self . stream : line = line . rstrip ( '\\n' ) parsed = False self . nline += 1 while not parsed : parsed = self . handlers [ self . state ] ( line ) if self . state == self . COMMIT and self . commit : commit = self . _build_commit ( ) logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) yield commit # Return the last commit, if any if self . commit : commit = self . _build_commit ( ) logger . debug ( \"Commit %s parsed\" , commit [ 'commit' ] ) yield commit", "nl": "Parse the Git log stream ."}}
{"translation": {"code": "def parse_git_log_from_file ( filepath ) : with open ( filepath , 'r' , errors = 'surrogateescape' , newline = os . linesep ) as f : parser = GitParser ( f ) for commit in parser . parse ( ) : yield commit", "nl": "Parse a Git log file ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_COMMIT , from_date = DEFAULT_DATETIME , to_date = DEFAULT_LAST_DATETIME , branches = None , latest_items = False , no_update = False ) : if not from_date : from_date = DEFAULT_DATETIME if not to_date : to_date = DEFAULT_LAST_DATETIME kwargs = { 'from_date' : from_date , 'to_date' : to_date , 'branches' : branches , 'latest_items' : latest_items , 'no_update' : no_update } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch commits ."}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , to_date = True ) # Optional arguments group = parser . parser . add_argument_group ( 'Git arguments' ) group . add_argument ( '--branches' , dest = 'branches' , nargs = '+' , type = str , default = None , help = \"Fetch commits only from these branches\" ) # Mutual exclusive parameters exgroup = group . add_mutually_exclusive_group ( ) exgroup . add_argument ( '--git-path' , dest = 'git_path' , help = \"Path where the Git repository will be cloned\" ) exgroup . add_argument ( '--git-log' , dest = 'git_log' , help = \"Path to the Git log file\" ) exgroup_fetch = group . add_mutually_exclusive_group ( ) exgroup_fetch . add_argument ( '--latest-items' , dest = 'latest_items' , action = 'store_true' , help = \"Fetch latest commits added to the repository\" ) exgroup_fetch . add_argument ( '--no-update' , dest = 'no_update' , action = 'store_true' , help = \"Fetch all commits without updating the repository\" ) # Required arguments parser . parser . add_argument ( 'uri' , help = \"URI of the Git log repository\" ) return parser", "nl": "Returns the Git argument parser ."}}
{"translation": {"code": "def _pre_init ( self ) : if self . parsed_args . git_log : git_path = self . parsed_args . git_log elif not self . parsed_args . git_path : base_path = os . path . expanduser ( '~/.perceval/repositories/' ) processed_uri = self . parsed_args . uri . lstrip ( '/' ) git_path = os . path . join ( base_path , processed_uri ) + '-git' else : git_path = self . parsed_args . git_path setattr ( self . parsed_args , 'gitpath' , git_path )", "nl": "Initialize repositories directory path"}}
{"translation": {"code": "def __get_user ( self , login ) : user = { } if not login : return user user_raw = self . client . user ( login ) user = json . loads ( user_raw ) user_orgs_raw = self . client . user_orgs ( login ) user [ 'organizations' ] = json . loads ( user_orgs_raw ) return user", "nl": "Get user and org data for the login"}}
{"translation": {"code": "def check_compressed_file_type ( filepath ) : def compressed_file_type ( content ) : magic_dict = { b'\\x1f\\x8b\\x08' : 'gz' , b'\\x42\\x5a\\x68' : 'bz2' , b'PK\\x03\\x04' : 'zip' } for magic , filetype in magic_dict . items ( ) : if content . startswith ( magic ) : return filetype return None with open ( filepath , mode = 'rb' ) as f : magic_number = f . read ( 4 ) return compressed_file_type ( magic_number )", "nl": "Check if filename is a compressed file supported by the tool ."}}
{"translation": {"code": "def remove_invalid_xml_chars ( raw_xml ) : illegal_unichrs = [ ( 0x00 , 0x08 ) , ( 0x0B , 0x1F ) , ( 0x7F , 0x84 ) , ( 0x86 , 0x9F ) ] illegal_ranges = [ '%s-%s' % ( chr ( low ) , chr ( high ) ) for ( low , high ) in illegal_unichrs if low < sys . maxunicode ] illegal_xml_re = re . compile ( '[%s]' % '' . join ( illegal_ranges ) ) purged_xml = '' for c in raw_xml : if illegal_xml_re . search ( c ) is not None : c = ' ' purged_xml += c return purged_xml", "nl": "Remove control and invalid characters from an xml stream ."}}
{"translation": {"code": "def get_questions ( self , from_date ) : page = 1 url = urijoin ( self . base_url , self . VERSION_API , \"questions\" ) req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) questions = req . text data = req . json ( ) tquestions = data [ 'total' ] nquestions = data [ 'page_size' ] self . __log_status ( data [ 'quota_remaining' ] , data [ 'quota_max' ] , nquestions , tquestions ) while questions : yield questions questions = None if data [ 'has_more' ] : page += 1 backoff = data . get ( 'backoff' , None ) if backoff : logger . debug ( \"Expensive query. Wait %s secs to send a new request\" , backoff ) time . sleep ( float ( backoff ) ) req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) data = req . json ( ) questions = req . text nquestions += data [ 'page_size' ] self . __log_status ( data [ 'quota_remaining' ] , data [ 'quota_max' ] , nquestions , tquestions )", "nl": "Retrieve all the questions from a given date ."}}
{"translation": {"code": "def parse_questions ( raw_page ) : raw_questions = json . loads ( raw_page ) questions = raw_questions [ 'items' ] for question in questions : yield question", "nl": "Parse a StackExchange API raw response ."}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True , archive = True ) # StackExchange options group = parser . parser . add_argument_group ( 'StackExchange arguments' ) group . add_argument ( '--site' , dest = 'site' , required = True , help = \"StackExchange site\" ) group . add_argument ( '--tagged' , dest = 'tagged' , help = \"filter items by question Tag\" ) group . add_argument ( '--max-questions' , dest = 'max_questions' , type = int , default = MAX_QUESTIONS , help = \"Maximum number of questions requested in the same query\" ) return parser", "nl": "Returns the StackExchange argument parser ."}}
{"translation": {"code": "def _fetch_and_parse_messages ( self , mailing_list , from_date ) : from_date = datetime_to_utc ( from_date ) nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) for mbox in mailing_list . mboxes : tmp_path = None try : tmp_path = self . _copy_mbox ( mbox ) for message in self . parse_mbox ( tmp_path ) : tmsgs += 1 if not self . _validate_message ( message ) : imsgs += 1 continue # Ignore those messages sent before the given date dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) if dt < from_date : logger . debug ( \"Message %s sent before %s; skipped\" , message [ 'unixfrom' ] , str ( from_date ) ) tmsgs -= 1 continue # Convert 'CaseInsensitiveDict' to dict message = self . _casedict_to_dict ( message ) nmsgs += 1 logger . debug ( \"Message %s parsed\" , message [ 'unixfrom' ] ) yield message except ( OSError , EOFError ) as e : logger . warning ( \"Ignoring %s mbox due to: %s\" , mbox . filepath , str ( e ) ) except Exception as e : if tmp_path and os . path . exists ( tmp_path ) : os . remove ( tmp_path ) raise e finally : if tmp_path and os . path . exists ( tmp_path ) : os . remove ( tmp_path ) logger . info ( \"Done. %s/%s messages fetched; %s ignored\" , nmsgs , tmsgs , imsgs )", "nl": "Fetch and parse the messages from a mailing list"}}
{"translation": {"code": "def _validate_message ( self , message ) : # This check is \"case insensitive\" because we're # using 'CaseInsensitiveDict' from requests.structures # module to store the contents of a message. if self . MESSAGE_ID_FIELD not in message : logger . warning ( \"Field 'Message-ID' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if not message [ self . MESSAGE_ID_FIELD ] : logger . warning ( \"Field 'Message-ID' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if self . DATE_FIELD not in message : logger . warning ( \"Field 'Date' not found in message %s; ignoring\" , message [ 'unixfrom' ] ) return False if not message [ self . DATE_FIELD ] : logger . warning ( \"Field 'Date' is empty in message %s; ignoring\" , message [ 'unixfrom' ] ) return False try : str_to_datetime ( message [ self . DATE_FIELD ] ) except InvalidDateError : logger . warning ( \"Invalid date %s in message %s; ignoring\" , message [ self . DATE_FIELD ] , message [ 'unixfrom' ] ) return False return True", "nl": "Check if the given message has the mandatory fields"}}
{"translation": {"code": "def _copy_mbox ( self , mbox ) : tmp_path = tempfile . mktemp ( prefix = 'perceval_' ) with mbox . container as f_in : with open ( tmp_path , mode = 'wb' ) as f_out : for l in f_in : f_out . write ( l ) return tmp_path", "nl": "Copy the contents of a mbox to a temporary file"}}
{"translation": {"code": "def parse_issues ( raw_page ) : raw_issues = json . loads ( raw_page ) issues = raw_issues [ 'issues' ] for issue in issues : yield issue", "nl": "Parse a JIRA API raw response ."}}
{"translation": {"code": "def clone ( cls , uri , dirpath ) : cmd = [ 'git' , 'clone' , '--bare' , uri , dirpath ] env = { 'LANG' : 'C' , 'HOME' : os . getenv ( 'HOME' , '' ) } cls . _exec ( cmd , env = env ) logger . debug ( \"Git %s repository cloned into %s\" , uri , dirpath ) return cls ( uri , dirpath )", "nl": "Clone a Git repository ."}}
{"translation": {"code": "def update ( self ) : cmd_update = [ 'git' , 'fetch' , 'origin' , '+refs/heads/*:refs/heads/*' , '--prune' ] self . _exec ( cmd_update , cwd = self . dirpath , env = self . gitenv ) logger . debug ( \"Git %s repository updated into %s\" , self . uri , self . dirpath )", "nl": "Update repository from its remote ."}}
{"translation": {"code": "def _exec ( cmd , cwd = None , env = None , ignored_error_codes = None , encoding = 'utf-8' ) : if ignored_error_codes is None : ignored_error_codes = [ ] logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) try : proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) ( outs , errs ) = proc . communicate ( ) except OSError as e : raise RepositoryError ( cause = str ( e ) ) if proc . returncode != 0 and proc . returncode not in ignored_error_codes : err = errs . decode ( encoding , errors = 'surrogateescape' ) cause = \"git command - %s\" % err raise RepositoryError ( cause = cause ) else : logger . debug ( errs . decode ( encoding , errors = 'surrogateescape' ) ) return outs", "nl": "Run a command ."}}
{"translation": {"code": "def log ( self , from_date = None , to_date = None , branches = None , encoding = 'utf-8' ) : if self . is_empty ( ) : logger . warning ( \"Git %s repository is empty; unable to get the log\" , self . uri ) raise EmptyRepositoryError ( repository = self . uri ) cmd_log = [ 'git' , 'log' , '--reverse' , '--topo-order' ] cmd_log . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) if from_date : dt = from_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) cmd_log . append ( '--since=' + dt ) if to_date : dt = to_date . strftime ( \"%Y-%m-%d %H:%M:%S %z\" ) cmd_log . append ( '--until=' + dt ) if branches is None : cmd_log . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) elif len ( branches ) == 0 : cmd_log . append ( '--max-count=0' ) else : branches = [ 'refs/heads/' + branch for branch in branches ] cmd_log . extend ( branches ) for line in self . _exec_nb ( cmd_log , cwd = self . dirpath , env = self . gitenv ) : yield line logger . debug ( \"Git log fetched from %s repository (%s)\" , self . uri , self . dirpath )", "nl": "Read the commit log from the repository ."}}
{"translation": {"code": "def _exec_nb ( self , cmd , cwd = None , env = None , encoding = 'utf-8' ) : self . failed_message = None logger . debug ( \"Running command %s (cwd: %s, env: %s)\" , ' ' . join ( cmd ) , cwd , str ( env ) ) try : self . proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = cwd , env = env ) err_thread = threading . Thread ( target = self . _read_stderr , kwargs = { 'encoding' : encoding } , daemon = True ) err_thread . start ( ) for line in self . proc . stdout : yield line . decode ( encoding , errors = 'surrogateescape' ) err_thread . join ( ) self . proc . communicate ( ) self . proc . stdout . close ( ) self . proc . stderr . close ( ) except OSError as e : err_thread . join ( ) raise RepositoryError ( cause = str ( e ) ) if self . proc . returncode != 0 : cause = \"git command - %s (return code: %d)\" % ( self . failed_message , self . proc . returncode ) raise RepositoryError ( cause = cause )", "nl": "Run a command with a non blocking call ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] logger . info ( \"Looking for topics at '%s', updated from '%s'\" , self . url , str ( from_date ) ) ntopics = 0 topics_ids = self . __fetch_and_parse_topics_ids ( from_date ) for topic_id in topics_ids : topic = self . __fetch_and_parse_topic ( topic_id ) ntopics += 1 yield topic logger . info ( \"Fetch process completed: %s topics fetched\" , ntopics )", "nl": "Fetch the topics"}}
{"translation": {"code": "def topic ( self , topic_id ) : params = { self . PKEY : self . api_key } # http://example.com/t/8.json response = self . _call ( self . TOPIC , topic_id , params = params ) return response", "nl": "Retrive the topic with topic_id identifier ."}}
{"translation": {"code": "def logout ( self ) : params = { self . PLOGOUT : '1' } self . call ( self . CGI_LOGIN , params ) self . _close_http_session ( ) logger . debug ( \"Bugzilla user logged out from %s\" , self . base_url )", "nl": "Logout from the server ."}}
{"translation": {"code": "def metadata_updated_on ( item ) : ts = item [ 'delta_ts' ] [ 0 ] [ '__text__' ] ts = str_to_datetime ( ts ) ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) return ts . timestamp ( )", "nl": "Extracts and coverts the update time from a Bugzilla item ."}}
{"translation": {"code": "def uuid ( * args ) : def check_value ( v ) : if not isinstance ( v , str ) : raise ValueError ( \"%s value is not a string instance\" % str ( v ) ) elif not v : raise ValueError ( \"value cannot be None or empty\" ) else : return v s = ':' . join ( map ( check_value , args ) ) sha1 = hashlib . sha1 ( s . encode ( 'utf-8' , errors = 'surrogateescape' ) ) uuid_sha1 = sha1 . hexdigest ( ) return uuid_sha1", "nl": "Generate a UUID based on the given parameters ."}}
{"translation": {"code": "def _casedict_to_dict ( self , message ) : message_id = message . pop ( self . MESSAGE_ID_FIELD ) date = message . pop ( self . DATE_FIELD ) msg = { k : v for k , v in message . items ( ) } msg [ self . MESSAGE_ID_FIELD ] = message_id msg [ self . DATE_FIELD ] = date return msg", "nl": "Convert a message in CaseInsensitiveDict to dict ."}}
{"translation": {"code": "def _read_stderr ( self , encoding = 'utf-8' ) : for line in self . proc . stderr : err_line = line . decode ( encoding , errors = 'surrogateescape' ) if self . proc . returncode != 0 : # If the subprocess didn't finish successfully, we expect # the last line in stderr to provide the cause if self . failed_message is not None : # We had a message, there is a newer line, print it logger . debug ( \"Git log stderr: \" + self . failed_message ) self . failed_message = err_line else : # The subprocess is successfully up to now, print the line logger . debug ( \"Git log stderr: \" + err_line )", "nl": "Reads self . proc . stderr ."}}
{"translation": {"code": "def _fetch_gerrit28 ( self , from_date = DEFAULT_DATETIME ) : # Convert date to Unix time from_ut = datetime_to_utc ( from_date ) from_ut = from_ut . timestamp ( ) filter_open = \"status:open\" filter_closed = \"status:closed\" last_item_open = self . client . next_retrieve_group_item ( ) last_item_closed = self . client . next_retrieve_group_item ( ) reviews_open = self . _get_reviews ( last_item_open , filter_open ) reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) last_nreviews_open = len ( reviews_open ) last_nreviews_closed = len ( reviews_closed ) while reviews_open or reviews_closed : if reviews_open and reviews_closed : if reviews_open [ 0 ] [ 'lastUpdated' ] >= reviews_closed [ 0 ] [ 'lastUpdated' ] : review_open = reviews_open . pop ( 0 ) review = review_open else : review_closed = reviews_closed . pop ( 0 ) review = review_closed elif reviews_closed : review_closed = reviews_closed . pop ( 0 ) review = review_closed else : review_open = reviews_open . pop ( 0 ) review = review_open updated = review [ 'lastUpdated' ] if updated <= from_ut : logger . debug ( \"No more updates for %s\" % ( self . hostname ) ) break else : yield review if not reviews_open and last_nreviews_open >= self . max_reviews : last_item_open = self . client . next_retrieve_group_item ( last_item_open , review_open ) reviews_open = self . _get_reviews ( last_item_open , filter_open ) last_nreviews_open = len ( reviews_open ) if not reviews_closed and last_nreviews_closed >= self . max_reviews : last_item_closed = self . client . next_retrieve_group_item ( last_item_closed , review_closed ) reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) last_nreviews_closed = len ( reviews_closed )", "nl": "Specific fetch for gerrit 2 . 8 version ."}}
{"translation": {"code": "def get_jobs ( self ) : url_jenkins = urijoin ( self . base_url , \"api\" , \"json\" ) response = self . fetch ( url_jenkins ) return response . text", "nl": "Retrieve all jobs"}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_BUILD ) : kwargs = { } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch the builds from the url ."}}
{"translation": {"code": "def get_builds ( self , job_name ) : if self . blacklist_jobs and job_name in self . blacklist_jobs : logger . warning ( \"Not getting blacklisted job: %s\" , job_name ) return payload = { 'depth' : self . detail_depth } url_build = urijoin ( self . base_url , \"job\" , job_name , \"api\" , \"json\" ) response = self . fetch ( url_build , payload = payload ) return response . text", "nl": "Retrieve all builds from a job"}}
{"translation": {"code": "def get_recent_pages ( self , namespaces , rccontinue = '' ) : namespaces . sort ( ) params = { \"action\" : \"query\" , \"list\" : \"recentchanges\" , \"rclimit\" : self . limit , \"rcnamespace\" : \"|\" . join ( namespaces ) , \"rcprop\" : \"title|timestamp|ids\" , \"format\" : \"json\" } if rccontinue : params [ 'rccontinue' ] = rccontinue return self . call ( params )", "nl": "Retrieve recent pages from all namespaces starting from rccontinue ."}}
{"translation": {"code": "def get_pages ( self , namespace , apcontinue = '' ) : params = { \"action\" : \"query\" , \"list\" : \"allpages\" , \"aplimit\" : self . limit , \"apnamespace\" : namespace , \"format\" : \"json\" } if apcontinue : params [ 'apcontinue' ] = apcontinue return self . call ( params )", "nl": "Retrieve all pages from a namespace starting from apcontinue ."}}
{"translation": {"code": "def __fetch_1_27 ( self , from_date = None ) : logger . info ( \"Looking for pages at url '%s'\" , self . url ) npages = 0 # number of pages processed tpages = 0 # number of total pages pages_done = [ ] # pages already retrieved in reviews API namespaces_contents = self . __get_namespaces_contents ( ) arvcontinue = '' # pagination for getting revisions and their pages while arvcontinue is not None : raw_pages = self . client . get_pages_from_allrevisions ( namespaces_contents , from_date , arvcontinue ) data_json = json . loads ( raw_pages ) arvcontinue = data_json [ 'continue' ] [ 'arvcontinue' ] if 'continue' in data_json else None pages_json = data_json [ 'query' ] [ 'allrevisions' ] for page in pages_json : if page [ 'pageid' ] in pages_done : logger . debug ( \"Page %s already processed; skipped\" , page [ 'pageid' ] ) continue tpages += 1 pages_done . append ( page [ 'pageid' ] ) page_reviews = self . __get_page_reviews ( page ) if not page_reviews : logger . warning ( \"Revisions not found in %s [page id: %s], page skipped\" , page [ 'title' ] , page [ 'pageid' ] ) continue yield page_reviews npages += 1 logger . info ( \"Total number of pages: %i, skipped %i\" , tpages , tpages - npages )", "nl": "Fetch the pages from the backend url for MediaWiki > = 1 . 27"}}
{"translation": {"code": "def __get_max_date ( self , reviews ) : max_ts = 0 for review in reviews : ts = str_to_datetime ( review [ 'timestamp' ] ) ts = datetime_to_utc ( ts ) if ts . timestamp ( ) > max_ts : max_ts = ts . timestamp ( ) return max_ts", "nl": "Get the max date in unixtime format from reviews ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] reviews_api = kwargs [ 'reviews_api' ] mediawiki_version = self . client . get_version ( ) logger . info ( \"MediaWiki version: %s\" , mediawiki_version ) if reviews_api : if ( ( mediawiki_version [ 0 ] == 1 and mediawiki_version [ 1 ] >= 27 ) or mediawiki_version [ 0 ] > 1 ) : fetcher = self . __fetch_1_27 ( from_date ) else : logger . warning ( \"Reviews API only available in MediaWiki >= 1.27\" ) logger . warning ( \"Using the Pages API instead\" ) fetcher = self . __fetch_pre1_27 ( from_date ) else : fetcher = self . __fetch_pre1_27 ( from_date ) for page_reviews in fetcher : yield page_reviews", "nl": "Fetch the pages"}}
{"translation": {"code": "def post ( self , post_id ) : params = { self . PKEY : self . api_key } # http://example.com/posts/10.json response = self . _call ( self . POSTS , post_id , params = params ) return response", "nl": "Retrieve the post whit post_id identifier ."}}
{"translation": {"code": "def __parse_topics_page ( self , raw_json ) : topics_page = json . loads ( raw_json ) topics_ids = [ ] for topic in topics_page [ 'topic_list' ] [ 'topics' ] : topic_id = topic [ 'id' ] if topic [ 'last_posted_at' ] is None : logger . warning ( \"Topic %s with last_posted_at null. Ignoring it.\" , topic [ 'title' ] ) continue updated_at = str_to_datetime ( topic [ 'last_posted_at' ] ) pinned = topic [ 'pinned' ] topics_ids . append ( ( topic_id , updated_at , pinned ) ) return topics_ids", "nl": "Parse a topics page stream ."}}
{"translation": {"code": "def bugs ( self , from_date = DEFAULT_DATETIME , offset = None , max_bugs = MAX_BUGS ) : date = datetime_to_utc ( from_date ) date = date . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" ) params = { self . PLAST_CHANGE_TIME : date , self . PLIMIT : max_bugs , self . PORDER : self . VCHANGE_DATE_ORDER , self . PINCLUDE_FIELDS : self . VINCLUDE_ALL } if offset : params [ self . POFFSET ] = offset response = self . call ( self . RBUG , params ) return response", "nl": "Get the information of a list of bugs ."}}
{"translation": {"code": "def comments ( self , * bug_ids ) : # Hack. The first value must be a valid bug id resource = urijoin ( self . RBUG , bug_ids [ 0 ] , self . RCOMMENT ) params = { self . PIDS : bug_ids } response = self . call ( resource , params ) return response", "nl": "Get the comments of the given bugs ."}}
{"translation": {"code": "def history ( self , * bug_ids ) : resource = urijoin ( self . RBUG , bug_ids [ 0 ] , self . RHISTORY ) params = { self . PIDS : bug_ids } response = self . call ( resource , params ) return response", "nl": "Get the history of the given bugs ."}}
{"translation": {"code": "def attachments ( self , * bug_ids ) : resource = urijoin ( self . RBUG , bug_ids [ 0 ] , self . RATTACHMENT ) params = { self . PIDS : bug_ids , self . PEXCLUDE_FIELDS : self . VEXCLUDE_ATTCH_DATA } response = self . call ( resource , params ) return response", "nl": "Get the attachments of the given bugs ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_BUG , from_date = DEFAULT_DATETIME ) : if not from_date : from_date = DEFAULT_DATETIME kwargs = { 'from_date' : from_date } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch the bugs from the repository ."}}
{"translation": {"code": "def _parse_supybot_msg ( self , line ) : patterns = [ ( self . SUPYBOT_COMMENT_REGEX , self . TCOMMENT ) , ( self . SUPYBOT_COMMENT_ACTION_REGEX , self . TCOMMENT ) , ( self . SUPYBOT_SERVER_REGEX , self . TSERVER ) , ( self . SUPYBOT_BOT_REGEX , self . TCOMMENT ) ] for p in patterns : m = p [ 0 ] . match ( line ) if not m : continue return p [ 1 ] , m . group ( 'nick' ) , m . group ( 'body' ) . strip ( ) msg = \"invalid message on line %s\" % ( str ( self . nline ) ) raise ParseError ( cause = msg )", "nl": "Parse message section"}}
{"translation": {"code": "def parse ( self ) : for line in self . stream : line = line . rstrip ( '\\n' ) self . nline += 1 if self . SUPYBOT_EMPTY_REGEX . match ( line ) : continue ts , msg = self . _parse_supybot_timestamp ( line ) if self . SUPYBOT_EMPTY_COMMENT_REGEX . match ( msg ) : continue elif self . SUPYBOT_EMPTY_COMMENT_ACTION_REGEX . match ( msg ) : continue elif self . SUPYBOT_EMPTY_BOT_REGEX . match ( msg ) : continue itype , nick , body = self . _parse_supybot_msg ( msg ) item = self . _build_item ( ts , itype , nick , body ) yield item", "nl": "Parse a Supybot IRC stream ."}}
{"translation": {"code": "def _parse_supybot_timestamp ( self , line ) : m = self . SUPYBOT_TIMESTAMP_REGEX . match ( line ) if not m : msg = \"date expected on line %s\" % ( str ( self . nline ) ) raise ParseError ( cause = msg ) ts = m . group ( 'ts' ) msg = m . group ( 'msg' ) return ts , msg", "nl": "Parse timestamp section"}}
{"translation": {"code": "def parse_supybot_log ( filepath ) : with open ( filepath , 'r' , errors = 'surrogateescape' , newline = os . linesep ) as f : parser = SupybotParser ( f ) try : for message in parser . parse ( ) : yield message except ParseError as e : cause = \"file: %s; reason: %s\" % ( filepath , str ( e ) ) raise ParseError ( cause = cause )", "nl": "Parse a Supybot IRC log file ."}}
{"translation": {"code": "def updates ( self , offset = None ) : params = { } if offset : params [ self . OFFSET ] = offset response = self . _call ( self . UPDATES_METHOD , params ) return response", "nl": "Fetch the messages that a bot can read ."}}
{"translation": {"code": "def parse_messages ( raw_json ) : result = json . loads ( raw_json ) messages = result [ 'result' ] for msg in messages : yield msg", "nl": "Parse a Telegram JSON messages list ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_MESSAGE , offset = DEFAULT_OFFSET , chats = None ) : if not offset : offset = DEFAULT_OFFSET kwargs = { \"offset\" : offset , \"chats\" : chats } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch the messages the bot can read from the server ."}}
{"translation": {"code": "def contents ( self , from_date = DEFAULT_DATETIME , offset = None , max_contents = MAX_CONTENTS ) : resource = self . RCONTENTS + '/' + self . MSEARCH # Set confluence query parameter (cql) date = from_date . strftime ( \"%Y-%m-%d %H:%M\" ) cql = self . VCQL % { 'date' : date } # Set parameters params = { self . PCQL : cql , self . PLIMIT : max_contents , self . PEXPAND : self . PANCESTORS } if offset : params [ self . PSTART ] = offset for response in self . _call ( resource , params ) : yield response", "nl": "Get the contents of a repository ."}}
{"translation": {"code": "def historical_content ( self , content_id , version ) : resource = self . RCONTENTS + '/' + str ( content_id ) params = { self . PVERSION : version , self . PSTATUS : self . VHISTORICAL , self . PEXPAND : ',' . join ( self . VEXPAND ) } # Only one item is returned response = [ response for response in self . _call ( resource , params ) ] return response [ 0 ]", "nl": "Get the snapshot of a content for the given version ."}}
{"translation": {"code": "def parse_contents_summary ( raw_json ) : summary = json . loads ( raw_json ) contents = summary [ 'results' ] for c in contents : yield c", "nl": "Parse a Confluence summary JSON list ."}}
{"translation": {"code": "def metadata_id ( item ) : cid = item [ 'id' ] cversion = item [ 'version' ] [ 'number' ] return str ( cid ) + '#v' + str ( cversion )", "nl": "Extracts the identifier from a Confluence item ."}}
{"translation": {"code": "def map_custom_field ( custom_fields , fields ) : def build_cf ( cf , v ) : return { 'id' : cf [ 'id' ] , 'name' : cf [ 'name' ] , 'value' : v } return { k : build_cf ( custom_fields [ k ] , v ) for k , v in fields . items ( ) if k in custom_fields }", "nl": "Add extra information for custom fields ."}}
{"translation": {"code": "def filter_custom_fields ( fields ) : custom_fields = { } sorted_fields = [ field for field in fields if field [ 'custom' ] is True ] for custom_field in sorted_fields : custom_fields [ custom_field [ 'id' ] ] = custom_field return custom_fields", "nl": "Filter custom fields from a given set of fields ."}}
{"translation": {"code": "def _call ( self , method , params ) : url = self . URL % { 'base' : self . base_url , 'method' : method } # Conduit and POST parameters params [ '__conduit__' ] = { 'token' : self . api_token } data = { 'params' : json . dumps ( params , sort_keys = True ) , 'output' : 'json' , '__conduit__' : True } logger . debug ( \"Phabricator Conduit client requests: %s params: %s\" , method , str ( data ) ) r = self . fetch ( url , payload = data , method = HttpClient . POST , verify = False ) # Check for possible Conduit API errors result = r . json ( ) if result [ 'error_code' ] : raise ConduitError ( error = result [ 'error_info' ] , code = result [ 'error_code' ] ) return r . text", "nl": "Call a method ."}}
{"translation": {"code": "def tasks ( self , from_date = DEFAULT_DATETIME ) : # Convert 'from_date' to epoch timestamp. # Zero value (1970-01-01 00:00:00) is not allowed for # 'modifiedStart' so it will be set to 1, by default. ts = int ( datetime_to_utc ( from_date ) . timestamp ( ) ) or 1 consts = { self . PMODIFIED_START : ts } attachments = { self . PPROJECTS : True } params = { self . PCONSTRAINTS : consts , self . PATTACHMENTS : attachments , self . PORDER : self . VOUTDATED , } while True : r = self . _call ( self . MANIPHEST_TASKS , params ) yield r j = json . loads ( r ) after = j [ 'result' ] [ 'cursor' ] [ 'after' ] if not after : break params [ self . PAFTER ] = after", "nl": "Retrieve tasks ."}}
{"translation": {"code": "def transactions ( self , * phids ) : params = { self . PIDS : phids } response = self . _call ( self . MANIPHEST_TRANSACTIONS , params ) return response", "nl": "Retrieve tasks transactions ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] logger . info ( \"Fetching tasks of '%s' from %s\" , self . url , str ( from_date ) ) ntasks = 0 for task in self . __fetch_tasks ( from_date ) : yield task ntasks += 1 logger . info ( \"Fetch process completed: %s tasks fetched\" , ntasks )", "nl": "Fetch the tasks"}}
{"translation": {"code": "def users ( self , * phids ) : params = { self . PHIDS : phids } response = self . _call ( self . PHAB_USERS , params ) return response", "nl": "Retrieve users ."}}
{"translation": {"code": "def issue ( self , issue_id ) : resource = urijoin ( self . RISSUES , str ( issue_id ) + self . CJSON ) params = { self . PINCLUDE : ',' . join ( [ self . CATTACHMENTS , self . CCHANGESETS , self . CCHILDREN , self . CJOURNALS , self . CRELATIONS , self . CWATCHERS ] ) } response = self . _call ( resource , params ) return response", "nl": "Get the information of the given issue ."}}
{"translation": {"code": "def issues ( self , from_date = DEFAULT_DATETIME , offset = None , max_issues = MAX_ISSUES ) : resource = self . RISSUES + self . CJSON ts = datetime_to_utc ( from_date ) ts = ts . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" ) # By default, Redmine returns open issues only. # Parameter 'status_id' is set to get all the statuses. params = { self . PSTATUS_ID : '*' , self . PSORT : self . PUPDATED_ON , self . PUPDATED_ON : '>=' + ts , self . PLIMIT : max_issues } if offset is not None : params [ self . POFFSET ] = offset response = self . _call ( resource , params ) return response", "nl": "Get the information of a list of issues ."}}
{"translation": {"code": "def _call ( self , resource , params ) : url = self . URL % { 'base' : self . base_url , 'resource' : resource } if self . api_token : params [ self . PKEY ] = self . api_token logger . debug ( \"Redmine client requests: %s params: %s\" , resource , str ( params ) ) r = self . fetch ( url , payload = params , verify = False ) return r . text", "nl": "Call to get a resource ."}}
{"translation": {"code": "def parse_issues ( raw_json ) : results = json . loads ( raw_json ) issues = results [ 'issues' ] for issue in issues : yield issue", "nl": "Parse a Redmine issues JSON stream ."}}
{"translation": {"code": "def phids ( self , * phids ) : params = { self . PHIDS : phids } response = self . _call ( self . PHAB_PHIDS , params ) return response", "nl": "Retrieve data about PHIDs ."}}
{"translation": {"code": "def user ( self , user_id ) : resource = urijoin ( self . RUSERS , str ( user_id ) + self . CJSON ) params = { } response = self . _call ( resource , params ) return response", "nl": "Get the information of the given user ."}}
{"translation": {"code": "def rsvps ( self , group , event_id ) : resource = urijoin ( group , self . REVENTS , event_id , self . RRSVPS ) # Same hack that in 'events' method fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP_FIELDS ) fixed_params += '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) resource += fixed_params params = { self . PPAGE : self . max_items } for page in self . _fetch ( resource , params ) : yield page", "nl": "Fetch the rsvps of a given event ."}}
{"translation": {"code": "def comments ( self , group , event_id ) : resource = urijoin ( group , self . REVENTS , event_id , self . RCOMMENTS ) params = { self . PPAGE : self . max_items } for page in self . _fetch ( resource , params ) : yield page", "nl": "Fetch the comments of a given event ."}}
{"translation": {"code": "def events ( self , group , from_date = DEFAULT_DATETIME ) : date = datetime_to_utc ( from_date ) date = date . strftime ( \"since:%Y-%m-%dT%H:%M:%S.000Z\" ) resource = urijoin ( group , self . REVENTS ) # Hack required due to Metup API does not support list # values with the format `?param=value1&param=value2`. # It only works with `?param=value1,value2`. # Morever, urrlib3 encodes comma characters when values # are given using params dict, which it doesn't work # with Meetup, either. fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT_FIELDS ) fixed_params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) resource += fixed_params params = { self . PORDER : self . VUPDATED , self . PSCROLL : date , self . PPAGE : self . max_items } try : for page in self . _fetch ( resource , params ) : yield page except requests . exceptions . HTTPError as error : if error . response . status_code == 410 : msg = \"Group is no longer accessible: {}\" . format ( error ) raise RepositoryError ( cause = msg ) else : raise error", "nl": "Fetch the events pages of a given group ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] to_date = kwargs [ 'to_date' ] logger . info ( \"Fetching events of '%s' group from %s to %s\" , self . group , str ( from_date ) , str ( to_date ) if to_date else '--' ) to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None nevents = 0 stop_fetching = False ev_pages = self . client . events ( self . group , from_date = from_date ) for evp in ev_pages : events = [ event for event in self . parse_json ( evp ) ] for event in events : event_id = event [ 'id' ] event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) # Check events updated before 'to_date' event_ts = self . metadata_updated_on ( event ) if to_date_ts and event_ts >= to_date_ts : stop_fetching = True continue yield event nevents += 1 if stop_fetching : break logger . info ( \"Fetch process completed: %s events fetched\" , nevents )", "nl": "Fetch the events"}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_EVENT , from_date = DEFAULT_DATETIME , to_date = None , filter_classified = False ) : if not from_date : from_date = DEFAULT_DATETIME from_date = datetime_to_utc ( from_date ) kwargs = { \"from_date\" : from_date , \"to_date\" : to_date } items = super ( ) . fetch ( category , filter_classified = filter_classified , * * kwargs ) return items", "nl": "Fetch the events from the server ."}}
{"translation": {"code": "def parse_user_info ( update_info ) : user_info = { } if update_info . select ( \"div.user-info\" ) : # Get all the <a> elements in the container. First <a> contains the user # information, second one (if exists), the website of the user. elements = update_info . select ( \"div.user-info\" ) [ 0 ] . find_all ( \"a\" ) href = elements [ 0 ] . attrs [ \"href\" ] user_info [ 'id' ] = re . search ( r'\\d+' , href ) . group ( 0 ) user_info [ 'username' ] = elements [ 0 ] . text user_info [ 'reputation' ] = update_info . select ( 'span.reputation-score' ) [ 0 ] . text user_info [ 'badges' ] = update_info . select ( \"span.badges\" ) [ 0 ] . attrs [ \"title\" ] try : elements [ 1 ] except IndexError : pass else : user_info [ 'website' ] = elements [ 1 ] . attrs [ \"href\" ] if update_info . select ( \"img.flag\" ) : flag = update_info . select ( \"img.flag\" ) [ 0 ] . attrs [ \"alt\" ] user_info [ 'country' ] = re . sub ( \"flag of \" , \"\" , flag ) return user_info", "nl": "Parse the user information of a given HTML container ."}}
{"translation": {"code": "def parse_number_of_html_pages ( html_question ) : bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) try : bs_question . select ( 'div.paginator' ) [ 0 ] except IndexError : return 1 else : return int ( bs_question . select ( 'div.paginator' ) [ 0 ] . attrs [ 'data-num-pages' ] )", "nl": "Parse number of answer pages to paginate over them ."}}
{"translation": {"code": "def parse_answers ( html_question ) : def parse_answer_container ( update_info ) : \"\"\"Parse the answer info container of a given HTML question.\n\n            The method parses the information available in the answer information\n            container. The container can have up to 2 elements: the first one\n            contains the information related with the user who generated the question\n            and the date (if any). The second one contains the date of the updated,\n            and the user who updated it (if not the same who generated the question).\n\n            :param update_info: beautiful soup update_info container element\n\n            :returns: an object with the parsed information\n            \"\"\" container_info = { } created = update_info [ 0 ] answered_at = created . abbr . attrs [ \"title\" ] # Convert date to UNIX timestamp container_info [ 'added_at' ] = str ( str_to_datetime ( answered_at ) . timestamp ( ) ) container_info [ 'answered_by' ] = AskbotParser . parse_user_info ( created ) try : update_info [ 1 ] except IndexError : pass else : updated = update_info [ 1 ] updated_at = updated . abbr . attrs [ \"title\" ] # Convert date to UNIX timestamp container_info [ 'updated_at' ] = str ( str_to_datetime ( updated_at ) . timestamp ( ) ) if AskbotParser . parse_user_info ( updated ) : container_info [ 'updated_by' ] = AskbotParser . parse_user_info ( updated ) return container_info answer_list = [ ] # Select all the answers bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) bs_answers = bs_question . select ( \"div.answer\" ) for bs_answer in bs_answers : answer_id = bs_answer . attrs [ \"data-post-id\" ] votes_element = bs_answer . select ( \"div.vote-number\" ) [ 0 ] . text accepted_answer = bs_answer . select ( \"div.answer-img-accept\" ) [ 0 ] . get ( 'title' ) . endswith ( \"correct\" ) # Select the body of the answer body = bs_answer . select ( \"div.post-body\" ) # Get the user information container and parse it update_info = body [ 0 ] . select ( \"div.post-update-info\" ) answer_container = parse_answer_container ( update_info ) # Remove the update-info-container div to be able to get the body body [ 0 ] . div . extract ( ) . select ( \"div.post-update-info-container\" ) # Override the body with a clean one body = body [ 0 ] . get_text ( strip = True ) # Generate the answer object answer = { 'id' : answer_id , 'score' : votes_element , 'summary' : body , 'accepted' : accepted_answer } # Update the object with the information in the answer container answer . update ( answer_container ) answer_list . append ( answer ) return answer_list", "nl": "Parse the answers of a given HTML question ."}}
{"translation": {"code": "def parse_question_container ( html_question ) : container_info = { } bs_question = bs4 . BeautifulSoup ( html_question , \"html.parser\" ) question = AskbotParser . _find_question_container ( bs_question ) container = question . select ( \"div.post-update-info\" ) created = container [ 0 ] container_info [ 'author' ] = AskbotParser . parse_user_info ( created ) try : container [ 1 ] except IndexError : pass else : updated = container [ 1 ] if AskbotParser . parse_user_info ( updated ) : container_info [ 'updated_by' ] = AskbotParser . parse_user_info ( updated ) return container_info", "nl": "Parse the question info container of a given HTML question ."}}
{"translation": {"code": "def find_backends ( top_package ) : candidates = pkgutil . walk_packages ( top_package . __path__ , prefix = top_package . __name__ + '.' ) modules = [ name for _ , name , is_pkg in candidates if not is_pkg ] return _import_backends ( modules )", "nl": "Find available backends ."}}
{"translation": {"code": "def get_api_questions ( self , path ) : npages = 1 next_request = True path = urijoin ( self . base_url , path ) while next_request : try : params = { 'page' : npages , 'sort' : self . ORDER_API } response = self . fetch ( path , payload = params ) whole_page = response . text raw_questions = json . loads ( whole_page ) tpages = raw_questions [ 'pages' ] logger . debug ( \"Fetching questions from '%s': page %s/%s\" , self . base_url , npages , tpages ) if npages == tpages : next_request = False npages = npages + 1 yield raw_questions except requests . exceptions . TooManyRedirects as e : logger . warning ( \"%s, data not retrieved for resource %s\" , e , path ) next_request = False", "nl": "Retrieve a question page using the API ."}}
{"translation": {"code": "def __fetch_question ( self , question ) : html_question_items = [ ] npages = 1 next_request = True while next_request : try : html_question = self . client . get_html_question ( question [ 'id' ] , npages ) html_question_items . append ( html_question ) tpages = self . ab_parser . parse_number_of_html_pages ( html_question ) if npages == tpages : next_request = False npages = npages + 1 except requests . exceptions . TooManyRedirects as e : logger . warning ( \"%s, data not retrieved for question %s\" , e , question [ 'id' ] ) next_request = False return html_question_items", "nl": "Fetch an Askbot HTML question body ."}}
{"translation": {"code": "def __build_question ( html_question , question , comments ) : question_object = { } # Parse the user info from the soup container question_container = AskbotParser . parse_question_container ( html_question [ 0 ] ) # Add the info to the question object question_object . update ( question_container ) # Add the comments of the question (if any) if comments [ int ( question [ 'id' ] ) ] : question_object [ 'comments' ] = comments [ int ( question [ 'id' ] ) ] answers = [ ] for page in html_question : answers . extend ( AskbotParser . parse_answers ( page ) ) if len ( answers ) != 0 : question_object [ 'answers' ] = answers for answer in question_object [ 'answers' ] : if comments [ int ( answer [ 'id' ] ) ] : answer [ 'comments' ] = comments [ int ( answer [ 'id' ] ) ] return question_object", "nl": "Build an Askbot HTML response ."}}
{"translation": {"code": "def get_html_question ( self , question_id , page = 1 ) : path = urijoin ( self . base_url , self . HTML_QUESTION , question_id ) params = { 'page' : page , 'sort' : self . ORDER_HTML } response = self . fetch ( path , payload = params ) return response . text", "nl": "Retrieve a raw HTML question and all it s information ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_ENTRY ) : kwargs = { } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch the entries from the url ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : logger . info ( \"Looking for rss entries at feed '%s'\" , self . url ) nentries = 0 # number of entries raw_entries = self . client . get_entries ( ) entries = self . parse_feed ( raw_entries ) [ 'entries' ] for item in entries : yield item nentries += 1 logger . info ( \"Total number of entries: %i\" , nentries )", "nl": "Fetch the entries"}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , archive = True ) # Required arguments parser . parser . add_argument ( 'url' , help = \"URL of the RSS feed\" ) return parser", "nl": "Returns the RSS argument parser ."}}
{"translation": {"code": "def __fetch_comments ( self , question ) : comments = { } comments [ question [ 'id' ] ] = json . loads ( self . client . get_comments ( question [ 'id' ] ) ) for object_id in question [ 'answer_ids' ] : comments [ object_id ] = json . loads ( self . client . get_comments ( object_id ) ) return comments", "nl": "Fetch all the comments of an Askbot question and answers ."}}
{"translation": {"code": "def get_comments ( self , post_id ) : path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) params = { 'post_id' : post_id , 'post_type' : 'answer' , 'avatar_size' : 0 } headers = { 'X-Requested-With' : 'XMLHttpRequest' } try : response = self . fetch ( path , payload = params , headers = headers ) raw = response . text except requests . exceptions . HTTPError as ex : if ex . response . status_code == 404 : logger . debug ( \"Comments URL did not work. Using old URL schema.\" ) self . _use_new_urls = False path = urijoin ( self . base_url , self . COMMENTS_OLD ) response = self . fetch ( path , payload = params , headers = headers ) raw = response . text elif ex . response . status_code == 500 : logger . warning ( \"Comments not retrieved due to %s\" , ex ) raw = '[]' else : raise ex return raw", "nl": "Retrieve a list of comments by a given id ."}}
{"translation": {"code": "def parse ( self , * args ) : parsed_args = self . parser . parse_args ( args ) # Category was not set, remove it if parsed_args . category is None : delattr ( parsed_args , 'category' ) if self . _from_date : parsed_args . from_date = str_to_datetime ( parsed_args . from_date ) if self . _to_date and parsed_args . to_date : parsed_args . to_date = str_to_datetime ( parsed_args . to_date ) if self . _archive and parsed_args . archived_since : parsed_args . archived_since = str_to_datetime ( parsed_args . archived_since ) if self . _archive and parsed_args . fetch_archive and parsed_args . no_archive : raise AttributeError ( \"fetch-archive and no-archive arguments are not compatible\" ) if self . _archive and parsed_args . fetch_archive and not parsed_args . category : raise AttributeError ( \"fetch-archive needs a category to work with\" ) # Set aliases for alias , arg in self . aliases . items ( ) : if ( alias not in parsed_args ) and ( arg in parsed_args ) : value = getattr ( parsed_args , arg , None ) setattr ( parsed_args , alias , value ) return parsed_args", "nl": "Parse a list of arguments ."}}
{"translation": {"code": "def run ( self ) : backend_args = vars ( self . parsed_args ) category = backend_args . pop ( 'category' , None ) filter_classified = backend_args . pop ( 'filter_classified' , False ) archived_since = backend_args . pop ( 'archived_since' , None ) if self . archive_manager and self . parsed_args . fetch_archive : items = fetch_from_archive ( self . BACKEND , backend_args , self . archive_manager , category , archived_since ) else : items = fetch ( self . BACKEND , backend_args , category , filter_classified = filter_classified , manager = self . archive_manager ) try : for item in items : if self . json_line : obj = json . dumps ( item , separators = ( ',' , ':' ) , sort_keys = True ) else : obj = json . dumps ( item , indent = 4 , sort_keys = True ) self . outfile . write ( obj ) self . outfile . write ( '\\n' ) except IOError as e : raise RuntimeError ( str ( e ) ) except Exception as e : raise RuntimeError ( str ( e ) )", "nl": "Fetch and write items ."}}
{"translation": {"code": "def _set_auth_arguments ( self , basic_auth = True , token_auth = False ) : group = self . parser . add_argument_group ( 'authentication arguments' ) if basic_auth : group . add_argument ( '-u' , '--backend-user' , dest = 'user' , help = \"backend user\" ) group . add_argument ( '-p' , '--backend-password' , dest = 'password' , help = \"backend password\" ) if token_auth : group . add_argument ( '-t' , '--api-token' , dest = 'api_token' , help = \"backend authentication token / API key\" )", "nl": "Activate authentication arguments parsing"}}
{"translation": {"code": "def _initialize_archive ( self ) : if 'archive_path' not in self . parsed_args : manager = None elif self . parsed_args . no_archive : manager = None else : if not self . parsed_args . archive_path : archive_path = os . path . expanduser ( ARCHIVES_DEFAULT_PATH ) else : archive_path = self . parsed_args . archive_path manager = ArchiveManager ( archive_path ) self . archive_manager = manager", "nl": "Initialize archive based on the parsed parameters"}}
{"translation": {"code": "def count_objects ( self ) : cmd_count = [ 'git' , 'count-objects' , '-v' ] outs = self . _exec ( cmd_count , cwd = self . dirpath , env = self . gitenv ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) try : cobjs = { k : v for k , v in ( x . split ( ': ' ) for x in outs . split ( '\\n' ) ) } nobjs = int ( cobjs [ 'count' ] ) + int ( cobjs [ 'in-pack' ] ) except KeyError as e : error = \"unable to parse 'count-objects' output; reason: '%s' entry not found\" % e . args [ 0 ] raise RepositoryError ( cause = error ) except ValueError as e : error = \"unable to parse 'count-objects' output; reason: %s\" % str ( e ) raise RepositoryError ( cause = error ) logger . debug ( \"Git %s repository has %s objects\" , self . uri , str ( nobjs ) ) return nobjs", "nl": "Count the objects of a repository ."}}
{"translation": {"code": "def get_message ( self , key ) : start , stop = self . _lookup ( key ) self . _file . seek ( start ) from_line = self . _file . readline ( ) . replace ( mailbox . linesep , b'' ) string = self . _file . read ( stop - self . _file . tell ( ) ) msg = self . _message_factory ( string . replace ( mailbox . linesep , b'\\n' ) ) try : msg . set_from ( from_line [ 5 : ] . decode ( 'ascii' ) ) return msg except UnicodeDecodeError : pass try : msg . set_from ( from_line [ 5 : ] . decode ( 'utf-8' ) ) except UnicodeDecodeError : msg . set_from ( from_line [ 5 : ] . decode ( 'iso-8859-1' ) ) return msg", "nl": "Return a Message representation or raise a KeyError ."}}
{"translation": {"code": "def months_range ( from_date , to_date ) : start = datetime . datetime ( from_date . year , from_date . month , 1 ) end = datetime . datetime ( to_date . year , to_date . month , 1 ) month_gen = dateutil . rrule . rrule ( freq = dateutil . rrule . MONTHLY , dtstart = start , until = end ) months = [ d for d in month_gen ] pos = 0 for x in range ( 1 , len ( months ) ) : yield months [ pos ] , months [ x ] pos = x", "nl": "Generate a months range ."}}
{"translation": {"code": "def is_detached ( self ) : cmd_sym = [ 'git' , 'symbolic-ref' , 'HEAD' ] try : self . _exec ( cmd_sym , cwd = self . dirpath , env = self . gitenv ) except RepositoryError as e : if e . msg . find ( \"ref HEAD is not a symbolic ref\" ) == - 1 : raise e return True else : return False", "nl": "Check if the repo is in a detached state ."}}
{"translation": {"code": "def message_to_dict ( msg ) : def parse_headers ( msg ) : headers = { } for header , value in msg . items ( ) : hv = [ ] for text , charset in email . header . decode_header ( value ) : if type ( text ) == bytes : charset = charset if charset else 'utf-8' try : text = text . decode ( charset , errors = 'surrogateescape' ) except ( UnicodeError , LookupError ) : # Try again with a 7bit encoding text = text . decode ( 'ascii' , errors = 'surrogateescape' ) hv . append ( text ) v = ' ' . join ( hv ) headers [ header ] = v if v else None return headers def parse_payload ( msg ) : body = { } if not msg . is_multipart ( ) : payload = decode_payload ( msg ) subtype = msg . get_content_subtype ( ) body [ subtype ] = [ payload ] else : # Include all the attached texts if it is multipart # Ignores binary parts by default for part in email . iterators . typed_subpart_iterator ( msg ) : payload = decode_payload ( part ) subtype = part . get_content_subtype ( ) body . setdefault ( subtype , [ ] ) . append ( payload ) return { k : '\\n' . join ( v ) for k , v in body . items ( ) } def decode_payload ( msg_or_part ) : charset = msg_or_part . get_content_charset ( 'utf-8' ) payload = msg_or_part . get_payload ( decode = True ) try : payload = payload . decode ( charset , errors = 'surrogateescape' ) except ( UnicodeError , LookupError ) : # Try again with a 7bit encoding payload = payload . decode ( 'ascii' , errors = 'surrogateescape' ) return payload # The function starts here message = requests . structures . CaseInsensitiveDict ( ) if isinstance ( msg , mailbox . mboxMessage ) : message [ 'unixfrom' ] = msg . get_from ( ) else : message [ 'unixfrom' ] = None try : for k , v in parse_headers ( msg ) . items ( ) : message [ k ] = v message [ 'body' ] = parse_payload ( msg ) except UnicodeError as e : raise ParseError ( cause = str ( e ) ) return message", "nl": "Convert an email message into a dictionary ."}}
{"translation": {"code": "def metadata ( self , item , filter_classified = False ) : item = super ( ) . metadata ( item , filter_classified = filter_classified ) item [ 'offset' ] = item [ 'data' ] [ 'offset' ] return item", "nl": "NNTP metadata ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : offset = kwargs [ 'offset' ] logger . info ( \"Fetching articles of '%s' group on '%s' offset %s\" , self . group , self . host , str ( offset ) ) narts , iarts , tarts = ( 0 , 0 , 0 ) _ , _ , first , last , _ = self . client . group ( self . group ) if offset <= last : first = max ( first , offset ) _ , overview = self . client . over ( ( first , last ) ) else : overview = [ ] tarts = len ( overview ) logger . debug ( \"Total number of articles to fetch: %s\" , tarts ) for article_id , _ in overview : try : article_raw = self . client . article ( article_id ) article = self . __parse_article ( article_raw ) except ParseError : logger . warning ( \"Error parsing %s article; skipping\" , article_id ) iarts += 1 continue except nntplib . NNTPTemporaryError as e : logger . warning ( \"Error '%s' fetching article %s; skipping\" , e . response , article_id ) iarts += 1 continue yield article narts += 1", "nl": "Fetch the articles"}}
{"translation": {"code": "def parse_article ( raw_article ) : try : message = email . message_from_string ( raw_article ) article = message_to_dict ( message ) except UnicodeEncodeError as e : raise ParseError ( cause = str ( e ) ) return article", "nl": "Parse a NNTP article ."}}
{"translation": {"code": "def user ( self , user_id ) : resource = self . RUSER_INFO params = { self . PUSER : user_id } response = self . _fetch ( resource , params ) return response", "nl": "Fetch user info ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_MESSAGE , from_date = DEFAULT_DATETIME ) : if not from_date : from_date = DEFAULT_DATETIME from_date = datetime_to_utc ( from_date ) latest = datetime_utcnow ( ) . timestamp ( ) kwargs = { 'from_date' : from_date , 'latest' : latest } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch the messages from the channel ."}}
{"translation": {"code": "def metadata_id ( item ) : if 'user' in item : nick = item [ 'user' ] elif 'comment' in item : nick = item [ 'comment' ] [ 'user' ] else : nick = item [ 'bot_id' ] return item [ 'ts' ] + nick", "nl": "Extracts the identifier from a Slack item ."}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True , archive = True ) # Backend token is required action = parser . parser . _option_string_actions [ '--api-token' ] action . required = True # Slack options group = parser . parser . add_argument_group ( 'Slack arguments' ) group . add_argument ( '--max-items' , dest = 'max_items' , type = int , default = MAX_ITEMS , help = \"Maximum number of items requested on the same query\" ) # Required arguments parser . parser . add_argument ( 'channel' , help = \"Slack channel identifier\" ) return parser", "nl": "Returns the Slack argument parser ."}}
{"translation": {"code": "def channel_info ( self , channel ) : resource = self . RCHANNEL_INFO params = { self . PCHANNEL : channel , } response = self . _fetch ( resource , params ) return response", "nl": "Fetch information about a channel ."}}
{"translation": {"code": "def repository ( self , owner , repository ) : url = urijoin ( self . base_url , self . RREPOSITORY , owner , repository ) logger . debug ( \"DockerHub client requests: %s\" , url ) response = self . fetch ( url ) return response . text", "nl": "Fetch information about a repository ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_DOCKERHUB_DATA ) : kwargs = { } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch data from a Docker Hub repository ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : logger . info ( \"Fetching data from '%s' repository of '%s' owner\" , self . repository , self . owner ) raw_data = self . client . repository ( self . owner , self . repository ) fetched_on = datetime_utcnow ( ) . timestamp ( ) data = self . parse_json ( raw_data ) data [ 'fetched_on' ] = fetched_on yield data logger . info ( \"Fetch process completed\" )", "nl": "Fetch the Dockher Hub items"}}
{"translation": {"code": "def show ( self , commits = None , encoding = 'utf-8' ) : if self . is_empty ( ) : logger . warning ( \"Git %s repository is empty; unable to run show\" , self . uri ) raise EmptyRepositoryError ( repository = self . uri ) if commits is None : commits = [ ] cmd_show = [ 'git' , 'show' ] cmd_show . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) cmd_show . extend ( commits ) for line in self . _exec_nb ( cmd_show , cwd = self . dirpath , env = self . gitenv ) : yield line logger . debug ( \"Git show fetched from %s repository (%s)\" , self . uri , self . dirpath )", "nl": "Show the data of a set of commits ."}}
{"translation": {"code": "def _discover_refs ( self , remote = False ) : if remote : cmd_refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] sep = '\\t' ignored_error_codes = [ 2 ] else : # Check first whether the local repo is empty; # Running 'show-ref' in empty repos gives an error if self . is_empty ( ) : raise EmptyRepositoryError ( repository = self . uri ) cmd_refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] sep = ' ' ignored_error_codes = [ 1 ] # Error codes returned when no matching refs (i.e, no heads # or tags) are found in a repository will be ignored. Otherwise, # the full process would fail for those situations. outs = self . _exec ( cmd_refs , cwd = self . dirpath , env = self . gitenv , ignored_error_codes = ignored_error_codes ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) outs = outs . split ( '\\n' ) if outs else [ ] refs = [ ] for line in outs : data = line . split ( sep ) ref = GitRef ( data [ 0 ] , data [ 1 ] ) refs . append ( ref ) return refs", "nl": "Get the current list of local or remote refs ."}}
{"translation": {"code": "def _read_commits_from_pack ( self , packet_name ) : filepath = 'objects/pack/pack-' + packet_name cmd_verify_pack = [ 'git' , 'verify-pack' , '-v' , filepath ] outs = self . _exec ( cmd_verify_pack , cwd = self . dirpath , env = self . gitenv ) outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) lines = [ line . split ( ' ' ) for line in outs . split ( '\\n' ) ] # Commits usually come in the pack ordered from newest to oldest commits = [ parts [ 0 ] for parts in lines if parts [ 1 ] == 'commit' ] commits . reverse ( ) return commits", "nl": "Read the commits of a pack ."}}
{"translation": {"code": "def _fetch_pack ( self ) : def prepare_refs ( refs ) : return [ ref . hash . encode ( 'utf-8' ) for ref in refs if not ref . refname . endswith ( '^{}' ) ] def determine_wants ( refs ) : remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) local_refs = prepare_refs ( self . _discover_refs ( ) ) wants = [ ref for ref in remote_refs if ref not in local_refs ] return wants client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) repo = dulwich . repo . Repo ( self . dirpath ) fd = io . BytesIO ( ) local_refs = self . _discover_refs ( ) graph_walker = _GraphWalker ( local_refs ) result = client . fetch_pack ( repo_path , determine_wants , graph_walker , fd . write ) refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) for ref_name , ref_hash in result . refs . items ( ) ] if len ( fd . getvalue ( ) ) > 0 : fd . seek ( 0 ) pack = repo . object_store . add_thin_pack ( fd . read , None ) pack_name = pack . name ( ) . decode ( 'utf-8' ) else : pack_name = None return ( pack_name , refs )", "nl": "Fetch changes and store them in a pack ."}}
{"translation": {"code": "def _update_ref ( self , ref , delete = False ) : cmd = [ 'git' , 'update-ref' ] if delete : cmd . extend ( [ '-d' , ref . refname ] ) action = 'deleted' else : cmd . extend ( [ ref . refname , ref . hash ] ) action = 'updated to %s' % ref . hash try : self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) except RepositoryError as e : logger . warning ( \"Git %s ref could not be %s during sync process in %s (%s); skipped\" , ref . refname , action , self . uri , self . dirpath ) else : logger . debug ( \"Git %s ref %s in %s (%s)\" , ref . refname , action , self . uri , self . dirpath )", "nl": "Update a reference ."}}
{"translation": {"code": "def _update_references ( self , refs ) : new_refs = [ ref . refname for ref in refs ] # Delete old references for old_ref in self . _discover_refs ( ) : if not old_ref . refname . startswith ( 'refs/heads/' ) : continue if old_ref . refname in new_refs : continue self . _update_ref ( old_ref , delete = True ) # Update new references for new_ref in refs : refname = new_ref . refname if refname . endswith ( '^{}' ) : logger . debug ( \"Annotated tag %s ignored for updating in sync process\" , refname ) continue elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : logger . debug ( \"Reference %s not needed; ignored for updating in sync process\" , refname ) continue else : self . _update_ref ( new_ref ) # Prune repository to remove old branches cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv )", "nl": "Update references removing old ones ."}}
{"translation": {"code": "def sync ( self ) : pack_name , refs = self . _fetch_pack ( ) if pack_name : commits = self . _read_commits_from_pack ( pack_name ) else : commits = [ ] logger . debug ( \"Git repository %s (%s) does not have any new object\" , self . uri , self . dirpath ) self . _update_references ( refs ) logger . debug ( \"Git repository %s (%s) is synced\" , self . uri , self . dirpath ) return commits", "nl": "Keep the repository in sync ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : from_date = kwargs [ 'from_date' ] to_date = kwargs [ 'to_date' ] branches = kwargs [ 'branches' ] latest_items = kwargs [ 'latest_items' ] no_update = kwargs [ 'no_update' ] ncommits = 0 try : if os . path . isfile ( self . gitpath ) : commits = self . __fetch_from_log ( ) else : commits = self . __fetch_from_repo ( from_date , to_date , branches , latest_items , no_update ) for commit in commits : yield commit ncommits += 1 except EmptyRepositoryError : pass logger . info ( \"Fetch process completed: %s commits fetched\" , ncommits )", "nl": "Fetch the commits"}}
{"translation": {"code": "def user_orgs ( self , login ) : if login in self . _users_orgs : return self . _users_orgs [ login ] url = urijoin ( self . base_url , 'users' , login , 'orgs' ) try : r = self . fetch ( url ) orgs = r . text except requests . exceptions . HTTPError as error : # 404 not found is wrongly received sometimes if error . response . status_code == 404 : logger . error ( \"Can't get github login orgs: %s\" , error ) orgs = '[]' else : raise error self . _users_orgs [ login ] = orgs return orgs", "nl": "Get the user public organizations"}}
{"translation": {"code": "def issues ( self , from_date = None ) : payload = { 'state' : 'all' , 'per_page' : PER_PAGE , 'direction' : 'asc' , 'sort' : 'updated' } if from_date : payload [ 'since' ] = from_date . isoformat ( ) path = urijoin ( \"issues\" ) return self . fetch_items ( path , payload )", "nl": "Fetch the issues from the repository ."}}
{"translation": {"code": "def user ( self , login ) : user = None if login in self . _users : return self . _users [ login ] url_user = urijoin ( self . base_url , 'users' , login ) logging . info ( \"Getting info for %s\" % ( url_user ) ) r = self . fetch ( url_user ) user = r . text self . _users [ login ] = user return user", "nl": "Get the user information and update the user cache"}}
{"translation": {"code": "def __get_issue_assignees ( self , raw_assignees ) : assignees = [ ] for ra in raw_assignees : assignees . append ( self . __get_user ( ra [ 'login' ] ) ) return assignees", "nl": "Get issue assignees"}}
{"translation": {"code": "def __get_issue_comment_reactions ( self , comment_id , total_count ) : reactions = [ ] if total_count == 0 : return reactions group_reactions = self . client . issue_comment_reactions ( comment_id ) for raw_reactions in group_reactions : for reaction in json . loads ( raw_reactions ) : reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "nl": "Get reactions on issue comments"}}
{"translation": {"code": "def issue_reactions ( self , issue_number ) : payload = { 'per_page' : PER_PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( \"issues\" , str ( issue_number ) , \"reactions\" ) return self . fetch_items ( path , payload )", "nl": "Get reactions of an issue"}}
{"translation": {"code": "def __get_issue_reactions ( self , issue_number , total_count ) : reactions = [ ] if total_count == 0 : return reactions group_reactions = self . client . issue_reactions ( issue_number ) for raw_reactions in group_reactions : for reaction in json . loads ( raw_reactions ) : reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "nl": "Get issue reactions"}}
{"translation": {"code": "def issue ( self , issue_id ) : path = urijoin ( \"bugs\" , str ( issue_id ) ) url_issue = self . __get_url ( path ) raw_text = self . __send_request ( url_issue ) return raw_text", "nl": "Get the issue data by its ID"}}
{"translation": {"code": "def issue_collection ( self , issue_id , collection_name ) : path = urijoin ( \"bugs\" , str ( issue_id ) , collection_name ) url_collection = self . __get_url ( path ) payload = { 'ws.size' : self . items_per_page , 'ws.start' : 0 , 'order_by' : 'date_last_updated' } raw_items = self . __fetch_items ( path = url_collection , payload = payload ) return raw_items", "nl": "Get a collection list of a given issue"}}
{"translation": {"code": "def __fetch_issue_activities ( self , issue_id ) : for activities_raw in self . client . issue_collection ( issue_id , \"activity\" ) : activities = json . loads ( activities_raw ) for act in activities [ 'entries' ] : act [ 'person_data' ] = self . __fetch_user_data ( '{PERSON}' , act [ 'person_link' ] ) yield act", "nl": "Get activities on an issue"}}
{"translation": {"code": "def __get_url_project ( self ) : if self . package : url = self . __get_url_distribution_package ( ) else : url = self . __get_url_distribution ( ) return url", "nl": "Build URL project"}}
{"translation": {"code": "def __fetch_items ( self , path , payload ) : page = 0 # current page url_next = path fetch_data = True while fetch_data : logger . debug ( \"Fetching page: %i\" , page ) try : raw_content = self . __send_request ( url_next , payload ) content = json . loads ( raw_content ) except requests . exceptions . HTTPError as e : if e . response . status_code in [ 410 ] : logger . warning ( \"Data is not available - %s\" , url_next ) raw_content = '{\"total_size\": 0, \"start\": 0, \"entries\": []}' content = json . loads ( raw_content ) else : raise e if 'next_collection_link' in content : url_next = content [ 'next_collection_link' ] payload = None else : fetch_data = False yield raw_content page += 1", "nl": "Return the items from Launchpad API using pagination"}}
{"translation": {"code": "def __fetch_issue_attachments ( self , issue_id ) : for attachments_raw in self . client . issue_collection ( issue_id , \"attachments\" ) : attachments = json . loads ( attachments_raw ) for attachment in attachments [ 'entries' ] : yield attachment", "nl": "Get attachments of an issue"}}
{"translation": {"code": "def __fetch_issue_data ( self , issue_id ) : raw_issue = self . client . issue ( issue_id ) issue = json . loads ( raw_issue ) return issue", "nl": "Get data associated to an issue"}}
{"translation": {"code": "def __fetch_user_data ( self , tag_type , user_link ) : user_name = self . client . user_name ( user_link ) user = { } if not user_name : return user user_raw = self . client . user ( user_name ) user = json . loads ( user_raw ) return user", "nl": "Get data associated to an user"}}
{"translation": {"code": "def user ( self , user_name ) : user = None if user_name in self . _users : return self . _users [ user_name ] url_user = self . __get_url ( \"~\" + user_name ) logger . info ( \"Getting info for %s\" % ( url_user ) ) try : raw_user = self . __send_request ( url_user ) user = raw_user except requests . exceptions . HTTPError as e : if e . response . status_code in [ 404 , 410 ] : logger . warning ( \"Data is not available - %s\" , url_user ) user = '{}' else : raise e self . _users [ user_name ] = user return user", "nl": "Get the user data by URL"}}
{"translation": {"code": "def __fetch_issue_messages ( self , issue_id ) : for messages_raw in self . client . issue_collection ( issue_id , \"messages\" ) : messages = json . loads ( messages_raw ) for msg in messages [ 'entries' ] : msg [ 'owner_data' ] = self . __fetch_user_data ( '{OWNER}' , msg [ 'owner_link' ] ) yield msg", "nl": "Get messages of an issue"}}
{"translation": {"code": "def metadata_category ( item ) : if \"base\" in item : category = CATEGORY_PULL_REQUEST elif \"forks_count\" in item : category = CATEGORY_REPO else : category = CATEGORY_ISSUE return category", "nl": "Extracts the category from a GitHub item ."}}
{"translation": {"code": "def metadata_updated_on ( item ) : if \"forks_count\" in item : return item [ 'fetched_on' ] else : ts = item [ 'updated_at' ] ts = str_to_datetime ( ts ) return ts . timestamp ( )", "nl": "Extracts the update time from a GitHub item ."}}
{"translation": {"code": "def __get_pull_review_comment_reactions ( self , comment_id , total_count ) : reactions = [ ] if total_count == 0 : return reactions group_reactions = self . client . pull_review_comment_reactions ( comment_id ) for raw_reactions in group_reactions : for reaction in json . loads ( raw_reactions ) : reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) reactions . append ( reaction ) return reactions", "nl": "Get pull review comment reactions"}}
{"translation": {"code": "def pull_requested_reviewers ( self , pr_number ) : requested_reviewers_url = urijoin ( \"pulls\" , str ( pr_number ) , \"requested_reviewers\" ) return self . fetch_items ( requested_reviewers_url , { } )", "nl": "Get pull requested reviewers"}}
{"translation": {"code": "def pulls ( self , from_date = None ) : issues_groups = self . issues ( from_date = from_date ) for raw_issues in issues_groups : issues = json . loads ( raw_issues ) for issue in issues : if \"pull_request\" not in issue : continue pull_number = issue [ \"number\" ] path = urijoin ( self . base_url , 'repos' , self . owner , self . repository , \"pulls\" , pull_number ) r = self . fetch ( path ) pull = r . text yield pull", "nl": "Fetch the pull requests from the repository ."}}
{"translation": {"code": "def __get_pull_requested_reviewers ( self , pr_number ) : requested_reviewers = [ ] group_requested_reviewers = self . client . pull_requested_reviewers ( pr_number ) for raw_requested_reviewers in group_requested_reviewers : group_requested_reviewers = json . loads ( raw_requested_reviewers ) for requested_reviewer in group_requested_reviewers [ 'users' ] : user_data = self . __get_user ( requested_reviewer [ 'login' ] ) requested_reviewers . append ( user_data ) return requested_reviewers", "nl": "Get pull request requested reviewers"}}
{"translation": {"code": "def __fetch_pull_requests ( self , from_date , to_date ) : raw_pulls = self . client . pulls ( from_date = from_date ) for raw_pull in raw_pulls : pull = json . loads ( raw_pull ) if str_to_datetime ( pull [ 'updated_at' ] ) > to_date : return self . __init_extra_pull_fields ( pull ) for field in TARGET_PULL_FIELDS : if not pull [ field ] : continue if field == 'user' : pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) elif field == 'merged_by' : pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) elif field == 'review_comments' : pull [ field + '_data' ] = self . __get_pull_review_comments ( pull [ 'number' ] ) elif field == 'requested_reviewers' : pull [ field + '_data' ] = self . __get_pull_requested_reviewers ( pull [ 'number' ] ) elif field == 'commits' : pull [ field + '_data' ] = self . __get_pull_commits ( pull [ 'number' ] ) yield pull", "nl": "Fetch the pull requests"}}
{"translation": {"code": "def pull_review_comment_reactions ( self , comment_id ) : payload = { 'per_page' : PER_PAGE , 'direction' : 'asc' , 'sort' : 'updated' } path = urijoin ( \"pulls\" , \"comments\" , str ( comment_id ) , \"reactions\" ) return self . fetch_items ( path , payload )", "nl": "Get reactions of a review comment"}}
{"translation": {"code": "def setup_rate_limit_handler ( self , sleep_for_rate = False , min_rate_to_sleep = MIN_RATE_LIMIT , rate_limit_header = RATE_LIMIT_HEADER , rate_limit_reset_header = RATE_LIMIT_RESET_HEADER ) : self . rate_limit = None self . rate_limit_reset_ts = None self . sleep_for_rate = sleep_for_rate self . rate_limit_header = rate_limit_header self . rate_limit_reset_header = rate_limit_reset_header if min_rate_to_sleep > self . MAX_RATE_LIMIT : msg = \"Minimum rate to sleep value exceeded (%d).\" msg += \"High values might cause the client to sleep forever.\" msg += \"Reset to %d.\" self . min_rate_to_sleep = self . MAX_RATE_LIMIT logger . warning ( msg , min_rate_to_sleep , self . MAX_RATE_LIMIT ) else : self . min_rate_to_sleep = min_rate_to_sleep", "nl": "Setup the rate limit handler ."}}
{"translation": {"code": "def update_rate_limit ( self , response ) : if self . rate_limit_header in response . headers : self . rate_limit = int ( response . headers [ self . rate_limit_header ] ) logger . debug ( \"Rate limit: %s\" , self . rate_limit ) else : self . rate_limit = None if self . rate_limit_reset_header in response . headers : self . rate_limit_reset_ts = int ( response . headers [ self . rate_limit_reset_header ] ) logger . debug ( \"Rate limit reset: %s\" , self . calculate_time_to_reset ( ) ) else : self . rate_limit_reset_ts = None", "nl": "Update the rate limit and the time to reset from the response headers ."}}
{"translation": {"code": "def _create_http_session ( self ) : self . session = requests . Session ( ) if self . headers : self . session . headers . update ( self . headers ) retries = urllib3 . util . Retry ( total = self . max_retries , connect = self . max_retries_on_connect , read = self . max_retries_on_read , redirect = self . max_retries_on_redirect , status = self . max_retries_on_status , method_whitelist = self . method_whitelist , status_forcelist = self . status_forcelist , backoff_factor = self . sleep_time , raise_on_redirect = self . raise_on_redirect , raise_on_status = self . raise_on_status , respect_retry_after_header = self . respect_retry_after_header ) self . session . mount ( 'http://' , requests . adapters . HTTPAdapter ( max_retries = retries ) ) self . session . mount ( 'https://' , requests . adapters . HTTPAdapter ( max_retries = retries ) )", "nl": "Create a http session and initialize the retry object ."}}
{"translation": {"code": "def sleep_for_rate_limit ( self ) : if self . rate_limit is not None and self . rate_limit <= self . min_rate_to_sleep : seconds_to_reset = self . calculate_time_to_reset ( ) if seconds_to_reset < 0 : logger . warning ( \"Value of sleep for rate limit is negative, reset it to 0\" ) seconds_to_reset = 0 cause = \"Rate limit exhausted.\" if self . sleep_for_rate : logger . info ( \"%s Waiting %i secs for rate limit reset.\" , cause , seconds_to_reset ) time . sleep ( seconds_to_reset ) else : raise RateLimitError ( cause = cause , seconds_to_reset = seconds_to_reset )", "nl": "The fetching process sleeps until the rate limit is restored or raises a RateLimitError exception if sleep_for_rate flag is disabled ."}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , from_date = True , token_auth = True , archive = True ) # GitLab options group = parser . parser . add_argument_group ( 'GitLab arguments' ) group . add_argument ( '--enterprise-url' , dest = 'base_url' , help = \"Base URL for GitLab Enterprise instance\" ) group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , action = 'store_true' , help = \"sleep for getting more rate\" ) group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , default = MIN_RATE_LIMIT , type = int , help = \"sleep until reset when the rate limit \\\n                               reaches this value\" ) group . add_argument ( '--blacklist-ids' , dest = 'blacklist_ids' , nargs = '*' , type = int , help = \"Ids of items that must not be retrieved.\" ) # Generic client options group . add_argument ( '--max-retries' , dest = 'max_retries' , default = MAX_RETRIES , type = int , help = \"number of API call retries\" ) group . add_argument ( '--sleep-time' , dest = 'sleep_time' , default = DEFAULT_SLEEP_TIME , type = int , help = \"sleeping time between API call retries\" ) # Positional arguments parser . parser . add_argument ( 'owner' , help = \"GitLab owner\" ) parser . parser . add_argument ( 'repository' , help = \"GitLab repository\" ) return parser", "nl": "Returns the GitLab argument parser ."}}
{"translation": {"code": "def notes ( self , item_type , item_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES ) return self . fetch_items ( path , payload )", "nl": "Get the notes from pagination"}}
{"translation": {"code": "def __get_issue_notes ( self , issue_id ) : notes = [ ] group_notes = self . client . notes ( GitLabClient . ISSUES , issue_id ) for raw_notes in group_notes : for note in json . loads ( raw_notes ) : note_id = note [ 'id' ] note [ 'award_emoji_data' ] = self . __get_note_award_emoji ( GitLabClient . ISSUES , issue_id , note_id ) notes . append ( note ) return notes", "nl": "Get issue notes"}}
{"translation": {"code": "def __get_merge_notes ( self , merge_id ) : notes = [ ] group_notes = self . client . notes ( GitLabClient . MERGES , merge_id ) for raw_notes in group_notes : for note in json . loads ( raw_notes ) : note_id = note [ 'id' ] note [ 'award_emoji_data' ] = self . __get_note_award_emoji ( GitLabClient . MERGES , merge_id , note_id ) notes . append ( note ) return notes", "nl": "Get merge notes"}}
{"translation": {"code": "def merges ( self , from_date = None ) : payload = { 'state' : 'all' , 'order_by' : 'updated_at' , 'sort' : 'asc' , 'view' : 'simple' , 'per_page' : PER_PAGE } if from_date : payload [ 'updated_after' ] = from_date . isoformat ( ) return self . fetch_items ( GitLabClient . MERGES , payload )", "nl": "Get the merge requests from pagination"}}
{"translation": {"code": "def merge ( self , merge_id ) : path = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , GitLabClient . MERGES , merge_id ) response = self . fetch ( path ) return response . text", "nl": "Get the merge full data"}}
{"translation": {"code": "def merge_versions ( self , merge_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( GitLabClient . MERGES , str ( merge_id ) , GitLabClient . VERSIONS ) return self . fetch_items ( path , payload )", "nl": "Get the merge versions from pagination"}}
{"translation": {"code": "def note_emojis ( self , item_type , item_id , note_id ) : payload = { 'order_by' : 'updated_at' , 'sort' : 'asc' , 'per_page' : PER_PAGE } path = urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES , str ( note_id ) , GitLabClient . EMOJI ) return self . fetch_items ( path , payload )", "nl": "Get emojis of a note"}}
{"translation": {"code": "def fetch_items ( self , path , payload ) : page = 0 # current page last_page = None # last page url_next = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , path ) logger . debug ( \"Get GitLab paginated items from \" + url_next ) response = self . fetch ( url_next , payload = payload ) items = response . text page += 1 if 'last' in response . links : last_url = response . links [ 'last' ] [ 'url' ] last_page = last_url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] last_page = int ( last_page ) logger . debug ( \"Page: %i/%i\" % ( page , last_page ) ) while items : yield items items = None if 'next' in response . links : url_next = response . links [ 'next' ] [ 'url' ] # Loving requests :) response = self . fetch ( url_next , payload = payload ) page += 1 items = response . text logger . debug ( \"Page: %i/%i\" % ( page , last_page ) )", "nl": "Return the items from GitLab API using links pagination"}}
{"translation": {"code": "def calculate_time_to_reset ( self ) : time_to_reset = self . rate_limit_reset_ts - ( datetime_utcnow ( ) . replace ( microsecond = 0 ) . timestamp ( ) + 1 ) if time_to_reset < 0 : time_to_reset = 0 return time_to_reset", "nl": "Calculate the seconds to reset the token requests by obtaining the different between the current date and the next date when the token is fully regenerated ."}}
{"translation": {"code": "def init_metadata ( self , origin , backend_name , backend_version , category , backend_params ) : created_on = datetime_to_utc ( datetime_utcnow ( ) ) created_on_dumped = created_on . isoformat ( ) backend_params_dumped = pickle . dumps ( backend_params , 0 ) metadata = ( origin , backend_name , backend_version , category , backend_params_dumped , created_on_dumped , ) try : cursor = self . _db . cursor ( ) insert_stmt = \"INSERT INTO \" + self . METADATA_TABLE + \" \" \"(origin, backend_name, backend_version, \" \"category, backend_params, created_on) \" \"VALUES (?, ?, ?, ?, ?, ?)\" cursor . execute ( insert_stmt , metadata ) self . _db . commit ( ) cursor . close ( ) except sqlite3 . DatabaseError as e : msg = \"metadata initialization error; cause: %s\" % str ( e ) raise ArchiveError ( cause = msg ) self . origin = origin self . backend_name = backend_name self . backend_version = backend_version self . category = category self . backend_params = backend_params self . created_on = created_on logger . debug ( \"Metadata of archive %s initialized to %s\" , self . archive_path , metadata )", "nl": "Init metadata information ."}}
{"translation": {"code": "def retrieve ( self , uri , payload , headers ) : hashcode = self . make_hashcode ( uri , payload , headers ) logger . debug ( \"Retrieving entry %s with %s %s %s in %s\" , hashcode , uri , payload , headers , self . archive_path ) self . _db . row_factory = sqlite3 . Row try : cursor = self . _db . cursor ( ) select_stmt = \"SELECT data \" \"FROM \" + self . ARCHIVE_TABLE + \" \" \"WHERE hashcode = ?\" cursor . execute ( select_stmt , ( hashcode , ) ) row = cursor . fetchone ( ) cursor . close ( ) except sqlite3 . DatabaseError as e : msg = \"data retrieval error; cause: %s\" % str ( e ) raise ArchiveError ( cause = msg ) if row : found = pickle . loads ( row [ 'data' ] ) else : msg = \"entry %s not found in archive %s\" % ( hashcode , self . archive_path ) raise ArchiveError ( cause = msg ) return found", "nl": "Retrieve a raw item from the archive ."}}
{"translation": {"code": "def store ( self , uri , payload , headers , data ) : hashcode = self . make_hashcode ( uri , payload , headers ) payload_dump = pickle . dumps ( payload , 0 ) headers_dump = pickle . dumps ( headers , 0 ) data_dump = pickle . dumps ( data , 0 ) logger . debug ( \"Archiving %s with %s %s %s in %s\" , hashcode , uri , payload , headers , self . archive_path ) try : cursor = self . _db . cursor ( ) insert_stmt = \"INSERT INTO \" + self . ARCHIVE_TABLE + \" (\" \"id, hashcode, uri, payload, headers, data) \" \"VALUES(?,?,?,?,?,?)\" cursor . execute ( insert_stmt , ( None , hashcode , uri , payload_dump , headers_dump , data_dump ) ) self . _db . commit ( ) cursor . close ( ) except sqlite3 . IntegrityError as e : msg = \"data storage error; cause: duplicated entry %s\" % hashcode raise ArchiveError ( cause = msg ) except sqlite3 . DatabaseError as e : msg = \"data storage error; cause: %s\" % str ( e ) raise ArchiveError ( cause = msg ) logger . debug ( \"%s data archived in %s\" , hashcode , self . archive_path )", "nl": "Store a raw item in this archive ."}}
{"translation": {"code": "def _count_table_rows ( self , table_name ) : cursor = self . _db . cursor ( ) select_stmt = \"SELECT COUNT(*) FROM \" + table_name try : cursor . execute ( select_stmt ) row = cursor . fetchone ( ) except sqlite3 . DatabaseError as e : msg = \"invalid archive file; cause: %s\" % str ( e ) raise ArchiveError ( cause = msg ) finally : cursor . close ( ) return row [ 0 ]", "nl": "Fetch the number of rows in a table"}}
{"translation": {"code": "def _load_metadata ( self ) : logger . debug ( \"Loading metadata infomation of archive %s\" , self . archive_path ) cursor = self . _db . cursor ( ) select_stmt = \"SELECT origin, backend_name, backend_version, \" \"category, backend_params, created_on \" \"FROM \" + self . METADATA_TABLE + \" \" \"LIMIT 1\" cursor . execute ( select_stmt ) row = cursor . fetchone ( ) cursor . close ( ) if row : self . origin = row [ 0 ] self . backend_name = row [ 1 ] self . backend_version = row [ 2 ] self . category = row [ 3 ] self . backend_params = pickle . loads ( row [ 4 ] ) self . created_on = str_to_datetime ( row [ 5 ] ) else : logger . debug ( \"Metadata of archive %s was empty\" , self . archive_path ) logger . debug ( \"Metadata of archive %s loaded\" , self . archive_path )", "nl": "Load metadata from the archive file"}}
{"translation": {"code": "def _verify_archive ( self ) : nentries = self . _count_table_rows ( self . ARCHIVE_TABLE ) nmetadata = self . _count_table_rows ( self . METADATA_TABLE ) if nmetadata > 1 : msg = \"archive %s metadata corrupted; multiple metadata entries\" % ( self . archive_path ) raise ArchiveError ( cause = msg ) if nmetadata == 0 and nentries > 0 : msg = \"archive %s metadata is empty but %s entries were achived\" % ( self . archive_path ) raise ArchiveError ( cause = msg ) logger . debug ( \"Integrity of archive %s OK; entries: %s rows, metadata: %s rows\" , self . archive_path , nentries , nmetadata )", "nl": "Check whether the archive is valid or not ."}}
{"translation": {"code": "def make_hashcode ( uri , payload , headers ) : def dict_to_json_str ( data ) : return json . dumps ( data , sort_keys = True ) content = ':' . join ( [ uri , dict_to_json_str ( payload ) , dict_to_json_str ( headers ) ] ) hashcode = hashlib . sha1 ( content . encode ( 'utf-8' ) ) return hashcode . hexdigest ( )", "nl": "Generate a SHA1 based on the given arguments ."}}
{"translation": {"code": "def create ( cls , archive_path ) : if os . path . exists ( archive_path ) : msg = \"archive %s already exists; remove it before creating a new one\" raise ArchiveError ( cause = msg % ( archive_path ) ) conn = sqlite3 . connect ( archive_path ) cursor = conn . cursor ( ) cursor . execute ( cls . METADATA_CREATE_STMT ) cursor . execute ( cls . ARCHIVE_CREATE_STMT ) conn . commit ( ) cursor . close ( ) conn . close ( ) logger . debug ( \"Creating archive %s\" , archive_path ) archive = cls ( archive_path ) logger . debug ( \"Achive %s was created\" , archive_path ) return archive", "nl": "Create a brand new archive ."}}
{"translation": {"code": "def _get_token_rate_limit ( self , token ) : rate_url = urijoin ( self . base_url , \"rate_limit\" ) self . session . headers . update ( { 'Authorization' : 'token ' + token } ) remaining = 0 try : headers = super ( ) . fetch ( rate_url ) . headers if self . rate_limit_header in headers : remaining = int ( headers [ self . rate_limit_header ] ) except requests . exceptions . HTTPError as error : logger . warning ( \"Rate limit not initialized: %s\" , error ) return remaining", "nl": "Return token s remaining API points"}}
{"translation": {"code": "def _update_current_rate_limit ( self ) : url = urijoin ( self . base_url , \"rate_limit\" ) try : # Turn off archiving when checking rates, because that would cause # archive key conflict (the same URLs giving different responses) arch = self . archive self . archive = None response = super ( ) . fetch ( url ) self . archive = arch self . update_rate_limit ( response ) self . last_rate_limit_checked = self . rate_limit except requests . exceptions . HTTPError as error : if error . response . status_code == 404 : logger . warning ( \"Rate limit not initialized: %s\" , error ) else : raise error", "nl": "Update rate limits data for the current token"}}
{"translation": {"code": "def get_fields ( self ) : url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , 'field' ) req = self . fetch ( url ) return req . text", "nl": "Retrieve all the fields available ."}}
{"translation": {"code": "def get_items ( self , from_date , url , expand_fields = True ) : start_at = 0 req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) issues = req . text data = req . json ( ) titems = data [ 'total' ] nitems = data [ 'maxResults' ] start_at += min ( nitems , titems ) self . __log_status ( start_at , titems , url ) while issues : yield issues issues = None if data [ 'startAt' ] + nitems < titems : req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) data = req . json ( ) start_at += nitems issues = req . text self . __log_status ( start_at , titems , url )", "nl": "Retrieve all the items from a given date ."}}
{"translation": {"code": "def parse_tasks ( raw_json ) : results = json . loads ( raw_json ) tasks = results [ 'result' ] [ 'data' ] for t in tasks : yield t", "nl": "Parse a Phabricator tasks JSON stream ."}}
{"translation": {"code": "def parse_users ( raw_json ) : results = json . loads ( raw_json ) users = results [ 'result' ] for u in users : yield u", "nl": "Parse a Phabricator users JSON stream ."}}
{"translation": {"code": "def _filter_message_by_chats ( self , message , chats ) : if chats is None : return True chat_id = message [ 'message' ] [ 'chat' ] [ 'id' ] return chat_id in chats", "nl": "Check if a message can be filtered based in a list of chats ."}}
{"translation": {"code": "def fetch ( self , category , filter_classified = False , * * kwargs ) : if category not in self . categories : cause = \"%s category not valid for %s\" % ( category , self . __class__ . __name__ ) raise BackendError ( cause = cause ) if filter_classified and self . archive : cause = \"classified fields filtering is not compatible with archiving items\" raise BackendError ( cause = cause ) if self . archive : self . archive . init_metadata ( self . origin , self . __class__ . __name__ , self . version , category , kwargs ) self . client = self . _init_client ( ) for item in self . fetch_items ( category , * * kwargs ) : if filter_classified : item = self . filter_classified_data ( item ) yield self . metadata ( item , filter_classified = filter_classified )", "nl": "Fetch items from the repository ."}}
{"translation": {"code": "def fetch_from_archive ( self ) : if not self . archive : raise ArchiveError ( cause = \"archive instance was not provided\" ) self . client = self . _init_client ( from_archive = True ) for item in self . fetch_items ( self . archive . category , * * self . archive . backend_params ) : yield self . metadata ( item )", "nl": "Fetch the questions from an archive ."}}
{"translation": {"code": "def _fetch_from_archive ( self , method , args ) : if not self . archive : raise ArchiveError ( cause = \"Archive not provided\" ) data = self . archive . retrieve ( method , args , None ) if isinstance ( data , nntplib . NNTPTemporaryError ) : raise data return data", "nl": "Fetch data from the archive"}}
{"translation": {"code": "def _fetch_from_remote ( self , method , args ) : try : if method == NNTTPClient . GROUP : data = self . handler . group ( args ) elif method == NNTTPClient . OVER : data = self . handler . over ( args ) elif method == NNTTPClient . ARTICLE : data = self . _fetch_article ( args ) except nntplib . NNTPTemporaryError as e : data = e raise e finally : if self . archive : self . archive . store ( method , args , None , data ) return data", "nl": "Fetch data from NNTP"}}
{"translation": {"code": "def _fetch_article ( self , article_id ) : fetched_data = self . handler . article ( article_id ) data = { 'number' : fetched_data [ 1 ] . number , 'message_id' : fetched_data [ 1 ] . message_id , 'lines' : fetched_data [ 1 ] . lines } return data", "nl": "Fetch article data"}}
{"translation": {"code": "def _fetch ( self , method , args ) : if self . from_archive : data = self . _fetch_from_archive ( method , args ) else : data = self . _fetch_from_remote ( method , args ) return data", "nl": "Fetch NNTP data from the server or from the archive"}}