{"translation": {"code": "def parse_mbox ( filepath ) : mbox = _MBox ( filepath , create = False ) for msg in mbox : message = message_to_dict ( msg ) yield message", "nl": "Parse a mbox file ."}}
{"translation": {"code": "def search ( self , origin , backend_name , category , archived_after ) : archives = self . _search_archives ( origin , backend_name , category , archived_after ) archives = [ ( fp , date ) for fp , date in archives ] archives = [ fp for fp , _ in sorted ( archives , key = lambda x : x [ 1 ] ) ] return archives", "nl": "Search archives ."}}
{"translation": {"code": "def fetch ( backend_class , backend_args , category , filter_classified = False , manager = None ) : init_args = find_signature_parameters ( backend_class . __init__ , backend_args ) archive = manager . create_archive ( ) if manager else None init_args [ 'archive' ] = archive backend = backend_class ( * * init_args ) if category : backend_args [ 'category' ] = category if filter_classified : backend_args [ 'filter_classified' ] = filter_classified fetch_args = find_signature_parameters ( backend . fetch , backend_args ) items = backend . fetch ( * * fetch_args ) try : for item in items : yield item except Exception as e : if manager : archive_path = archive . archive_path manager . remove_archive ( archive_path ) raise e", "nl": "Fetch items using the given backend ."}}
{"translation": {"code": "def __execute ( self , cmd ) : if self . from_archive : response = self . __execute_from_archive ( cmd ) else : response = self . __execute_from_remote ( cmd ) return response", "nl": "Execute gerrit command"}}
{"translation": {"code": "def user ( self , user ) : entrypoint = self . RUSERS + '/' + user response = self . _fetch ( entrypoint , None ) return response", "nl": "Fetch user data ."}}
{"translation": {"code": "def pull_commits ( self , pr_number ) : payload = { 'per_page' : PER_PAGE , } commit_url = urijoin ( \"pulls\" , str ( pr_number ) , \"commits\" ) return self . fetch_items ( commit_url , payload )", "nl": "Get pull request commits"}}
{"translation": {"code": "def setup_cmd_parser ( cls ) : parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , token_auth = True , archive = True ) # Backend token is required action = parser . parser . _option_string_actions [ '--api-token' ] action . required = True # Meetup options group = parser . parser . add_argument_group ( 'Twitter arguments' ) group . add_argument ( '--max-items' , dest = 'max_items' , type = int , default = MAX_ITEMS , help = \"Maximum number of items requested on the same query\" ) group . add_argument ( '--no-entities' , dest = 'include_entities' , action = 'store_false' , help = \" Exclude entities node\" ) group . add_argument ( '--geo-code' , dest = 'geocode' , help = \"Select tweets by users located at latitude,longitude,radius\" ) group . add_argument ( '--lang' , dest = 'lang' , help = \"Select tweets to the given language in ISO 639-1 code\" ) group . add_argument ( '--tweets-type' , dest = 'tweets_type' , default = TWEET_TYPE_MIXED , help = \"Type of tweets returned. Default is 'mixed', others are 'recent' and 'popular'\" ) group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , action = 'store_true' , help = \"sleep for getting more rate\" ) group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , default = MIN_RATE_LIMIT , type = int , help = \"sleep until reset when the rate limit reaches this value\" ) group . add_argument ( '--sleep-time' , dest = 'sleep_time' , default = SLEEP_TIME , type = int , help = \"minimun sleeping time to avoid too many request exception\" ) # Required arguments parser . parser . add_argument ( 'query' , help = \"Search query including operators, max 500 chars\" ) return parser", "nl": "Returns the Twitter argument parser ."}}
{"translation": {"code": "def fetch ( self , category = CATEGORY_TWEET , since_id = None , max_id = None , geocode = None , lang = None , include_entities = True , tweets_type = TWEET_TYPE_MIXED ) : kwargs = { \"since_id\" : since_id , \"max_id\" : max_id , \"geocode\" : geocode , \"lang\" : lang , \"include_entities\" : include_entities , \"result_type\" : tweets_type } items = super ( ) . fetch ( category , * * kwargs ) return items", "nl": "Fetch the tweets from the server ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : since_id = kwargs [ 'since_id' ] max_id = kwargs [ 'max_id' ] geocode = kwargs [ 'geocode' ] lang = kwargs [ 'lang' ] entities = kwargs [ 'include_entities' ] tweets_type = kwargs [ 'result_type' ] logger . info ( \"Fetching tweets %s from %s to %s\" , self . query , str ( since_id ) , str ( max_id ) if max_id else '--' ) tweets_ids = [ ] min_date = None max_date = None group_tweets = self . client . tweets ( self . query , since_id = since_id , max_id = max_id , geocode = geocode , lang = lang , include_entities = entities , result_type = tweets_type ) for tweets in group_tweets : for i in range ( len ( tweets ) ) : tweet = tweets [ i ] tweets_ids . append ( tweet [ 'id' ] ) if tweets [ - 1 ] == tweet : min_date = str_to_datetime ( tweets [ - 1 ] [ 'created_at' ] ) if tweets [ 0 ] == tweet and not max_date : max_date = str_to_datetime ( tweets [ 0 ] [ 'created_at' ] ) yield tweet logger . info ( \"Fetch process completed: %s (unique %s) tweets fetched, from %s to %s\" , len ( tweets_ids ) , len ( list ( set ( tweets_ids ) ) ) , min_date , max_date )", "nl": "Fetch the tweets"}}
{"translation": {"code": "def tweets ( self , query , since_id = None , max_id = None , geocode = None , lang = None , include_entities = True , result_type = TWEET_TYPE_MIXED ) : resource = self . base_url params = { 'q' : query , 'count' : self . max_items } if since_id : params [ 'since_id' ] = since_id if max_id : params [ 'max_id' ] = max_id if geocode : params [ 'geocode' ] = geocode if lang : params [ 'lang' ] = lang params [ 'include_entities' ] = include_entities params [ 'result_type' ] = result_type while True : raw_tweets = self . _fetch ( resource , params = params ) tweets = json . loads ( raw_tweets ) if not tweets [ 'statuses' ] : break params [ 'max_id' ] = tweets [ 'statuses' ] [ - 1 ] [ 'id' ] - 1 yield tweets [ 'statuses' ]", "nl": "Fetch tweets for a given query between since_id and max_id ."}}
{"translation": {"code": "def fetch_items ( self , category , * * kwargs ) : logger . info ( \"Fetching data for '%s'\" , self . keywords ) hits_raw = self . client . hits ( self . keywords ) hits = self . __parse_hits ( hits_raw ) yield hits logger . info ( \"Fetch process completed\" )", "nl": "Fetch Google hit items"}}
{"translation": {"code": "def __parse_hits ( self , hit_raw ) : # Create the soup and get the desired div bs_result = bs4 . BeautifulSoup ( hit_raw , 'html.parser' ) hit_string = bs_result . find ( \"div\" , id = \"resultStats\" ) . text # Remove commas or dots hit_string = hit_string . replace ( ',' , u'' ) hit_string = hit_string . replace ( '.' , u'' ) fetched_on = datetime_utcnow ( ) . timestamp ( ) id_args = self . keywords [ : ] id_args . append ( str ( fetched_on ) ) hits_json = { 'fetched_on' : fetched_on , 'id' : uuid ( * id_args ) , 'keywords' : self . keywords , 'type' : 'googleSearchHits' } if not hit_string : logger . warning ( \"No hits for %s\" , self . keywords ) hits_json [ 'hits' ] = 0 return hits_json str_hits = re . search ( r'\\d+' , hit_string ) . group ( 0 ) hits = int ( str_hits ) hits_json [ 'hits' ] = hits return hits_json", "nl": "Parse the hits returned by the Google Search API"}}
{"translation": {"code": "def hits ( self , keywords ) : if len ( keywords ) == 1 : query_str = keywords [ 0 ] else : query_str = ' ' . join ( [ k for k in keywords ] ) logger . info ( \"Fetching hits for '%s'\" , query_str ) params = { 'q' : query_str } # Make the request req = self . fetch ( GOOGLE_SEARCH_URL , payload = params ) return req . text", "nl": "Fetch information about a list of keywords ."}}
{"translation": {"code": "def conversation_members ( self , conversation ) : members = 0 resource = self . RCONVERSATION_INFO params = { self . PCHANNEL : conversation , } raw_response = self . _fetch ( resource , params ) response = json . loads ( raw_response ) members += len ( response [ \"members\" ] ) while 'next_cursor' in response [ 'response_metadata' ] and response [ 'response_metadata' ] [ 'next_cursor' ] : params [ 'cursor' ] = response [ 'response_metadata' ] [ 'next_cursor' ] raw_response = self . _fetch ( resource , params ) response = json . loads ( raw_response ) members += len ( response [ \"members\" ] ) return members", "nl": "Fetch the number of members in a conversation which is a supertype for public and private ones DM and group DM ."}}
{"translation": {"code": "def __fetch_merge_requests ( self , from_date ) : merges_groups = self . client . merges ( from_date = from_date ) for raw_merges in merges_groups : merges = json . loads ( raw_merges ) for merge in merges : merge_id = merge [ 'iid' ] if self . blacklist_ids and merge_id in self . blacklist_ids : logger . warning ( \"Skipping blacklisted merge request %s\" , merge_id ) continue # The single merge_request API call returns a more # complete merge request, thus we inflate it with # other data (e.g., notes, emojis, versions) merge_full_raw = self . client . merge ( merge_id ) merge_full = json . loads ( merge_full_raw ) self . __init_merge_extra_fields ( merge_full ) merge_full [ 'notes_data' ] = self . __get_merge_notes ( merge_id ) merge_full [ 'award_emoji_data' ] = self . __get_award_emoji ( GitLabClient . MERGES , merge_id ) merge_full [ 'versions_data' ] = self . __get_merge_versions ( merge_id ) yield merge_full", "nl": "Fetch the merge requests"}}
{"translation": {"code": "def merge_version ( self , merge_id , version_id ) : path = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , GitLabClient . MERGES , merge_id , GitLabClient . VERSIONS , version_id ) response = self . fetch ( path ) return response . text", "nl": "Get merge version detail"}}
{"translation": {"code": "def subscriptions ( self , per_page = PER_PAGE ) : url = urijoin ( GROUPSIO_API_URL , self . GET_SUBSCRIPTIONS ) logger . debug ( \"Get groupsio paginated subscriptions from \" + url ) keep_fetching = True payload = { \"limit\" : per_page } while keep_fetching : r = self . __fetch ( url , payload ) response_raw = r . json ( ) subscriptions = response_raw [ 'data' ] yield subscriptions total_subscriptions = response_raw [ 'total_count' ] logger . debug ( \"Subscriptions: %i/%i\" % ( response_raw [ 'end_item' ] , total_subscriptions ) ) payload [ 'page_token' ] = response_raw [ 'next_page_token' ] keep_fetching = response_raw [ 'has_more' ]", "nl": "Fetch the groupsio paginated subscriptions for a given token"}}
{"translation": {"code": "def __fetch ( self , url , payload ) : r = requests . get ( url , params = payload , auth = self . auth , verify = self . verify ) try : r . raise_for_status ( ) except requests . exceptions . HTTPError as e : raise e return r", "nl": "Fetch requests from groupsio API"}}
{"translation": {"code": "def __find_group_id ( self ) : group_subscriptions = self . subscriptions ( self . auth ) for subscriptions in group_subscriptions : for sub in subscriptions : if sub [ 'group_name' ] == self . group_name : return sub [ 'group_id' ] msg = \"Group id not found for group name %s\" % self . group_name raise BackendError ( cause = msg )", "nl": "Find the id of a group given its name by iterating on the list of subscriptions"}}
{"translation": {"code": "def _get_tokens_rate_limits ( self ) : remainings = [ 0 ] * self . n_tokens # Turn off archiving when checking rates, because that would cause # archive key conflict (the same URLs giving different responses) arch = self . archive self . archive = None for idx , token in enumerate ( self . tokens ) : # Pass flag to skip disabling archiving because this function doies it remainings [ idx ] = self . _get_token_rate_limit ( token ) # Restore archiving to whatever state it was self . archive = arch logger . debug ( \"Remaining API points: {}\" . format ( remainings ) ) return remainings", "nl": "Return array of all tokens remaining API points"}}
{"translation": {"code": "def _choose_best_api_token ( self ) : # Return if no tokens given if self . n_tokens == 0 : return # If multiple tokens given, choose best token_idx = 0 if self . n_tokens > 1 : remainings = self . _get_tokens_rate_limits ( ) token_idx = remainings . index ( max ( remainings ) ) logger . debug ( \"Remaining API points: {}, choosen index: {}\" . format ( remainings , token_idx ) ) # If we have any tokens - use best of them self . current_token = self . tokens [ token_idx ] self . session . headers . update ( { 'Authorization' : 'token ' + self . current_token } ) # Update rate limit data for the current token self . _update_current_rate_limit ( )", "nl": "Check all API tokens defined and choose one with most remaining API points"}}
{"translation": {"code": "def _need_check_tokens ( self ) : if self . n_tokens <= 1 or self . rate_limit is None : return False elif self . last_rate_limit_checked is None : self . last_rate_limit_checked = self . rate_limit return True # If approaching minimum rate limit for sleep approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 if self . rate_limit <= approaching_limit : self . last_rate_limit_checked = self . rate_limit return True # Only switch token when used predefined factor of the current token's remaining API points ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : self . last_rate_limit_checked = self . rate_limit return True elif ratio > 1.0 : self . last_rate_limit_checked = self . rate_limit return False else : return False", "nl": "Check if we need to switch GitHub API tokens"}}
{"translation": {"code": "def rev_list ( self , branches = None ) : if self . is_empty ( ) : logger . warning ( \"Git %s repository is empty; unable to get the rev-list\" , self . uri ) raise EmptyRepositoryError ( repository = self . uri ) cmd_rev_list = [ 'git' , 'rev-list' , '--topo-order' ] if branches is None : cmd_rev_list . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) elif len ( branches ) == 0 : cmd_rev_list . extend ( [ '--branches' , '--tags' , '--max-count=0' ] ) else : branches = [ 'refs/heads/' + branch for branch in branches ] cmd_rev_list . extend ( branches ) for line in self . _exec_nb ( cmd_rev_list , cwd = self . dirpath , env = self . gitenv ) : yield line . rstrip ( '\\n' ) logger . debug ( \"Git rev-list fetched from %s repository (%s)\" , self . uri , self . dirpath )", "nl": "Read the list commits from the repository"}}
{"translation": {"code": "def get_comments ( self , issue_id ) : url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , self . ISSUE , issue_id , self . COMMENT ) comments = self . get_items ( DEFAULT_DATETIME , url , expand_fields = False ) return comments", "nl": "Retrieve all the comments of a given issue ."}}