{"translation": {"code": "def main ( ) : # magic_files = {} for filename in locate_files ( ROOT_DIR ) : print ( \"Processing %s\" % filename ) with open ( filename , \"rt\" ) as f : tokens = list ( tokenize . generate_tokens ( f . readline ) ) text1 = tokenize . untokenize ( tokens ) ntokens = normalize_tokens ( tokens ) text2 = tokenize . untokenize ( ntokens ) assert text1 == text2", "nl": "Executed when script is run as - is ."}}
{"translation": {"code": "def unparse ( self ) : ut = Untokenizer ( start_row = self . _tokens [ 0 ] . start_row ) self . _unparse ( ut ) return ut . result ( )", "nl": "Convert the parsed representation back into the source code ."}}
{"translation": {"code": "def move ( self , drow , dcol = 0 ) : self . _start_row += drow self . _start_col += dcol self . _end_row += drow self . _end_col += dcol", "nl": "Move the token by drow rows and dcol columns ."}}
{"translation": {"code": "def parse_file ( filename ) : assert isinstance ( filename , _str_type ) , \"`filename` parameter should be a string, got %r\" % type ( filename ) with open ( filename , \"rt\" , encoding = \"utf-8\" ) as f : return Code ( _tokenize ( f . readline ) )", "nl": "Parse the provided file and return Code object ."}}
{"translation": {"code": "def train ( self , x = None , y = None , training_frame = None , offset_column = None , fold_column = None , weights_column = None , validation_frame = None , max_runtime_secs = None , ignored_columns = None , model_id = None , verbose = False ) : self . _train ( x = x , y = y , training_frame = training_frame , offset_column = offset_column , fold_column = fold_column , weights_column = weights_column , validation_frame = validation_frame , max_runtime_secs = max_runtime_secs , ignored_columns = ignored_columns , model_id = model_id , verbose = verbose )", "nl": "Train the H2O model ."}}
{"translation": {"code": "def join ( self ) : self . _future = False self . _job . poll ( ) model_key = self . _job . dest_key self . _job = None model_json = h2o . api ( \"GET /%d/Models/%s\" % ( self . _rest_version , model_key ) ) [ \"models\" ] [ 0 ] self . _resolve_model ( model_key , model_json )", "nl": "Wait until job s completion ."}}
{"translation": {"code": "def available ( ) : builder_json = h2o . api ( \"GET /3/ModelBuilders\" , data = { \"algo\" : \"deepwater\" } ) visibility = builder_json [ \"model_builders\" ] [ \"deepwater\" ] [ \"visibility\" ] if visibility == \"Experimental\" : print ( \"Cannot build a Deep Water model - no backend found.\" ) return False else : return True", "nl": "Returns True if a deep water model can be built or False otherwise ."}}
{"translation": {"code": "def difflag1 ( self ) : if self . ncols > 1 : raise H2OValueError ( \"Only single-column frames supported\" ) if self . types [ self . columns [ 0 ] ] not in { \"real\" , \"int\" , \"bool\" } : raise H2OValueError ( \"Numeric column expected\" ) fr = H2OFrame . _expr ( expr = ExprNode ( \"difflag1\" , self ) , cache = self . _ex . _cache ) return fr", "nl": "Conduct a diff - 1 transform on a numeric frame column ."}}
{"translation": {"code": "def check ( self , var ) : if not isinstance ( var , _str_type ) : return False return _enum_mangle ( var ) in self . _consts", "nl": "Check whether the provided value is a valid enum constant ."}}
{"translation": {"code": "def refresh ( self ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True )", "nl": "Reload frame information from the backend H2O server ."}}
{"translation": {"code": "def _read_config ( self ) : self . _config_loaded = True conf = [ ] for f in self . _candidate_log_files ( ) : if os . path . isfile ( f ) : self . _logger . info ( \"Reading config file %s\" % f ) section_rx = re . compile ( r\"^\\[(\\w+)\\]$\" ) keyvalue_rx = re . compile ( r\"^(\\w+:)?([\\w.]+)\\s*=(.*)$\" ) with io . open ( f , \"rt\" , encoding = \"utf-8\" ) as config_file : section_name = None for lineno , line in enumerate ( config_file ) : line = line . strip ( ) if line == \"\" or line . startswith ( \"#\" ) : continue m1 = section_rx . match ( line ) if m1 : section_name = m1 . group ( 1 ) continue m2 = keyvalue_rx . match ( line ) if m2 : lng = m2 . group ( 1 ) key = m2 . group ( 2 ) val = m2 . group ( 3 ) . strip ( ) if lng and lng . lower ( ) != \"py:\" : continue if section_name : key = section_name + \".\" + key if key in H2OConfigReader . _allowed_config_keys : conf . append ( ( key , val ) ) else : self . _logger . error ( \"Key %s is not a valid config key\" % key ) continue self . _logger . error ( \"Syntax error in config file line %d: %s\" % ( lineno , line ) ) self . _config = dict ( conf ) return", "nl": "Find and parse config file storing all variables in self . _config ."}}
{"translation": {"code": "def get_config ( ) : self = H2OConfigReader . _get_instance ( ) if not self . _config_loaded : self . _read_config ( ) return self . _config", "nl": "Retrieve the config as a dictionary of key - value pairs ."}}
{"translation": {"code": "def _candidate_log_files ( ) : # Search for .h2oconfig in the current directory and all parent directories relpath = \".h2oconfig\" prevpath = None while True : abspath = os . path . abspath ( relpath ) if abspath == prevpath : break prevpath = abspath relpath = \"../\" + relpath yield abspath # Also check if .h2oconfig exists in the user's directory yield os . path . expanduser ( \"~/.h2oconfig\" )", "nl": "Return possible locations for the . h2oconfig file one at a time ."}}
{"translation": {"code": "def dedent ( ind , text ) : text2 = textwrap . dedent ( text ) if ind == 0 : return text2 indent_str = \" \" * ind return \"\\n\" . join ( indent_str + line for line in text2 . split ( \"\\n\" ) )", "nl": "Dedent text to the specific indentation level ."}}
{"translation": {"code": "def main ( argv ) : global g_test_root_dir global g_temp_filename if len ( argv ) < 2 : print ( \"invoke this script as python extractGLRMRuntimeJavaLog.py javatextlog.\\n\" ) sys . exit ( 1 ) else : # we may be in business javaLogText = argv [ 1 ] # filename while java log is stored print ( \"your java text is {0}\" . format ( javaLogText ) ) extractRunInto ( javaLogText )", "nl": "Main program . Take user input parse it and call other functions to execute the commands and extract run summary and store run result in json file"}}
{"translation": {"code": "def find_synonyms ( self , word , count = 20 ) : j = h2o . api ( \"GET /3/Word2VecSynonyms\" , data = { 'model' : self . model_id , 'word' : word , 'count' : count } ) return OrderedDict ( sorted ( zip ( j [ 'synonyms' ] , j [ 'scores' ] ) , key = lambda t : t [ 1 ] , reverse = True ) )", "nl": "Find synonyms using a word2vec model ."}}
{"translation": {"code": "def minute ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"minute\" , self ) , cache = self . _ex . _cache ) if fr . _ex . _cache . types_valid ( ) : fr . _ex . _cache . types = { k : \"int\" for k in self . _ex . _cache . types . keys ( ) } return fr", "nl": "Extract the minute part from a date column ."}}
{"translation": {"code": "def save_model_details ( self , path = \"\" , force = False ) : assert_is_type ( path , str ) assert_is_type ( force , bool ) path = os . path . join ( os . getcwd ( ) if path == \"\" else path , self . model_id + \".json\" ) return h2o . api ( \"GET /99/Models/%s/json\" % self . model_id , data = { \"dir\" : path , \"force\" : force } ) [ \"dir\" ]", "nl": "Save Model Details of an H2O Model in JSON Format to disk ."}}
{"translation": {"code": "def extractPrintSaveIntermittens ( ) : # extract intermittents from collected failed tests global g_summary_dict_intermittents localtz = time . tzname [ 0 ] for ind in range ( len ( g_summary_dict_all [ \"TestName\" ] ) ) : if g_summary_dict_all [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] >= g_threshold_failure : addFailedTests ( g_summary_dict_intermittents , g_summary_dict_all , ind ) # save dict in file if len ( g_summary_dict_intermittents [ \"TestName\" ] ) > 0 : json . dump ( g_summary_dict_intermittents , open ( g_summary_dict_name , 'w' ) ) with open ( g_summary_csv_filename , 'w' ) as summaryFile : for ind in range ( len ( g_summary_dict_intermittents [ \"TestName\" ] ) ) : testName = g_summary_dict_intermittents [ \"TestName\" ] [ ind ] numberFailure = g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"FailureCount\" ] firstFailedTS = parser . parse ( time . ctime ( min ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) firstFailedStr = firstFailedTS . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) recentFail = parser . parse ( time . ctime ( max ( g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"Timestamp\" ] ) ) + ' ' + localtz ) recentFailStr = recentFail . strftime ( \"%a %b %d %H:%M:%S %Y %Z\" ) eachTest = \"{0}, {1}, {2}, {3}\\n\" . format ( testName , recentFailStr , numberFailure , g_summary_dict_intermittents [ \"TestInfo\" ] [ ind ] [ \"TestCategory\" ] [ 0 ] ) summaryFile . write ( eachTest ) print ( \"Intermittent: {0}, Last failed: {1}, Failed {2} times since \" \"{3}\" . format ( testName , recentFailStr , numberFailure , firstFailedStr ) )", "nl": "This function will print out the intermittents onto the screen for casual viewing . It will also print out where the giant summary dictionary is going to be stored ."}}
{"translation": {"code": "def summarizeFailedRuns ( ) : global g_summary_dict_all onlyFiles = [ x for x in listdir ( g_test_root_dir ) if isfile ( join ( g_test_root_dir , x ) ) ] # grab files for f in onlyFiles : for fileStart in g_file_start : if ( fileStart in f ) and ( os . path . getsize ( f ) > 10 ) : # found the file containing failed tests fFullPath = os . path . join ( g_test_root_dir , f ) try : temp_dict = json . load ( open ( fFullPath , 'r' ) ) # scrape through temp_dict and see if we need to add the test to intermittents for ind in range ( len ( temp_dict [ \"TestName\" ] ) ) : addFailedTests ( g_summary_dict_all , temp_dict , ind ) except : continue break", "nl": "This function will look at the local directory and pick out files that have the correct start name and summarize the results into one giant dict ."}}
{"translation": {"code": "def metalearner_params ( self ) : if self . _parms . get ( \"metalearner_params\" ) != None : metalearner_params_dict = ast . literal_eval ( self . _parms . get ( \"metalearner_params\" ) ) for k in metalearner_params_dict : if len ( metalearner_params_dict [ k ] ) == 1 : #single parameter metalearner_params_dict [ k ] = metalearner_params_dict [ k ] [ 0 ] return metalearner_params_dict else : return self . _parms . get ( \"metalearner_params\" )", "nl": "Parameters for metalearner algorithm"}}
{"translation": {"code": "def rename ( self , columns = None ) : assert_is_type ( columns , None , dict ) new_names = self . names ncols = self . ncols for col , name in columns . items ( ) : col_index = None if is_type ( col , int ) and ( - ncols <= col < ncols ) : col_index = ( col + ncols ) % ncols # handle negative indices elif is_type ( col , str ) and col in self . names : col_index = self . names . index ( col ) # lookup the name if col_index is not None : new_names [ col_index ] = name return self . set_names ( new_names )", "nl": "Change names of columns in the frame ."}}
{"translation": {"code": "def mojo_predict_pandas ( dataframe , mojo_zip_path , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : tmp_dir = tempfile . mkdtemp ( ) try : if not can_use_pandas ( ) : raise RuntimeException ( 'Cannot import pandas' ) import pandas assert_is_type ( dataframe , pandas . DataFrame ) input_csv_path = os . path . join ( tmp_dir , 'input.csv' ) prediction_csv_path = os . path . join ( tmp_dir , 'prediction.csv' ) dataframe . to_csv ( input_csv_path ) mojo_predict_csv ( input_csv_path = input_csv_path , mojo_zip_path = mojo_zip_path , output_csv_path = prediction_csv_path , genmodel_jar_path = genmodel_jar_path , classpath = classpath , java_options = java_options , verbose = verbose ) return pandas . read_csv ( prediction_csv_path ) finally : shutil . rmtree ( tmp_dir )", "nl": "MOJO scoring function to take a Pandas frame and use MOJO model as zip file to score ."}}
{"translation": {"code": "def mojo_predict_csv ( input_csv_path , mojo_zip_path , output_csv_path = None , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : default_java_options = '-Xmx4g -XX:ReservedCodeCacheSize=256m' prediction_output_file = 'prediction.csv' # Checking java java = H2OLocalServer . _find_java ( ) H2OLocalServer . _check_java ( java = java , verbose = verbose ) # Ensure input_csv exists if verbose : print ( \"input_csv:\\t%s\" % input_csv_path ) if not os . path . isfile ( input_csv_path ) : raise RuntimeError ( \"Input csv cannot be found at %s\" % input_csv_path ) # Ensure mojo_zip exists mojo_zip_path = os . path . abspath ( mojo_zip_path ) if verbose : print ( \"mojo_zip:\\t%s\" % mojo_zip_path ) if not os . path . isfile ( mojo_zip_path ) : raise RuntimeError ( \"MOJO zip cannot be found at %s\" % mojo_zip_path ) parent_dir = os . path . dirname ( mojo_zip_path ) # Set output_csv if necessary if output_csv_path is None : output_csv_path = os . path . join ( parent_dir , prediction_output_file ) # Set path to h2o-genmodel.jar if necessary and check it's valid if genmodel_jar_path is None : genmodel_jar_path = os . path . join ( parent_dir , gen_model_file_name ) if verbose : print ( \"genmodel_jar:\\t%s\" % genmodel_jar_path ) if not os . path . isfile ( genmodel_jar_path ) : raise RuntimeError ( \"Genmodel jar cannot be found at %s\" % genmodel_jar_path ) if verbose and output_csv_path is not None : print ( \"output_csv:\\t%s\" % output_csv_path ) # Set classpath if necessary if classpath is None : classpath = genmodel_jar_path if verbose : print ( \"classpath:\\t%s\" % classpath ) # Set java_options if necessary if java_options is None : java_options = default_java_options if verbose : print ( \"java_options:\\t%s\" % java_options ) # Construct command to invoke java cmd = [ java ] for option in java_options . split ( ' ' ) : cmd += [ option ] cmd += [ \"-cp\" , classpath , h2o_predictor_class , \"--mojo\" , mojo_zip_path , \"--input\" , input_csv_path , '--output' , output_csv_path , '--decimal' ] if verbose : cmd_str = \" \" . join ( cmd ) print ( \"java cmd:\\t%s\" % cmd_str ) # invoke the command subprocess . check_call ( cmd , shell = False ) # load predictions in form of a dict with open ( output_csv_path ) as csv_file : result = list ( csv . DictReader ( csv_file ) ) return result", "nl": "MOJO scoring function to take a CSV file and use MOJO model as zip file to score ."}}
{"translation": {"code": "def get_automl ( project_name ) : automl_json = h2o . api ( \"GET /99/AutoML/%s\" % project_name ) project_name = automl_json [ \"project_name\" ] leaderboard_list = [ key [ \"name\" ] for key in automl_json [ 'leaderboard' ] [ 'models' ] ] if leaderboard_list is not None and len ( leaderboard_list ) > 0 : leader_id = leaderboard_list [ 0 ] else : leader_id = None leader = h2o . get_model ( leader_id ) # Intentionally mask the progress bar here since showing multiple progress bars is confusing to users. # If any failure happens, revert back to user's original setting for progress and display the error message. is_progress = H2OJob . __PROGRESS_BAR__ h2o . no_progress ( ) try : # Parse leaderboard H2OTwoDimTable & return as an H2OFrame leaderboard = h2o . H2OFrame ( automl_json [ \"leaderboard_table\" ] . cell_values , column_names = automl_json [ \"leaderboard_table\" ] . col_header ) except Exception as ex : raise ex finally : if is_progress is True : h2o . show_progress ( ) leaderboard = leaderboard [ 1 : ] automl_dict = { 'project_name' : project_name , \"leader\" : leader , \"leaderboard\" : leaderboard } return automl_dict", "nl": "Retrieve information about an AutoML instance ."}}
{"translation": {"code": "def fit ( self , frame = None ) : self . _teColumns = list ( map ( lambda i : frame . names [ i ] , self . _teColumns ) ) if all ( isinstance ( n , int ) for n in self . _teColumns ) else self . _teColumns self . _responseColumnName = frame . names [ self . _responseColumnName ] if isinstance ( self . _responseColumnName , int ) else self . _responseColumnName self . _foldColumnName = frame . names [ self . _foldColumnName ] if isinstance ( self . _foldColumnName , int ) else self . _foldColumnName self . _encodingMap = ExprNode ( \"target.encoder.fit\" , frame , self . _teColumns , self . _responseColumnName , self . _foldColumnName ) . _eager_map_frame ( ) return self . _encodingMap", "nl": "Returns encoding map as an object that maps column_name - > frame_with_encoding_map_for_this_column_name"}}
{"translation": {"code": "def list_jobs ( self ) : res = h2o . api ( \"GET /3/Jobs\" ) table = [ [ \"type\" ] , [ \"dest\" ] , [ \"description\" ] , [ \"status\" ] ] for job in res [ \"jobs\" ] : job_dest = job [ \"dest\" ] table [ 0 ] . append ( self . _translate_job_type ( job_dest [ \"type\" ] ) ) table [ 1 ] . append ( job_dest [ \"name\" ] ) table [ 2 ] . append ( job [ \"description\" ] ) table [ 3 ] . append ( job [ \"status\" ] ) return table", "nl": "List all jobs performed by the cluster ."}}