{"translation": {"code": "def stabilize ( self , test_func , error , timeoutSecs = 10 , retryDelaySecs = 0.5 ) : start = time . time ( ) numberOfRetries = 0 while h2o_args . no_timeout or ( time . time ( ) - start < timeoutSecs ) : if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : break time . sleep ( retryDelaySecs ) numberOfRetries += 1 # hey, check the sandbox if we've been waiting a long time...rather than wait for timeout # to find the badness?. can check_sandbox_for_errors at any time if ( ( numberOfRetries % 50 ) == 0 ) : check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) else : timeTakenSecs = time . time ( ) - start if isinstance ( error , type ( '' ) ) : raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) else : msg = error ( self , timeTakenSecs , numberOfRetries ) raise Exception ( msg )", "nl": "Repeatedly test a function waiting for it to return True ."}}
{"translation": {"code": "def summary ( self , key , column = \"C1\" , timeoutSecs = 10 , * * kwargs ) : params_dict = { # 'offset': 0, # 'len': 100 } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'summary' , True ) result = self . do_json_request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeoutSecs , params = params_dict ) h2o_sandbox . check_sandbox_for_errors ( ) return result", "nl": "Return the summary for a single column for a single Frame in the h2o cluster ."}}
{"translation": {"code": "def delete_model ( self , key , ignoreMissingKey = True , timeoutSecs = 60 , * * kwargs ) : assert key is not None , '\"key\" parameter is null' result = self . do_json_request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) # TODO: look for what? if not ignoreMissingKey and 'f00b4r' in result : raise ValueError ( 'Model key not found: ' + key ) verboseprint ( \"delete_model result:\" , dump_json ( result ) ) return result", "nl": "Delete a model on the h2o cluster given its key ."}}
{"translation": {"code": "def model_metrics ( self , timeoutSecs = 60 , * * kwargs ) : result = self . do_json_request ( '/3/ModelMetrics.json' , cmd = 'get' , timeout = timeoutSecs ) h2o_sandbox . check_sandbox_for_errors ( ) return result", "nl": "ModelMetrics list ."}}
{"translation": {"code": "def compute_model_metrics ( self , model , frame , timeoutSecs = 60 , * * kwargs ) : assert model is not None , '\"model\" parameter is null' assert frame is not None , '\"frame\" parameter is null' models = self . models ( key = model , timeoutSecs = timeoutSecs ) assert models is not None , \"/Models REST call failed\" assert models [ 'models' ] [ 0 ] [ 'model_id' ] [ 'name' ] == model , \"/Models/{0} returned Model {1} rather than Model {2}\" . format ( model , models [ 'models' ] [ 0 ] [ 'key' ] [ 'name' ] , model ) # TODO: test this assert, I don't think this is working. . . frames = self . frames ( key = frame ) assert frames is not None , \"/Frames/{0} REST call failed\" . format ( frame ) print \"frames:\" , dump_json ( frames ) # is the name not there? # assert frames['frames'][0]['model_id']['name'] == frame, \"/Frames/{0} returned Frame {1} rather than Frame {2}\".format(frame, models['models'][0]['key']['name'], frame) result = self . do_json_request ( '/3/ModelMetrics.json/models/' + model + '/frames/' + frame , cmd = 'post' , timeout = timeoutSecs ) mm = result [ 'model_metrics' ] [ 0 ] verboseprint ( \"model metrics: \" + repr ( mm ) ) h2o_sandbox . check_sandbox_for_errors ( ) return mm", "nl": "Score a model on the h2o cluster on the given Frame and return only the model metrics ."}}
{"translation": {"code": "def validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60 , * * kwargs ) : assert algo is not None , '\"algo\" parameter is null' # Allow this now: assert training_frame is not None, '\"training_frame\" parameter is null' assert parameters is not None , '\"parameters\" parameter is null' model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) assert model_builders is not None , \"/ModelBuilders REST call failed\" assert algo in model_builders [ 'model_builders' ] builder = model_builders [ 'model_builders' ] [ algo ] # TODO: test this assert, I don't think this is working. . . if training_frame is not None : frames = self . frames ( key = training_frame ) assert frames is not None , \"/Frames/{0} REST call failed\" . format ( training_frame ) key_name = frames [ 'frames' ] [ 0 ] [ 'key' ] [ 'name' ] assert key_name == training_frame , \"/Frames/{0} returned Frame {1} rather than Frame {2}\" . format ( training_frame , key_name , training_frame ) parameters [ 'training_frame' ] = training_frame # TODO: add parameter existence checks # TODO: add parameter value validation # FIX! why ignoreH2oError here? result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + \"/parameters\" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = True , noExtraErrorCheck = True ) verboseprint ( \"model parameters validation: \" + repr ( result ) ) return result", "nl": "Check a dictionary of model builder parameters on the h2o cluster using the given algorithm and model parameters ."}}
{"translation": {"code": "def model_builders ( self , algo = None , timeoutSecs = 10 , * * kwargs ) : params_dict = { } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'model_builders' , False ) request = '3/ModelBuilders.json' if algo : request += \"/\" + algo result = self . do_json_request ( request , timeout = timeoutSecs , params = params_dict ) # verboseprint(request, \"result:\", dump_json(result)) h2o_sandbox . check_sandbox_for_errors ( ) return result", "nl": "Return a model builder or all of the model builders known to the h2o cluster . The model builders are contained in a dictionary called model_builders at the top level of the result . The dictionary maps algorithm names to parameters lists . Each of the parameters contains all the metdata required by a client to present a model building interface to the user ."}}
{"translation": {"code": "def delete_frame ( self , key , ignoreMissingKey = True , timeoutSecs = 60 , * * kwargs ) : assert key is not None , '\"key\" parameter is null' result = self . do_json_request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) # TODO: look for what? if not ignoreMissingKey and 'f00b4r' in result : raise ValueError ( 'Frame key not found: ' + key ) return result", "nl": "Delete a frame on the h2o cluster given its key ."}}
{"translation": {"code": "def stop_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( \"Stopping instances {0}.\" . format ( instances ) ) conn . stop_instances ( instances ) log ( \"Done\" )", "nl": "stop all the instances given by its ids"}}
{"translation": {"code": "def terminate_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( \"Terminating instances {0}.\" . format ( instances ) ) conn . terminate_instances ( instances ) log ( \"Done\" )", "nl": "terminate all the instances given by its ids"}}
{"translation": {"code": "def run_instances ( count , ec2_config , region , waitForSSH = True , tags = None ) : ec2params = inheritparams ( ec2_config , EC2_API_RUN_INSTANCE ) ec2params . setdefault ( 'min_count' , count ) ec2params . setdefault ( 'max_count' , count ) reservation = None conn = ec2_connect ( region ) try : reservation = conn . run_instances ( * * ec2params ) log ( 'Reservation: {0}' . format ( reservation . id ) ) log ( 'Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.' . format ( len ( reservation . instances ) , reservation . instances ) ) start = time . time ( ) time . sleep ( 1 ) for instance in reservation . instances : while instance . update ( ) == 'pending' : time . sleep ( 1 ) h2o_cmd . dot ( ) if not instance . state == 'running' : raise Exception ( '\\033[91m[ec2] Error waiting for running state. Instance is in state {0}.\\033[0m' . format ( instance . state ) ) log ( 'Instances started in {0} seconds' . format ( time . time ( ) - start ) ) log ( 'Instances: ' ) for inst in reservation . instances : log ( \"   {0} ({1}) : public ip: {2}, private ip: {3}\" . format ( inst . public_dns_name , inst . id , inst . ip_address , inst . private_ip_address ) ) if waitForSSH : # kbn: changing to private address, so it should fail if not in right domain # used to have the public ip address wait_for_ssh ( [ i . private_ip_address for i in reservation . instances ] ) # Tag instances try : if tags : conn . create_tags ( [ i . id for i in reservation . instances ] , tags ) except : warn ( 'Something wrong during tagging instances. Exceptions IGNORED!' ) print sys . exc_info ( ) pass return reservation except : print \"\\033[91mUnexpected error\\033[0m :\" , sys . exc_info ( ) if reservation : terminate_reservation ( reservation , region ) raise", "nl": "Create a new reservation for count instances"}}
{"translation": {"code": "def wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) for ip in ips : if not skipAlive or not ssh_live ( ip , port ) : log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) count = 0 while count < requiredsuccess : if ssh_live ( ip , port ) : count += 1 else : count = 0 time . sleep ( 1 ) h2o_cmd . dot ( )", "nl": "Wait for ssh service to appear on given hosts"}}
{"translation": {"code": "def reboot_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( \"Rebooting instances {0}.\" . format ( instances ) ) conn . reboot_instances ( instances ) log ( \"Done\" )", "nl": "Reboot all the instances given by its ids"}}
{"translation": {"code": "def start_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( \"Starting instances {0}.\" . format ( instances ) ) conn . start_instances ( instances ) log ( \"Done\" )", "nl": "Start all the instances given by its ids"}}
{"translation": {"code": "def percentileOnSortedList ( N , percent , key = lambda x : x , interpolate = 'mean' ) : # 5 ways of resolving fractional # floor, ceil, funky, linear, mean interpolateChoices = [ 'floor' , 'ceil' , 'funky' , 'linear' , 'mean' ] if interpolate not in interpolateChoices : print \"Bad choice for interpolate:\" , interpolate print \"Supported choices:\" , interpolateChoices if N is None : return None k = ( len ( N ) - 1 ) * percent f = int ( math . floor ( k ) ) c = int ( math . ceil ( k ) ) if f == c : d = key ( N [ f ] ) msg = \"aligned:\" elif interpolate == 'floor' : d = key ( N [ f ] ) msg = \"fractional with floor:\" elif interpolate == 'ceil' : d = key ( N [ c ] ) msg = \"fractional with ceil:\" elif interpolate == 'funky' : d0 = key ( N [ f ] ) * ( c - k ) d1 = key ( N [ c ] ) * ( k - f ) d = d0 + d1 msg = \"fractional with Tung(floor and ceil) :\" elif interpolate == 'linear' : assert ( c - f ) == 1 assert ( k >= f ) and ( k <= c ) pctDiff = k - f dDiff = pctDiff * ( key ( N [ c ] ) - key ( N [ f ] ) ) d = key ( N [ f ] + dDiff ) msg = \"fractional %s with linear(floor and ceil):\" % pctDiff elif interpolate == 'mean' : d = ( key ( N [ c ] ) + key ( N [ f ] ) ) / 2.0 msg = \"fractional with mean(floor and ceil):\" # print 3 around the floored k, for eyeballing when we're close flooredK = int ( f ) # print the 3 around the median if flooredK > 0 : print \"prior->\" , key ( N [ flooredK - 1 ] ) , \" \" else : print \"prior->\" , \"<bof>\" print \"floor->\" , key ( N [ flooredK ] ) , \" \" , msg , 'result:' , d , \"f:\" , f , \"len(N):\" , len ( N ) if flooredK + 1 < len ( N ) : print \" ceil->\" , key ( N [ flooredK + 1 ] ) , \"c:\" , c else : print \" ceil-> <eof>\" , \"c:\" , c return d", "nl": "Find the percentile of a list of values ."}}
{"translation": {"code": "def show ( self , header = True ) : # if h2o.can_use_pandas(): #  import pandas #  pandas.options.display.max_rows = 20 #  print pandas.DataFrame(self._cell_values,columns=self._col_header) #  return if header and self . _table_header : print ( self . _table_header + \":\" , end = ' ' ) if self . _table_description : print ( self . _table_description ) print ( ) table = copy . deepcopy ( self . _cell_values ) nr = 0 if _is_list_of_lists ( table ) : nr = len ( table ) # only set if we truly have multiple rows... not just one long row :) if nr > 20 : # create a truncated view of the table, first/last 5 rows trunc_table = [ ] trunc_table += [ v for v in table [ : 5 ] ] trunc_table . append ( [ \"---\" ] * len ( table [ 0 ] ) ) trunc_table += [ v for v in table [ ( nr - 5 ) : ] ] table = trunc_table H2ODisplay ( table , self . _col_header , numalign = \"left\" , stralign = \"left\" ) if nr > 20 and can_use_pandas ( ) : print ( '\\nSee the whole table with table.as_data_frame()' )", "nl": "Print the contents of this table ."}}
{"translation": {"code": "def lazy_import ( path , pattern = None ) : assert_is_type ( path , str , [ str ] ) assert_is_type ( pattern , str , None ) paths = [ path ] if is_type ( path , str ) else path return _import_multi ( paths , pattern )", "nl": "Import a single file or collection of files ."}}
{"translation": {"code": "def signal_handler ( signum , stackframe ) : global g_runner global g_handling_signal if g_handling_signal : # Don't do this recursively. return g_handling_signal = True print ( \"\" ) print ( \"----------------------------------------------------------------------\" ) print ( \"\" ) print ( \"SIGNAL CAUGHT (\" + str ( signum ) + \").  TEARING DOWN CLOUDS.\" ) print ( \"\" ) print ( \"----------------------------------------------------------------------\" ) g_runner . terminate ( )", "nl": "Helper function to handle caught signals ."}}
{"translation": {"code": "def wipe_output_dir ( ) : print ( \"Wiping output directory.\" ) try : if os . path . exists ( g_output_dir ) : shutil . rmtree ( str ( g_output_dir ) ) except OSError as e : print ( \"ERROR: Removing output directory %s failed: \" % g_output_dir ) print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) print ( \"\" ) sys . exit ( 1 )", "nl": "Clear the output directory ."}}
{"translation": {"code": "def scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : retries = 60 while retries > 0 : if self . terminated : return f = open ( self . output_file_name , \"r\" ) s = f . readline ( ) while len ( s ) > 0 : if self . terminated : return match_groups = re . search ( r\"Cloud of size (\\d+) formed\" , s ) if match_groups is not None : size = match_groups . group ( 1 ) if size is not None : size = int ( size ) if size == nodes_per_cloud : f . close ( ) return s = f . readline ( ) f . close ( ) retries -= 1 if self . terminated : return time . sleep ( 1 ) print ( \"\" ) print ( \"ERROR: Too many retries starting cloud.\" ) print ( \"\" ) sys . exit ( 1 )", "nl": "Look at the stdout log and wait until the cluster of proper size is formed . This call is blocking . Exit if this fails ."}}
{"translation": {"code": "def stop ( self ) : if self . pid > 0 : print ( \"Killing JVM with PID {}\" . format ( self . pid ) ) try : self . child . terminate ( ) self . child . wait ( ) except OSError : pass self . pid = - 1", "nl": "Normal node shutdown . Ignore failures for now ."}}
{"translation": {"code": "def stop ( self ) : for node in self . nodes : node . stop ( ) for node in self . client_nodes : node . stop ( )", "nl": "Normal cluster shutdown ."}}
{"translation": {"code": "def get_ip ( self ) : if len ( self . client_nodes ) > 0 : node = self . client_nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get_ip ( )", "nl": "Return an ip to use to talk to this cluster ."}}
{"translation": {"code": "def get_port ( self ) : if len ( self . client_nodes ) > 0 : node = self . client_nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get_port ( )", "nl": "Return a port to use to talk to this cluster ."}}
{"translation": {"code": "def scrape_port_from_stdout ( self ) : regex = re . compile ( r\"Open H2O Flow in your web browser: https?://([^:]+):(\\d+)\" ) retries_left = 30 while retries_left and not self . terminated : with open ( self . output_file_name , \"r\" ) as f : for line in f : mm = re . search ( regex , line ) if mm is not None : self . port = mm . group ( 2 ) print ( \"H2O cloud %d node %d listening on port %s\\n    with output file %s\" % ( self . cloud_num , self . node_num , self . port , self . output_file_name ) ) return if self . terminated : break retries_left -= 1 time . sleep ( 1 ) if self . terminated : return print ( \"\\nERROR: Too many retries starting cloud %d.\\nCheck the output log %s.\\n\" % ( self . cloud_num , self . output_file_name ) ) sys . exit ( 1 )", "nl": "Look at the stdout log and figure out which port the JVM chose ."}}
{"translation": {"code": "def as_list ( data , use_pandas = True , header = True ) : assert_is_type ( data , H2OFrame ) assert_is_type ( use_pandas , bool ) assert_is_type ( header , bool ) return H2OFrame . as_data_frame ( data , use_pandas = use_pandas , header = header )", "nl": "Convert an H2O data object into a python - specific object ."}}
{"translation": {"code": "def get_model ( model_id ) : assert_is_type ( model_id , str ) model_json = api ( \"GET /3/Models/%s\" % model_id ) [ \"models\" ] [ 0 ] algo = model_json [ \"algo\" ] if algo == \"svd\" : m = H2OSVD ( ) elif algo == \"pca\" : m = H2OPrincipalComponentAnalysisEstimator ( ) elif algo == \"drf\" : m = H2ORandomForestEstimator ( ) elif algo == \"naivebayes\" : m = H2ONaiveBayesEstimator ( ) elif algo == \"kmeans\" : m = H2OKMeansEstimator ( ) elif algo == \"glrm\" : m = H2OGeneralizedLowRankEstimator ( ) elif algo == \"glm\" : m = H2OGeneralizedLinearEstimator ( ) elif algo == \"gbm\" : m = H2OGradientBoostingEstimator ( ) elif algo == \"deepwater\" : m = H2ODeepWaterEstimator ( ) elif algo == \"xgboost\" : m = H2OXGBoostEstimator ( ) elif algo == \"word2vec\" : m = H2OWord2vecEstimator ( ) elif algo == \"generic\" : m = H2OGenericEstimator ( ) elif algo == \"deeplearning\" : if model_json [ \"output\" ] [ \"model_category\" ] == \"AutoEncoder\" : m = H2OAutoEncoderEstimator ( ) else : m = H2ODeepLearningEstimator ( ) elif algo == \"stackedensemble\" : m = H2OStackedEnsembleEstimator ( ) elif algo == \"isolationforest\" : m = H2OIsolationForestEstimator ( ) else : raise ValueError ( \"Unknown algo type: \" + algo ) m . _resolve_model ( model_id , model_json ) return m", "nl": "Load a model from the server ."}}
{"translation": {"code": "def download_csv ( data , filename ) : assert_is_type ( data , H2OFrame ) assert_is_type ( filename , str ) url = h2oconn . make_url ( \"DownloadDataset\" , 3 ) + \"?frame_id={}&hex_string=false\" . format ( data . frame_id ) with open ( filename , \"wb\" ) as f : f . write ( urlopen ( ) ( url ) . read ( ) )", "nl": "Download an H2O data set to a CSV file on the local disk ."}}
{"translation": {"code": "def screeplot ( self , type = \"barplot\" , * * kwargs ) : # check for matplotlib. exit if absent. is_server = kwargs . pop ( \"server\" ) if kwargs : raise ValueError ( \"Unknown arguments %s to screeplot()\" % \", \" . join ( kwargs . keys ( ) ) ) try : import matplotlib if is_server : matplotlib . use ( 'Agg' , warn = False ) import matplotlib . pyplot as plt except ImportError : print ( \"matplotlib is required for this function!\" ) return variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] plt . xlabel ( 'Components' ) plt . ylabel ( 'Variances' ) plt . title ( 'Scree Plot' ) plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) if type == \"barplot\" : plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) elif type == \"lines\" : plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) if not is_server : plt . show ( )", "nl": "Produce the scree plot ."}}
{"translation": {"code": "def _wait_for_keypress ( ) : result = None if os . name == \"nt\" : # noinspection PyUnresolvedReferences import msvcrt result = msvcrt . getch ( ) else : import termios fd = sys . stdin . fileno ( ) oldterm = termios . tcgetattr ( fd ) newattr = termios . tcgetattr ( fd ) newattr [ 3 ] = newattr [ 3 ] & ~ termios . ICANON & ~ termios . ECHO termios . tcsetattr ( fd , termios . TCSANOW , newattr ) try : result = sys . stdin . read ( 1 ) except IOError : pass finally : termios . tcsetattr ( fd , termios . TCSAFLUSH , oldterm ) return result", "nl": "Wait for a key press on the console and return it ."}}
{"translation": {"code": "def h2o_mean_absolute_error ( y_actual , y_predicted , weights = None ) : ModelBase . _check_targets ( y_actual , y_predicted ) return _colmean ( ( y_predicted - y_actual ) . abs ( ) )", "nl": "Mean absolute error regression loss ."}}
{"translation": {"code": "def h2o_explained_variance_score ( y_actual , y_predicted , weights = None ) : ModelBase . _check_targets ( y_actual , y_predicted ) _ , numerator = _mean_var ( y_actual - y_predicted , weights ) _ , denominator = _mean_var ( y_actual , weights ) if denominator == 0.0 : return 1. if numerator == 0 else 0. # 0/0 => 1, otherwise, 0 return 1 - numerator / denominator", "nl": "Explained variance regression score function ."}}
{"translation": {"code": "def h2o_median_absolute_error ( y_actual , y_predicted ) : ModelBase . _check_targets ( y_actual , y_predicted ) return ( y_predicted - y_actual ) . abs ( ) . median ( )", "nl": "Median absolute error regression loss"}}
{"translation": {"code": "def h2o_mean_squared_error ( y_actual , y_predicted , weights = None ) : ModelBase . _check_targets ( y_actual , y_predicted ) return _colmean ( ( y_predicted - y_actual ) ** 2 )", "nl": "Mean squared error regression loss"}}
{"translation": {"code": "def download_all_logs ( dirname = \".\" , filename = None ) : assert_is_type ( dirname , str ) assert_is_type ( filename , str , None ) url = \"%s/3/Logs/download\" % h2oconn . base_url opener = urlopen ( ) response = opener ( url ) if not os . path . exists ( dirname ) : os . mkdir ( dirname ) if filename is None : if PY3 : headers = [ h [ 1 ] for h in response . headers . _headers ] else : headers = response . headers . headers for h in headers : if \"filename=\" in h : filename = h . split ( \"filename=\" ) [ 1 ] . strip ( ) break path = os . path . join ( dirname , filename ) response = opener ( url ) . read ( ) print ( \"Writing H2O logs to \" + path ) with open ( path , \"wb\" ) as f : f . write ( response ) return path", "nl": "Download H2O log files to disk ."}}
{"translation": {"code": "def upload_file ( path , destination_frame = None , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , skipped_columns = None ) : coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) natype = U ( str , [ str ] ) assert_is_type ( path , str ) assert_is_type ( destination_frame , str , None ) assert_is_type ( header , - 1 , 0 , 1 ) assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == 1 ) ) assert_is_type ( col_names , [ str ] , None ) assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) assert ( skipped_columns == None ) or isinstance ( skipped_columns , list ) , \"The skipped_columns should be an list of column names!\" check_frame_id ( destination_frame ) if path . startswith ( \"~\" ) : path = os . path . expanduser ( path ) return H2OFrame ( ) . _upload_parse ( path , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns )", "nl": "Upload a dataset from the provided local path to the H2O cluster ."}}
{"translation": {"code": "def import_sql_select ( connection_url , select_query , username , password , optimize = True , use_temp_table = None , temp_table_name = None , fetch_mode = None ) : assert_is_type ( connection_url , str ) assert_is_type ( select_query , str ) assert_is_type ( username , str ) assert_is_type ( password , str ) assert_is_type ( optimize , bool ) assert_is_type ( use_temp_table , bool , None ) assert_is_type ( temp_table_name , str , None ) assert_is_type ( fetch_mode , str , None ) p = { \"connection_url\" : connection_url , \"select_query\" : select_query , \"username\" : username , \"password\" : password , \"use_temp_table\" : use_temp_table , \"temp_table_name\" : temp_table_name , \"fetch_mode\" : fetch_mode } j = H2OJob ( api ( \"POST /99/ImportSQLTable\" , data = p ) , \"Import SQL Table\" ) . poll ( ) return get_frame ( j . dest_key )", "nl": "Import the SQL table that is the result of the specified SQL query to H2OFrame in memory ."}}
{"translation": {"code": "def get_frame ( frame_id , * * kwargs ) : assert_is_type ( frame_id , str ) return H2OFrame . get_frame ( frame_id , * * kwargs )", "nl": "Obtain a handle to the frame in H2O with the frame_id key ."}}
{"translation": {"code": "def proj_archetypes ( self , test_data , reverse_transform = False ) : if test_data is None or test_data . nrow == 0 : raise ValueError ( \"Must specify test data\" ) j = h2o . api ( \"POST /3/Predictions/models/%s/frames/%s\" % ( self . model_id , test_data . frame_id ) , data = { \"project_archetypes\" : True , \"reverse_transform\" : reverse_transform } ) return h2o . get_frame ( j [ \"model_metrics\" ] [ 0 ] [ \"predictions\" ] [ \"frame_id\" ] [ \"name\" ] )", "nl": "Convert archetypes of the model into original feature space ."}}
{"translation": {"code": "def update_summary_file ( ) : global g_summary_text_filename global g_output_filename_failed_tests global g_output_filename_passed_tests with open ( g_summary_text_filename , 'a' ) as tempfile : write_file_content ( tempfile , g_output_filename_failed_tests ) write_file_content ( tempfile , g_output_filename_passed_tests )", "nl": "Concatecate all log file into a summary text file to be sent to users at the end of a daily log scraping ."}}
{"translation": {"code": "def extract_true_string ( string_content ) : startL , found , endL = string_content . partition ( '[0m' ) if found : return endL else : return string_content", "nl": "remove extra characters before the actual string we are looking for . The Jenkins console output is encoded using utf - 8 . However the stupid redirect function can only encode using ASCII . I have googled for half a day with no results to how to resolve the issue . Hence we are going to the heat and just manually get rid of the junk ."}}
{"translation": {"code": "def find_node_name ( each_line , temp_func_list ) : global g_node_name global g_failed_test_info_dict if g_node_name in each_line : temp_strings = each_line . split ( ) [ start , found , endstr ] = each_line . partition ( g_node_name ) if found : temp_strings = endstr . split ( ) g_failed_test_info_dict [ \"6.node_name\" ] = extract_true_string ( temp_strings [ 1 ] ) temp_func_list . remove ( find_node_name ) return True", "nl": "Find the slave machine where a Jenkins job was executed on . It will save this information in g_failed_test_info_dict . In addition it will delete this particular function handle off the temp_func_list as we do not need to perform this action again ."}}
{"translation": {"code": "def find_git_hash_branch ( each_line , temp_func_list ) : global g_git_hash_branch global g_failed_test_info_dict if g_git_hash_branch in each_line : [ start , found , endstr ] = each_line . partition ( g_git_hash_branch ) temp_strings = endstr . strip ( ) . split ( ) if len ( temp_strings ) > 1 : g_failed_test_info_dict [ \"4.git_hash\" ] = temp_strings [ 0 ] g_failed_test_info_dict [ \"5.git_branch\" ] = temp_strings [ 1 ] temp_func_list . remove ( find_git_hash_branch ) return True", "nl": "Find the git hash and branch info that a Jenkins job was taken from . It will save this information in g_failed_test_info_dict . In addition it will delete this particular function handle off the temp_func_list as we do not need to perform this action again ."}}
{"translation": {"code": "def find_build_timeout ( each_line , temp_func_list ) : global g_build_timeout global g_failed_test_info_dict global g_failure_occurred if g_build_timeout in each_line : g_failed_test_info_dict [ \"8.build_timeout\" ] = 'Yes' g_failure_occurred = True return False # build timeout was found, no need to continue mining the console text else : return True", "nl": "Find if a Jenkins job has taken too long to finish and was killed . It will save this information in g_failed_test_info_dict ."}}
{"translation": {"code": "def find_build_failure ( each_line , temp_func_list ) : global g_build_success global g_build_success_tests global g_failed_test_info_dict global g_failure_occurred global g_build_failed_message for ind in range ( 0 , len ( g_build_failed_message ) ) : if g_build_failed_message [ ind ] in each_line . lower ( ) : if ( ( ind == 0 ) and ( len ( g_failed_jobs ) > 0 ) ) : continue else : g_failure_occurred = True g_failed_test_info_dict [ \"7.build_failure\" ] = 'Yes' temp_func_list . remove ( find_build_failure ) return False return True", "nl": "Find if a Jenkins job has failed to build . It will save this information in g_failed_test_info_dict . In addition it will delete this particular function handle off the temp_func_list as we do not need to perform this action again ."}}
{"translation": {"code": "def find_build_id ( each_line , temp_func_list ) : global g_before_java_file global g_java_filenames global g_build_id_text global g_jenkins_url global g_output_filename global g_output_pickle_filename if g_build_id_text in each_line : [ startStr , found , endStr ] = each_line . partition ( g_build_id_text ) g_failed_test_info_dict [ \"2.build_id\" ] = endStr . strip ( ) temp_func_list . remove ( find_build_id ) g_jenkins_url = os . path . join ( 'http://' , g_jenkins_url , 'view' , g_view_name , 'job' , g_failed_test_info_dict [ \"1.jobName\" ] , g_failed_test_info_dict [ \"2.build_id\" ] , 'artifact' ) return True", "nl": "Find the build id of a jenkins job . It will save this information in g_failed_test_info_dict . In addition it will delete this particular function handle off the temp_func_list as we do not need to perform this action again ."}}
{"translation": {"code": "def extract_job_build_url ( url_string ) : global g_failed_test_info_dict global g_jenkins_url global g_view_name tempString = url_string . strip ( '/' ) . split ( '/' ) if len ( tempString ) < 6 : print \"Illegal URL resource address.\\n\" sys . exit ( 1 ) g_failed_test_info_dict [ \"1.jobName\" ] = tempString [ 6 ] g_jenkins_url = tempString [ 2 ] g_view_name = tempString [ 4 ]", "nl": "From user input grab the jenkins job name and saved it in g_failed_test_info_dict . In addition it will grab the jenkins url and the view name into g_jenkins_url and g_view_name ."}}
{"translation": {"code": "def grab_java_message ( ) : global g_temp_filename global g_current_testname global g_java_start_text global g_ok_java_messages global g_java_general_bad_messages # store bad java messages not associated with running a unit test global g_java_general_bad_message_types global g_failure_occurred global g_java_message_type global g_all_java_message_type global g_toContinue java_messages = [ ] # store all bad java messages associated with running a unit test java_message_types = [ ] # store all bad java message types associated with running a unit test if os . path . isfile ( g_temp_filename ) : # open temp file containing content of some java_*_0.out.txt java_file = open ( g_temp_filename , 'r' ) g_toContinue = False # denote if a multi-line message starts tempMessage = \"\" messageType = \"\" for each_line in java_file : if ( g_java_start_text in each_line ) : startStr , found , endStr = each_line . partition ( g_java_start_text ) if len ( found ) > 0 : if len ( g_current_testname ) > 0 : # a new unit test is being started.  Save old info and move on associate_test_with_java ( g_current_testname , java_messages , java_message_types ) g_current_testname = endStr . strip ( ) # record the test name java_messages = [ ] java_message_types = [ ] temp_strings = each_line . strip ( ) . split ( ) if ( len ( temp_strings ) >= 6 ) and ( temp_strings [ 5 ] in g_all_java_message_type ) : if g_toContinue == True : # at the end of last message fragment addJavaMessages ( tempMessage , messageType , java_messages , java_message_types ) tempMessage = \"\" messageType = \"\" # start of new message fragment g_toContinue = False else : # non standard output.  Continuation of last java message, add it to bad java message list if g_toContinue : tempMessage += each_line # add more java message here # if len(g_current_testname) == 0: #     addJavaMessages(each_line.strip(),\"\",java_messages,java_message_types) # else: #     addJavaMessages(each_line.strip(),\"\",java_messages,java_message_types) if ( ( len ( temp_strings ) > 5 ) and ( temp_strings [ 5 ] in g_java_message_type ) ) : # find a bad java message startStr , found , endStr = each_line . partition ( temp_strings [ 5 ] ) # can be WARN,ERRR,FATAL,TRACE if found and ( len ( endStr . strip ( ) ) > 0 ) : tempMessage += endStr messageType = temp_strings [ 5 ] #                    if (tempMessage not in g_ok_java_messages[\"general\"]):  # found new bad messages that cannot be ignored g_toContinue = True # add tempMessage to bad java message list #                        addJavaMessages(tempMessage,temp_strings[5],java_messages,java_message_types) java_file . close ( )", "nl": "scan through the java output text and extract the bad java messages that may or may not happened when unit tests are run . It will not record any bad java messages that are stored in g_ok_java_messages ."}}
{"translation": {"code": "def save_dict ( ) : global g_test_root_dir global g_output_filename_failed_tests global g_output_filename_passed_tests global g_output_pickle_filename global g_failed_test_info_dict # some build can fail really early that no buid id info is stored in the console text. if \"2.build_id\" not in g_failed_test_info_dict . keys ( ) : g_failed_test_info_dict [ \"2.build_id\" ] = \"unknown\" build_id = g_failed_test_info_dict [ \"2.build_id\" ] g_output_filename_failed_tests = g_output_filename_failed_tests + '_build_' + build_id + '_failed_tests.log' g_output_filename_passed_tests = g_output_filename_passed_tests + '_build_' + build_id + '_passed_tests.log' g_output_pickle_filename = g_output_pickle_filename + '_build_' + build_id + '.pickle' allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) # write out the jenkins job info into log files. with open ( g_output_pickle_filename , 'wb' ) as test_file : pickle . dump ( g_failed_test_info_dict , test_file ) # write out the failure report as text into a text file text_file_failed_tests = open ( g_output_filename_failed_tests , 'w' ) text_file_passed_tests = None allKeys = sorted ( g_failed_test_info_dict . keys ( ) ) write_passed_tests = False if ( \"passed_tests_info *********\" in allKeys ) : text_file_passed_tests = open ( g_output_filename_passed_tests , 'w' ) write_passed_tests = True for keyName in allKeys : val = g_failed_test_info_dict [ keyName ] if isinstance ( val , list ) : # writing one of the job lists if ( len ( val ) == 3 ) : # it is a message for a test if keyName == \"failed_tests_info *********\" : write_test_java_message ( keyName , val , text_file_failed_tests ) if keyName == \"passed_tests_info *********\" : write_test_java_message ( keyName , val , text_file_passed_tests ) elif ( len ( val ) == 2 ) : # it is a general bad java message write_java_message ( keyName , val , text_file_failed_tests ) if write_passed_tests : write_java_message ( keyName , val , text_file_passed_tests ) else : write_general_build_message ( keyName , val , text_file_failed_tests ) if write_passed_tests : write_general_build_message ( keyName , val , text_file_passed_tests ) text_file_failed_tests . close ( ) if write_passed_tests : text_file_passed_tests . close ( )", "nl": "Save the log scraping results into logs denoted by g_output_filename_failed_tests and g_output_filename_passed_tests ."}}
{"translation": {"code": "def write_file_content ( fhandle , file2read ) : if os . path . isfile ( file2read ) : # write summary of failed tests logs with open ( file2read , 'r' ) as tfile : fhandle . write ( '============ Content of ' + file2read ) fhandle . write ( '\\n' ) fhandle . write ( tfile . read ( ) ) fhandle . write ( '\\n\\n' )", "nl": "Write one log file into the summary text file ."}}
{"translation": {"code": "def load_dict ( ) : global g_load_java_message_filename global g_ok_java_messages if os . path . isfile ( g_load_java_message_filename ) : # only load dict from file if it exists. with open ( g_load_java_message_filename , 'rb' ) as ofile : g_ok_java_messages = pickle . load ( ofile ) else : # no previous java messages to be excluded are found g_ok_java_messages [ \"general\" ] = [ ]", "nl": "Load java messages that can be ignored pickle file into a dict structure g_ok_java_messages ."}}
{"translation": {"code": "def add_new_message ( ) : global g_new_messages_to_exclude # filename containing text file from user containing new java ignored messages global g_dict_changed # True if new ignored java messages are added. new_message_dict = extract_message_to_dict ( g_new_messages_to_exclude ) if new_message_dict : g_dict_changed = True update_message_dict ( new_message_dict , 1 )", "nl": "Add new java messages to ignore from user text file . It first reads in the new java ignored messages from the user text file and generate a dict structure to out of the new java ignored messages . This is achieved by function extract_message_to_dict . Next new java messages will be added to the original ignored java messages dict g_ok_java_messages . Again this is achieved by function update_message_dict ."}}
{"translation": {"code": "def write_java_message ( key , val , text_file ) : text_file . write ( key ) text_file . write ( '\\n' ) if ( len ( val [ 0 ] ) > 0 ) and ( len ( val ) >= 3 ) : for index in range ( len ( val [ 0 ] ) ) : text_file . write ( \"Java Message Type: \" ) text_file . write ( val [ 1 ] [ index ] ) text_file . write ( '\\n' ) text_file . write ( \"Java Message: \" ) for jmess in val [ 2 ] [ index ] : text_file . write ( jmess ) text_file . write ( '\\n' ) text_file . write ( '\\n \\n' )", "nl": "Loop through all java messages that are not associated with a unit test and write them into a log file ."}}
{"translation": {"code": "def load_java_messages_to_ignore ( ) : global g_ok_java_messages global g_java_message_pickle_filename if os . path . isfile ( g_java_message_pickle_filename ) : with open ( g_java_message_pickle_filename , 'rb' ) as tfile : g_ok_java_messages = pickle . load ( tfile ) else : g_ok_java_messages [ \"general\" ] = [ ]", "nl": "Load in pickle file that contains dict structure with bad java messages to ignore per unit test or for all cases . The ignored bad java info is stored in g_ok_java_messages dict ."}}
{"translation": {"code": "def usage ( ) : global g_script_name # name of the script being run. print ( \"\" ) print ( \"Usage:  \" + g_script_name + \" [...options...]\" ) print ( \"\" ) print ( \"     --help print out this help menu and show all the valid flags and inputs.\" ) print ( \"\" ) print ( \"    --inputfileadd filename where the new java messages to ignore are stored in.\" ) print ( \"\" ) print ( \"    --inputfilerm filename where the java messages are removed from the ignored list.\" ) print ( \"\" ) print ( \"    --loadjavamessage filename pickle file that stores the dict structure containing java messages to include.\" ) print ( \"\" ) print ( \"    --savejavamessage filename pickle file that saves the final dict structure after update.\" ) print ( \"\" ) print ( \"    --printjavamessage filename print java ignored java messages stored in pickle file filenam onto console and save into a text file.\" ) print ( \"\" ) sys . exit ( 1 )", "nl": "Illustrate what the various input flags are and the options should be ."}}
{"translation": {"code": "def parse_args ( argv ) : global g_new_messages_to_exclude global g_old_messages_to_remove global g_load_java_message_filename global g_save_java_message_filename global g_print_java_messages if len ( argv ) < 2 : # print out help menu if user did not enter any arguments. usage ( ) i = 1 while ( i < len ( argv ) ) : s = argv [ i ] if ( s == \"--inputfileadd\" ) : # input text file where new java messages are stored i += 1 if ( i > len ( argv ) ) : usage ( ) g_new_messages_to_exclude = argv [ i ] elif ( s == \"--inputfilerm\" ) : # input text file containing java messages to be removed from the ignored list i += 1 if ( i > len ( argv ) ) : usage ( ) g_old_messages_to_remove = argv [ i ] elif ( s == \"--loadjavamessage\" ) : # load previously saved java message pickle file from file other than i += 1 # the default one before performing update if i > len ( argv ) : usage ( ) g_load_java_message_filename = argv [ i ] elif ( s == \"--savejavamessage\" ) : # save updated java message in this file instead of default file i += 1 if ( i > len ( argv ) ) : usage ( ) g_save_java_message_filename = argv [ i ] elif ( s == '--printjavamessage' ) : # will print java message out to console and save in a text file i += 1 g_print_java_messages = True g_load_java_message_filename = argv [ i ] elif ( s == '--help' ) : # print help menu and exit usage ( ) else : unknown_arg ( s ) i += 1", "nl": "Parse user inputs and set the corresponing global variables to perform the necessary tasks ."}}
{"translation": {"code": "def print_dict ( ) : global g_ok_java_messages global g_java_messages_to_ignore_text_filename allKeys = sorted ( g_ok_java_messages . keys ( ) ) with open ( g_java_messages_to_ignore_text_filename , 'w' ) as ofile : for key in allKeys : for mess in g_ok_java_messages [ key ] : ofile . write ( 'KeyName: ' + key + '\\n' ) ofile . write ( 'IgnoredMessage: ' + mess + '\\n' ) print ( 'KeyName: ' , key ) print ( 'IgnoredMessage: ' , g_ok_java_messages [ key ] ) print ( '\\n' )", "nl": "Write the java ignored messages in g_ok_java_messages into a text file for humans to read ."}}
{"translation": {"code": "def save_dict ( ) : global g_ok_java_messages global g_save_java_message_filename global g_dict_changed if g_dict_changed : with open ( g_save_java_message_filename , 'wb' ) as ofile : pickle . dump ( g_ok_java_messages , ofile )", "nl": "Save the ignored java message dict stored in g_ok_java_messages into a pickle file for future use ."}}
{"translation": {"code": "def extract_message_to_dict ( filename ) : message_dict = { } if os . path . isfile ( filename ) : # open file to read in new exclude messages if it exists with open ( filename , 'r' ) as wfile : key = \"\" val = \"\" startMess = False while 1 : each_line = wfile . readline ( ) if not each_line : # reached EOF if startMess : add_to_dict ( val . strip ( ) , key , message_dict ) break # found a test name or general with values to follow if \"keyname\" in each_line . lower ( ) : # name of test file or the word \"general\" temp_strings = each_line . strip ( ) . split ( '=' ) if ( len ( temp_strings ) > 1 ) : # make sure the line is formatted sort of correctly if startMess : # this is the start of a new key/value pair add_to_dict ( val . strip ( ) , key , message_dict ) val = \"\" key = temp_strings [ 1 ] . strip ( ) startMess = False if ( len ( each_line ) > 1 ) and startMess : val += each_line if \"ignoredmessage\" in each_line . lower ( ) : startMess = True # start of a Java message. temp_mess = each_line . split ( '=' ) if ( len ( temp_mess ) > 1 ) : val = temp_mess [ 1 ] return message_dict", "nl": "Read in a text file that java messages to be ignored and generate a dictionary structure out of it with key and value pairs . The keys are test names and the values are lists of java message strings associated with that test name where we are either going to add to the existing java messages to ignore or remove them from g_ok_java_messages ."}}
{"translation": {"code": "def update_message_dict ( message_dict , action ) : global g_ok_java_messages allKeys = g_ok_java_messages . keys ( ) for key in message_dict . keys ( ) : if key in allKeys : # key already exists, just add to it for message in message_dict [ key ] : if action == 1 : if message not in g_ok_java_messages [ key ] : g_ok_java_messages [ key ] . append ( message ) if action == 2 : if message in g_ok_java_messages [ key ] : g_ok_java_messages [ key ] . remove ( message ) else : # new key here.  Can only add and cannot remove if action == 1 : g_ok_java_messages [ key ] = message_dict [ key ]", "nl": "Update the g_ok_java_messages dict structure by 1 . add the new java ignored messages stored in message_dict if action == 1 2 . remove the java ignored messages stired in message_dict if action == 2 ."}}
{"translation": {"code": "def remove_sandbox ( parent_dir , dir_name ) : if \"Rsandbox\" in dir_name : rsandbox_dir = os . path . join ( parent_dir , dir_name ) try : if sys . platform == \"win32\" : os . system ( r'C:/cygwin64/bin/rm.exe -r -f \"{0}\"' . format ( rsandbox_dir ) ) else : shutil . rmtree ( rsandbox_dir ) except OSError as e : print ( \"\" ) print ( \"ERROR: Removing RSandbox directory failed: \" + rsandbox_dir ) print ( \"       (errno {0}): {1}\" . format ( e . errno , e . strerror ) ) print ( \"\" ) sys . exit ( 1 )", "nl": "This function is written to remove sandbox directories if they exist under the parent_dir ."}}
{"translation": {"code": "def download_pojo ( model , path = \"\" , get_jar = True , jar_name = \"\" ) : assert_is_type ( model , ModelBase ) assert_is_type ( path , str ) assert_is_type ( get_jar , bool ) if not model . have_pojo : raise H2OValueError ( \"Export to POJO not supported\" ) if path == \"\" : java_code = api ( \"GET /3/Models.java/%s\" % model . model_id ) print ( java_code ) return None else : filename = api ( \"GET /3/Models.java/%s\" % model . model_id , save_to = path ) if get_jar : if jar_name == \"\" : api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) ) else : api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , jar_name ) ) return filename", "nl": "Download the POJO for this model to the directory specified by path ; if path is then dump to screen ."}}
{"translation": {"code": "def import_file ( path = None , destination_frame = None , parse = True , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , pattern = None , skipped_columns = None , custom_non_data_line_markers = None ) : coltype = U ( None , \"unknown\" , \"uuid\" , \"string\" , \"float\" , \"real\" , \"double\" , \"int\" , \"numeric\" , \"categorical\" , \"factor\" , \"enum\" , \"time\" ) natype = U ( str , [ str ] ) assert_is_type ( path , str , [ str ] ) assert_is_type ( pattern , str , None ) assert_is_type ( destination_frame , str , None ) assert_is_type ( parse , bool ) assert_is_type ( header , - 1 , 0 , 1 ) assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == 1 ) ) assert_is_type ( col_names , [ str ] , None ) assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) assert isinstance ( skipped_columns , ( type ( None ) , list ) ) , \"The skipped_columns should be an list of column names!\" check_frame_id ( destination_frame ) patharr = path if isinstance ( path , list ) else [ path ] if any ( os . path . split ( p ) [ 0 ] == \"~\" for p in patharr ) : raise H2OValueError ( \"Paths relative to a current user (~) are not valid in the server environment. \" \"Please use absolute paths if possible.\" ) if not parse : return lazy_import ( path , pattern ) else : return H2OFrame ( ) . _import_parse ( path , pattern , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns , custom_non_data_line_markers )", "nl": "Import a dataset that is already on the cluster ."}}
{"translation": {"code": "def get_grid ( grid_id ) : assert_is_type ( grid_id , str ) grid_json = api ( \"GET /99/Grids/%s\" % grid_id ) models = [ get_model ( key [ \"name\" ] ) for key in grid_json [ \"model_ids\" ] ] # get first model returned in list of models from grid search to get model class (binomial, multinomial, etc) first_model_json = api ( \"GET /3/Models/%s\" % grid_json [ \"model_ids\" ] [ 0 ] [ \"name\" ] ) [ \"models\" ] [ 0 ] gs = H2OGridSearch ( None , { } , grid_id ) gs . _resolve_grid ( grid_id , grid_json , first_model_json ) gs . models = models hyper_params = { param : set ( ) for param in gs . hyper_names } for param in gs . hyper_names : for model in models : if isinstance ( model . full_parameters [ param ] [ \"actual_value\" ] , list ) : hyper_params [ param ] . add ( model . full_parameters [ param ] [ \"actual_value\" ] [ 0 ] ) else : hyper_params [ param ] . add ( model . full_parameters [ param ] [ \"actual_value\" ] ) hyper_params = { str ( param ) : list ( vals ) for param , vals in hyper_params . items ( ) } gs . hyper_params = hyper_params gs . model = model . __class__ ( ) return gs", "nl": "Return the specified grid ."}}
{"translation": {"code": "def discover_modules ( self ) : modules = [ self . package_name ] # raw directory parsing for dirpath , dirnames , filenames in os . walk ( self . root_path ) : # Check directory names for packages root_uri = self . _path2uri ( os . path . join ( self . root_path , dirpath ) ) for dirname in dirnames [ : ] : # copy list - we modify inplace package_uri = '.' . join ( ( root_uri , dirname ) ) if ( self . _uri2path ( package_uri ) and self . _survives_exclude ( package_uri , 'package' ) ) : modules . append ( package_uri ) else : dirnames . remove ( dirname ) # Check filenames for modules for filename in filenames : module_name = filename [ : - 3 ] module_uri = '.' . join ( ( root_uri , module_name ) ) if ( self . _uri2path ( module_uri ) and self . _survives_exclude ( module_uri , 'module' ) ) : modules . append ( module_uri ) return sorted ( modules )", "nl": "Return module sequence discovered from self . package_name"}}
{"translation": {"code": "def write_api_docs ( self , outdir ) : if not os . path . exists ( outdir ) : os . mkdir ( outdir ) # compose list of modules modules = self . discover_modules ( ) self . write_modules_api ( modules , outdir )", "nl": "Generate API reST files ."}}
{"translation": {"code": "def write_index ( self , outdir , froot = 'gen' , relative_to = None ) : if self . written_modules is None : raise ValueError ( 'No modules written' ) # Get full filename path path = os . path . join ( outdir , froot + self . rst_extension ) # Path written into index is relative to rootpath if relative_to is not None : relpath = outdir . replace ( relative_to + os . path . sep , '' ) else : relpath = outdir idx = open ( path , 'wt' ) w = idx . write w ( '.. AUTO-GENERATED FILE -- DO NOT EDIT!\\n\\n' ) w ( '.. toctree::\\n\\n' ) for f in self . written_modules : w ( '   %s\\n' % os . path . join ( relpath , f ) ) idx . close ( )", "nl": "Make a reST API index file from written files"}}
{"translation": {"code": "def _uri2path ( self , uri ) : if uri == self . package_name : return os . path . join ( self . root_path , '__init__.py' ) path = uri . replace ( '.' , os . path . sep ) path = path . replace ( self . package_name + os . path . sep , '' ) path = os . path . join ( self . root_path , path ) # XXX maybe check for extensions as well? if os . path . exists ( path + '.py' ) : # file path += '.py' elif os . path . exists ( os . path . join ( path , '__init__.py' ) ) : path = os . path . join ( path , '__init__.py' ) else : return None return path", "nl": "Convert uri to absolute filepath"}}
{"translation": {"code": "def _path2uri ( self , dirpath ) : relpath = dirpath . replace ( self . root_path , self . package_name ) if relpath . startswith ( os . path . sep ) : relpath = relpath [ 1 : ] return relpath . replace ( os . path . sep , '.' )", "nl": "Convert directory path to uri"}}
{"translation": {"code": "def _parse_lines ( self , linesource ) : functions = [ ] classes = [ ] for line in linesource : if line . startswith ( 'def ' ) and line . count ( '(' ) : # exclude private stuff name = self . _get_object_name ( line ) if not name . startswith ( '_' ) : functions . append ( name ) elif line . startswith ( 'class ' ) : # exclude private stuff name = self . _get_object_name ( line ) if not name . startswith ( '_' ) : classes . append ( name ) else : pass functions . sort ( ) classes . sort ( ) return functions , classes", "nl": "Parse lines of text for functions and classes"}}
{"translation": {"code": "def generate_api_doc ( self , uri ) : # get the names of all classes and functions functions , classes = self . _parse_module ( uri ) if not len ( functions ) and not len ( classes ) : print 'WARNING: Empty -' , uri # dbg return '' # Make a shorter version of the uri that omits the package name for # titles  uri_short = re . sub ( r'^%s\\.' % self . package_name , '' , uri ) ad = '.. AUTO-GENERATED FILE -- DO NOT EDIT!\\n\\n' chap_title = uri_short ad += ( chap_title + '\\n' + self . rst_section_levels [ 1 ] * len ( chap_title ) + '\\n\\n' ) # Set the chapter title to read 'module' for all modules except for the # main packages if '.' in uri : title = 'Module: :mod:`' + uri_short + '`' else : title = ':mod:`' + uri_short + '`' ad += title + '\\n' + self . rst_section_levels [ 2 ] * len ( title ) if len ( classes ) : ad += '\\nInheritance diagram for ``%s``:\\n\\n' % uri ad += '.. inheritance-diagram:: %s \\n' % uri ad += '   :parts: 3\\n' ad += '\\n.. automodule:: ' + uri + '\\n' ad += '\\n.. currentmodule:: ' + uri + '\\n' multi_class = len ( classes ) > 1 multi_fx = len ( functions ) > 1 if multi_class : ad += '\\n' + 'Classes' + '\\n' + self . rst_section_levels [ 2 ] * 7 + '\\n' elif len ( classes ) and multi_fx : ad += '\\n' + 'Class' + '\\n' + self . rst_section_levels [ 2 ] * 5 + '\\n' for c in classes : ad += '\\n:class:`' + c + '`\\n' + self . rst_section_levels [ multi_class + 2 ] * ( len ( c ) + 9 ) + '\\n\\n' ad += '\\n.. autoclass:: ' + c + '\\n' # must NOT exclude from index to keep cross-refs working ad += '  :members:\\n' '  :undoc-members:\\n' '  :show-inheritance:\\n' '  :inherited-members:\\n' '\\n' '  .. automethod:: __init__\\n' if multi_fx : ad += '\\n' + 'Functions' + '\\n' + self . rst_section_levels [ 2 ] * 9 + '\\n\\n' elif len ( functions ) and multi_class : ad += '\\n' + 'Function' + '\\n' + self . rst_section_levels [ 2 ] * 8 + '\\n\\n' for f in functions : # must NOT exclude from index to keep cross-refs working ad += '\\n.. autofunction:: ' + uri + '.' + f + '\\n\\n' return ad", "nl": "Make autodoc documentation template string for a module"}}
{"translation": {"code": "def endpoint_groups ( ) : groups = defaultdict ( list ) for e in endpoints ( ) : groups [ e [ \"class_name\" ] ] . append ( e ) return groups", "nl": "Return endpoints grouped by the class which handles them ."}}
{"translation": {"code": "def translate_name ( name ) : parts = name . split ( \"_\" ) i = 0 while parts [ i ] == \"\" : parts [ i ] = \"_\" i += 1 parts [ i ] = parts [ i ] . lower ( ) for j in range ( i + 1 , len ( parts ) ) : parts [ j ] = parts [ j ] . capitalize ( ) i = len ( parts ) - 1 while parts [ i ] == \"\" : parts [ i ] = \"_\" i -= 1 return \"\" . join ( parts )", "nl": "Convert names with underscores into camelcase ."}}
{"translation": {"code": "def getGLMRegularizationPath ( model ) : x = h2o . api ( \"GET /3/GetGLMRegPath\" , data = { \"model\" : model . _model_json [ \"model_id\" ] [ \"name\" ] } ) ns = x . pop ( \"coefficient_names\" ) res = { \"lambdas\" : x [ \"lambdas\" ] , \"explained_deviance_train\" : x [ \"explained_deviance_train\" ] , \"explained_deviance_valid\" : x [ \"explained_deviance_valid\" ] , \"coefficients\" : [ dict ( zip ( ns , y ) ) for y in x [ \"coefficients\" ] ] , } if \"coefficients_std\" in x : res [ \"coefficients_std\" ] = [ dict ( zip ( ns , y ) ) for y in x [ \"coefficients_std\" ] ] return res", "nl": "Extract full regularization path explored during lambda search from glm model ."}}
{"translation": {"code": "def makeGLMModel ( model , coefs , threshold = .5 ) : model_json = h2o . api ( \"POST /3/MakeGLMModel\" , data = { \"model\" : model . _model_json [ \"model_id\" ] [ \"name\" ] , \"names\" : list ( coefs . keys ( ) ) , \"beta\" : list ( coefs . values ( ) ) , \"threshold\" : threshold } ) m = H2OGeneralizedLinearEstimator ( ) m . _resolve_model ( model_json [ \"model_id\" ] [ \"name\" ] , model_json ) return m", "nl": "Create a custom GLM model using the given coefficients ."}}
{"translation": {"code": "def import_sql_table ( connection_url , table , username , password , columns = None , optimize = True , fetch_mode = None ) : assert_is_type ( connection_url , str ) assert_is_type ( table , str ) assert_is_type ( username , str ) assert_is_type ( password , str ) assert_is_type ( columns , [ str ] , None ) assert_is_type ( optimize , bool ) assert_is_type ( fetch_mode , str , None ) p = { \"connection_url\" : connection_url , \"table\" : table , \"username\" : username , \"password\" : password , \"fetch_mode\" : fetch_mode } if columns : p [ \"columns\" ] = \", \" . join ( columns ) j = H2OJob ( api ( \"POST /99/ImportSQLTable\" , data = p ) , \"Import SQL Table\" ) . poll ( ) return get_frame ( j . dest_key )", "nl": "Import SQL table to H2OFrame in memory ."}}
{"translation": {"code": "def parse_raw ( setup , id = None , first_line_is_header = 0 ) : assert_is_type ( setup , dict ) assert_is_type ( id , str , None ) assert_is_type ( first_line_is_header , - 1 , 0 , 1 ) check_frame_id ( id ) if id : setup [ \"destination_frame\" ] = id if first_line_is_header != ( - 1 , 0 , 1 ) : if first_line_is_header not in ( - 1 , 0 , 1 ) : raise ValueError ( \"first_line_is_header should be -1, 0, or 1\" ) setup [ \"check_header\" ] = first_line_is_header fr = H2OFrame ( ) fr . _parse_raw ( setup ) return fr", "nl": "Parse dataset using the parse setup structure ."}}
{"translation": {"code": "def export_file ( frame , path , force = False , parts = 1 ) : assert_is_type ( frame , H2OFrame ) assert_is_type ( path , str ) assert_is_type ( force , bool ) assert_is_type ( parts , int ) H2OJob ( api ( \"POST /3/Frames/%s/export\" % ( frame . frame_id ) , data = { \"path\" : path , \"num_parts\" : parts , \"force\" : force } ) , \"Export File\" ) . poll ( )", "nl": "Export a given H2OFrame to a path on the machine this python session is currently connected to ."}}
{"translation": {"code": "def _prepare_data_payload ( data ) : if not data : return None res = { } for key , value in viewitems ( data ) : if value is None : continue # don't send args set to None so backend defaults take precedence if isinstance ( value , list ) : value = stringify_list ( value ) elif isinstance ( value , dict ) : if \"__meta\" in value and value [ \"__meta\" ] [ \"schema_name\" ] . endswith ( \"KeyV3\" ) : value = value [ \"name\" ] else : value = stringify_dict ( value ) else : value = str ( value ) res [ key ] = value return res", "nl": "Make a copy of the data object preparing it to be sent to the server ."}}
{"translation": {"code": "def close ( self ) : if self . _session_id : try : # If the server gone bad, we don't want to wait forever... if self . _timeout is None : self . _timeout = 1 self . request ( \"DELETE /4/sessions/%s\" % self . _session_id ) self . _print ( \"H2O session %s closed.\" % self . _session_id ) except Exception : pass self . _session_id = None self . _stage = - 1", "nl": "Close an existing connection ; once closed it cannot be used again ."}}
{"translation": {"code": "def _process_response ( response , save_to ) : status_code = response . status_code if status_code == 200 and save_to : if save_to . startswith ( \"~\" ) : save_to = os . path . expanduser ( save_to ) if os . path . isdir ( save_to ) or save_to . endswith ( os . path . sep ) : dirname = os . path . abspath ( save_to ) filename = H2OConnection . _find_file_name ( response ) else : dirname , filename = os . path . split ( os . path . abspath ( save_to ) ) fullname = os . path . join ( dirname , filename ) try : if not os . path . exists ( dirname ) : os . makedirs ( dirname ) with open ( fullname , \"wb\" ) as f : for chunk in response . iter_content ( chunk_size = 65536 ) : if chunk : # Empty chunks may occasionally happen f . write ( chunk ) except OSError as e : raise H2OValueError ( \"Cannot write to file %s: %s\" % ( fullname , e ) ) return fullname content_type = response . headers . get ( \"Content-Type\" , \"\" ) if \";\" in content_type : # Remove a \";charset=...\" part content_type = content_type [ : content_type . index ( \";\" ) ] # Auto-detect response type by its content-type. Decode JSON, all other responses pass as-is. if content_type == \"application/json\" : try : data = response . json ( object_pairs_hook = H2OResponse ) except ( JSONDecodeError , requests . exceptions . ContentDecodingError ) as e : raise H2OServerError ( \"Malformed JSON from server (%s):\\n%s\" % ( str ( e ) , response . text ) ) else : data = response . text # Success (200 = \"Ok\", 201 = \"Created\", 202 = \"Accepted\", 204 = \"No Content\") if status_code in { 200 , 201 , 202 , 204 } : return data # Client errors (400 = \"Bad Request\", 404 = \"Not Found\", 412 = \"Precondition Failed\") if status_code in { 400 , 404 , 412 } and isinstance ( data , ( H2OErrorV3 , H2OModelBuilderErrorV3 ) ) : raise H2OResponseError ( data ) # Server errors (notably 500 = \"Server Error\") # Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server # did not provide the correct status code. raise H2OServerError ( \"HTTP %d %s:\\n%r\" % ( status_code , response . reason , data ) )", "nl": "Given a response object prepare it to be handed over to the external caller ."}}
{"translation": {"code": "def get_human_readable_bytes ( size ) : if size == 0 : return \"0\" if size is None : return \"\" assert_is_type ( size , int ) assert size >= 0 , \"`size` cannot be negative, got %d\" % size suffixes = \"PTGMk\" maxl = len ( suffixes ) for i in range ( maxl + 1 ) : shift = ( maxl - i ) * 10 if size >> shift == 0 : continue ndigits = 0 for nd in [ 3 , 2 , 1 ] : if size >> ( shift + 12 - nd * 3 ) == 0 : ndigits = nd break if ndigits == 0 or size == ( size >> shift ) << shift : rounded_val = str ( size >> shift ) else : rounded_val = \"%.*f\" % ( ndigits , size / ( 1 << shift ) ) return \"%s %sb\" % ( rounded_val , suffixes [ i ] if i < maxl else \"\" )", "nl": "Convert given number of bytes into a human readable representation i . e . add prefix such as kb Mb Gb etc . The size argument must be a non - negative integer ."}}
{"translation": {"code": "def _log_start_transaction ( self , endpoint , data , json , files , params ) : # TODO: add information about the caller, i.e. which module + line of code called the .request() method #       This can be done by fetching current traceback and then traversing it until we find the request function self . _requests_counter += 1 if not self . _is_logging : return msg = \"\\n---- %d --------------------------------------------------------\\n\" % self . _requests_counter msg += \"[%s] %s\\n\" % ( time . strftime ( \"%H:%M:%S\" ) , endpoint ) if params is not None : msg += \"     params: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( params ) ) if data is not None : msg += \"     body: {%s}\\n\" % \", \" . join ( \"%s:%s\" % item for item in viewitems ( data ) ) if json is not None : import json as j msg += \"     json: %s\\n\" % j . dumps ( json ) if files is not None : msg += \"     file: %s\\n\" % \", \" . join ( f . name for f in viewvalues ( files ) ) self . _log_message ( msg + \"\\n\" )", "nl": "Log the beginning of an API request ."}}
{"translation": {"code": "def connect ( server = None , url = None , ip = None , port = None , https = None , verify_ssl_certificates = None , auth = None , proxy = None , cookies = None , verbose = True , config = None ) : global h2oconn if config : if \"connect_params\" in config : h2oconn = _connect_with_conf ( config [ \"connect_params\" ] ) else : h2oconn = _connect_with_conf ( config ) else : h2oconn = H2OConnection . open ( server = server , url = url , ip = ip , port = port , https = https , auth = auth , verify_ssl_certificates = verify_ssl_certificates , proxy = proxy , cookies = cookies , verbose = verbose ) if verbose : h2oconn . cluster . show_status ( ) return h2oconn", "nl": "Connect to an existing H2O server remote or local ."}}
{"translation": {"code": "def version_check ( ) : from . __init__ import __version__ as ver_pkg ci = h2oconn . cluster if not ci : raise H2OConnectionError ( \"Connection not initialized. Did you run h2o.connect()?\" ) ver_h2o = ci . version if ver_pkg == \"SUBST_PROJECT_VERSION\" : ver_pkg = \"UNKNOWN\" if str ( ver_h2o ) != str ( ver_pkg ) : branch_name_h2o = ci . branch_name build_number_h2o = ci . build_number if build_number_h2o is None or build_number_h2o == \"unknown\" : raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Upgrade H2O and h2o-Python to latest stable version - \" \"http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\" \"\" . format ( ver_h2o , ver_pkg ) ) elif build_number_h2o == \"99999\" : raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"This is a developer build, please contact your developer.\" \"\" . format ( ver_h2o , ver_pkg ) ) else : raise H2OConnectionError ( \"Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. \" \"Install the matching h2o-Python version from - \" \"http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html.\" \"\" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) # Check age of the install if ci . build_too_old : print ( \"Warning: Your H2O cluster version is too old ({})! Please download and install the latest \" \"version from http://h2o.ai/download/\" . format ( ci . build_age ) )", "nl": "Used to verify that h2o - python module and the H2O server are compatible with each other ."}}
{"translation": {"code": "def session_id ( self ) : if self . _session_id is None : req = self . request ( \"POST /4/sessions\" ) self . _session_id = req . get ( \"session_key\" ) or req . get ( \"session_id\" ) return CallableString ( self . _session_id )", "nl": "Return the session id of the current connection ."}}
{"translation": {"code": "def _prepare_file_payload ( filename ) : if not filename : return None absfilename = os . path . abspath ( filename ) if not os . path . exists ( absfilename ) : raise H2OValueError ( \"File %s does not exist\" % filename , skip_frames = 1 ) return { os . path . basename ( absfilename ) : open ( absfilename , \"rb\" ) }", "nl": "Prepare filename to be sent to the server ."}}
{"translation": {"code": "def _log_end_transaction ( self , start_time , response ) : if not self . _is_logging : return elapsed_time = int ( ( time . time ( ) - start_time ) * 1000 ) msg = \"<<< HTTP %d %s   (%d ms)\\n\" % ( response . status_code , response . reason , elapsed_time ) if \"Content-Type\" in response . headers : msg += \"    Content-Type: %s\\n\" % response . headers [ \"Content-Type\" ] msg += response . text self . _log_message ( msg + \"\\n\\n\" )", "nl": "Log response from an API request ."}}
{"translation": {"code": "def start_logging ( self , dest = None ) : assert_is_type ( dest , None , str , type ( sys . stdout ) ) if dest is None : dest = os . path . join ( tempfile . mkdtemp ( ) , \"h2o-connection.log\" ) self . _print ( \"Now logging all API requests to file %r\" % dest ) self . _is_logging = True self . _logging_dest = dest", "nl": "Start logging all API requests to the provided destination ."}}
{"translation": {"code": "def _log_message ( self , msg ) : if is_type ( self . _logging_dest , str ) : with open ( self . _logging_dest , \"at\" , encoding = \"utf-8\" ) as f : f . write ( msg ) else : self . _logging_dest . write ( msg )", "nl": "Log the message msg to the destination self . _logging_dest ."}}
{"translation": {"code": "def poll ( self , verbose_model_scoring_history = False ) : try : hidden = not H2OJob . __PROGRESS_BAR__ pb = ProgressBar ( title = self . _job_type + \" progress\" , hidden = hidden ) if verbose_model_scoring_history : pb . execute ( self . _refresh_job_status , print_verbose_info = lambda x : self . _print_verbose_info ( ) if int ( x * 10 ) % 5 == 0 else \" \" ) else : pb . execute ( self . _refresh_job_status ) except StopIteration as e : if str ( e ) == \"cancelled\" : h2o . api ( \"POST /3/Jobs/%s/cancel\" % self . job_key ) self . status = \"CANCELLED\" # Potentially we may want to re-raise the exception here assert self . status in { \"DONE\" , \"CANCELLED\" , \"FAILED\" } or self . _poll_count <= 0 , \"Polling finished while the job has status %s\" % self . status if self . warnings : for w in self . warnings : warnings . warn ( w ) # check if failed... and politely print relevant message if self . status == \"CANCELLED\" : raise H2OJobCancelled ( \"Job<%s> was cancelled by the user.\" % self . job_key ) if self . status == \"FAILED\" : if ( isinstance ( self . job , dict ) ) and ( \"stacktrace\" in list ( self . job ) ) : raise EnvironmentError ( \"Job with key {} failed with an exception: {}\\nstacktrace: \" \"\\n{}\" . format ( self . job_key , self . exception , self . job [ \"stacktrace\" ] ) ) else : raise EnvironmentError ( \"Job with key %s failed with an exception: %s\" % ( self . job_key , self . exception ) ) return self", "nl": "Wait until the job finishes ."}}
{"translation": {"code": "def csv_dict_writer ( f , fieldnames , * * kwargs ) : import csv if \"delimiter\" in kwargs : kwargs [ \"delimiter\" ] = str ( kwargs [ \"delimiter\" ] ) return csv . DictWriter ( f , fieldnames , * * kwargs )", "nl": "Equivalent of csv . DictWriter but allows delimiter to be a unicode string on Py2 ."}}
{"translation": {"code": "def deeplearning ( interactive = True , echo = True , testing = False ) : def demo_body ( go ) : \"\"\"\n        Demo of H2O's Deep Learning model.\n\n        This demo uploads a dataset to h2o, parses it, and shows a description.\n        Then it divides the dataset into training and test sets, builds a GLM\n        from the training set, and makes predictions for the test set.\n        Finally, default performance metrics are displayed.\n        \"\"\" go ( ) # Connect to H2O h2o . init ( ) go ( ) # Upload the prostate dataset that comes included in the h2o python package prostate = h2o . load_dataset ( \"prostate\" ) go ( ) # Print a description of the prostate data prostate . describe ( ) go ( ) # Randomly split the dataset into ~70/30, training/test sets train , test = prostate . split_frame ( ratios = [ 0.70 ] ) go ( ) # Convert the response columns to factors (for binary classification problems) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) # Build a (classification) GLM from h2o . estimators import H2ODeepLearningEstimator prostate_dl = H2ODeepLearningEstimator ( activation = \"Tanh\" , hidden = [ 10 , 10 , 10 ] , epochs = 10000 ) prostate_dl . train ( x = list ( set ( prostate . col_names ) - { \"ID\" , \"CAPSULE\" } ) , y = \"CAPSULE\" , training_frame = train ) go ( ) # Show the model prostate_dl . show ( ) go ( ) # Predict on the test set and show the first ten predictions predictions = prostate_dl . predict ( test ) predictions . show ( ) go ( ) # Show default performance metrics performance = prostate_dl . model_performance ( test ) performance . show ( ) # Execute: _run_demo ( demo_body , interactive , echo , testing )", "nl": "Deep Learning model demo ."}}
{"translation": {"code": "def glm ( interactive = True , echo = True , testing = False ) : def demo_body ( go ) : \"\"\"\n        Demo of H2O's Generalized Linear Estimator.\n\n        This demo uploads a dataset to h2o, parses it, and shows a description.\n        Then it divides the dataset into training and test sets, builds a GLM\n        from the training set, and makes predictions for the test set.\n        Finally, default performance metrics are displayed.\n        \"\"\" go ( ) # Connect to H2O h2o . init ( ) go ( ) # Upload the prostate dataset that comes included in the h2o python package prostate = h2o . load_dataset ( \"prostate\" ) go ( ) # Print a description of the prostate data prostate . describe ( ) go ( ) # Randomly split the dataset into ~70/30, training/test sets train , test = prostate . split_frame ( ratios = [ 0.70 ] ) go ( ) # Convert the response columns to factors (for binary classification problems) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) # Build a (classification) GLM from h2o . estimators import H2OGeneralizedLinearEstimator prostate_glm = H2OGeneralizedLinearEstimator ( family = \"binomial\" , alpha = [ 0.5 ] ) prostate_glm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training_frame = train ) go ( ) # Show the model prostate_glm . show ( ) go ( ) # Predict on the test set and show the first ten predictions predictions = prostate_glm . predict ( test ) predictions . show ( ) go ( ) # Show default performance metrics performance = prostate_glm . model_performance ( test ) performance . show ( ) # Execute: _run_demo ( demo_body , interactive , echo , testing )", "nl": "GLM model demo ."}}
{"translation": {"code": "def demo ( funcname , interactive = True , echo = True , test = False ) : import h2o . demos as h2odemo assert_is_type ( funcname , str ) assert_is_type ( interactive , bool ) assert_is_type ( echo , bool ) assert_is_type ( test , bool ) demo_function = getattr ( h2odemo , funcname , None ) if demo_function and type ( demo_function ) is type ( demo ) : demo_function ( interactive , echo , test ) else : print ( \"Demo for %s is not available.\" % funcname )", "nl": "H2O built - in demo facility ."}}
{"translation": {"code": "def gbm ( interactive = True , echo = True , testing = False ) : def demo_body ( go ) : \"\"\"\n        Demo of H2O's Gradient Boosting estimator.\n\n        This demo uploads a dataset to h2o, parses it, and shows a description.\n        Then it divides the dataset into training and test sets, builds a GLM\n        from the training set, and makes predictions for the test set.\n        Finally, default performance metrics are displayed.\n        \"\"\" go ( ) # Connect to H2O h2o . init ( ) go ( ) # Upload the prostate dataset that comes included in the h2o python package prostate = h2o . load_dataset ( \"prostate\" ) go ( ) # Print a description of the prostate data prostate . describe ( ) go ( ) # Randomly split the dataset into ~70/30, training/test sets train , test = prostate . split_frame ( ratios = [ 0.70 ] ) go ( ) # Convert the response columns to factors (for binary classification problems) train [ \"CAPSULE\" ] = train [ \"CAPSULE\" ] . asfactor ( ) test [ \"CAPSULE\" ] = test [ \"CAPSULE\" ] . asfactor ( ) go ( ) # Build a (classification) GLM from h2o . estimators import H2OGradientBoostingEstimator prostate_gbm = H2OGradientBoostingEstimator ( distribution = \"bernoulli\" , ntrees = 10 , max_depth = 8 , min_rows = 10 , learn_rate = 0.2 ) prostate_gbm . train ( x = [ \"AGE\" , \"RACE\" , \"PSA\" , \"VOL\" , \"GLEASON\" ] , y = \"CAPSULE\" , training_frame = train ) go ( ) # Show the model prostate_gbm . show ( ) go ( ) # Predict on the test set and show the first ten predictions predictions = prostate_gbm . predict ( test ) predictions . show ( ) go ( ) # Fetch a tree, print number of tree nodes, show root node description from h2o . tree import H2OTree , H2ONode tree = H2OTree ( prostate_gbm , 0 , \"0\" ) len ( tree ) tree . left_children tree . right_children tree . root_node . show ( ) go ( ) # Show default performance metrics performance = prostate_gbm . model_performance ( test ) performance . show ( ) # Execute: _run_demo ( demo_body , interactive , echo , testing )", "nl": "GBM model demo ."}}
{"translation": {"code": "def load_dataset ( relative_path ) : assert_is_type ( relative_path , str ) h2o_dir = os . path . split ( __file__ ) [ 0 ] for possible_file in [ os . path . join ( h2o_dir , relative_path ) , os . path . join ( h2o_dir , \"h2o_data\" , relative_path ) , os . path . join ( h2o_dir , \"h2o_data\" , relative_path + \".csv\" ) ] : if os . path . exists ( possible_file ) : return upload_file ( possible_file ) # File not found -- raise an error! raise H2OValueError ( \"Data file %s cannot be found\" % relative_path )", "nl": "Imports a data file within the h2o_data folder ."}}
{"translation": {"code": "def deprecated ( message ) : from traceback import extract_stack assert message , \"`message` argument in @deprecated is required.\" def deprecated_decorator ( fun ) : def decorator_invisible ( * args , * * kwargs ) : stack = extract_stack ( ) assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == \"decorator_invisible\" , \"Got confusing stack... %r\" % stack print ( \"[WARNING] in %s line %d:\" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) print ( \"    >>> %s\" % ( stack [ - 2 ] [ 3 ] or \"????\" ) ) print ( \"        ^^^^ %s\" % message ) return fun ( * args , * * kwargs ) decorator_invisible . __doc__ = message decorator_invisible . __name__ = fun . __name__ decorator_invisible . __module__ = fun . __module__ decorator_invisible . __deprecated__ = True return decorator_invisible return deprecated_decorator", "nl": "The decorator to mark deprecated functions ."}}
{"translation": {"code": "def show ( self ) : hyper_combos = itertools . product ( * list ( self . hyper_params . values ( ) ) ) if not self . models : c_values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper_combos ) ] print ( H2OTwoDimTable ( col_header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ] , table_header = 'Grid Search of Model ' + self . model . __class__ . __name__ , cell_values = c_values ) ) else : print ( self . sorted_metric_table ( ) )", "nl": "Print models sorted by metric ."}}
{"translation": {"code": "def get_grid ( self , sort_by = None , decreasing = None ) : if sort_by is None and decreasing is None : return self grid_json = h2o . api ( \"GET /99/Grids/%s\" % self . _id , data = { \"sort_by\" : sort_by , \"decreasing\" : decreasing } ) grid = H2OGridSearch ( self . model , self . hyper_params , self . _id ) grid . models = [ h2o . get_model ( key [ 'name' ] ) for key in grid_json [ 'model_ids' ] ] # reordered first_model_json = h2o . api ( \"GET /99/Models/%s\" % grid_json [ 'model_ids' ] [ 0 ] [ 'name' ] ) [ 'models' ] [ 0 ] model_class = H2OGridSearch . _metrics_class ( first_model_json ) m = model_class ( ) m . _id = self . _id m . _grid_json = grid_json # m._metrics_class = metrics_class m . _parms = grid . _parms H2OEstimator . mixin ( grid , model_class ) grid . __dict__ . update ( m . __dict__ . copy ( ) ) return grid", "nl": "Retrieve an H2OGridSearch instance ."}}
{"translation": {"code": "def get_hyperparams_dict ( self , id , display = True ) : idx = id if is_type ( id , int ) else self . model_ids . index ( id ) model = self [ idx ] model_params = dict ( ) # if cross-validation is turned on, parameters in one of the fold model actual contains the max_runtime_secs # parameter and not the main model that is returned. if model . _is_xvalidated : model = h2o . get_model ( model . _xval_keys [ 0 ] ) for param_name in self . hyper_names : model_params [ param_name ] = model . params [ param_name ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ param_name ] [ 'actual' ] , list ) else model . params [ param_name ] [ 'actual' ] if display : print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) return model_params", "nl": "Derived and returned the model parameters used to train the particular grid search model ."}}
{"translation": {"code": "def get_hyperparams ( self , id , display = True ) : idx = id if is_type ( id , int ) else self . model_ids . index ( id ) model = self [ idx ] # if cross-validation is turned on, parameters in one of the fold model actuall contains the max_runtime_secs # parameter and not the main model that is returned. if model . _is_xvalidated : model = h2o . get_model ( model . _xval_keys [ 0 ] ) res = [ model . params [ h ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ h ] [ 'actual' ] , list ) else model . params [ h ] [ 'actual' ] for h in self . hyper_params ] if display : print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) return res", "nl": "Get the hyperparameters of a model explored by grid search ."}}
{"translation": {"code": "def summary ( self , header = True ) : table = [ ] for model in self . models : model_summary = model . _model_json [ \"output\" ] [ \"model_summary\" ] r_values = list ( model_summary . cell_values [ 0 ] ) r_values [ 0 ] = model . model_id table . append ( r_values ) # if h2o.can_use_pandas(): #  import pandas #  pandas.options.display.max_rows = 20 #  print pandas.DataFrame(table,columns=self.col_header) #  return print ( ) if header : print ( 'Grid Summary:' ) print ( ) H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ 1 : ] , numalign = \"left\" , stralign = \"left\" )", "nl": "Print a detailed summary of the explored models ."}}
{"translation": {"code": "def deepfeatures ( self , test_data , layer ) : return { model . model_id : model . deepfeatures ( test_data , layer ) for model in self . models }", "nl": "Obtain a hidden layer s details on a dataset ."}}
{"translation": {"code": "def F1 ( self , thresholds = None , train = False , valid = False , xval = False ) : return { model . model_id : model . F1 ( thresholds , train , valid , xval ) for model in self . models }", "nl": "Get the F1 values for a set of thresholds for the models explored ."}}
{"translation": {"code": "def join ( self ) : self . _future = False self . _job . poll ( ) self . _job = None", "nl": "Wait until grid finishes computing ."}}
{"translation": {"code": "def fit ( self , X , y = None , * * params ) : if isinstance ( self . parms [ \"center\" ] , ( tuple , list ) ) : self . _means = self . parms [ \"center\" ] if isinstance ( self . parms [ \"scale\" ] , ( tuple , list ) ) : self . _stds = self . parms [ \"scale\" ] if self . means is None and self . parms [ \"center\" ] : self . _means = X . mean ( return_frame = True ) . getrow ( ) else : self . _means = False if self . stds is None and self . parms [ \"scale\" ] : self . _stds = X . sd ( ) else : self . _stds = False return self", "nl": "Fit this object by computing the means and standard deviations used by the transform method ."}}
{"translation": {"code": "def transform ( self , X , y = None , * * params ) : return X . scale ( self . means , self . stds )", "nl": "Scale an H2OFrame with the fitted means and standard deviations ."}}
{"translation": {"code": "def confusion_matrix ( self , metrics = None , thresholds = None ) : # make lists out of metrics and thresholds arguments if metrics is None and thresholds is None : metrics = [ 'f1' ] if isinstance ( metrics , list ) : metrics_list = metrics elif metrics is None : metrics_list = [ ] else : metrics_list = [ metrics ] if isinstance ( thresholds , list ) : thresholds_list = thresholds elif thresholds is None : thresholds_list = [ ] else : thresholds_list = [ thresholds ] # error check the metrics_list and thresholds_list assert_is_type ( thresholds_list , [ numeric ] ) assert_satisfies ( thresholds_list , all ( 0 <= t <= 1 for t in thresholds_list ) ) if not all ( m . lower ( ) in H2OBinomialModelMetrics . max_metrics for m in metrics_list ) : raise ValueError ( \"The only allowable metrics are {}\" , ', ' . join ( H2OBinomialModelMetrics . max_metrics ) ) # make one big list that combines the thresholds and metric-thresholds metrics_thresholds = [ self . find_threshold_by_max_metric ( m ) for m in metrics_list ] for mt in metrics_thresholds : thresholds_list . append ( mt ) first_metrics_thresholds_offset = len ( thresholds_list ) - len ( metrics_thresholds ) thresh2d = self . _metric_json [ 'thresholds_and_metric_scores' ] actual_thresholds = [ float ( e [ 0 ] ) for i , e in enumerate ( thresh2d . cell_values ) ] cms = [ ] for i , t in enumerate ( thresholds_list ) : idx = self . find_idx_by_threshold ( t ) row = thresh2d . cell_values [ idx ] tns = row [ 11 ] fns = row [ 12 ] fps = row [ 13 ] tps = row [ 14 ] p = tps + fns n = tns + fps c0 = n - fps c1 = p - tps if t in metrics_thresholds : m = metrics_list [ i - first_metrics_thresholds_offset ] table_header = \"Confusion Matrix (Act/Pred) for max {} @ threshold = {}\" . format ( m , actual_thresholds [ idx ] ) else : table_header = \"Confusion Matrix (Act/Pred) @ threshold = {}\" . format ( actual_thresholds [ idx ] ) cms . append ( ConfusionMatrix ( cm = [ [ c0 , fps ] , [ c1 , tps ] ] , domains = self . _metric_json [ 'domain' ] , table_header = table_header ) ) if len ( cms ) == 1 : return cms [ 0 ] else : return cms", "nl": "Get the confusion matrix for the specified metric"}}
{"translation": {"code": "def plot ( self , type = \"roc\" , server = False ) : # TODO: add more types (i.e. cutoffs) assert_is_type ( type , \"roc\" ) # check for matplotlib. exit if absent. try : imp . find_module ( 'matplotlib' ) import matplotlib if server : matplotlib . use ( 'Agg' , warn = False ) import matplotlib . pyplot as plt except ImportError : print ( \"matplotlib is required for this function!\" ) return if type == \"roc\" : plt . xlabel ( 'False Positive Rate (FPR)' ) plt . ylabel ( 'True Positive Rate (TPR)' ) plt . title ( 'ROC Curve' ) plt . text ( 0.5 , 0.5 , r'AUC={0:.4f}' . format ( self . _metric_json [ \"AUC\" ] ) ) plt . plot ( self . fprs , self . tprs , 'b--' ) plt . axis ( [ 0 , 1 , 0 , 1 ] ) if not server : plt . show ( )", "nl": "Produce the desired metric plot ."}}
{"translation": {"code": "def to_list ( self ) : return [ [ int ( self . table . cell_values [ 0 ] [ 1 ] ) , int ( self . table . cell_values [ 0 ] [ 2 ] ) ] , [ int ( self . table . cell_values [ 1 ] [ 1 ] ) , int ( self . table . cell_values [ 1 ] [ 2 ] ) ] ]", "nl": "Convert this confusion matrix into a 2x2 plain list of values ."}}
{"translation": {"code": "def _tabulate ( self , tablefmt = \"simple\" , rollups = False , rows = 10 ) : if not self . is_valid ( ) : self . fill ( rows = rows ) # Pretty print cached data d = collections . OrderedDict ( ) # If also printing the rollup stats, build a full row-header if rollups : col = next ( iter ( viewvalues ( self . _data ) ) ) # Get a sample column lrows = len ( col [ 'data' ] ) # Cached rows being displayed d [ \"\" ] = [ \"type\" , \"mins\" , \"mean\" , \"maxs\" , \"sigma\" , \"zeros\" , \"missing\" ] + list ( map ( str , range ( lrows ) ) ) # For all columns... for k , v in viewitems ( self . _data ) : x = v [ 'data' ] # Data to display t = v [ \"type\" ] # Column type if t == \"enum\" : domain = v [ 'domain' ] # Map to cat strings as needed x = [ \"\" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] elif t == \"time\" : x = [ \"\" if math . isnan ( z ) else time . strftime ( \"%Y-%m-%d %H:%M:%S\" , time . gmtime ( z / 1000 ) ) for z in x ] if rollups : # Rollups, if requested mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ \"type\" ] != \"enum\" else None maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ \"type\" ] != \"enum\" else None #Cross check type with mean and sigma. Set to None if of type enum. if v [ 'type' ] == \"enum\" : v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x d [ k ] = x # Insert into ordered-dict return tabulate . tabulate ( d , headers = \"keys\" , tablefmt = tablefmt )", "nl": "Pretty tabulated string of all the cached data and column names"}}
{"translation": {"code": "def hit_ratio_table ( self , train = False , valid = False , xval = False ) : tm = ModelBase . _get_metrics ( self , train , valid , xval ) m = { } for k , v in zip ( list ( tm . keys ( ) ) , list ( tm . values ( ) ) ) : m [ k ] = None if v is None else v . hit_ratio_table ( ) return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m", "nl": "Retrieve the Hit Ratios ."}}
{"translation": {"code": "def as_data_frame ( self ) : if can_use_pandas ( ) : import pandas pandas . options . display . max_colwidth = 70 return pandas . DataFrame ( self . _cell_values , columns = self . _col_header ) return self", "nl": "Convert to a python data frame ."}}
{"translation": {"code": "def cross_validation_models ( self ) : cvmodels = self . _model_json [ \"output\" ] [ \"cross_validation_models\" ] if cvmodels is None : return None m = [ ] for p in cvmodels : m . append ( h2o . get_model ( p [ \"name\" ] ) ) return m", "nl": "Obtain a list of cross - validation models ."}}
{"translation": {"code": "def _check_targets ( y_actual , y_predicted ) : if len ( y_actual ) != len ( y_predicted ) : raise ValueError ( \"Row mismatch: [{},{}]\" . format ( len ( y_actual ) , len ( y_predicted ) ) )", "nl": "Check that y_actual and y_predicted have the same length ."}}
{"translation": {"code": "def download_pojo ( self , path = \"\" , get_genmodel_jar = False , genmodel_name = \"\" ) : assert_is_type ( path , str ) assert_is_type ( get_genmodel_jar , bool ) path = path . rstrip ( \"/\" ) return h2o . download_pojo ( self , path , get_jar = get_genmodel_jar , jar_name = genmodel_name )", "nl": "Download the POJO for this model to the directory specified by path ."}}
{"translation": {"code": "def coef ( self ) : tbl = self . _model_json [ \"output\" ] [ \"coefficients_table\" ] if tbl is None : return None return { name : coef for name , coef in zip ( tbl [ \"names\" ] , tbl [ \"coefficients\" ] ) }", "nl": "Return the coefficients which can be applied to the non - standardized data ."}}
{"translation": {"code": "def residual_degrees_of_freedom ( self , train = False , valid = False , xval = False ) : if xval : raise H2OValueError ( \"Cross-validation metrics are not available.\" ) if not train and not valid : train = True if train and valid : train = True if train : return self . _model_json [ \"output\" ] [ \"training_metrics\" ] . residual_degrees_of_freedom ( ) else : return self . _model_json [ \"output\" ] [ \"validation_metrics\" ] . residual_degrees_of_freedom ( )", "nl": "Retreive the residual degress of freedom if this model has the attribute or None otherwise ."}}
{"translation": {"code": "def varimp ( self , use_pandas = False ) : model = self . _model_json [ \"output\" ] if self . algo == 'glm' or \"variable_importances\" in list ( model . keys ( ) ) and model [ \"variable_importances\" ] : if self . algo == 'glm' : tempvals = model [ \"standardized_coefficient_magnitudes\" ] . cell_values maxVal = 0 sum = 0 for item in tempvals : sum = sum + item [ 1 ] if item [ 1 ] > maxVal : maxVal = item [ 1 ] vals = [ ] for item in tempvals : tempT = ( item [ 0 ] , item [ 1 ] , item [ 1 ] / maxVal , item [ 1 ] / sum ) vals . append ( tempT ) header = [ \"variable\" , \"relative_importance\" , \"scaled_importance\" , \"percentage\" ] else : vals = model [ \"variable_importances\" ] . cell_values header = model [ \"variable_importances\" ] . col_header if use_pandas and can_use_pandas ( ) : import pandas return pandas . DataFrame ( vals , columns = header ) else : return vals else : print ( \"Warning: This model doesn't have variable importances\" )", "nl": "Pretty print the variable importances or return them in a list ."}}
{"translation": {"code": "def show ( self ) : if self . _future : self . _job . poll_once ( ) return if self . _model_json is None : print ( \"No model trained yet\" ) return if self . model_id is None : print ( \"This H2OEstimator has been removed.\" ) return model = self . _model_json [ \"output\" ] print ( \"Model Details\" ) print ( \"=============\" ) print ( self . __class__ . __name__ , \": \" , self . _model_json [ \"algo_full_name\" ] ) print ( \"Model Key: \" , self . _id ) self . summary ( ) print ( ) # training metrics tm = model [ \"training_metrics\" ] if tm : tm . show ( ) vm = model [ \"validation_metrics\" ] if vm : vm . show ( ) xm = model [ \"cross_validation_metrics\" ] if xm : xm . show ( ) xms = model [ \"cross_validation_metrics_summary\" ] if xms : xms . show ( ) if \"scoring_history\" in model and model [ \"scoring_history\" ] : model [ \"scoring_history\" ] . show ( ) if \"variable_importances\" in model and model [ \"variable_importances\" ] : model [ \"variable_importances\" ] . show ( )", "nl": "Print innards of model without regards to type ."}}
{"translation": {"code": "def scoring_history ( self ) : model = self . _model_json [ \"output\" ] if \"scoring_history\" in model and model [ \"scoring_history\" ] is not None : return model [ \"scoring_history\" ] . as_data_frame ( ) print ( \"No score history for this model\" )", "nl": "Retrieve Model Score History ."}}
{"translation": {"code": "def deepfeatures ( self , test_data , layer ) : if test_data is None : raise ValueError ( \"Must specify test data\" ) if str ( layer ) . isdigit ( ) : j = H2OJob ( h2o . api ( \"POST /4/Predictions/models/%s/frames/%s\" % ( self . _id , test_data . frame_id ) , data = { \"deep_features_hidden_layer\" : layer } ) , \"deepfeatures\" ) else : j = H2OJob ( h2o . api ( \"POST /4/Predictions/models/%s/frames/%s\" % ( self . _id , test_data . frame_id ) , data = { \"deep_features_hidden_layer_name\" : layer } ) , \"deepfeatures\" ) j . poll ( ) return h2o . get_frame ( j . dest_key )", "nl": "Return hidden layer details ."}}
{"translation": {"code": "def fit ( self , fr ) : assert_is_type ( fr , H2OFrame ) steps = \"[%s]\" % \",\" . join ( quoted ( step [ 1 ] . to_rest ( step [ 0 ] ) . replace ( '\"' , \"'\" ) ) for step in self . steps ) j = h2o . api ( \"POST /99/Assembly\" , data = { \"steps\" : steps , \"frame\" : fr . frame_id } ) self . id = j [ \"assembly\" ] [ \"name\" ] return H2OFrame . get_frame ( j [ \"result\" ] [ \"name\" ] )", "nl": "To perform the munging operations on a frame specified in steps on the frame fr ."}}
{"translation": {"code": "def to_pojo ( self , pojo_name = \"\" , path = \"\" , get_jar = True ) : assert_is_type ( pojo_name , str ) assert_is_type ( path , str ) assert_is_type ( get_jar , bool ) if pojo_name == \"\" : pojo_name = \"AssemblyPOJO_\" + str ( uuid . uuid4 ( ) ) java = h2o . api ( \"GET /99/Assembly.java/%s/%s\" % ( self . id , pojo_name ) ) file_path = path + \"/\" + pojo_name + \".java\" if path == \"\" : print ( java ) else : with open ( file_path , 'w' , encoding = \"utf-8\" ) as f : f . write ( java ) # this had better be utf-8 ? if get_jar and path != \"\" : h2o . api ( \"GET /3/h2o-genmodel.jar\" , save_to = os . path . join ( path , \"h2o-genmodel.jar\" ) )", "nl": "Convert the munging operations performed on H2OFrame into a POJO ."}}
{"translation": {"code": "def roc ( self , train = False , valid = False , xval = False ) : tm = ModelBase . _get_metrics ( self , train , valid , xval ) m = { } for k , v in viewitems ( tm ) : if v is not None : m [ k ] = ( v . fprs , v . tprs ) return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m", "nl": "Return the coordinates of the ROC curve for a given set of data ."}}
{"translation": {"code": "def inverse_transform ( self , X , y = None , * * params ) : for i in range ( X . ncol ) : X [ i ] = self . means [ i ] + self . stds [ i ] * X [ i ] return X", "nl": "Undo the scale transformation ."}}
{"translation": {"code": "def centers_std ( self ) : o = self . _model_json [ \"output\" ] cvals = o [ \"centers_std\" ] . cell_values centers_std = [ list ( cval [ 1 : ] ) for cval in cvals ] centers_std = [ list ( x ) for x in zip ( * centers_std ) ] return centers_std", "nl": "The standardized centers for the kmeans model ."}}
{"translation": {"code": "def centers ( self ) : o = self . _model_json [ \"output\" ] cvals = o [ \"centers\" ] . cell_values centers = [ list ( cval [ 1 : ] ) for cval in cvals ] return centers", "nl": "The centers for the KMeans model ."}}
{"translation": {"code": "def size ( self , train = False , valid = False , xval = False ) : tm = ModelBase . _get_metrics ( self , train , valid , xval ) m = { } for k , v in tm . items ( ) : m [ k ] = None if v is None else [ v [ 2 ] for v in v . _metric_json [ \"centroid_stats\" ] . cell_values ] return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m", "nl": "Get the sizes of each cluster ."}}
{"translation": {"code": "def set_levels ( self , levels ) : assert_is_type ( levels , [ str ] ) return H2OFrame . _expr ( expr = ExprNode ( \"setDomain\" , self , False , levels ) , cache = self . _ex . _cache )", "nl": "Replace the levels of a categorical column ."}}
{"translation": {"code": "def describe ( self , chunk_summary = False ) : if self . _has_content ( ) : res = h2o . api ( \"GET /3/Frames/%s\" % self . frame_id , data = { \"row_count\" : 10 } ) [ \"frames\" ] [ 0 ] self . _ex . _cache . _fill_data ( res ) print ( \"Rows:{}\" . format ( self . nrow ) ) print ( \"Cols:{}\" . format ( self . ncol ) ) #The chunk & distribution summaries are not cached, so must be pulled if chunk_summary=True. if chunk_summary : res [ \"chunk_summary\" ] . show ( ) res [ \"distribution_summary\" ] . show ( ) print ( \"\\n\" ) self . summary ( )", "nl": "Generate an in - depth description of this H2OFrame ."}}
{"translation": {"code": "def impute ( self , column = - 1 , method = \"mean\" , combine_method = \"interpolate\" , by = None , group_by_frame = None , values = None ) : if is_type ( column , str ) : column = self . names . index ( column ) if is_type ( by , str ) : by = self . names . index ( by ) if values is None : values = \"_\" else : assert len ( values ) == len ( self . columns ) , \"Length of values does not match length of columns\" # convert string values to categorical num values values2 = [ ] for i in range ( 0 , len ( values ) ) : if self . type ( i ) == \"enum\" : try : values2 . append ( self . levels ( ) [ i ] . index ( values [ i ] ) ) except : raise H2OValueError ( \"Impute value of: \" + values [ i ] + \" not found in existing levels of\" \" column: \" + self . col_names [ i ] ) else : values2 . append ( values [ i ] ) values = values2 if group_by_frame is None : group_by_frame = \"_\" # This code below is needed to ensure the frame (self) exists on the server. Without it, self._ex._cache.fill() # fails with an assertion that ._id is None. # This code should be removed / reworked once we have a more consistent strategy of dealing with frames. self . _ex . _eager_frame ( ) if by is not None or group_by_frame is not \"_\" : res = H2OFrame . _expr ( expr = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) ) . _frame ( ) else : res = ExprNode ( \"h2o.impute\" , self , column , method , combine_method , by , group_by_frame , values ) . _eager_scalar ( ) self . _ex . _cache . flush ( ) self . _ex . _cache . fill ( 10 ) return res", "nl": "Impute missing values into the frame modifying it in - place ."}}
{"translation": {"code": "def merge ( self , other , all_x = False , all_y = False , by_x = None , by_y = None , method = \"auto\" ) : if by_x is None and by_y is None : common_names = list ( set ( self . names ) & set ( other . names ) ) if not common_names : raise H2OValueError ( \"No columns in common to merge on!\" ) if by_x is None : by_x = [ self . names . index ( c ) for c in common_names ] else : by_x = _getValidCols ( by_x , self ) if by_y is None : by_y = [ other . names . index ( c ) for c in common_names ] else : by_y = _getValidCols ( by_y , other ) return H2OFrame . _expr ( expr = ExprNode ( \"merge\" , self , other , all_x , all_y , by_x , by_y , method ) )", "nl": "Merge two datasets based on common column names . We do not support all_x = True and all_y = True . Only one can be True or none is True . The default merge method is auto and it will default to the radix method . The radix method will return the correct merge result regardless of duplicated rows in the right frame . In addition the radix method can perform merge even if you have string columns in your frames . If there are duplicated rows in your rite frame they will not be included if you use the hash method . The hash method cannot perform merge if you have string columns in your left frame . Hence we consider the radix method superior to the hash method and is the default method to use ."}}
{"translation": {"code": "def nlevels ( self ) : levels = self . levels ( ) return [ len ( l ) for l in levels ] if levels else 0", "nl": "Get the number of factor levels for each categorical column ."}}
{"translation": {"code": "def levels ( self ) : lol = H2OFrame . _expr ( expr = ExprNode ( \"levels\" , self ) ) . as_data_frame ( False ) lol . pop ( 0 ) # Remove column headers lol = list ( zip ( * lol ) ) return [ [ ll for ll in l if ll != '' ] for l in lol ]", "nl": "Get the factor levels ."}}
{"translation": {"code": "def relevel ( self , y ) : return H2OFrame . _expr ( expr = ExprNode ( \"relevel\" , self , quote ( y ) ) )", "nl": "Reorder levels of an H2O factor for one single column of a H2O frame"}}
{"translation": {"code": "def toupper ( self ) : return H2OFrame . _expr ( expr = ExprNode ( \"toupper\" , self ) , cache = self . _ex . _cache )", "nl": "Translate characters from lower to upper case for a particular column ."}}
{"translation": {"code": "def sub ( self , pattern , replacement , ignore_case = False ) : return H2OFrame . _expr ( expr = ExprNode ( \"replacefirst\" , self , pattern , replacement , ignore_case ) )", "nl": "Substitute the first occurrence of pattern in a string with replacement ."}}
{"translation": {"code": "def isax ( self , num_words , max_cardinality , optimize_card = False , * * kwargs ) : if num_words <= 0 : raise H2OValueError ( \"num_words must be greater than 0\" ) if max_cardinality <= 0 : raise H2OValueError ( \"max_cardinality must be greater than 0\" ) return H2OFrame . _expr ( expr = ExprNode ( \"isax\" , self , num_words , max_cardinality , optimize_card ) )", "nl": "Compute the iSAX index for DataFrame which is assumed to be numeric time series data ."}}
{"translation": {"code": "def hist ( self , breaks = \"sturges\" , plot = True , * * kwargs ) : server = kwargs . pop ( \"server\" ) if \"server\" in kwargs else False assert_is_type ( breaks , int , [ numeric ] , Enum ( \"sturges\" , \"rice\" , \"sqrt\" , \"doane\" , \"fd\" , \"scott\" ) ) assert_is_type ( plot , bool ) assert_is_type ( server , bool ) if kwargs : raise H2OValueError ( \"Unknown parameters to hist(): %r\" % kwargs ) hist = H2OFrame . _expr ( expr = ExprNode ( \"hist\" , self , breaks ) ) . _frame ( ) if plot : try : import matplotlib if server : matplotlib . use ( \"Agg\" , warn = False ) import matplotlib . pyplot as plt except ImportError : print ( \"ERROR: matplotlib is required to make the histogram plot. \" \"Set `plot` to False, if a plot is not desired.\" ) return hist [ \"widths\" ] = hist [ \"breaks\" ] . difflag1 ( ) # [2:] because we're removing the title and the first row (which consists of NaNs) lefts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"breaks\" ] , use_pandas = False ) [ 2 : ] ] widths = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"widths\" ] , use_pandas = False ) [ 2 : ] ] counts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ \"counts\" ] , use_pandas = False ) [ 2 : ] ] plt . xlabel ( self . names [ 0 ] ) plt . ylabel ( \"Frequency\" ) plt . title ( \"Histogram of %s\" % self . names [ 0 ] ) plt . bar ( left = lefts , width = widths , height = counts , bottom = 0 ) if not server : plt . show ( ) else : hist [ \"density\" ] = hist [ \"counts\" ] / ( hist [ \"breaks\" ] . difflag1 ( ) * hist [ \"counts\" ] . sum ( ) ) return hist", "nl": "Compute a histogram over a numeric column ."}}
{"translation": {"code": "def insert_missing_values ( self , fraction = 0.1 , seed = None ) : kwargs = { } kwargs [ 'dataset' ] = self . frame_id # Eager; forces eval now for following REST call kwargs [ 'fraction' ] = fraction if seed is not None : kwargs [ 'seed' ] = seed job = { } job [ 'job' ] = h2o . api ( \"POST /3/MissingInserter\" , data = kwargs ) H2OJob ( job , job_type = ( \"Insert Missing Values\" ) ) . poll ( ) self . _ex . _cache . flush ( ) return self", "nl": "Insert missing values into the current frame modifying it in - place ."}}
{"translation": {"code": "def table ( self , data2 = None , dense = True ) : return H2OFrame . _expr ( expr = ExprNode ( \"table\" , self , data2 , dense ) ) if data2 is not None else H2OFrame . _expr ( expr = ExprNode ( \"table\" , self , dense ) )", "nl": "Compute the counts of values appearing in a column or co - occurence counts between two columns ."}}
{"translation": {"code": "def num_valid_substrings ( self , path_to_words ) : assert_is_type ( path_to_words , str ) fr = H2OFrame . _expr ( expr = ExprNode ( \"num_valid_substrings\" , self , path_to_words ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr", "nl": "For each string find the count of all possible substrings with 2 characters or more that are contained in the line - separated text file whose path is given ."}}
{"translation": {"code": "def entropy ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"entropy\" , self ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr", "nl": "For each string compute its Shannon entropy if the string is empty the entropy is 0 ."}}
{"translation": {"code": "def lstrip ( self , set = \" \" ) : # work w/ None; parity with python lstrip if set is None : set = \" \" fr = H2OFrame . _expr ( expr = ExprNode ( \"lstrip\" , self , set ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr", "nl": "Return a copy of the column with leading characters removed ."}}
{"translation": {"code": "def head ( self , rows = 10 , cols = 200 ) : assert_is_type ( rows , int ) assert_is_type ( cols , int ) nrows = min ( self . nrows , rows ) ncols = min ( self . ncols , cols ) newdt = self [ : nrows , : ncols ] return newdt . _frame ( rows = nrows , cols = cols , fill_cache = True )", "nl": "Return the first rows and cols of the frame as a new H2OFrame ."}}
{"translation": {"code": "def var ( self , y = None , na_rm = False , use = None ) : symmetric = False if y is None : y = self symmetric = True if use is None : use = \"complete.obs\" if na_rm else \"everything\" if self . nrow == 1 or ( self . ncol == 1 and y . ncol == 1 ) : return ExprNode ( \"var\" , self , y , use , symmetric ) . _eager_scalar ( ) return H2OFrame . _expr ( expr = ExprNode ( \"var\" , self , y , use , symmetric ) ) . _frame ( )", "nl": "Compute the variance - covariance matrix of one or two H2OFrames ."}}
{"translation": {"code": "def cor ( self , y = None , na_rm = False , use = None ) : assert_is_type ( y , H2OFrame , None ) assert_is_type ( na_rm , bool ) assert_is_type ( use , None , \"everything\" , \"all.obs\" , \"complete.obs\" ) if y is None : y = self if use is None : use = \"complete.obs\" if na_rm else \"everything\" if self . nrow == 1 or ( self . ncol == 1 and y . ncol == 1 ) : return ExprNode ( \"cor\" , self , y , use ) . _eager_scalar ( ) return H2OFrame . _expr ( expr = ExprNode ( \"cor\" , self , y , use ) ) . _frame ( )", "nl": "Compute the correlation matrix of one or two H2OFrames ."}}
{"translation": {"code": "def mult ( self , matrix ) : if self . ncols != matrix . nrows : raise H2OValueError ( \"Matrix is not compatible for multiplication with the current frame\" ) return H2OFrame . _expr ( expr = ExprNode ( \"x\" , self , matrix ) )", "nl": "Multiply this frame viewed as a matrix by another matrix ."}}
{"translation": {"code": "def asfactor ( self ) : for colname in self . names : t = self . types [ colname ] if t not in { \"bool\" , \"int\" , \"string\" , \"enum\" } : raise H2OValueError ( \"Only 'int' or 'string' are allowed for \" \"asfactor(), got %s:%s \" % ( colname , t ) ) fr = H2OFrame . _expr ( expr = ExprNode ( \"as.factor\" , self ) , cache = self . _ex . _cache ) if fr . _ex . _cache . types_valid ( ) : fr . _ex . _cache . types = { name : \"enum\" for name in self . types } else : raise H2OTypeError ( \"Types are not available in result\" ) return fr", "nl": "Convert columns in the current frame to categoricals ."}}
{"translation": {"code": "def substring ( self , start_index , end_index = None ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"substring\" , self , start_index , end_index ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr", "nl": "For each string return a new string that is a substring of the original string ."}}
{"translation": {"code": "def countmatches ( self , pattern ) : assert_is_type ( pattern , str , [ str ] ) fr = H2OFrame . _expr ( expr = ExprNode ( \"countmatches\" , self , pattern ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncols = self . ncol return fr", "nl": "For each string in the frame count the occurrences of the provided pattern . If countmathces is applied to a frame all columns of the frame must be type string otherwise the returned frame will contain errors ."}}
{"translation": {"code": "def na_omit ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"na.omit\" , self ) , cache = self . _ex . _cache ) fr . _ex . _cache . nrows = - 1 return fr", "nl": "Remove rows with NAs from the H2OFrame ."}}
{"translation": {"code": "def strsplit ( self , pattern ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"strsplit\" , self , pattern ) ) fr . _ex . _cache . nrows = self . nrow return fr", "nl": "Split the strings in the target column on the given regular expression pattern ."}}
{"translation": {"code": "def as_data_frame ( self , use_pandas = True , header = True ) : if can_use_pandas ( ) and use_pandas : import pandas return pandas . read_csv ( StringIO ( self . get_frame_data ( ) ) , low_memory = False , skip_blank_lines = False ) from h2o . utils . csv . readers import reader frame = [ row for row in reader ( StringIO ( self . get_frame_data ( ) ) ) ] if not header : frame . pop ( 0 ) return frame", "nl": "Obtain the dataset as a python - local object ."}}
{"translation": {"code": "def stratified_kfold_column ( self , n_folds = 3 , seed = - 1 ) : return H2OFrame . _expr ( expr = ExprNode ( \"stratified_kfold_column\" , self , n_folds , seed ) ) . _frame ( )", "nl": "Build a fold assignment column with the constraint that each fold has the same class distribution as the fold column ."}}
{"translation": {"code": "def summary ( self , return_data = False ) : if not self . _has_content ( ) : print ( \"This H2OFrame is empty and not initialized.\" ) return self . _ex . _cache . _data if not self . _ex . _cache . is_valid ( ) : self . _frame ( ) . _ex . _cache . fill ( ) if not return_data : if self . nrows == 0 : print ( \"This H2OFrame is empty.\" ) elif H2ODisplay . _in_ipy ( ) : import IPython . display IPython . display . display_html ( self . _ex . _cache . _tabulate ( \"html\" , True ) , raw = True ) else : print ( self . _ex . _cache . _tabulate ( \"simple\" , True ) ) else : return self . _ex . _cache . _data", "nl": "Display summary information about the frame ."}}
{"translation": {"code": "def structure ( self ) : df = self . as_data_frame ( use_pandas = False ) cn = df . pop ( 0 ) nr = self . nrow nc = self . ncol width = max ( [ len ( c ) for c in cn ] ) isfactor = self . isfactor ( ) numlevels = self . nlevels ( ) lvls = self . levels ( ) print ( \"H2OFrame: '{}' \\nDimensions: {} obs. of {} variables\" . format ( self . frame_id , nr , nc ) ) for i in range ( nc ) : print ( \"$ {} {}: \" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) if isfactor [ i ] : nl = numlevels [ i ] print ( \"Factor w/ {} level(s) {} \" . format ( nl , '\"' + '\",\"' . join ( lvls [ i ] ) + '\"' ) , end = '\\n' ) else : print ( \"num {}\" . format ( \" \" . join ( it [ 0 ] if it else \"nan\" for it in h2o . as_list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) )", "nl": "Compactly display the internal structure of an H2OFrame ."}}
{"translation": {"code": "def cut ( self , breaks , labels = None , include_lowest = False , right = True , dig_lab = 3 ) : assert_is_type ( breaks , [ numeric ] ) if self . ncols != 1 : raise H2OValueError ( \"Single-column frame is expected\" ) if self . types [ self . names [ 0 ] ] not in { \"int\" , \"real\" } : raise H2OValueError ( \"A numeric column is expected\" ) fr = H2OFrame . _expr ( expr = ExprNode ( \"cut\" , self , breaks , labels , include_lowest , right , dig_lab ) , cache = self . _ex . _cache ) fr . _ex . _cache . types = { k : \"enum\" for k in self . names } return fr", "nl": "Cut a numeric vector into categorical buckets ."}}
{"translation": {"code": "def runif ( self , seed = None ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"h2o.runif\" , self , - 1 if seed is None else seed ) ) fr . _ex . _cache . ncols = 1 fr . _ex . _cache . nrows = self . nrow return fr", "nl": "Generate a column of random numbers drawn from a uniform distribution [ 0 1 ) and having the same data layout as the source frame ."}}
{"translation": {"code": "def pop ( self , i ) : if is_type ( i , str ) : i = self . names . index ( i ) col = H2OFrame . _expr ( expr = ExprNode ( \"cols\" , self , i ) ) old_cache = self . _ex . _cache self . _ex = ExprNode ( \"cols\" , self , - ( i + 1 ) ) self . _ex . _cache . ncols -= 1 self . _ex . _cache . names = old_cache . names [ : i ] + old_cache . names [ i + 1 : ] self . _ex . _cache . types = { name : old_cache . types [ name ] for name in self . _ex . _cache . names } self . _ex . _cache . _data = None col . _ex . _cache . ncols = 1 col . _ex . _cache . names = [ old_cache . names [ i ] ] return col", "nl": "Pop a column from the H2OFrame at index i ."}}
{"translation": {"code": "def quantile ( self , prob = None , combine_method = \"interpolate\" , weights_column = None ) : if len ( self ) == 0 : return self if prob is None : prob = [ 0.01 , 0.1 , 0.25 , 0.333 , 0.5 , 0.667 , 0.75 , 0.9 , 0.99 ] if weights_column is None : weights_column = \"_\" else : assert_is_type ( weights_column , str , I ( H2OFrame , lambda wc : wc . ncol == 1 and wc . nrow == self . nrow ) ) if isinstance ( weights_column , H2OFrame ) : merged = self . cbind ( weights_column ) weights_column = merged . names [ - 1 ] return H2OFrame . _expr ( expr = ExprNode ( \"quantile\" , merged , prob , combine_method , weights_column ) ) return H2OFrame . _expr ( expr = ExprNode ( \"quantile\" , self , prob , combine_method , weights_column ) )", "nl": "Compute quantiles ."}}
{"translation": {"code": "def concat ( self , frames , axis = 1 ) : if len ( frames ) == 0 : raise ValueError ( \"Input list of frames is empty! Nothing to concat.\" ) if axis == 1 : df = self . cbind ( frames ) else : df = self . rbind ( frames ) return df", "nl": "Append multiple H2OFrames to this frame column - wise or row - wise ."}}
{"translation": {"code": "def set_level ( self , level ) : return H2OFrame . _expr ( expr = ExprNode ( \"setLevel\" , self , level ) , cache = self . _ex . _cache )", "nl": "A method to set all column values to one of the levels ."}}
{"translation": {"code": "def rbind ( self , data ) : assert_is_type ( data , H2OFrame , [ H2OFrame ] ) frames = [ data ] if not isinstance ( data , list ) else data for frame in frames : if frame . ncol != self . ncol : raise H2OValueError ( \"Cannot row-bind a dataframe with %d columns to a data frame with %d columns: \" \"the columns must match\" % ( frame . ncol , self . ncol ) ) if frame . columns != self . columns or frame . types != self . types : raise H2OValueError ( \"Column names and types must match for rbind() to work\" ) fr = H2OFrame . _expr ( expr = ExprNode ( \"rbind\" , self , * frames ) , cache = self . _ex . _cache ) fr . _ex . _cache . nrows = self . nrow + sum ( frame . nrow for frame in frames ) return fr", "nl": "Append data to this frame row - wise ."}}
{"translation": {"code": "def split_frame ( self , ratios = None , destination_frames = None , seed = None ) : assert_is_type ( ratios , [ numeric ] , None ) assert_is_type ( destination_frames , [ str ] , None ) assert_is_type ( seed , int , None ) if ratios is None : ratios = [ 0.75 ] if not ratios : raise ValueError ( \"Ratios array may not be empty\" ) if destination_frames is not None : if len ( ratios ) + 1 != len ( destination_frames ) : raise ValueError ( \"The number of provided destination_frames must be one more \" \"than the number of provided ratios\" ) num_slices = len ( ratios ) + 1 boundaries = [ ] last_boundary = 0 i = 0 while i < num_slices - 1 : ratio = ratios [ i ] if ratio < 0 : raise ValueError ( \"Ratio must be greater than 0\" ) boundary = last_boundary + ratio if boundary >= 1.0 : raise ValueError ( \"Ratios must add up to less than 1.0\" ) boundaries . append ( boundary ) last_boundary = boundary i += 1 splits = [ ] tmp_runif = self . runif ( seed ) tmp_runif . frame_id = \"%s_splitter\" % _py_tmp_key ( h2o . connection ( ) . session_id ) i = 0 while i < num_slices : if i == 0 : # lower_boundary is 0.0 upper_boundary = boundaries [ i ] tmp_slice = self [ ( tmp_runif <= upper_boundary ) , : ] elif i == num_slices - 1 : lower_boundary = boundaries [ i - 1 ] # upper_boundary is 1.0 tmp_slice = self [ ( tmp_runif > lower_boundary ) , : ] else : lower_boundary = boundaries [ i - 1 ] upper_boundary = boundaries [ i ] tmp_slice = self [ ( ( tmp_runif > lower_boundary ) & ( tmp_runif <= upper_boundary ) ) , : ] if destination_frames is None : splits . append ( tmp_slice ) else : destination_frame_id = destination_frames [ i ] tmp_slice . frame_id = destination_frame_id splits . append ( tmp_slice ) i += 1 del tmp_runif return splits", "nl": "Split a frame into distinct subsets of size determined by the given ratios ."}}
{"translation": {"code": "def group_by ( self , by ) : assert_is_type ( by , str , int , [ str , int ] ) return GroupBy ( self , by )", "nl": "Return a new GroupBy object using this frame and the desired grouping columns ."}}
{"translation": {"code": "def apply ( self , fun = None , axis = 0 ) : from . astfun import lambda_to_expr assert_is_type ( axis , 0 , 1 ) assert_is_type ( fun , FunctionType ) assert_satisfies ( fun , fun . __name__ == \"<lambda>\" ) res = lambda_to_expr ( fun ) return H2OFrame . _expr ( expr = ExprNode ( \"apply\" , self , 1 + ( axis == 0 ) , * res ) )", "nl": "Apply a lambda expression to an H2OFrame ."}}
{"translation": {"code": "def isin ( self , item ) : if is_type ( item , list , tuple , set ) : if self . ncols == 1 and ( self . type ( 0 ) == 'str' or self . type ( 0 ) == 'enum' ) : return self . match ( item ) else : return functools . reduce ( H2OFrame . __or__ , ( self == i for i in item ) ) else : return self == item", "nl": "Test whether elements of an H2OFrame are contained in the item ."}}
{"translation": {"code": "def set_name ( self , col = None , name = None ) : assert_is_type ( col , None , int , str ) assert_is_type ( name , str ) ncols = self . ncols col_index = None if is_type ( col , int ) : if not ( - ncols <= col < ncols ) : raise H2OValueError ( \"Index %d is out of bounds for a frame with %d columns\" % ( col , ncols ) ) col_index = ( col + ncols ) % ncols # handle negative indices elif is_type ( col , str ) : if col not in self . names : raise H2OValueError ( \"Column %s doesn't exist in the frame.\" % col ) col_index = self . names . index ( col ) # lookup the name else : assert col is None if ncols != 1 : raise H2OValueError ( \"The frame has %d columns; please specify which one to rename\" % ncols ) col_index = 0 if name != self . names [ col_index ] and name in self . types : raise H2OValueError ( \"Column '%s' already exists in the frame\" % name ) oldname = self . names [ col_index ] old_cache = self . _ex . _cache self . _ex = ExprNode ( \"colnames=\" , self , col_index , name ) # Update-in-place, but still lazy self . _ex . _cache . fill_from ( old_cache ) if self . names is None : self . _frame ( ) . _ex . _cache . fill ( ) else : self . _ex . _cache . _names = self . names [ : col_index ] + [ name ] + self . names [ col_index + 1 : ] self . _ex . _cache . _types [ name ] = self . _ex . _cache . _types . pop ( oldname ) return", "nl": "Set a new name for a column ."}}
{"translation": {"code": "def set_names ( self , names ) : assert_is_type ( names , [ str ] ) assert_satisfies ( names , len ( names ) == self . ncol ) self . _ex = ExprNode ( \"colnames=\" , self , range ( self . ncol ) , names ) # Update-in-place, but still lazy return self", "nl": "Change names of all columns in the frame ."}}
{"translation": {"code": "def isna ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( \"is.na\" , self ) ) fr . _ex . _cache . nrows = self . _ex . _cache . nrows fr . _ex . _cache . ncols = self . _ex . _cache . ncols if self . _ex . _cache . names : fr . _ex . _cache . names = [ \"isNA(%s)\" % n for n in self . _ex . _cache . names ] fr . _ex . _cache . types = { \"isNA(%s)\" % n : \"int\" for n in self . _ex . _cache . names } return fr", "nl": "For each element in an H2OFrame determine if it is NA or not ."}}
{"translation": {"code": "def stratified_split ( self , test_frac = 0.2 , seed = - 1 ) : return H2OFrame . _expr ( expr = ExprNode ( 'h2o.random_stratified_split' , self , test_frac , seed ) )", "nl": "Construct a column that can be used to perform a random stratified split ."}}
{"translation": {"code": "def modulo_kfold_column ( self , n_folds = 3 ) : return H2OFrame . _expr ( expr = ExprNode ( \"modulo_kfold_column\" , self , n_folds ) ) . _frame ( )", "nl": "Build a fold assignments column for cross - validation ."}}
{"translation": {"code": "def _find_jar ( self , path0 = None ) : jar_paths = [ path0 ] if path0 else self . _jar_paths ( ) searched_paths = [ ] for jp in jar_paths : searched_paths . append ( jp ) if os . path . exists ( jp ) : return jp raise H2OStartupError ( \"Cannot start local server: h2o.jar not found. Paths searched:\\n\" + \"\" . join ( \"    %s\\n\" % s for s in searched_paths ) )", "nl": "Return the location of an h2o . jar executable ."}}
{"translation": {"code": "def start ( jar_path = None , nthreads = - 1 , enable_assertions = True , max_mem_size = None , min_mem_size = None , ice_root = None , log_dir = None , log_level = None , port = \"54321+\" , name = None , extra_classpath = None , verbose = True , jvm_custom_args = None , bind_to_localhost = True ) : assert_is_type ( jar_path , None , str ) assert_is_type ( port , None , int , str ) assert_is_type ( name , None , str ) assert_is_type ( nthreads , - 1 , BoundInt ( 1 , 4096 ) ) assert_is_type ( enable_assertions , bool ) assert_is_type ( min_mem_size , None , int ) assert_is_type ( max_mem_size , None , BoundInt ( 1 << 25 ) ) assert_is_type ( log_dir , str , None ) assert_is_type ( log_level , str , None ) assert_satisfies ( log_level , log_level in [ None , \"TRACE\" , \"DEBUG\" , \"INFO\" , \"WARN\" , \"ERRR\" , \"FATA\" ] ) assert_is_type ( ice_root , None , I ( str , os . path . isdir ) ) assert_is_type ( extra_classpath , None , [ str ] ) assert_is_type ( jvm_custom_args , list , None ) assert_is_type ( bind_to_localhost , bool ) if jar_path : assert_satisfies ( jar_path , jar_path . endswith ( \"h2o.jar\" ) ) if min_mem_size is not None and max_mem_size is not None and min_mem_size > max_mem_size : raise H2OValueError ( \"`min_mem_size`=%d is larger than the `max_mem_size`=%d\" % ( min_mem_size , max_mem_size ) ) if port is None : port = \"54321+\" baseport = None # TODO: get rid of this port gimmick and have 2 separate parameters. if is_type ( port , str ) : if port . isdigit ( ) : port = int ( port ) else : if not ( port [ - 1 ] == \"+\" and port [ : - 1 ] . isdigit ( ) ) : raise H2OValueError ( \"`port` should be of the form 'DDDD+', where D is a digit. Got: %s\" % port ) baseport = int ( port [ : - 1 ] ) port = 0 hs = H2OLocalServer ( ) hs . _verbose = bool ( verbose ) hs . _jar_path = hs . _find_jar ( jar_path ) hs . _extra_classpath = extra_classpath hs . _ice_root = ice_root hs . _name = name if not ice_root : hs . _ice_root = tempfile . mkdtemp ( ) hs . _tempdir = hs . _ice_root if verbose : print ( \"Attempting to start a local H2O server...\" ) hs . _launch_server ( port = port , baseport = baseport , nthreads = int ( nthreads ) , ea = enable_assertions , mmax = max_mem_size , mmin = min_mem_size , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost , log_dir = log_dir , log_level = log_level ) if verbose : print ( \"  Server is running at %s://%s:%d\" % ( hs . scheme , hs . ip , hs . port ) ) atexit . register ( lambda : hs . shutdown ( ) ) return hs", "nl": "Start new H2O server on the local machine ."}}
{"translation": {"code": "def _jar_paths ( ) : # PUBDEV-3534 hook to use arbitrary h2o.jar own_jar = os . getenv ( \"H2O_JAR_PATH\" , \"\" ) if own_jar != \"\" : if not os . path . isfile ( own_jar ) : raise H2OStartupError ( \"Environment variable H2O_JAR_PATH is set to '%d' but file does not exists, unset environment variable or provide valid path to h2o.jar file.\" % own_jar ) yield own_jar # Check if running from an h2o-3 src folder (or any subfolder), in which case use the freshly-built h2o.jar cwd_chunks = os . path . abspath ( \".\" ) . split ( os . path . sep ) for i in range ( len ( cwd_chunks ) , 0 , - 1 ) : if cwd_chunks [ i - 1 ] == \"h2o-3\" : yield os . path . sep . join ( cwd_chunks [ : i ] + [ \"build\" , \"h2o.jar\" ] ) # Then check the backend/bin folder: # (the following works assuming this code is located in h2o/backend/server.py file) backend_dir = os . path . split ( os . path . realpath ( __file__ ) ) [ 0 ] yield os . path . join ( backend_dir , \"bin\" , \"h2o.jar\" ) # Then try several old locations where h2o.jar might have been installed prefix1 = prefix2 = sys . prefix # On Unix-like systems Python typically gets installed into /Library/... or /System/Library/... If one of # those paths is sys.prefix, then we also build its counterpart. if prefix1 . startswith ( os . path . sep + \"Library\" ) : prefix2 = os . path . join ( \"\" , \"System\" , prefix1 ) elif prefix1 . startswith ( os . path . sep + \"System\" ) : prefix2 = prefix1 [ len ( os . path . join ( \"\" , \"System\" ) ) : ] yield os . path . join ( prefix1 , \"h2o_jar\" , \"h2o.jar\" ) yield os . path . join ( os . path . abspath ( os . sep ) , \"usr\" , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) yield os . path . join ( prefix1 , \"local\" , \"h2o_jar\" , \"h2o.jar\" ) yield os . path . join ( get_config_var ( \"userbase\" ) , \"h2o_jar\" , \"h2o.jar\" ) yield os . path . join ( prefix2 , \"h2o_jar\" , \"h2o.jar\" )", "nl": "Produce potential paths for an h2o . jar executable ."}}
{"translation": {"code": "def assert_is_type ( var , * types , * * kwargs ) : assert types , \"The list of expected types was not provided\" expected_type = types [ 0 ] if len ( types ) == 1 else U ( * types ) if _check_type ( var , expected_type ) : return # Type check failed => Create a nice error message assert set ( kwargs ) . issubset ( { \"message\" , \"skip_frames\" } ) , \"Unexpected keyword arguments: %r\" % kwargs message = kwargs . get ( \"message\" , None ) skip_frames = kwargs . get ( \"skip_frames\" , 1 ) args = _retrieve_assert_arguments ( ) vname = args [ 0 ] etn = _get_type_name ( expected_type , dump = \", \" . join ( args [ 1 : ] ) ) vtn = _get_type_name ( type ( var ) ) raise H2OTypeError ( var_name = vname , var_value = var , var_type_name = vtn , exp_type_name = etn , message = message , skip_frames = skip_frames )", "nl": "Assert that the argument has the specified type ."}}
{"translation": {"code": "def _retrieve_assert_arguments ( ) : try : raise RuntimeError ( \"Catch me!\" ) except RuntimeError : # Walk up the stacktrace until we are outside of this file tb = sys . exc_info ( ) [ 2 ] assert tb . tb_frame . f_code . co_name == \"_retrieve_assert_arguments\" this_filename = tb . tb_frame . f_code . co_filename fr = tb . tb_frame while fr is not None and fr . f_code . co_filename == this_filename : fr = fr . f_back # Read the source file and tokenize it, extracting the expressions. try : with io . open ( fr . f_code . co_filename , \"r\" , encoding = \"utf-8\" ) as f : # Skip initial lines that are irrelevant for i in range ( fr . f_lineno - 1 ) : next ( f ) # Create tokenizer g = tokenize . generate_tokens ( f . readline ) step = 0 args_tokens = [ ] level = 0 for ttt in g : if step == 0 : if ttt [ 0 ] != tokenize . NAME : continue if not ttt [ 1 ] . startswith ( \"assert_\" ) : continue step = 1 elif step == 1 : assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \"(\" args_tokens . append ( [ ] ) step = 2 elif step == 2 : if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \",\" : args_tokens . append ( [ ] ) elif level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == \")\" : break else : if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \"([{\" : level += 1 if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in \")]}\" : level -= 1 assert level >= 0 , \"Parse error: parentheses level became negative\" args_tokens [ - 1 ] . append ( ttt ) args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( \"\\n\" , \" \" ) for at in args_tokens ] return args except IOError : return \"arg\" ,", "nl": "Magic variable name retrieval ."}}
{"translation": {"code": "def _check_type ( var , vtype ) : if vtype is None : return var is None if isinstance ( vtype , _primitive_type ) : return var == vtype if vtype is str : return isinstance ( var , _str_type ) if vtype is int : return isinstance ( var , _int_type ) if vtype is numeric : return isinstance ( var , _num_type ) if isinstance ( vtype , MagicType ) : return vtype . check ( var ) if isinstance ( vtype , type ) : # ``vtype`` is a name of the class, or a built-in type such as \"list\", \"tuple\", etc return isinstance ( var , vtype ) if isinstance ( vtype , list ) : # ``vtype`` is a list literal elem_type = U ( * vtype ) return isinstance ( var , list ) and all ( _check_type ( item , elem_type ) for item in var ) if isinstance ( vtype , set ) : # ``vtype`` is a set literal elem_type = U ( * vtype ) return isinstance ( var , set ) and all ( _check_type ( item , elem_type ) for item in var ) if isinstance ( vtype , tuple ) : # ``vtype`` is a tuple literal return ( isinstance ( var , tuple ) and len ( vtype ) == len ( var ) and all ( _check_type ( var [ i ] , vtype [ i ] ) for i in range ( len ( vtype ) ) ) ) if isinstance ( vtype , dict ) : # ``vtype`` is a dict literal ttkv = U ( * viewitems ( vtype ) ) return isinstance ( var , dict ) and all ( _check_type ( kv , ttkv ) for kv in viewitems ( var ) ) if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : return vtype ( var ) raise RuntimeError ( \"Ivalid type %r in _check_type()\" % vtype )", "nl": "Return True if the variable is of the specified type and False otherwise ."}}
{"translation": {"code": "def make_metrics ( predicted , actual , domain = None , distribution = None ) : assert_is_type ( predicted , H2OFrame ) assert_is_type ( actual , H2OFrame ) # assert predicted.ncol == 1, \"`predicted` frame should have exactly 1 column\" assert actual . ncol == 1 , \"`actual` frame should have exactly 1 column\" assert_is_type ( distribution , str , None ) assert_satisfies ( actual . ncol , actual . ncol == 1 ) if domain is None and any ( actual . isfactor ( ) ) : domain = actual . levels ( ) [ 0 ] res = api ( \"POST /3/ModelMetrics/predictions_frame/%s/actuals_frame/%s\" % ( predicted . frame_id , actual . frame_id ) , data = { \"domain\" : domain , \"distribution\" : distribution } ) return res [ \"model_metrics\" ]", "nl": "Create Model Metrics from predicted and actual values in H2O ."}}
{"translation": {"code": "def _print ( self , msg , flush = False , end = \"\\n\" ) : if self . _verbose : print2 ( msg , end = end , flush = flush )", "nl": "Helper function to print connection status messages when in verbose mode ."}}
{"translation": {"code": "def _get_method_full_name ( func ) : # Python 3.3 already has this information available... if hasattr ( func , \"__qualname__\" ) : return func . __qualname__ module = inspect . getmodule ( func ) if module is None : return \"?.%s\" % getattr ( func , \"__name__\" , \"?\" ) for cls_name in dir ( module ) : cls = getattr ( module , cls_name ) if not inspect . isclass ( cls ) : continue for method_name in dir ( cls ) : cls_method = getattr ( cls , method_name ) if cls_method == func : return \"%s.%s\" % ( cls_name , method_name ) if hasattr ( func , \"__name__\" ) : return \"%s.%s\" % ( module . __name__ , func . __name__ ) return \"<unknown>\"", "nl": "Return fully qualified function name ."}}
{"translation": {"code": "def _wrap ( text , wrap_at = 120 , indent = 4 ) : out = \"\" curr_line_length = indent space_needed = False for word in text . split ( ) : if curr_line_length + len ( word ) > wrap_at : out += \"\\n\" + \" \" * indent curr_line_length = indent space_needed = False if space_needed : out += \" \" curr_line_length += 1 out += word curr_line_length += len ( word ) space_needed = True return out", "nl": "Return piece of text wrapped around if needed ."}}
{"translation": {"code": "def _get_args_str ( func , highlight = None ) : if not func : return \"\" s = str ( inspect . signature ( func ) ) [ 1 : - 1 ] if highlight : s = re . sub ( r\"\\b%s\\b\" % highlight , Style . BRIGHT + Fore . WHITE + highlight + Fore . LIGHTBLACK_EX + Style . NORMAL , s ) return s", "nl": "Return function s declared arguments as a string ."}}
{"translation": {"code": "def _find_function_from_code ( frame , code ) : def find_code ( iterable , depth = 0 ) : if depth > 3 : return # Avoid potential infinite loops, or generally objects that are too deep. for item in iterable : if item is None : continue found = None if hasattr ( item , \"__code__\" ) and item . __code__ == code : found = item elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : # class / module try : found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) except Exception : # Sometimes merely getting module's attributes may cause an exception. For example :mod:`six.moves` # is such an offender... continue elif isinstance ( item , ( list , tuple , set ) ) : found = find_code ( item , depth + 1 ) elif isinstance ( item , dict ) : found = find_code ( item . values ( ) , depth + 1 ) if found : return found return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) )", "nl": "Given a frame and a compiled function code find the corresponding function object within the frame ."}}
{"translation": {"code": "def assert_matches ( v , regex ) : m = re . match ( regex , v ) if m is None : vn = _retrieve_assert_arguments ( ) [ 0 ] message = \"Argument `{var}` (= {val!r}) did not match /{regex}/\" . format ( var = vn , regex = regex , val = v ) raise H2OValueError ( message , var_name = vn , skip_frames = 1 ) return m", "nl": "Assert that string variable matches the provided regular expression ."}}
{"translation": {"code": "def assert_satisfies ( v , cond , message = None ) : if not cond : vname , vexpr = _retrieve_assert_arguments ( ) if not message : message = \"Argument `{var}` (= {val!r}) does not satisfy the condition {expr}\" . format ( var = vname , val = v , expr = vexpr ) raise H2OValueError ( message = message , var_name = vname , skip_frames = 1 )", "nl": "Assert that variable satisfies the provided condition ."}}
{"translation": {"code": "def deep_copy ( data , xid ) : assert_is_type ( data , H2OFrame ) assert_is_type ( xid , str ) assert_satisfies ( xid , xid != data . frame_id ) check_frame_id ( xid ) duplicate = data . apply ( lambda x : x ) duplicate . _ex = ExprNode ( \"assign\" , xid , duplicate ) . _eval_driver ( False ) duplicate . _ex . _cache . _id = xid duplicate . _ex . _children = None return duplicate", "nl": "Create a deep clone of the frame data ."}}
{"translation": {"code": "def set_encoding ( self , encoding ) : self . _bar_ends = \"[]\" self . _bar_symbols = \"#\" if not encoding : return s1 = \"\\u258F\\u258E\\u258D\\u258C\\u258B\\u258A\\u2589\\u2588\" s2 = \"\\u258C\\u2588\" s3 = \"\\u2588\" if self . _file_mode : s1 = s2 = None assert len ( s3 ) == 1 for s in ( s1 , s2 , s3 ) : if s is None : continue try : s . encode ( encoding ) self . _bar_ends = \"||\" self . _bar_symbols = s return except UnicodeEncodeError : pass except LookupError : print ( \"Warning: unknown encoding %s\" % encoding )", "nl": "Inform the widget about the encoding of the underlying character stream ."}}
{"translation": {"code": "def _get_terminal_size ( ) : # If output is not terminal but a regular file, assume 100 chars width if not sys . stdout . isatty ( ) : return 80 # Otherwise, first try getting the dimensions from shell command `stty`: try : import subprocess ret = subprocess . check_output ( [ \"stty\" , \"size\" ] ) . strip ( ) . split ( \" \" ) if len ( ret ) == 2 : return int ( ret [ 1 ] ) except : pass # Otherwise try using ioctl try : from termios import TIOCGWINSZ from fcntl import ioctl from struct import unpack res = unpack ( \"hh\" , ioctl ( sys . stdout , TIOCGWINSZ , b\"1234\" ) ) return int ( res [ 1 ] ) except : pass # Finally check the COLUMNS environment variable return int ( os . environ . get ( \"COLUMNS\" , 80 ) )", "nl": "Find current STDOUT s width in characters ."}}
{"translation": {"code": "def _compute_widget_sizes ( self ) : wl = [ 0 ] * len ( self . _widgets ) flex_count = 0 # First render all non-flexible widgets for i , widget in enumerate ( self . _widgets ) : if isinstance ( widget , ProgressBarFlexibleWidget ) : flex_count += 1 else : wl [ i ] = widget . render ( 1 ) . length remaining_width = self . _width - sum ( wl ) remaining_width -= len ( self . _widgets ) - 1 # account for 1-space interval between widgets if remaining_width < 10 * flex_count : if self . _file_mode : remaining_width = 10 * flex_count else : # The window is too small to accomodate the widget: try to split it into several lines, otherwise # switch to \"file mode\". If we don't do this, then rendering the widget will cause it to wrap, and # then when we use \\r to go to the beginning of the line, only part of the widget will be overwritten, # which means we'll have many (possibly hundreds) of progress bar lines in the end. widget0 = self . _widgets [ 0 ] if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( 0 ) . length >= 10 * flex_count : remaining_width += widget0 . render ( 0 ) . length + 1 self . _to_render = widget0 . render ( 0 ) . rendered + \"\\n\" self . _widgets = self . _widgets [ 1 : ] if remaining_width < 10 * flex_count : self . _file_mode = True remaining_width = 10 * flex_count remaining_width = max ( remaining_width , 10 * flex_count ) # Ensure at least 10 chars per flexible widget for i , widget in enumerate ( self . _widgets ) : if isinstance ( widget , ProgressBarFlexibleWidget ) : target_length = int ( remaining_width / flex_count ) result = widget . render ( 1 , target_length ) wl [ i ] = result . length remaining_width -= result . length flex_count -= 1 return wl", "nl": "Initial rendering stage done in order to compute widths of all widgets ."}}
{"translation": {"code": "def _draw ( self , txt , final = False ) : if not self . _file_mode : # If the user presses Ctrl+C this ensures we still start writing from the beginning of the line sys . stdout . write ( \"\\r\" ) sys . stdout . write ( txt ) if final and not isinstance ( self . _widget , _HiddenWidget ) : sys . stdout . write ( \"\\n\" ) else : if not self . _file_mode : sys . stdout . write ( \"\\r\" ) sys . stdout . flush ( )", "nl": "Print the rendered string to the stdout ."}}
{"translation": {"code": "def _get_time_at_progress ( self , x_target ) : t , x , v = self . _t0 , self . _x0 , self . _v0 # The convergence should be achieved in just few iterations, however in unlikely situation that it doesn't # we don't want to loop forever... for _ in range ( 20 ) : if v == 0 : return 1e20 # make time prediction assuming the progress will continue at a linear speed ``v`` t += ( x_target - x ) / v # calculate the actual progress at that time x , v = self . _compute_progress_at_time ( t ) # iterate until convergence if abs ( x - x_target ) < 1e-3 : return t return time . time ( ) + 100", "nl": "Return the projected time when progress level x_target will be reached ."}}
{"translation": {"code": "def _guess_next_poll_interval ( self ) : time_elapsed = self . _progress_data [ - 1 ] [ 0 ] - self . _progress_data [ 0 ] [ 0 ] real_progress = self . _get_real_progress ( ) return min ( 0.2 * time_elapsed , 0.5 + ( 1 - real_progress ) ** 0.5 )", "nl": "Determine when to query the progress status next ."}}
{"translation": {"code": "def _estimate_progress_completion_time ( self , now ) : assert self . _next_poll_time >= now tlast , wlast = self . _progress_data [ - 1 ] # If reached 100%, make sure that we finish as soon as possible, but maybe not immediately if wlast == self . _maxval : current_completion_time = ( 1 - self . _x0 ) / self . _v0 + self . _t0 return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) # Calculate the approximate speed of the raw progress based on recent data tacc , wacc = 0 , 0 factor = self . GAMMA for t , x in self . _progress_data [ - 2 : : - 1 ] : tacc += factor * ( tlast - t ) wacc += factor * ( wlast - x ) factor *= self . GAMMA if factor < 1e-2 : break # If there was no progress at all, then just assume it's 5 minutes from now if wacc == 0 : return now + 300 # Estimate the completion time assuming linear progress t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc # Adjust the estimate if it looks like it may happen too soon if t_estimate <= self . _next_poll_time : t_estimate = self . _next_poll_time + self . FINISH_DELAY return t_estimate", "nl": "Estimate the moment when the underlying process is expected to reach completion ."}}
{"translation": {"code": "def _recalculate_model_parameters ( self , now ) : time_until_end = self . _estimate_progress_completion_time ( now ) - now assert time_until_end >= 0 , \"Estimated progress completion cannot be in the past.\" x_real = self . _get_real_progress ( ) if x_real == 1 : t0 , x0 , v0 , ve = now , 1 , 0 , 0 else : x0 , v0 = self . _compute_progress_at_time ( now ) t0 = now if x0 >= 1 : # On rare occasion, the model's progress may have reached 100% by ``now``. This can happen if # (1) the progress is close to 100% initially and has high speed, (2) on the previous call we # estimated that the process completion time will be right after the next poll time, and (3) # the polling itself took so much time that the process effectively \"overshoot\". # If this happens, then we adjust x0, v0 to the previous valid data checkpoint. t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 time_until_end += now - t0 z = self . BETA * time_until_end max_speed = ( 1 - x_real ** 2 ) / self . FINISH_DELAY ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) if ve < 0 : # Current speed is too high -- reduce v0 (violate non-smoothness of speed) v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) ve = 0 if ve > max_speed : # Current speed is too low: finish later, but do not allow ``ve`` to be higher than ``max_speed`` ve = max_speed self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve", "nl": "Compute t0 x0 v0 ve ."}}
{"translation": {"code": "def _compute_progress_at_time ( self , t ) : t0 , x0 , v0 , ve = self . _t0 , self . _x0 , self . _v0 , self . _ve z = ( v0 - ve ) * math . exp ( - self . BETA * ( t - t0 ) ) vt = ve + z xt = clamp ( x0 + ve * ( t - t0 ) + ( v0 - ve - z ) / self . BETA , 0 , 1 ) return xt , vt", "nl": "Calculate the modelled progress state for the given time moment ."}}
{"translation": {"code": "def _store_model_progress ( self , res , now ) : raw_progress , delay = res raw_progress = clamp ( raw_progress , 0 , self . _maxval ) self . _progress_data . append ( ( now , raw_progress ) ) if delay < 0 : # calculation of ``_guess_next_poll_interval()`` should be done only *after* we pushed the fresh data to # ``self._progress_data``. delay = self . _guess_next_poll_interval ( ) self . _next_poll_time = now + clamp ( delay , self . MIN_PROGRESS_CHECK_INTERVAL , self . MAX_PROGRESS_CHECK_INTERVAL )", "nl": "Save the current model progress into self . _progress_data and update self . _next_poll_time ."}}
{"translation": {"code": "def execute ( self , progress_fn , print_verbose_info = None ) : assert_is_type ( progress_fn , FunctionType , GeneratorType , MethodType ) if isinstance ( progress_fn , GeneratorType ) : # Convert generator to a regular function progress_fn = ( lambda g : lambda : next ( g ) ) ( progress_fn ) # Initialize the execution context self . _next_poll_time = 0 self . _t0 = time . time ( ) self . _x0 = 0 self . _v0 = 0.01 # corresponds to 100s completion time self . _ve = 0.01 progress = 0 status = None # Status message in case the job gets interrupted. try : while True : # We attempt to synchronize all helper functions, ensuring that each of them has the same idea # for what the current time moment is. Otherwise we could have some corner cases when one method # says that something must happen right now, while the other already sees that moment in the past. now = time . time ( ) # Query the progress level, but only if it's time already if self . _next_poll_time <= now : res = progress_fn ( ) # may raise StopIteration assert_is_type ( res , ( numeric , numeric ) , numeric ) if not isinstance ( res , tuple ) : res = ( res , - 1 ) # Progress querying could have taken some time, so update the current time moment now = time . time ( ) self . _store_model_progress ( res , now ) self . _recalculate_model_parameters ( now ) # Render the widget regardless of whether it's too early or not progress = min ( self . _compute_progress_at_time ( now ) [ 0 ] , 1 ) if progress == 1 and self . _get_real_progress ( ) >= 1 : # Do not exit until both the model and the actual progress reach 100% mark. break result = self . _widget . render ( progress ) assert_is_type ( result , RenderResult ) time0 = result . next_time time1 = self . _get_time_at_progress ( result . next_progress ) next_render_time = min ( time0 , time1 ) self . _draw ( result . rendered ) # Wait until the next rendering/querying cycle wait_time = min ( next_render_time , self . _next_poll_time ) - now if wait_time > 0 : time . sleep ( wait_time ) if print_verbose_info is not None : print_verbose_info ( progress ) except KeyboardInterrupt : # If the user presses Ctrl+C, we interrupt the progress bar. status = \"cancelled\" except StopIteration as e : # If the generator raises StopIteration before reaching 100%, then the progress display will # reamin incomplete. status = str ( e ) # Do one final rendering before we exit result = self . _widget . render ( progress = progress , status = status ) self . _draw ( result . rendered , final = True ) if status == \"cancelled\" : # Re-raise the exception, to inform the upstream caller that something unexpected happened. raise StopIteration ( status )", "nl": "Start the progress bar and return only when the progress reaches 100% ."}}
{"translation": {"code": "def show_status ( self , detailed = False ) : if self . _retrieved_at + self . REFRESH_INTERVAL < time . time ( ) : # Info is stale, need to refresh new_info = h2o . api ( \"GET /3/Cloud\" ) self . _fill_from_h2ocluster ( new_info ) ncpus = sum ( node [ \"num_cpus\" ] for node in self . nodes ) allowed_cpus = sum ( node [ \"cpus_allowed\" ] for node in self . nodes ) free_mem = sum ( node [ \"free_mem\" ] for node in self . nodes ) unhealthy_nodes = sum ( not node [ \"healthy\" ] for node in self . nodes ) status = \"locked\" if self . locked else \"accepting new members\" if unhealthy_nodes == 0 : status += \", healthy\" else : status += \", %d nodes are not healthy\" % unhealthy_nodes api_extensions = self . list_api_extensions ( ) H2ODisplay ( [ [ \"H2O cluster uptime:\" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ \"H2O cluster timezone:\" , self . cloud_internal_timezone ] , [ \"H2O data parsing timezone:\" , self . datafile_parser_timezone ] , [ \"H2O cluster version:\" , self . version ] , [ \"H2O cluster version age:\" , \"{} {}\" . format ( self . build_age , ( \"!!!\" if self . build_too_old else \"\" ) ) ] , [ \"H2O cluster name:\" , self . cloud_name ] , [ \"H2O cluster total nodes:\" , self . cloud_size ] , [ \"H2O cluster free memory:\" , get_human_readable_bytes ( free_mem ) ] , [ \"H2O cluster total cores:\" , str ( ncpus ) ] , [ \"H2O cluster allowed cores:\" , str ( allowed_cpus ) ] , [ \"H2O cluster status:\" , status ] , [ \"H2O connection url:\" , h2o . connection ( ) . base_url ] , [ \"H2O connection proxy:\" , h2o . connection ( ) . proxy ] , [ \"H2O internal security:\" , self . internal_security_enabled ] , [ \"H2O API Extensions:\" , ', ' . join ( api_extensions ) ] , [ \"Python version:\" , \"%d.%d.%d %s\" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) if detailed : keys = [ \"h2o\" , \"healthy\" , \"last_ping\" , \"num_cpus\" , \"sys_load\" , \"mem_value_size\" , \"free_mem\" , \"pojo_mem\" , \"swap_mem\" , \"free_disk\" , \"max_disk\" , \"pid\" , \"num_keys\" , \"tcps_active\" , \"open_fds\" , \"rpcs_active\" ] header = [ \"Nodes info:\" ] + [ \"Node %d\" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] table = [ [ k ] for k in keys ] for node in self . nodes : for i , k in enumerate ( keys ) : table [ i ] . append ( node [ k ] ) H2ODisplay ( table = table , header = header )", "nl": "Print current cluster status information ."}}
{"translation": {"code": "def from_kvs ( keyvals ) : obj = H2OCluster ( ) obj . _retrieved_at = time . time ( ) for k , v in keyvals : if k in { \"__meta\" , \"_exclude_fields\" , \"__schema\" } : continue if k in _cloud_v3_valid_keys : obj . _props [ k ] = v else : raise AttributeError ( \"Attribute %s cannot be set on H2OCluster (= %r)\" % ( k , v ) ) return obj", "nl": "Create H2OCluster object from a list of key - value pairs ."}}
{"translation": {"code": "def _fill_from_h2ocluster ( self , other ) : self . _props = other . _props self . _retrieved_at = other . _retrieved_at other . _props = { } other . _retrieved_at = None", "nl": "Update information in this object from another H2OCluster instance ."}}
{"translation": {"code": "def list_timezones ( self ) : from h2o . expr import ExprNode return h2o . H2OFrame . _expr ( expr = ExprNode ( \"listTimeZones\" ) ) . _frame ( )", "nl": "Return the list of all known timezones ."}}
{"translation": {"code": "def shutdown ( self , prompt = False ) : if not self . is_running ( ) : return assert_is_type ( prompt , bool ) if prompt : question = \"Are you sure you want to shutdown the H2O instance running at %s (Y/N)? \" % h2o . connection ( ) . base_url response = input ( question ) # works in Py2 & Py3 because redefined in h2o.utils.compatibility module else : response = \"Y\" if response . lower ( ) in { \"y\" , \"yes\" } : h2o . api ( \"POST /3/Shutdown\" ) h2o . connection ( ) . close ( )", "nl": "Shut down the server ."}}
{"translation": {"code": "def is_running ( self ) : try : if h2o . connection ( ) . local_server and not h2o . connection ( ) . local_server . is_running ( ) : return False h2o . api ( \"GET /\" ) return True except ( H2OConnectionError , H2OServerError ) : return False", "nl": "Determine if the H2O cluster is running or not ."}}
{"translation": {"code": "def api ( endpoint , data = None , json = None , filename = None , save_to = None ) : # type checks are performed in H2OConnection class _check_connection ( ) return h2oconn . request ( endpoint , data = data , json = json , filename = filename , save_to = save_to )", "nl": "Perform a REST API request to a previously connected server ."}}
{"translation": {"code": "def check ( self , var ) : return not any ( _check_type ( var , tt ) for tt in self . _types )", "nl": "Return True if the variable does not match any of the types and False otherwise ."}}
{"translation": {"code": "def _get_type_name ( vtype , dump = None ) : if vtype is None : return \"None\" if vtype is str : return \"string\" if vtype is int : return \"integer\" if vtype is numeric : return \"numeric\" if is_type ( vtype , str ) : return '\"%s\"' % repr ( vtype ) [ 1 : - 1 ] if is_type ( vtype , int ) : return str ( vtype ) if isinstance ( vtype , MagicType ) : return vtype . name ( dump ) if isinstance ( vtype , type ) : return vtype . __name__ if isinstance ( vtype , list ) : return \"list(%s)\" % _get_type_name ( U ( * vtype ) , dump ) if isinstance ( vtype , set ) : return \"set(%s)\" % _get_type_name ( U ( * vtype ) , dump ) if isinstance ( vtype , tuple ) : return \"(%s)\" % \", \" . join ( _get_type_name ( item , dump ) for item in vtype ) if isinstance ( vtype , dict ) : return \"dict(%s)\" % \", \" . join ( \"%s: %s\" % ( _get_type_name ( tk , dump ) , _get_type_name ( tv , dump ) ) for tk , tv in viewitems ( vtype ) ) if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : if vtype . __name__ == \"<lambda>\" : return _get_lambda_source_code ( vtype , dump ) else : return vtype . __name__ raise RuntimeError ( \"Unexpected `vtype`: %r\" % vtype )", "nl": "Return the name of the provided type ."}}
{"translation": {"code": "def cbind ( self , data ) : assert_is_type ( data , H2OFrame , numeric , [ H2OFrame , numeric ] ) frames = [ data ] if not isinstance ( data , list ) else data new_cols = list ( self . columns ) new_types = dict ( self . types ) for frame in frames : if isinstance ( frame , H2OFrame ) : if frame . nrow != self . nrow : raise H2OValueError ( \"Cannot bind a dataframe with %d rows to a data frame with %d rows: \" \"the number of rows should match\" % ( frame . nrow , self . nrow ) ) new_cols += frame . columns new_types . update ( frame . types ) else : new_cols += [ None ] unique_cols = set ( new_cols ) fr = H2OFrame . _expr ( expr = ExprNode ( \"cbind\" , self , * frames ) , cache = self . _ex . _cache ) fr . _ex . _cache . ncols = len ( new_cols ) if len ( new_cols ) == len ( unique_cols ) and None not in unique_cols : fr . _ex . _cache . names = new_cols fr . _ex . _cache . types = new_types else : # Invalidate names and types since they contain duplicate / unknown names, and the server will choose those. fr . _ex . _cache . names = None fr . _ex . _cache . types = None return fr", "nl": "Append data to this frame column - wise ."}}
{"translation": {"code": "def get_frame ( frame_id , rows = 10 , rows_offset = 0 , cols = - 1 , full_cols = - 1 , cols_offset = 0 , light = False ) : fr = H2OFrame ( ) fr . _ex . _cache . _id = frame_id try : fr . _ex . _cache . fill ( rows = rows , rows_offset = rows_offset , cols = cols , full_cols = full_cols , cols_offset = cols_offset , light = light ) except EnvironmentError : return None return fr", "nl": "Retrieve an existing H2OFrame from the H2O cluster using the frame s id ."}}
{"translation": {"code": "def _get_lambda_source_code ( lambda_fn , src ) : def gen_lambdas ( ) : def gen ( ) : yield src + \"\\n\" g = gen ( ) step = 0 tokens = [ ] for tok in tokenize . generate_tokens ( getattr ( g , \"next\" , getattr ( g , \"__next__\" , None ) ) ) : if step == 0 : if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == \"lambda\" : step = 1 tokens = [ tok ] level = 0 elif step == 1 : if tok [ 0 ] == tokenize . NAME : tokens . append ( tok ) step = 2 else : step = 0 elif step == 2 : if tok [ 0 ] == tokenize . OP and tok [ 1 ] == \":\" : tokens . append ( tok ) step = 3 else : step = 0 elif step == 3 : if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in \",)\" or tok [ 0 ] == tokenize . ENDMARKER ) : yield tokenize . untokenize ( tokens ) . strip ( ) step = 0 else : tokens . append ( tok ) if tok [ 0 ] == tokenize . OP : if tok [ 1 ] in \"[({\" : level += 1 if tok [ 1 ] in \"])}\" : level -= 1 assert not tokens actual_code = lambda_fn . __code__ . co_code for lambda_src in gen_lambdas ( ) : try : fn = eval ( lambda_src , globals ( ) , locals ( ) ) if fn . __code__ . co_code == actual_code : return lambda_src . split ( \":\" , 1 ) [ 1 ] . strip ( ) except Exception : pass return \"<lambda>\"", "nl": "Attempt to find the source code of the lambda_fn within the string src ."}}
{"translation": {"code": "def default_params ( self ) : params = { } for p in self . parms : params [ p ] = self . parms [ p ] [ \"default_value\" ] return params", "nl": "Dictionary of the default parameters of the model ."}}
{"translation": {"code": "def actual_params ( self ) : params_to_select = { \"model_id\" : \"name\" , \"response_column\" : \"column_name\" , \"training_frame\" : \"name\" , \"validation_frame\" : \"name\" } params = { } for p in self . parms : if p in params_to_select . keys ( ) : params [ p ] = self . parms [ p ] [ \"actual_value\" ] . get ( params_to_select [ p ] , None ) else : params [ p ] = self . parms [ p ] [ \"actual_value\" ] return params", "nl": "Dictionary of actual parameters of the model ."}}