{"translation": {"code": "def _event_shape ( self , shape , static_perm_to_shape ) : rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) if tensorshape_util . rank ( shape ) is None or rightmost_ is None : return tf . TensorShape ( None ) if tensorshape_util . rank ( shape ) < rightmost_ : raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) perm_ = tf . get_static_value ( self . perm , partial = True ) if perm_ is None : return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) # We can use elimination to reidentify a single None dimension. if sum ( p is None for p in perm_ ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) for i , p in enumerate ( present [ 1 : ] ) : # The -1 sorts to position 0. if i != p : perm_ = [ i if p is None else p for p in perm_ ] break return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) )", "nl": "Helper for _forward and _inverse_event_shape ."}}
{"translation": {"code": "def get_config ( self ) : return { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate_args' : self . validate_args , }", "nl": "Returns initializer configuration as a JSON - serializable dict ."}}
{"translation": {"code": "def _apply_slice_sequence ( dist , params_event_ndims , slice_overrides_seq ) : for slices , overrides in slice_overrides_seq : dist = _apply_single_step ( dist , params_event_ndims , slices , overrides ) return dist", "nl": "Applies a sequence of slice or copy - with - overrides operations to dist ."}}
{"translation": {"code": "def batch_slice ( dist , params_event_ndims , params_overrides , slices ) : if not isinstance ( slices , collections . Sequence ) : slices = ( slices , ) # We track the history of slice and copy(**param_overrides) in order to trace # back to the original distribution's source variables. orig_dist , slice_overrides_seq = getattr ( dist , PROVENANCE_ATTR , ( dist , [ ] ) ) slice_overrides_seq += [ ( slices , params_overrides ) ] # Re-doing the full sequence of slice+copy override work here enables # gradients all the way back to the original distribution's arguments. dist = _apply_slice_sequence ( orig_dist , params_event_ndims , slice_overrides_seq ) setattr ( dist , PROVENANCE_ATTR , ( orig_dist , slice_overrides_seq ) ) return dist", "nl": "Slices dist along its batch dimensions . Helper for tfd . Distribution ."}}
{"translation": {"code": "def _slice_params_to_dict ( dist , params_event_ndims , slices ) : override_dict = { } for param_name , param_event_ndims in six . iteritems ( params_event_ndims ) : # Verify that either None or a legit value is in the parameters dict. if param_name not in dist . parameters : raise ValueError ( 'Distribution {} is missing advertised ' 'parameter {}' . format ( dist , param_name ) ) param = dist . parameters [ param_name ] if param is None : # some distributions have multiple possible parameterizations; this # param was not provided continue dtype = None if hasattr ( dist , param_name ) : attr = getattr ( dist , param_name ) dtype = getattr ( attr , 'dtype' , None ) if dtype is None : dtype = dist . dtype warnings . warn ( 'Unable to find property getter for parameter Tensor {} ' 'on {}, falling back to Distribution.dtype {}' . format ( param_name , dist , dtype ) ) param = tf . convert_to_tensor ( value = param , dtype = dtype ) override_dict [ param_name ] = _slice_single_param ( param , param_event_ndims , slices , dist . batch_shape_tensor ( ) ) return override_dict", "nl": "Computes the override dictionary of sliced parameters ."}}
{"translation": {"code": "def make_ar_transition_matrix ( coefficients ) : top_row = tf . expand_dims ( coefficients , - 2 ) coef_shape = dist_util . prefer_static_shape ( coefficients ) batch_shape , order = coef_shape [ : - 1 ] , coef_shape [ - 1 ] remaining_rows = tf . concat ( [ tf . eye ( order - 1 , dtype = coefficients . dtype , batch_shape = batch_shape ) , tf . zeros ( tf . concat ( [ batch_shape , ( order - 1 , 1 ) ] , axis = 0 ) , dtype = coefficients . dtype ) ] , axis = - 1 ) ar_matrix = tf . concat ( [ top_row , remaining_rows ] , axis = - 2 ) return ar_matrix", "nl": "Build transition matrix for an autoregressive StateSpaceModel ."}}
{"translation": {"code": "def plot_generated_images ( images , fname ) : fig = plt . figure ( figsize = ( 4 , 4 ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) for i , image in enumerate ( images ) : ax = fig . add_subplot ( 4 , 4 , i + 1 ) plt . axis ( 'off' ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) ax . imshow ( image . reshape ( IMAGE_SHAPE [ : - 1 ] ) , cmap = 'Greys_r' ) fig . tight_layout ( ) plt . subplots_adjust ( wspace = 0.05 , hspace = 0.05 ) canvas . print_figure ( fname , format = 'png' )", "nl": "Save a synthetic image as a PNG file ."}}
{"translation": {"code": "def build_input_pipeline ( train_images , batch_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( train_images ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) images = training_iterator . get_next ( ) return images", "nl": "Build an iterator over training batches ."}}
{"translation": {"code": "def decompose_forecast_by_component ( model , forecast_dist , parameter_samples ) : with tf . compat . v1 . name_scope ( 'decompose_forecast_by_component' ) : try : forecast_lgssm = forecast_dist . components_distribution forecast_latent_mean , _ = forecast_lgssm . _joint_mean ( ) # pylint: disable=protected-access forecast_latent_covs , _ = forecast_lgssm . _joint_covariances ( ) # pylint: disable=protected-access except AttributeError as e : raise ValueError ( 'Forecast distribution must be a MixtureSameFamily of' 'LinearGaussianStateSpaceModel distributions, such as returned by' '`tfp.sts.forecast()`. (saw exception: {})' . format ( e ) ) # Since `parameter_samples` will have sample shape `[num_posterior_draws]`, # we need to move the `num_posterior_draws` dimension of the forecast # moments from the trailing batch dimension, where it's currently put by # `sts.forecast`, back to the leading (sample shape) dimension. forecast_latent_mean = dist_util . move_dimension ( forecast_latent_mean , source_idx = - 3 , dest_idx = 0 ) forecast_latent_covs = dist_util . move_dimension ( forecast_latent_covs , source_idx = - 4 , dest_idx = 0 ) return _decompose_from_posterior_marginals ( model , forecast_latent_mean , forecast_latent_covs , parameter_samples )", "nl": "Decompose a forecast distribution into contributions from each component ."}}
{"translation": {"code": "def make_component_state_space_models ( self , num_timesteps , param_vals , initial_step = 0 ) : with tf . compat . v1 . name_scope ( 'make_component_state_space_models' ) : # List the model parameters in canonical order param_map = self . _canonicalize_param_vals_as_map ( param_vals ) param_vals_list = [ param_map [ p . name ] for p in self . parameters ] # Build SSMs for each component model. We process the components in # canonical order, extracting the parameters for each component from the # (ordered) list of parameters. remaining_param_vals = param_vals_list [ 1 : ] component_ssms = [ ] for component in self . components : num_parameters = len ( component . parameters ) component_param_vals = remaining_param_vals [ : num_parameters ] remaining_param_vals = remaining_param_vals [ num_parameters : ] component_ssms . append ( component . make_state_space_model ( num_timesteps , param_vals = component_param_vals , initial_step = initial_step ) ) return component_ssms", "nl": "Build an ordered list of Distribution instances for component models ."}}
{"translation": {"code": "def _split_covariance_into_marginals ( covariance , block_sizes ) : start_dim = 0 marginals = [ ] for size in block_sizes : end_dim = start_dim + size marginals . append ( covariance [ ... , start_dim : end_dim , start_dim : end_dim ] ) start_dim = end_dim return marginals", "nl": "Split a covariance matrix into block - diagonal marginals of given sizes ."}}
{"translation": {"code": "def build_effects_to_residuals_matrix ( num_seasons , dtype ) : # Build the matrix that converts effects `e_i` into differences from the mean # effect `(e_i - sum(e_i)) / num_seasons`, with the mean effect in the last # row so that the transformation is invertible. effects_to_residuals_fullrank = np . eye ( num_seasons ) - 1. / num_seasons effects_to_residuals_fullrank [ - 1 , : ] = 1. / num_seasons # compute mean effect residuals_to_effects_fullrank = np . linalg . inv ( effects_to_residuals_fullrank ) # Drop the final dimension, effectively setting the mean effect to zero. effects_to_residuals = effects_to_residuals_fullrank [ : - 1 , : ] residuals_to_effects = residuals_to_effects_fullrank [ : , : - 1 ] # Return Tensor values of the specified dtype. effects_to_residuals = tf . cast ( effects_to_residuals , dtype = dtype , name = 'effects_to_residuals' ) residuals_to_effects = tf . cast ( residuals_to_effects , dtype = dtype , name = 'residuals_to_effects' ) return effects_to_residuals , residuals_to_effects", "nl": "Build change - of - basis matrices for constrained seasonal effects ."}}
{"translation": {"code": "def build_is_last_day_of_season ( num_steps_per_season ) : num_steps_per_cycle = np . sum ( num_steps_per_season ) changepoints = np . cumsum ( np . ravel ( num_steps_per_season ) ) - 1 def is_last_day_of_season ( t ) : t_ = dist_util . maybe_get_static_value ( t ) if t_ is not None : # static case step_in_cycle = t_ % num_steps_per_cycle return any ( step_in_cycle == changepoints ) else : step_in_cycle = tf . math . floormod ( t , num_steps_per_cycle ) return tf . reduce_any ( input_tensor = tf . equal ( step_in_cycle , changepoints ) ) return is_last_day_of_season", "nl": "Build utility method to compute whether the season is changing ."}}
{"translation": {"code": "def build_seasonal_transition_matrix ( num_seasons , is_last_day_of_season , dtype , basis_change_matrix = None , basis_change_matrix_inv = None ) : with tf . compat . v1 . name_scope ( 'build_seasonal_transition_matrix' ) : # If the season is changing, the transition matrix permutes the latent # state to shift all seasons up by a dimension, and sends the current # season's effect to the bottom. seasonal_permutation = np . concatenate ( [ np . arange ( 1 , num_seasons ) , [ 0 ] ] , axis = 0 ) seasonal_permutation_matrix = tf . constant ( np . eye ( num_seasons ) [ seasonal_permutation ] , dtype = dtype ) # Optionally transform the transition matrix into a reparameterized space, # enforcing the zero-sum constraint for ConstrainedSeasonalStateSpaceModel. if basis_change_matrix is not None : seasonal_permutation_matrix = tf . matmul ( basis_change_matrix , tf . matmul ( seasonal_permutation_matrix , basis_change_matrix_inv ) ) identity_matrix = tf . eye ( tf . shape ( input = seasonal_permutation_matrix ) [ - 1 ] , dtype = dtype ) def seasonal_transition_matrix ( t ) : return tf . linalg . LinearOperatorFullMatrix ( matrix = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , seasonal_permutation_matrix , identity_matrix ) ) return seasonal_transition_matrix", "nl": "Build a function computing transitions for a seasonal effect model ."}}
{"translation": {"code": "def make_innermost_getter ( getter ) : @ functools . wraps ( getter ) def _new_getter ( kernel_results , * args , * * kwargs ) : \"\"\"Wrapped getter.\"\"\" results_stack = [ ] while hasattr ( kernel_results , 'inner_results' ) : results_stack . append ( kernel_results ) kernel_results = kernel_results . inner_results return getter ( kernel_results , * args , * * kwargs ) return _new_getter", "nl": "Wraps a getter so it applies to the inner - most results in kernel_results ."}}
{"translation": {"code": "def _copy_docstring ( original_fn , new_fn ) : original_spec = tf_inspect . getfullargspec ( original_fn ) new_spec = tf_inspect . getfullargspec ( new_fn ) if original_spec != new_spec : raise ValueError ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original_spec , new_spec , original_fn ) ) @ decorator . decorator def wrap ( wrapped_fn , * args , * * kwargs ) : del wrapped_fn return new_fn ( * args , * * kwargs ) return wrap ( original_fn )", "nl": "Wraps new_fn with the doc of original_fn ."}}
{"translation": {"code": "def _prefer_static ( original_fn , static_fn ) : original_spec = tf_inspect . getfullargspec ( original_fn ) static_spec = tf_inspect . getfullargspec ( static_fn ) if original_spec != static_spec : raise ValueError ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original_spec , static_spec , original_fn ) ) @ decorator . decorator def wrap ( wrapped_fn , * args , * * kwargs ) : del wrapped_fn [ args_ , kwargs_ ] , all_static = _maybe_get_static_args ( [ args , kwargs ] ) if all_static : return static_fn ( * args_ , * * kwargs_ ) return original_fn ( * args , * * kwargs ) return wrap ( original_fn )", "nl": "Wraps original_fn preferring to call static_fn when inputs are static ."}}
{"translation": {"code": "def moments_of_masked_time_series ( time_series_tensor , broadcast_mask ) : num_unmasked_entries = tf . cast ( tf . reduce_sum ( input_tensor = tf . cast ( ~ broadcast_mask , tf . int32 ) , axis = - 1 ) , time_series_tensor . dtype ) # Manually compute mean and variance, excluding masked entries. mean = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , time_series_tensor ) , axis = - 1 ) / num_unmasked_entries ) variance = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , ( time_series_tensor - mean [ ... , tf . newaxis ] ) ** 2 ) , axis = - 1 ) / num_unmasked_entries ) return mean , variance", "nl": "Compute mean and variance accounting for a mask ."}}
{"translation": {"code": "def canonicalize_observed_time_series_with_mask ( maybe_masked_observed_time_series ) : with tf . compat . v1 . name_scope ( 'canonicalize_observed_time_series_with_mask' ) : if hasattr ( maybe_masked_observed_time_series , 'is_missing' ) : observed_time_series = ( maybe_masked_observed_time_series . time_series ) is_missing = maybe_masked_observed_time_series . is_missing else : observed_time_series = maybe_masked_observed_time_series is_missing = None observed_time_series = tf . convert_to_tensor ( value = observed_time_series , name = 'observed_time_series' ) observed_time_series = _maybe_expand_trailing_dim ( observed_time_series ) if is_missing is not None : is_missing = tf . convert_to_tensor ( value = is_missing , name = 'is_missing' , dtype_hint = tf . bool ) return missing_values_util . MaskedTimeSeries ( observed_time_series , is_missing = is_missing )", "nl": "Extract a Tensor with canonical shape and optional mask ."}}
{"translation": {"code": "def _create_input_order ( input_size , input_order = \"left-to-right\" ) : if isinstance ( input_order , six . string_types ) : if input_order == \"left-to-right\" : return np . arange ( start = 1 , stop = input_size + 1 ) elif input_order == \"right-to-left\" : return np . arange ( start = input_size , stop = 0 , step = - 1 ) elif input_order == \"random\" : ret = np . arange ( start = 1 , stop = input_size + 1 ) np . random . shuffle ( ret ) return ret elif np . all ( np . sort ( input_order ) == np . arange ( 1 , input_size + 1 ) ) : return np . array ( input_order ) raise ValueError ( \"Invalid input order: '{}'.\" . format ( input_order ) )", "nl": "Returns a degree vectors for the input ."}}
{"translation": {"code": "def _create_degrees ( input_size , hidden_units = None , input_order = \"left-to-right\" , hidden_degrees = \"equal\" ) : input_order = _create_input_order ( input_size , input_order ) degrees = [ input_order ] if hidden_units is None : hidden_units = [ ] for units in hidden_units : if isinstance ( hidden_degrees , six . string_types ) : if hidden_degrees == \"random\" : # samples from: [low, high) degrees . append ( np . random . randint ( low = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) , high = input_size , size = units ) ) elif hidden_degrees == \"equal\" : min_degree = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) degrees . append ( np . maximum ( min_degree , # Evenly divide the range `[1, input_size - 1]` in to `units + 1` # segments, and pick the boundaries between the segments as degrees. np . ceil ( np . arange ( 1 , units + 1 ) * ( input_size - 1 ) / float ( units + 1 ) ) . astype ( np . int32 ) ) ) else : raise ValueError ( 'Invalid hidden order: \"{}\".' . format ( hidden_degrees ) ) return degrees", "nl": "Returns a list of degree vectors one for each input and hidden layer ."}}
{"translation": {"code": "def one_step ( self , current_state , previous_kernel_results ) : @ tfp . mcmc . internal . util . make_innermost_setter def set_num_leapfrog_steps ( kernel_results , num_leapfrog_steps ) : return kernel_results . _replace ( accepted_results = kernel_results . accepted_results . _replace ( num_leapfrog_steps = num_leapfrog_steps ) ) step_size = previous_kernel_results . new_step_size previous_kernel_results = set_num_leapfrog_steps ( previous_kernel_results , self . _num_leapfrog_steps ( step_size ) ) new_state , kernel_results = self . _kernel . one_step ( self . _flatten_state ( current_state ) , previous_kernel_results ) return self . _unflatten_state ( new_state ) , kernel_results", "nl": "Runs one iteration of NeuTra ."}}
{"translation": {"code": "def _matmul ( a , b , transpose_a = False , transpose_b = False , adjoint_a = False , adjoint_b = False , a_is_sparse = False , b_is_sparse = False , name = None ) : # pylint: disable=unused-argument if a_is_sparse or b_is_sparse : raise NotImplementedError ( 'Numpy backend does not support sparse matmul.' ) if transpose_a or adjoint_a : a = _matrix_transpose ( a , conjugate = adjoint_a ) if transpose_b or adjoint_b : b = _matrix_transpose ( b , conjugate = adjoint_b ) return np . matmul ( a , b )", "nl": "Numpy matmul wrapper ."}}
{"translation": {"code": "def _sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : # pylint: disable=unused-argument if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) if direction == 'DESCENDING' : return np . negative ( result ) return result", "nl": "Numpy implementation of tf . sort ."}}
{"translation": {"code": "def _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : # pylint: disable=unused-argument if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )", "nl": "Numpy implementation of tf . argsort ."}}
{"translation": {"code": "def assert_rank_at_most ( x , rank , data = None , summarize = None , message = None , name = None ) : with tf . compat . v2 . name_scope ( name or 'assert_rank_at_most' ) : return tf . compat . v1 . assert_less_equal ( tf . rank ( x ) , rank , data = data , summarize = summarize , message = message )", "nl": "Assert x has rank equal to rank or smaller ."}}
{"translation": {"code": "def _kl_joint_joint ( d0 , d1 , name = None ) : if len ( d0 . _dist_fn_wrapped ) != len ( d1 . _dist_fn_wrapped ) : # pylint: disable=protected-access raise ValueError ( 'Can only compute KL divergence between when each has the' 'same number of component distributions.' ) if ( not all ( a is None for a in d0 . _dist_fn_args ) or # pylint: disable=protected-access not all ( a is None for a in d1 . _dist_fn_args ) ) : # pylint: disable=protected-access raise ValueError ( 'Can only compute KL divergence when all distributions are ' 'independent.' ) with tf . name_scope ( name or 'kl_jointseq_jointseq' ) : return sum ( kullback_leibler . kl_divergence ( d0_ ( ) , d1_ ( ) ) for d0_ , d1_ in zip ( d0 . _dist_fn_wrapped , d1 . _dist_fn_wrapped ) )", "nl": "Calculate the KL divergence between two JointDistributionSequential s ."}}
{"translation": {"code": "def _get_required_args ( fn ) : argspec = tf_inspect . getfullargspec ( fn ) args = argspec . args if tf_inspect . isclass ( fn ) : args = args [ 1 : ] # Remove the `self` arg. if argspec . defaults : # Remove the args which have defaults. By convention we only feed # *required args*. This means some distributions must always be wrapped # with a `lambda`, e.g., `lambda logits: tfd.Bernoulli(logits=logits)` # or `lambda probs: tfd.Bernoulli(probs=probs)`. args = args [ : - len ( argspec . defaults ) ] return tuple ( args )", "nl": "Returns the distribution s required args ."}}
{"translation": {"code": "def _make_summary_statistic ( attr ) : def _fn ( self ) : if any ( self . _dist_fn_args ) : # pylint: disable=protected-access raise ValueError ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . _unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . _dist_fn_wrapped ) # pylint: disable=protected-access return _fn", "nl": "Factory for making summary statistics eg mean mode stddev ."}}
{"translation": {"code": "def _unify_call_signature ( i , dist_fn ) : if distribution_util . is_distribution_instance ( dist_fn ) : return ( lambda * _ : dist_fn ) , None if not callable ( dist_fn ) : raise TypeError ( '{} must be either `tfd.Distribution`-like or ' '`callable`.' . format ( dist_fn ) ) args = _get_required_args ( dist_fn ) if not args : return ( lambda * _ : dist_fn ( ) ) , ( ) @ functools . wraps ( dist_fn ) def dist_fn_wrapped ( * xs ) : \"\"\"Calls `dist_fn` with reversed and truncated args.\"\"\" if i != len ( xs ) : raise ValueError ( 'Internal Error: Unexpected number of inputs provided to {}-th ' 'distribution maker (dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , i , len ( xs ) ) ) if len ( xs ) < len ( args ) : raise ValueError ( 'Internal Error: Too few inputs provided to {}-th distribution maker ' '(dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , len ( args ) , len ( xs ) ) ) return dist_fn ( * reversed ( xs [ - len ( args ) : ] ) ) return dist_fn_wrapped , args", "nl": "Creates dist_fn_wrapped which calls dist_fn with all prev nodes ."}}
{"translation": {"code": "def maybe_broadcast_structure ( from_structure : Any , to_structure : Any ) -> Any : flat_from = tf . nest . flatten ( from_structure ) flat_to = tf . nest . flatten ( to_structure ) if len ( flat_from ) == 1 : flat_from *= len ( flat_to ) return tf . nest . pack_sequence_as ( to_structure , flat_from )", "nl": "Maybe broadcasts from_structure to to_structure ."}}
{"translation": {"code": "def sign_adaptation ( control : FloatNest , output : FloatTensor , set_point : FloatTensor , adaptation_rate : FloatTensor = 0.01 ) -> FloatNest : def _get_new_control ( control , output , set_point ) : new_control = mcmc_util . choose ( output > set_point , control * ( 1. + adaptation_rate ) , control / ( 1. + adaptation_rate ) ) return new_control output = maybe_broadcast_structure ( output , control ) set_point = maybe_broadcast_structure ( set_point , control ) return tf . nest . map_structure ( _get_new_control , control , output , set_point )", "nl": "A function to do simple sign - based control of a variable ."}}
{"translation": {"code": "def hamiltonian_monte_carlo ( hmc_state : HamiltonianMonteCarloState , target_log_prob_fn : PotentialFn , step_size : Any , num_leapfrog_steps : IntTensor , momentum : State = None , kinetic_energy_fn : PotentialFn = None , momentum_sample_fn : MomentumSampleFn = None , leapfrog_trace_fn : Callable [ [ LeapFrogStepState , LeapFrogStepExtras ] , TensorNest ] = lambda * args : ( ) , seed = None , ) -> Tuple [ HamiltonianMonteCarloState , HamiltonianMonteCarloExtra ] : state = hmc_state . state state_grads = hmc_state . state_grads target_log_prob = hmc_state . target_log_prob state_extra = hmc_state . state_extra if kinetic_energy_fn is None : # pylint: disable=function-redefined def kinetic_energy_fn ( * momentum ) : return tf . add_n ( [ tf . reduce_sum ( input_tensor = tf . square ( x ) , axis = - 1 ) / 2. for x in tf . nest . flatten ( momentum ) ] ) , ( ) if momentum_sample_fn is None : # pylint: disable=function-redefined def momentum_sample_fn ( * momentum ) : ret = tf . nest . map_structure ( lambda x : tf . random . normal ( tf . shape ( input = x ) , dtype = x . dtype ) , momentum ) if len ( ret ) == 1 : return ret [ 0 ] else : return ret if momentum is None : momentum = call_fn ( momentum_sample_fn , tf . nest . map_structure ( tf . zeros_like , state ) ) if target_log_prob is None : target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) kinetic_energy , _ = call_fn ( kinetic_energy_fn , momentum ) current_energy = - target_log_prob + kinetic_energy current_state = HamiltonianMonteCarloState ( state = state , state_grads = state_grads , state_extra = state_extra , target_log_prob = target_log_prob ) def leapfrog_wrapper ( leapfrog_state , target_log_prob , state_extra ) : \"\"\"Leapfrog wrapper that tracks extra state.\"\"\" del target_log_prob del state_extra leapfrog_state , leapfrog_extra = leapfrog_step ( leapfrog_state , step_size = step_size , target_log_prob_fn = target_log_prob_fn , kinetic_energy_fn = kinetic_energy_fn ) return [ leapfrog_state , leapfrog_extra . target_log_prob , leapfrog_extra . state_extra ] , leapfrog_extra def leapfrog_trace_wrapper_fn ( args , leapfrog_extra ) : return leapfrog_trace_fn ( args [ 0 ] , leapfrog_extra ) leapfrog_wrapper_state = ( LeapFrogStepState ( state , state_grads , momentum ) , target_log_prob , state_extra ) [ [ leapfrog_state , target_log_prob , state_extra ] , _ ] , leapfrog_trace = trace ( leapfrog_wrapper_state , leapfrog_wrapper , num_leapfrog_steps , trace_fn = leapfrog_trace_wrapper_fn ) kinetic_energy , _ = call_fn ( kinetic_energy_fn , leapfrog_state . momentum ) proposed_energy = - target_log_prob + kinetic_energy proposed_state = HamiltonianMonteCarloState ( state = leapfrog_state . state , state_grads = leapfrog_state . state_grads , target_log_prob = target_log_prob , state_extra = state_extra ) energy_change = proposed_energy - current_energy hmc_state , is_accepted , _ = metropolis_hastings_step ( current_state , proposed_state , energy_change , seed = seed ) hmc_state = hmc_state # type: HamiltonianMonteCarloState return hmc_state , HamiltonianMonteCarloExtra ( is_accepted = is_accepted , proposed_hmc_state = proposed_state , log_accept_ratio = - energy_change , leapfrog_trace = leapfrog_trace )", "nl": "Hamiltonian Monte Carlo TransitionOperator ."}}
{"translation": {"code": "def leapfrog_step ( leapfrog_step_state : LeapFrogStepState , step_size : FloatTensor , target_log_prob_fn : PotentialFn , kinetic_energy_fn : PotentialFn ) -> Tuple [ LeapFrogStepState , LeapFrogStepExtras ] : state = leapfrog_step_state . state state_grads = leapfrog_step_state . state_grads momentum = leapfrog_step_state . momentum step_size = maybe_broadcast_structure ( step_size , state ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) momentum = tf . nest . map_structure ( tf . convert_to_tensor , momentum ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) if state_grads is None : _ , _ , state_grads = call_and_grads ( target_log_prob_fn , state ) else : state_grads = tf . nest . map_structure ( tf . convert_to_tensor , state_grads ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) kinetic_energy , kinetic_energy_extra , momentum_grads = call_and_grads ( kinetic_energy_fn , momentum ) state = tf . nest . map_structure ( lambda x , mg , s : x + mg * s , state , momentum_grads , step_size ) target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) return LeapFrogStepState ( state , state_grads , momentum ) , LeapFrogStepExtras ( target_log_prob , state_extra , kinetic_energy , kinetic_energy_extra )", "nl": "Leapfrog TransitionOperator ."}}
{"translation": {"code": "def call_and_grads ( fn : TransitionOperator , args : Union [ Tuple [ Any ] , Any ] ) -> Tuple [ tf . Tensor , TensorNest , TensorNest ] : with tf . GradientTape ( ) as tape : tape . watch ( args ) ret , extra = call_fn ( fn , args ) grads = tape . gradient ( ret , args ) return ret , extra , grads", "nl": "Calls fn and returns the gradients with respect to fn s first output ."}}
{"translation": {"code": "def _zero_dimensional_mvndiag ( dtype ) : dummy_mvndiag = tfd . MultivariateNormalDiag ( scale_diag = tf . ones ( [ 0 ] , dtype = dtype ) ) dummy_mvndiag . covariance = lambda : dummy_mvndiag . variance ( ) [ ... , tf . newaxis ] return dummy_mvndiag", "nl": "Build a zero - dimensional MVNDiag object ."}}
{"translation": {"code": "def params_to_weights ( self , global_scale_variance , global_scale_noncentered , local_scale_variances , local_scales_noncentered , weights_noncentered ) : global_scale = ( global_scale_noncentered * tf . sqrt ( global_scale_variance ) * self . weights_prior_scale ) local_scales = local_scales_noncentered * tf . sqrt ( local_scale_variances ) return weights_noncentered * local_scales * global_scale [ ... , tf . newaxis ]", "nl": "Build regression weights from model parameters ."}}
{"translation": {"code": "def size ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize", "nl": "Returns the number of bytes to represent this dtype ."}}
{"translation": {"code": "def max ( dtype ) : # pylint: disable=redefined-builtin dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'max' ) : return dtype . max use_finfo = is_floating ( dtype ) or is_complex ( dtype ) return np . finfo ( dtype ) . max if use_finfo else np . iinfo ( dtype ) . max", "nl": "Returns the maximum representable value in this data type ."}}
{"translation": {"code": "def is_complex ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_complex' ) : return dtype . is_complex return np . issubdtype ( np . dtype ( dtype ) , np . complex )", "nl": "Returns whether this is a complex floating point type ."}}
{"translation": {"code": "def name ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'name' ) : return dtype . name if hasattr ( dtype , '__name__' ) : return dtype . __name__ return str ( dtype )", "nl": "Returns the string name for this dtype ."}}
{"translation": {"code": "def concatenate ( x , other ) : return type ( x ) ( tf . TensorShape ( x ) . concatenate ( other ) )", "nl": "Returns the concatenation of the dimension in x and other ."}}
{"translation": {"code": "def dims ( x ) : if isinstance ( x , tf . TensorShape ) : return x . dims r = tf . TensorShape ( x ) . dims return None if r is None else list ( map ( tf . compat . dimension_value , r ) )", "nl": "Returns a list of dimension sizes or None if rank is unknown ."}}
{"translation": {"code": "def maybe_get_common_dtype ( arg_list ) : # Note that `all` defaults to `True` if `arg_list` is empty. if all ( a is None for a in arg_list ) : return None return dtype_util . common_dtype ( arg_list , tf . float32 )", "nl": "Return common dtype of arg_list or None ."}}
{"translation": {"code": "def _max_mask_non_finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) needs_masking = ~ np . isfinite ( m ) if needs_masking . ndim > 0 : m [ needs_masking ] = mask elif needs_masking : m = mask return m", "nl": "Returns max or mask if max is not finite ."}}
{"translation": {"code": "def _assert_same_base_type ( items , expected_type = None ) : original_expected_type = expected_type mismatch = False for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type elif expected_type != item_type : mismatch = True break if mismatch : # Loop back through and build up an informative error message (this is very # slow, so we don't do it unless we found an error above). expected_type = original_expected_type original_item_str = None get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type original_item_str = get_name ( item ) elif expected_type != item_type : raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) return expected_type # Should be unreachable else : return expected_type", "nl": "r Asserts all items are of the same base type ."}}
{"translation": {"code": "def _validate_observation_data ( kernel , observation_index_points , observations ) : # Check that observation index points and observation counts broadcast. ndims = kernel . feature_ndims if ( tensorshape_util . is_fully_defined ( observation_index_points . shape [ : - ndims ] ) and tensorshape_util . is_fully_defined ( observations . shape ) ) : index_point_count = observation_index_points . shape [ : - ndims ] observation_count = observations . shape try : tf . broadcast_static_shape ( index_point_count , observation_count ) except ValueError : # Re-raise with our own more contextual error message. raise ValueError ( 'Observation index point and observation counts are not ' 'broadcastable: {} and {}, respectively.' . format ( index_point_count , observation_count ) )", "nl": "Ensure that observation data and locations have consistent shapes ."}}
{"translation": {"code": "def _is_empty_observation_data ( feature_ndims , observation_index_points , observations ) : # If both input locations and observations are `None`, we consider this # \"empty\" observation data. if observation_index_points is None and observations is None : return True num_obs = tf . compat . dimension_value ( observation_index_points . shape [ - ( feature_ndims + 1 ) ] ) if num_obs is not None and num_obs == 0 : return True return False", "nl": "Returns True if given observation data is empty ."}}
{"translation": {"code": "def _build ( self , model ) : if not _is_dict_like ( model ) : raise TypeError ( '`model` must be convertible to `dict` (saw: {}).' . format ( type ( model ) . __name__ ) ) [ self . _dist_fn , self . _dist_fn_wrapped , self . _dist_fn_args , self . _dist_fn_name , # JointDistributionSequential doesn't have this. ] = _prob_chain_rule_flatten ( model )", "nl": "Creates dist_fn dist_fn_wrapped dist_fn_args dist_fn_name ."}}
{"translation": {"code": "def _depth ( g ) : def _explore ( v ) : if v . depth < 0 : v . depth = ( ( 1 + max ( [ - 1 ] + [ _explore ( annotated_graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) return v . depth annotated_graph = { k : _Node ( k , v ) for k , v in g . items ( ) } for v in annotated_graph . values ( ) : _explore ( v ) return annotated_graph", "nl": "Computes the number of edges on longest path from node to root ."}}
{"translation": {"code": "def broadcast_structure ( to_structure , from_structure ) : from_parts = tf . nest . flatten ( from_structure ) if len ( from_parts ) == 1 : from_structure = tf . nest . map_structure ( lambda _ : from_parts [ 0 ] , to_structure ) return from_structure", "nl": "Broadcasts from_structure to to_structure ."}}
{"translation": {"code": "def _recursively_replace_dict_for_pretty_dict ( x ) : # We use \"PrettyDict\" because collections.OrderedDict repr/str has the word # \"OrderedDict\" in it. We only want to print \"OrderedDict\" if in fact the # input really is an OrderedDict. if isinstance ( x , dict ) : return _PrettyDict ( { k : _recursively_replace_dict_for_pretty_dict ( v ) for k , v in x . items ( ) } ) if ( isinstance ( x , collections . Sequence ) and not isinstance ( x , six . string_types ) ) : args = ( _recursively_replace_dict_for_pretty_dict ( x_ ) for x_ in x ) is_named_tuple = ( isinstance ( x , tuple ) and hasattr ( x , \"_asdict\" ) and hasattr ( x , \"_fields\" ) ) return type ( x ) ( * args ) if is_named_tuple else type ( x ) ( args ) if isinstance ( x , collections . Mapping ) : return type ( x ) ( * * { k : _recursively_replace_dict_for_pretty_dict ( v ) for k , v in x . items ( ) } ) return x", "nl": "Recursively replace dict s with _PrettyDict ."}}
{"translation": {"code": "def _maybe_validate_args ( outcomes , logits , probs , validate_args ) : assertions = [ ] def validate_equal_last_dim ( tensor_a , tensor_b , message ) : if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : if tensor_a . shape [ - 1 ] != tensor_b . shape [ - 1 ] : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - 1 ] , tf . shape ( input = tensor_b ) [ - 1 ] , message = message ) ) if logits is not None : validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) if probs is not None : validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) message = 'Rank of outcomes must be 1.' if outcomes . shape . ndims is not None : if outcomes . shape . ndims != 1 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) message = 'Size of outcomes must be greater than 0.' if outcomes . shape . num_elements ( ) is not None : if outcomes . shape . num_elements ( ) == 0 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) if validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) return assertions", "nl": "Validate outcomes logits and probs s shapes ."}}