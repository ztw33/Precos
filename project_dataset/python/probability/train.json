{"translation": {"code": "def _get_samples ( dist , z , n , seed ) : with tf . compat . v1 . name_scope ( 'get_samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise ValueError ( 'Must specify exactly one of arguments \"n\" and \"z\".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert_to_tensor ( value = z , name = 'z' )", "nl": "Check args and return samples ."}}
{"translation": {"code": "def multivariate_normal_tril ( x , dims , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = tril_with_diag_softplus_and_shift , name = None ) : with tf . compat . v1 . name_scope ( name , 'multivariate_normal_tril' , [ x , dims ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) x = layer_fn ( x , dims + dims * ( dims + 1 ) // 2 ) return tfd . MultivariateNormalTriL ( loc = loc_fn ( x [ ... , : dims ] ) , scale_tril = scale_fn ( x [ ... , dims : ] ) )", "nl": "Constructs a trainable tfd . MultivariateNormalTriL distribution ."}}
{"translation": {"code": "def bernoulli ( x , layer_fn = tf . compat . v1 . layers . dense , name = None ) : with tf . compat . v1 . name_scope ( name , 'bernoulli' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) logits = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) return tfd . Bernoulli ( logits = logits )", "nl": "Constructs a trainable tfd . Bernoulli distribution ."}}
{"translation": {"code": "def triangular ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , \"triangular\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) return pearson ( logu ) / ( 1. + tf . exp ( logu ) )", "nl": "The Triangular Csiszar - function in log - space ."}}
{"translation": {"code": "def squared_hellinger ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , \"squared_hellinger\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) return pearson ( 0.5 * logu )", "nl": "The Squared - Hellinger Csiszar - function in log - space ."}}
{"translation": {"code": "def jensen_shannon ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , \"jensen_shannon\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) npdt = logu . dtype . as_numpy_dtype y = tf . nn . softplus ( logu ) if self_normalized : y -= np . log ( 2 ) . astype ( npdt ) return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y", "nl": "The Jensen - Shannon Csiszar - function in log - space ."}}
{"translation": {"code": "def t_power ( logu , t , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , \"t_power\" , [ logu , t ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = \"t\" ) fu = tf . math . expm1 ( t * logu ) if self_normalized : fu -= t * tf . math . expm1 ( logu ) fu *= tf . where ( tf . logical_and ( 0. < t , t < 1. ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) return fu", "nl": "The T - Power Csiszar - function in log - space ."}}
{"translation": {"code": "def log1p_abs ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , \"log1p_abs\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) return tf . math . expm1 ( tf . abs ( logu ) )", "nl": "The log1p - abs Csiszar - function in log - space ."}}
{"translation": {"code": "def jeffreys ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , \"jeffreys\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) return 0.5 * tf . math . expm1 ( logu ) * logu", "nl": "The Jeffreys Csiszar - function in log - space ."}}
{"translation": {"code": "def modified_gan ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , \"chi_square\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) y = tf . nn . softplus ( logu ) - logu if self_normalized : y += 0.5 * tf . math . expm1 ( logu ) return y", "nl": "The Modified - GAN Csiszar - function in log - space ."}}
{"translation": {"code": "def dual_csiszar_function ( logu , csiszar_function , name = None ) : with tf . compat . v1 . name_scope ( name , \"dual_csiszar_function\" , [ logu ] ) : return tf . exp ( logu ) * csiszar_function ( - logu )", "nl": "Calculates the dual Csiszar - function in log - space ."}}
{"translation": {"code": "def monte_carlo_csiszar_f_divergence ( f , p_log_prob , q , num_draws , use_reparametrization = None , seed = None , name = None ) : reparameterization_types = tf . nest . flatten ( q . reparameterization_type ) with tf . compat . v1 . name_scope ( name , \"monte_carlo_csiszar_f_divergence\" , [ num_draws ] ) : if use_reparametrization is None : use_reparametrization = all ( reparameterization_type == tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) elif ( use_reparametrization and any ( reparameterization_type != tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) ) : # TODO(jvdillon): Consider only raising an exception if the gradient is # requested. raise ValueError ( \"Distribution `q` must be reparameterized, i.e., a diffeomorphic \" \"transformation of a parameterless distribution. (Otherwise this \" \"function has a biased gradient.)\" ) if not callable ( p_log_prob ) : raise TypeError ( \"`p_log_prob` must be a Python `callable` function.\" ) return monte_carlo . expectation ( f = lambda q_samples : f ( p_log_prob ( q_samples ) - q . log_prob ( q_samples ) ) , samples = q . sample ( num_draws , seed = seed ) , log_prob = q . log_prob , # Only used if use_reparametrization=False. use_reparametrization = use_reparametrization )", "nl": "Monte - Carlo approximation of the Csiszar f - Divergence ."}}
{"translation": {"code": "def kl_reverse ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , \"kl_reverse\" , [ logu ] ) : return amari_alpha ( logu , alpha = 0. , self_normalized = self_normalized )", "nl": "The reverse Kullback - Leibler Csiszar - function in log - space ."}}
{"translation": {"code": "def amari_alpha ( logu , alpha = 1. , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , \"amari_alpha\" , [ logu ] ) : if alpha is None or tf . is_tensor ( alpha ) : raise TypeError ( \"`alpha` cannot be `None` or `Tensor` type.\" ) if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : raise TypeError ( \"`self_normalized` cannot be `None` or `Tensor` type.\" ) logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) if alpha == 0. : f = - logu elif alpha == 1. : f = tf . exp ( logu ) * logu else : f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) if not self_normalized : return f if alpha == 0. : return f + tf . math . expm1 ( logu ) elif alpha == 1. : return f - tf . math . expm1 ( logu ) else : return f - tf . math . expm1 ( logu ) / ( alpha - 1. )", "nl": "The Amari - alpha Csiszar - function in log - space ."}}
{"translation": {"code": "def csiszar_vimco_helper ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , \"csiszar_vimco_helper\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( 1 ) [ 0 ] ) if n is None : n = tf . shape ( input = logu ) [ 0 ] log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) nm1 = tf . cast ( n - 1 , dtype = logu . dtype ) else : log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) nm1 = np . asarray ( n - 1 , dtype = logu . dtype . as_numpy_dtype ) # Throughout we reduce across axis=0 since this is presumed to be iid # samples. log_max_u = tf . reduce_max ( input_tensor = logu , axis = 0 ) log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = 0 ) # log_loosum_u[i] = # = logsumexp(logu[j] : j != i) # = log( exp(logsumexp(logu)) - exp(logu[i]) ) # = log( exp(logsumexp(logu - logu[i])) exp(logu[i])  - exp(logu[i])) # = logu[i] + log(exp(logsumexp(logu - logu[i])) - 1) # = logu[i] + log(exp(logsumexp(logu) - logu[i]) - 1) # = logu[i] + softplus_inverse(logsumexp(logu) - logu[i]) d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) # We use `d != 0` rather than `d > 0.` because `d < 0.` should never # happens; if it does we want to complain loudly (which `softplus_inverse` # will). d_ok = tf . not_equal ( d , 0. ) safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) d_ok_result = logu + tfd . softplus_inverse ( safe_d ) inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) # When not(d_ok) and is_positive_and_largest then we manually compute the # log_loosum_u. (We can efficiently do this for any one point but not all, # hence we still need the above calculation.) This is good because when # this condition is met, we cannot use the above calculation; its -inf. # We now compute the log-leave-out-max-sum, replicate it to every # point and make sure to select it only when we need to. is_positive_and_largest = tf . logical_and ( logu > 0. , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( input = logu ) , - inf ) , logu ) , axis = 0 , keepdims = True ) log_lomsum_u = tf . tile ( log_lomsum_u , multiples = 1 + tf . pad ( tensor = [ n - 1 ] , paddings = [ [ 0 , tf . rank ( logu ) - 1 ] ] ) ) d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( input = d ) , - inf ) ) log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) # The swap-one-out-sum (\"soosum\") is n different sums, each of which # replaces the i-th item with the i-th-left-out average, i.e., # soo_sum_u[i] = [exp(logu) - exp(logu[i])] + exp(mean(logu[!=i])) #              =  exp(log_loosum_u[i])      + exp(looavg_logu[i]) looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = 0 ) - logu ) / nm1 log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = 0 ) log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n log_sooavg_u = log_soosum_u - log_n log_avg_u . set_shape ( logu . shape . with_rank_at_least ( 1 ) [ 1 : ] ) log_sooavg_u . set_shape ( logu . shape ) return log_avg_u , log_sooavg_u", "nl": "Helper to csiszar_vimco ; computes log_avg_u log_sooavg_u ."}}
{"translation": {"code": "def symmetrized_csiszar_function ( logu , csiszar_function , name = None ) : with tf . compat . v1 . name_scope ( name , \"symmetrized_csiszar_function\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) return 0.5 * ( csiszar_function ( logu ) + dual_csiszar_function ( logu , csiszar_function ) )", "nl": "Symmetrizes a Csiszar - function in log - space ."}}
{"translation": {"code": "def pearson ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , \"pearson\" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = \"logu\" ) return tf . square ( tf . math . expm1 ( logu ) )", "nl": "The Pearson Csiszar - function in log - space ."}}
{"translation": {"code": "def _choose_base_case ( is_accepted , accepted , rejected , name = None ) : def _expand_is_accepted_like ( x ) : \"\"\"Helper to expand `is_accepted` like the shape of some input arg.\"\"\" with tf . compat . v1 . name_scope ( 'expand_is_accepted_like' ) : expand_shape = tf . concat ( [ tf . shape ( input = is_accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is_accepted ) : ] , ] , axis = 0 ) m = tf . tile ( tf . reshape ( is_accepted , expand_shape ) , multiples ) m . set_shape ( m . shape . merge_with ( x . shape ) ) return m def _where ( accepted , rejected ) : if accepted is rejected : return accepted accepted = tf . convert_to_tensor ( value = accepted , name = 'accepted' ) rejected = tf . convert_to_tensor ( value = rejected , name = 'rejected' ) r = tf . where ( _expand_is_accepted_like ( accepted ) , accepted , rejected ) r . set_shape ( r . shape . merge_with ( accepted . shape . merge_with ( rejected . shape ) ) ) return r with tf . compat . v1 . name_scope ( name , 'choose' , values = [ is_accepted , accepted , rejected ] ) : if not is_list_like ( accepted ) : return _where ( accepted , rejected ) return [ ( choose ( is_accepted , a , r , name = name ) if is_namedtuple_like ( a ) else _where ( a , r ) ) for a , r in zip ( accepted , rejected ) ]", "nl": "Helper to choose which expand_dims is_accepted and applies tf . where ."}}
{"translation": {"code": "def sample_chain ( num_results , current_state , previous_kernel_results = None , kernel = None , num_burnin_steps = 0 , num_steps_between_results = 0 , trace_fn = lambda current_state , kernel_results : kernel_results , return_final_kernel_results = False , parallel_iterations = 10 , name = None , ) : if not kernel . is_calibrated : warnings . warn ( \"supplied `TransitionKernel` is not calibrated. Markov \" \"chain may not converge to intended target distribution.\" ) with tf . compat . v1 . name_scope ( name , \"mcmc_sample_chain\" , [ num_results , num_burnin_steps , num_steps_between_results ] ) : num_results = tf . convert_to_tensor ( value = num_results , dtype = tf . int32 , name = \"num_results\" ) num_burnin_steps = tf . convert_to_tensor ( value = num_burnin_steps , dtype = tf . int32 , name = \"num_burnin_steps\" ) num_steps_between_results = tf . convert_to_tensor ( value = num_steps_between_results , dtype = tf . int32 , name = \"num_steps_between_results\" ) current_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = \"current_state\" ) , current_state ) if previous_kernel_results is None : previous_kernel_results = kernel . bootstrap_results ( current_state ) if trace_fn is None : # It simplifies the logic to use a dummy function here. trace_fn = lambda * args : ( ) no_trace = True else : no_trace = False if trace_fn is sample_chain . __defaults__ [ 4 ] : warnings . warn ( \"Tracing all kernel results by default is deprecated. Set \" \"the `trace_fn` argument to None (the future default \" \"value) or an explicit callback that traces the values \" \"you are interested in.\" ) def _trace_scan_fn ( state_and_results , num_steps ) : next_state , current_kernel_results = mcmc_util . smart_for_loop ( loop_num_iter = num_steps , body_fn = kernel . one_step , initial_loop_vars = list ( state_and_results ) , parallel_iterations = parallel_iterations ) return next_state , current_kernel_results ( _ , final_kernel_results ) , ( all_states , trace ) = mcmc_util . trace_scan ( loop_fn = _trace_scan_fn , initial_state = ( current_state , previous_kernel_results ) , elems = tf . one_hot ( indices = 0 , depth = num_results , on_value = 1 + num_burnin_steps , off_value = 1 + num_steps_between_results , dtype = tf . int32 ) , # pylint: disable=g-long-lambda trace_fn = lambda state_and_results : ( state_and_results [ 0 ] , trace_fn ( * state_and_results ) ) , # pylint: enable=g-long-lambda parallel_iterations = parallel_iterations ) if return_final_kernel_results : return CheckpointableStatesAndTrace ( all_states = all_states , trace = trace , final_kernel_results = final_kernel_results ) else : if no_trace : return all_states else : return StatesAndTrace ( all_states = all_states , trace = trace )", "nl": "Implements Markov chain Monte Carlo via repeated TransitionKernel steps ."}}
{"translation": {"code": "def _leapfrog_integrator_one_step ( target_log_prob_fn , independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts , state_gradients_are_stopped = False , name = None ) : # Note on per-variable step sizes: # # Using per-variable step sizes is equivalent to using the same step # size for all variables and adding a diagonal mass matrix in the # kinetic energy term of the Hamiltonian being integrated. This is # hinted at by Neal (2011) but not derived in detail there. # # Let x and v be position and momentum variables respectively. # Let g(x) be the gradient of `target_log_prob_fn(x)`. # Let S be a diagonal matrix of per-variable step sizes. # Let the Hamiltonian H(x, v) = -target_log_prob_fn(x) + 0.5 * ||v||**2. # # Using per-variable step sizes gives the updates # v'  = v  + 0.5 * matmul(S, g(x)) # x'' = x  + matmul(S, v') # v'' = v' + 0.5 * matmul(S, g(x'')) # # Let u = matmul(inv(S), v). # Multiplying v by inv(S) in the updates above gives the transformed dynamics # u'  = matmul(inv(S), v')  = matmul(inv(S), v) + 0.5 * g(x) #                           = u + 0.5 * g(x) # x'' = x + matmul(S, v') = x + matmul(S**2, u') # u'' = matmul(inv(S), v'') = matmul(inv(S), v') + 0.5 * g(x'') #                           = u' + 0.5 * g(x'') # # These are exactly the leapfrog updates for the Hamiltonian # H'(x, u) = -target_log_prob_fn(x) + 0.5 * u^T S**2 u #          = -target_log_prob_fn(x) + 0.5 * ||v||**2 = H(x, v). # # To summarize: # # * Using per-variable step sizes implicitly simulates the dynamics #   of the Hamiltonian H' (which are energy-conserving in H'). We #   keep track of v instead of u, but the underlying dynamics are #   the same if we transform back. # * The value of the Hamiltonian H'(x, u) is the same as the value #   of the original Hamiltonian H(x, v) after we transform back from #   u to v. # * Sampling v ~ N(0, I) is equivalent to sampling u ~ N(0, S**-2). # # So using per-variable step sizes in HMC will give results that are # exactly identical to explicitly using a diagonal mass matrix. with tf . compat . v1 . name_scope ( name , 'hmc_leapfrog_integrator_one_step' , [ independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts ] ) : # Step 1: Update momentum. proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( current_momentum_parts , step_sizes , current_target_log_prob_grad_parts ) ] # Step 2: Update state. proposed_state_parts = [ x + tf . cast ( eps , v . dtype ) * v for x , eps , v in zip ( current_state_parts , step_sizes , proposed_momentum_parts ) ] if state_gradients_are_stopped : proposed_state_parts = [ tf . stop_gradient ( x ) for x in proposed_state_parts ] # Step 3a: Re-evaluate target-log-prob (and grad) at proposed state. [ proposed_target_log_prob , proposed_target_log_prob_grad_parts , ] = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , proposed_state_parts ) if not proposed_target_log_prob . dtype . is_floating : raise TypeError ( '`target_log_prob_fn` must produce a `Tensor` ' 'with `float` `dtype`.' ) if any ( g is None for g in proposed_target_log_prob_grad_parts ) : raise ValueError ( 'Encountered `None` gradient. Does your target `target_log_prob_fn` ' 'access all `tf.Variable`s via `tf.get_variable`?\\n' '  current_state_parts: {}\\n' '  proposed_state_parts: {}\\n' '  proposed_target_log_prob_grad_parts: {}' . format ( current_state_parts , proposed_state_parts , proposed_target_log_prob_grad_parts ) ) # Step 3b: Update momentum (again). proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( proposed_momentum_parts , step_sizes , proposed_target_log_prob_grad_parts ) ] return [ proposed_momentum_parts , proposed_state_parts , proposed_target_log_prob , proposed_target_log_prob_grad_parts , ]", "nl": "Applies num_leapfrog_steps of the leapfrog integrator ."}}
{"translation": {"code": "def safe_sum ( x , alt_value = - np . inf , name = None ) : with tf . compat . v1 . name_scope ( name , 'safe_sum' , [ x , alt_value ] ) : if not is_list_like ( x ) : raise TypeError ( 'Expected list input.' ) if not x : raise ValueError ( 'Input should not be empty.' ) in_shape = x [ 0 ] . shape x = tf . stack ( x , axis = - 1 ) x = tf . reduce_sum ( input_tensor = x , axis = - 1 ) alt_value = np . array ( alt_value , x . dtype . as_numpy_dtype ) alt_fill = tf . fill ( tf . shape ( input = x ) , value = alt_value ) x = tf . where ( tf . math . is_finite ( x ) , x , alt_fill ) x . set_shape ( x . shape . merge_with ( in_shape ) ) return x", "nl": "Elementwise adds list members replacing non - finite results with alt_value ."}}
{"translation": {"code": "def one_step ( self , current_state , previous_kernel_results ) : previous_step_size_assign = ( [ ] if self . step_size_update_fn is None else ( previous_kernel_results . extra . step_size_assign if mcmc_util . is_list_like ( previous_kernel_results . extra . step_size_assign ) else [ previous_kernel_results . extra . step_size_assign ] ) ) with tf . control_dependencies ( previous_step_size_assign ) : next_state , kernel_results = self . _impl . one_step ( current_state , previous_kernel_results ) if self . step_size_update_fn is not None : step_size_assign = self . step_size_update_fn ( # pylint: disable=not-callable self . step_size , kernel_results ) kernel_results = kernel_results . _replace ( extra = HamiltonianMonteCarloExtraKernelResults ( step_size_assign = step_size_assign ) ) return next_state , kernel_results", "nl": "Runs one iteration of Hamiltonian Monte Carlo ."}}
{"translation": {"code": "def _compute_log_acceptance_correction ( current_momentums , proposed_momentums , independent_chain_ndims , name = None ) : with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ independent_chain_ndims , current_momentums , proposed_momentums ] ) : log_current_kinetic , log_proposed_kinetic = [ ] , [ ] for current_momentum , proposed_momentum in zip ( current_momentums , proposed_momentums ) : axis = tf . range ( independent_chain_ndims , tf . rank ( current_momentum ) ) log_current_kinetic . append ( _log_sum_sq ( current_momentum , axis ) ) log_proposed_kinetic . append ( _log_sum_sq ( proposed_momentum , axis ) ) current_kinetic = 0.5 * tf . exp ( tf . reduce_logsumexp ( input_tensor = tf . stack ( log_current_kinetic , axis = - 1 ) , axis = - 1 ) ) proposed_kinetic = 0.5 * tf . exp ( tf . reduce_logsumexp ( input_tensor = tf . stack ( log_proposed_kinetic , axis = - 1 ) , axis = - 1 ) ) return mcmc_util . safe_sum ( [ current_kinetic , - proposed_kinetic ] )", "nl": "Helper to kernel which computes the log acceptance - correction ."}}
{"translation": {"code": "def _broadcast_maybelist_arg ( states , secondary_arg , name ) : if _is_list_like ( secondary_arg ) : if len ( secondary_arg ) != len ( states ) : raise ValueError ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) else : secondary_arg = [ secondary_arg ] * len ( states ) return secondary_arg", "nl": "Broadcast a listable secondary_arg to that of states ."}}
{"translation": {"code": "def _potential_scale_reduction_single_state ( state , independent_chain_ndims ) : with tf . compat . v1 . name_scope ( 'potential_scale_reduction_single_state' , values = [ state , independent_chain_ndims ] ) : # We assume exactly one leading dimension indexes e.g. correlated samples # from each Markov chain. state = tf . convert_to_tensor ( value = state , name = 'state' ) sample_ndims = 1 sample_axis = tf . range ( 0 , sample_ndims ) chain_axis = tf . range ( sample_ndims , sample_ndims + independent_chain_ndims ) sample_and_chain_axis = tf . range ( 0 , sample_ndims + independent_chain_ndims ) n = _axis_size ( state , sample_axis ) m = _axis_size ( state , chain_axis ) # In the language of Brooks and Gelman (1998), # B / n is the between chain variance, the variance of the chain means. # W is the within sequence variance, the mean of the chain variances. b_div_n = _reduce_variance ( tf . reduce_mean ( input_tensor = state , axis = sample_axis , keepdims = True ) , sample_and_chain_axis , biased = False ) w = tf . reduce_mean ( input_tensor = _reduce_variance ( state , sample_axis , keepdims = True , biased = True ) , axis = sample_and_chain_axis ) # sigma^2_+ is an estimate of the true variance, which would be unbiased if # each chain was drawn from the target.  c.f. \"law of total variance.\" sigma_2_plus = w + b_div_n return ( ( m + 1. ) / m ) * sigma_2_plus / w - ( n - 1. ) / ( m * n )", "nl": "potential_scale_reduction for one single state Tensor ."}}
{"translation": {"code": "def _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : states = tf . convert_to_tensor ( value = states , name = 'states' ) dt = states . dtype # filter_beyond_lag == None ==> auto_corr is the full sequence. auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) if filter_threshold is not None : filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) # Get a binary mask to zero out values of auto_corr below the threshold. #   mask[i, ...] = 1 if auto_corr[j, ...] > threshold for all j <= i, #   mask[i, ...] = 0, otherwise. # So, along dimension zero, the mask will look like [1, 1, ..., 0, 0,...] # Building step by step, #   Assume auto_corr = [1, 0.5, 0.0, 0.3], and filter_threshold = 0.2. # Step 1:  mask = [False, False, True, False] mask = auto_corr < filter_threshold # Step 2:  mask = [0, 0, 1, 1] mask = tf . cast ( mask , dtype = dt ) # Step 3:  mask = [0, 0, 1, 2] mask = tf . cumsum ( mask , axis = 0 ) # Step 4:  mask = [1, 1, 0, 0] mask = tf . maximum ( 1. - mask , 0. ) auto_corr *= mask # With R[k] := auto_corr[k, ...], # ESS = N / {1 + 2 * Sum_{k=1}^N (N - k) / N * R[k]} #     = N / {-1 + 2 * Sum_{k=0}^N (N - k) / N * R[k]} (since R[0] = 1) #     approx N / {-1 + 2 * Sum_{k=0}^M (N - k) / N * R[k]} # where M is the filter_beyond_lag truncation point chosen above. # Get the factor (N - k) / N, and give it shape [M, 1,...,1], having total # ndims the same as auto_corr n = _axis_size ( states , axis = 0 ) k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) nk_factor = ( n - k ) / n if auto_corr . shape . ndims is not None : new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) else : new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk_factor = tf . reshape ( nk_factor , new_shape ) return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) )", "nl": "ESS computation for one single Tensor argument ."}}
{"translation": {"code": "def effective_sample_size ( states , filter_threshold = 0. , filter_beyond_lag = None , name = None ) : states_was_list = _is_list_like ( states ) # Convert all args to lists. if not states_was_list : states = [ states ] filter_beyond_lag = _broadcast_maybelist_arg ( states , filter_beyond_lag , 'filter_beyond_lag' ) filter_threshold = _broadcast_maybelist_arg ( states , filter_threshold , 'filter_threshold' ) # Process items, one at a time. with tf . compat . v1 . name_scope ( name , 'effective_sample_size' ) : ess_list = [ _effective_sample_size_single_state ( s , ml , mlt ) for ( s , ml , mlt ) in zip ( states , filter_beyond_lag , filter_threshold ) ] if states_was_list : return ess_list return ess_list [ 0 ]", "nl": "Estimate a lower bound on effective sample size for each independent chain ."}}
{"translation": {"code": "def _axis_size ( x , axis = None ) : if axis is None : return tf . cast ( tf . size ( input = x ) , x . dtype ) return tf . cast ( tf . reduce_prod ( input_tensor = tf . gather ( tf . shape ( input = x ) , axis ) ) , x . dtype )", "nl": "Get number of elements of x in axis as type x . dtype ."}}
{"translation": {"code": "def _primes_less_than ( n ) : # Based on # https://stackoverflow.com/questions/2068372/fastest-way-to-list-all-primes-below-n-in-python/3035188#3035188 small_primes = np . array ( ( 2 , 3 , 5 ) ) if n <= 6 : return small_primes [ small_primes < n ] sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) sieve [ 0 ] = False m = int ( n ** 0.5 ) // 3 + 1 for i in range ( m ) : if not sieve [ i ] : continue k = 3 * i + 1 | 1 sieve [ k ** 2 // 3 : : 2 * k ] = False sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = False return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ]", "nl": "Returns sorted array of primes such that 2 < = prime < n ."}}
{"translation": {"code": "def _base_expansion_size ( num , bases ) : return tf . floor ( tf . math . log ( num ) / tf . math . log ( bases ) ) + 1", "nl": "Computes the number of terms in the place value expansion ."}}
{"translation": {"code": "def _get_indices ( num_results , sequence_indices , dtype , name = None ) : with tf . compat . v1 . name_scope ( name , '_get_indices' , [ num_results , sequence_indices ] ) : if sequence_indices is None : num_results = tf . cast ( num_results , dtype = dtype ) sequence_indices = tf . range ( num_results , dtype = dtype ) else : sequence_indices = tf . cast ( sequence_indices , dtype ) # Shift the indices so they are 1 based. indices = sequence_indices + 1 # Reshape to make space for the event dimension and the place value # coefficients. return tf . reshape ( indices , [ - 1 , 1 , 1 ] )", "nl": "Generates starting points for the Halton sequence procedure ."}}
{"translation": {"code": "def _get_permutations ( num_results , dims , seed = None ) : sample_range = tf . range ( num_results ) stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence3' ) def generate_one ( d ) : seed = stream ( ) fn = lambda _ : tf . random . shuffle ( tf . range ( d ) , seed = seed ) return tf . map_fn ( fn , sample_range , parallel_iterations = 1 if seed is not None else 10 ) return tf . concat ( [ generate_one ( d ) for d in tf . unstack ( dims ) ] , axis = - 1 )", "nl": "Uniform iid sample from the space of permutations ."}}
{"translation": {"code": "def sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = True , seed = None , name = None ) : if dim < 1 or dim > _MAX_DIMENSION : raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) if ( num_results is None ) == ( sequence_indices is None ) : raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) if not dtype . is_floating : raise ValueError ( 'dtype must be of `float`-type' ) with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : # Here and in the following, the shape layout is as follows: # [sample dimension, event dimension, coefficient dimension]. # The coefficient dimension is an intermediate axes which will hold the # weights of the starting integer when expressed in the (prime) base for # an event dimension. if num_results is not None : num_results = tf . convert_to_tensor ( value = num_results ) if sequence_indices is not None : sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) indices = _get_indices ( num_results , sequence_indices , dtype ) radixes = tf . constant ( _PRIMES [ 0 : dim ] , dtype = dtype , shape = [ dim , 1 ] ) max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) # The powers of the radixes that we will need. Note that there is a bit # of an excess here. Suppose we need the place value coefficients of 7 # in base 2 and 3. For 2, we will have 3 digits but we only need 2 digits # for base 3. However, we can only create rectangular tensors so we # store both expansions in a [2, 3] tensor. This leads to the problem that # we might end up attempting to raise large numbers to large powers. For # example, base 2 expansion of 1024 has 10 digits. If we were in 10 # dimensions, then the 10th prime (29) we will end up computing 29^10 even # though we don't need it. We avoid this by setting the exponents for each # axes to 0 beyond the maximum value needed for that dimension. exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , 1 ] ) # The mask is true for those coefficients that are irrelevant. weight_mask = exponents_by_axes >= max_sizes_by_axes capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) weights = radixes ** capped_exponents # The following computes the base b expansion of the indices. Suppose, # x = a0 + a1*b + a2*b^2 + ... Then, performing a floor div of x with # the vector (1, b, b^2, b^3, ...) will produce # (a0 + s1 * b, a1 + s2 * b, ...) where s_i are coefficients we don't care # about. Noting that all a_i < b by definition of place value expansion, # we see that taking the elements mod b of the above vector produces the # place value expansion coefficients. coeffs = tf . math . floordiv ( indices , weights ) coeffs *= 1. - tf . cast ( weight_mask , dtype ) coeffs %= radixes if not randomized : coeffs /= radixes return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) # Remove the contribution from randomizing the trailing zero for the # axes where max_size_by_axes < max_size. This will be accounted # for separately below (using zero_correction). coeffs *= 1. - tf . cast ( weight_mask , dtype ) coeffs /= radixes base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) # The randomization used in Owen (2017) does not leave 0 invariant. While # we have accounted for the randomization of the first `max_size_by_axes` # coefficients, we still need to correct for the trailing zeros. Luckily, # this is equivalent to adding a uniform random value scaled so the first # `max_size_by_axes` coefficients are zero. The following statements perform # this correction. zero_correction = tf . random . uniform ( [ dim , 1 ] , seed = stream ( ) , dtype = dtype ) zero_correction /= radixes ** max_sizes_by_axes return base_values + tf . reshape ( zero_correction , [ - 1 ] )", "nl": "r Returns a sample from the dim dimensional Halton sequence ."}}
{"translation": {"code": "def default_loc_scale_fn ( is_singular = False , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : def _fn ( dtype , shape , name , trainable , add_variable_fn ) : \"\"\"Creates `loc`, `scale` parameters.\"\"\" loc = add_variable_fn ( name = name + '_loc' , shape = shape , initializer = loc_initializer , regularizer = loc_regularizer , constraint = loc_constraint , dtype = dtype , trainable = trainable ) if is_singular : return loc , None untransformed_scale = add_variable_fn ( name = name + '_untransformed_scale' , shape = shape , initializer = untransformed_scale_initializer , regularizer = untransformed_scale_regularizer , constraint = untransformed_scale_constraint , dtype = dtype , trainable = trainable ) scale = ( np . finfo ( dtype . as_numpy_dtype ) . eps + tf . nn . softplus ( untransformed_scale ) ) return loc , scale return _fn", "nl": "Makes closure which creates loc scale params from tf . get_variable ."}}
{"translation": {"code": "def default_mean_field_normal_fn ( is_singular = False , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : loc_scale_fn = default_loc_scale_fn ( is_singular = is_singular , loc_initializer = loc_initializer , untransformed_scale_initializer = untransformed_scale_initializer , loc_regularizer = loc_regularizer , untransformed_scale_regularizer = untransformed_scale_regularizer , loc_constraint = loc_constraint , untransformed_scale_constraint = untransformed_scale_constraint ) def _fn ( dtype , shape , name , trainable , add_variable_fn ) : \"\"\"Creates multivariate `Deterministic` or `Normal` distribution.\n\n    Args:\n      dtype: Type of parameter's event.\n      shape: Python `list`-like representing the parameter's event shape.\n      name: Python `str` name prepended to any created (or existing)\n        `tf.Variable`s.\n      trainable: Python `bool` indicating all created `tf.Variable`s should be\n        added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n      add_variable_fn: `tf.get_variable`-like `callable` used to create (or\n        access existing) `tf.Variable`s.\n\n    Returns:\n      Multivariate `Deterministic` or `Normal` distribution.\n    \"\"\" loc , scale = loc_scale_fn ( dtype , shape , name , trainable , add_variable_fn ) if scale is None : dist = tfd . Deterministic ( loc = loc ) else : dist = tfd . Normal ( loc = loc , scale = scale ) batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) return _fn", "nl": "Creates a function to build Normal distributions with trainable params ."}}
{"translation": {"code": "def expand_docstring ( * * kwargs ) : def _fn_wrapped ( fn ) : \"\"\"Original function with modified `__doc__` attribute.\"\"\" doc = inspect . cleandoc ( fn . __doc__ ) for k , v in six . iteritems ( kwargs ) : # Capture each ${k} reference to replace with v. # We wrap the replacement in a function so no backslash escapes # are processed. pattern = r'\\$\\{' + str ( k ) + r'\\}' doc = re . sub ( pattern , lambda match : v , doc ) # pylint: disable=cell-var-from-loop fn . __doc__ = doc return fn return _fn_wrapped", "nl": "Decorator to programmatically expand the docstring ."}}
{"translation": {"code": "def bootstrap_results ( self , init_state ) : kernel_results = self . _impl . bootstrap_results ( init_state ) if self . step_size_update_fn is not None : step_size_assign = self . step_size_update_fn ( self . step_size , None ) # pylint: disable=not-callable kernel_results = kernel_results . _replace ( extra = HamiltonianMonteCarloExtraKernelResults ( step_size_assign = step_size_assign ) ) return kernel_results", "nl": "Creates initial previous_kernel_results using a supplied state ."}}
{"translation": {"code": "def custom_gradient ( fx , gx , x , fx_gx_manually_stopped = False , name = None ) : def maybe_stop ( x ) : if fx_gx_manually_stopped : return x return tf . stop_gradient ( x ) with tf . compat . v1 . name_scope ( name , 'custom_gradient' , [ fx , gx , x ] ) : fx = tf . convert_to_tensor ( value = fx , name = 'fx' ) # We don't want to bother eagerly computing `gx` since we may not even need # it. with tf . control_dependencies ( [ fx ] ) : if is_list_like ( x ) : x = [ identity ( x_ , name = 'x' ) for x_ in x ] else : x = [ identity ( x , name = 'x' ) ] if is_list_like ( gx ) : gx = [ identity ( gx_ , dtype = fx . dtype , name = 'gx' ) for gx_ in gx ] else : gx = [ identity ( gx , dtype = fx . dtype , name = 'gx' ) ] override_grad = [ ] for x_ , gx_ in zip ( x , gx ) : # Observe: tf.gradients(f(x), x)[i].shape == x[i].shape # thus we check that the user is supplying correct shapes. equal_shape = tf . compat . v1 . assert_equal ( tf . shape ( input = x_ ) , tf . shape ( input = gx_ ) , message = 'Each `x` must have the same shape as each `gx`.' ) with tf . control_dependencies ( [ equal_shape ] ) : # IEEE754 ensures `(x-x)==0.` and that `0.*x==0.` so we make sure to # write the code this way, rather than, e.g., # `sum_x * stop(gx) + stop(fx - sum_x * gx)`. # For more discussion regarding the relevant portions of the IEEE754 # standard, see the StackOverflow question, # \"Is there a floating point value of x, for which x-x == 0 is false?\" # http://stackoverflow.com/q/2686644 zeros_like_x_ = x_ - tf . stop_gradient ( x_ ) override_grad . append ( tf . reduce_sum ( input_tensor = maybe_stop ( gx_ ) * zeros_like_x_ ) ) override_grad = sum ( override_grad ) override_grad /= tf . cast ( tf . size ( input = fx ) , dtype = fx . dtype . base_dtype ) # Proof of correctness: # #  f(x) = x * stop[gx] + stop[fx - x * gx] #       = stop[fx] # #  g(x) = grad[fx] #       = stop[gx] + grad[stop[fx - x * gx]] #       = stop[gx] + 0 # # Notice that when x is zero it still works: # grad[x * stop(gx) + stop(fx - x * gx)] = 1 * stop[gx] + 0 = stop[gx] # # The proof is similar for the tensor-domain case, except that we # `reduce_sum` the `stop[gx] * (x - stop[x])` then rescale by # `tf.size(fx)` since this reduced version is broadcast to `fx`. return maybe_stop ( fx ) + override_grad", "nl": "Embeds a custom gradient into a Tensor ."}}
{"translation": {"code": "def normal ( x , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = 1. , name = None ) : with tf . compat . v1 . name_scope ( name , 'normal' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if callable ( scale_fn ) : y = layer_fn ( x , 2 ) loc = loc_fn ( y [ ... , 0 ] ) scale = scale_fn ( y [ ... , 1 ] ) else : y = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) loc = loc_fn ( y ) scale = tf . cast ( scale_fn , loc . dtype . base_dtype ) return tfd . Normal ( loc = loc , scale = scale )", "nl": "Constructs a trainable tfd . Normal distribution ."}}
{"translation": {"code": "def poisson ( x , layer_fn = tf . compat . v1 . layers . dense , log_rate_fn = lambda x : x , name = None ) : with tf . compat . v1 . name_scope ( name , 'poisson' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) log_rate = log_rate_fn ( tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) ) return tfd . Poisson ( log_rate = log_rate )", "nl": "Constructs a trainable tfd . Poisson distribution ."}}
{"translation": {"code": "def _numpy_text ( tensor , is_repr = False ) : if tensor . dtype . is_numpy_compatible : text = repr ( tensor . numpy ( ) ) if is_repr else str ( tensor . numpy ( ) ) else : text = \"<unprintable>\" if \"\\n\" in text : text = \"\\n\" + text return text", "nl": "Human - readable representation of a tensor s numpy value ."}}
{"translation": {"code": "def numpy ( self ) : if not isinstance ( self . value , ops . EagerTensor ) : raise NotImplementedError ( \"value argument must be a EagerTensor.\" ) return self . value . numpy ( )", "nl": "Value as NumPy array only available for TF Eager ."}}
{"translation": {"code": "def eval ( self , session = None , feed_dict = None ) : return self . value . eval ( session = session , feed_dict = feed_dict )", "nl": "In a session computes and returns the value of this random variable ."}}
{"translation": {"code": "def value ( self ) : if self . _value is None : try : self . _value = self . distribution . sample ( self . sample_shape_tensor ( ) ) except NotImplementedError : raise NotImplementedError ( \"sample is not implemented for {0}. You must either pass in the \" \"value argument or implement sample for {0}.\" . format ( self . distribution . __class__ . __name__ ) ) return self . _value", "nl": "Get tensor that the random variable corresponds to ."}}
{"translation": {"code": "def sample_shape ( self ) : if isinstance ( self . _sample_shape , tf . Tensor ) : return tf . TensorShape ( tf . get_static_value ( self . _sample_shape ) ) return tf . TensorShape ( self . _sample_shape )", "nl": "Sample shape of random variable as a TensorShape ."}}
{"translation": {"code": "def _operator ( attr ) : @ functools . wraps ( attr ) def func ( a , * args ) : return attr ( a . value , * args ) return func", "nl": "Defers an operator overload to attr ."}}
{"translation": {"code": "def _make_random_variable ( distribution_cls ) : @ interceptable @ functools . wraps ( distribution_cls , assigned = ( '__module__' , '__name__' ) ) @ docstring_util . expand_docstring ( cls = distribution_cls . __name__ , doc = inspect . cleandoc ( distribution_cls . __init__ . __doc__ or '' ) ) def func ( * args , * * kwargs ) : # pylint: disable=g-doc-args \"\"\"Create a random variable for ${cls}.\n\n    See ${cls} for more details.\n\n    Returns:\n      RandomVariable.\n\n    #### Original Docstring for Distribution\n\n    ${doc}\n    \"\"\" # pylint: enable=g-doc-args sample_shape = kwargs . pop ( 'sample_shape' , ( ) ) value = kwargs . pop ( 'value' , None ) return RandomVariable ( distribution = distribution_cls ( * args , * * kwargs ) , sample_shape = sample_shape , value = value ) return func", "nl": "Factory function to make random variable given distribution class ."}}
{"translation": {"code": "def get_next_interceptor ( ) : try : interceptor = _interceptor_stack . stack . pop ( ) yield interceptor finally : _interceptor_stack . stack . append ( interceptor )", "nl": "Yields the top - most interceptor on the thread - local interceptor stack ."}}
{"translation": {"code": "def interceptable ( func ) : @ functools . wraps ( func ) def func_wrapped ( * args , * * kwargs ) : with get_next_interceptor ( ) as interceptor : return interceptor ( func , * args , * * kwargs ) return func_wrapped", "nl": "Decorator that wraps func so that its execution is intercepted ."}}
{"translation": {"code": "def choose ( is_accepted , accepted , rejected , name = None ) : if not is_namedtuple_like ( accepted ) : return _choose_base_case ( is_accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise TypeError ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . __name__ , type ( rejected ) . __name__ ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is_accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . _fields ] ) )", "nl": "Helper which expand_dims is_accepted then applies tf . where ."}}
{"translation": {"code": "def is_namedtuple_like ( x ) : try : for fn in x . _fields : _ = getattr ( x , fn ) return True except AttributeError : return False", "nl": "Helper which returns True if input is collections . namedtuple - like ."}}
{"translation": {"code": "def _get_function_inputs ( f , src_kwargs ) : if hasattr ( f , \"_func\" ) : # functions returned by tf.make_template f = f . _func # pylint: disable=protected-access try : # getargspec was deprecated in Python 3.6 argspec = inspect . getfullargspec ( f ) except AttributeError : argspec = inspect . getargspec ( f ) fkwargs = { k : v for k , v in six . iteritems ( src_kwargs ) if k in argspec . args } return fkwargs", "nl": "Filters inputs to be compatible with function f s signature ."}}
{"translation": {"code": "def make_log_joint_fn ( model ) : def log_joint_fn ( * args , * * kwargs ) : \"\"\"Log-probability of inputs according to a joint probability distribution.\n\n    Args:\n      *args: Positional arguments. They are the model's original inputs and can\n        alternatively be specified as part of `kwargs`.\n      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,\n        `v` is passed as a `value` to the random variable(s) whose keyword\n        argument `name` during construction is equal to `k`.\n\n    Returns:\n      Scalar tf.Tensor, which represents the model's log-probability summed\n      over all Edward random variables and their dimensions.\n\n    Raises:\n      TypeError: If a random variable in the model has no specified value in\n        `**kwargs`.\n    \"\"\" log_probs = [ ] def interceptor ( rv_constructor , * rv_args , * * rv_kwargs ) : \"\"\"Overrides a random variable's `value` and accumulates its log-prob.\"\"\" # Set value to keyword argument indexed by `name` (an input tensor). rv_name = rv_kwargs . get ( \"name\" ) if rv_name is None : raise KeyError ( \"Random variable constructor {} has no name \" \"in its arguments.\" . format ( rv_constructor . __name__ ) ) # If no value is explicitly passed in for an RV, default to the value # from the RV constructor. This may have been set explicitly by the user # or forwarded from a lower-level interceptor. previously_specified_value = rv_kwargs . get ( \"value\" ) value = kwargs . get ( rv_name , previously_specified_value ) if value is None : raise LookupError ( \"Keyword argument specifying value for {} is \" \"missing.\" . format ( rv_name ) ) rv_kwargs [ \"value\" ] = value rv = rv_constructor ( * rv_args , * * rv_kwargs ) log_prob = tf . reduce_sum ( input_tensor = rv . distribution . log_prob ( rv . value ) ) log_probs . append ( log_prob ) return rv model_kwargs = _get_function_inputs ( model , kwargs ) with interception ( interceptor ) : model ( * args , * * model_kwargs ) log_prob = sum ( log_probs ) return log_prob return log_joint_fn", "nl": "Takes Edward probabilistic program and returns its log joint function ."}}
{"translation": {"code": "def random_walk_uniform_fn ( scale = 1. , name = None ) : def _fn ( state_parts , seed ) : \"\"\"Adds a uniform perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\" with tf . compat . v1 . name_scope ( name , 'random_walk_uniform_fn' , values = [ state_parts , scale , seed ] ) : scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] if len ( scales ) == 1 : scales *= len ( state_parts ) if len ( state_parts ) != len ( scales ) : raise ValueError ( '`scale` must broadcast with `state_parts`.' ) seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkUniformFn' ) next_state_parts = [ tf . random . uniform ( minval = state_part - scale_part , maxval = state_part + scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] return next_state_parts return _fn", "nl": "Returns a callable that adds a random uniform perturbation to the input ."}}
{"translation": {"code": "def random_walk_normal_fn ( scale = 1. , name = None ) : def _fn ( state_parts , seed ) : \"\"\"Adds a normal perturbation to the input state.\n\n    Args:\n      state_parts: A list of `Tensor`s of any shape and real dtype representing\n        the state parts of the `current_state` of the Markov chain.\n      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is\n        applied.\n        Default value: `None`.\n\n    Returns:\n      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same\n        shape and type as the `state_parts`.\n\n    Raises:\n      ValueError: if `scale` does not broadcast with `state_parts`.\n    \"\"\" with tf . compat . v1 . name_scope ( name , 'random_walk_normal_fn' , values = [ state_parts , scale , seed ] ) : scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] if len ( scales ) == 1 : scales *= len ( state_parts ) if len ( state_parts ) != len ( scales ) : raise ValueError ( '`scale` must broadcast with `state_parts`.' ) seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkNormalFn' ) next_state_parts = [ tf . random . normal ( mean = state_part , stddev = scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] return next_state_parts return _fn", "nl": "Returns a callable that adds a random normal perturbation to the input ."}}
{"translation": {"code": "def build_fake_data ( num_examples = 10 ) : class Dummy ( object ) : pass num_examples = 10 mnist_data = Dummy ( ) mnist_data . train = Dummy ( ) mnist_data . train . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) mnist_data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) mnist_data . train . num_examples = num_examples mnist_data . validation = Dummy ( ) mnist_data . validation . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) mnist_data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) mnist_data . validation . num_examples = num_examples return mnist_data", "nl": "Build fake MNIST - style data for unit testing ."}}
{"translation": {"code": "def plot_heldout_prediction ( input_vals , probs , fname , n = 10 , title = \"\" ) : fig = figure . Figure ( figsize = ( 9 , 3 * n ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) for i in range ( n ) : ax = fig . add_subplot ( n , 3 , 3 * i + 1 ) ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - 1 ] ) , interpolation = \"None\" ) ax = fig . add_subplot ( n , 3 , 3 * i + 2 ) for prob_sample in probs : sns . barplot ( np . arange ( 10 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) ax . set_ylim ( [ 0 , 1 ] ) ax . set_title ( \"posterior samples\" ) ax = fig . add_subplot ( n , 3 , 3 * i + 3 ) sns . barplot ( np . arange ( 10 ) , np . mean ( probs [ : , i , : ] , axis = 0 ) , ax = ax ) ax . set_ylim ( [ 0 , 1 ] ) ax . set_title ( \"predictive probs\" ) fig . suptitle ( title ) fig . tight_layout ( ) canvas . print_figure ( fname , format = \"png\" ) print ( \"saved {}\" . format ( fname ) )", "nl": "Save a PNG plot visualizing posterior uncertainty on heldout data ."}}
{"translation": {"code": "def toy_logistic_data ( num_examples , input_size = 2 , weights_prior_stddev = 5.0 ) : random_weights = weights_prior_stddev * np . random . randn ( input_size ) random_bias = np . random . randn ( ) design_matrix = np . random . rand ( num_examples , input_size ) * 2 - 1 logits = np . reshape ( np . dot ( design_matrix , random_weights ) + random_bias , ( - 1 , 1 ) ) p_labels = 1. / ( 1 + np . exp ( - logits ) ) labels = np . int32 ( p_labels > np . random . rand ( num_examples , 1 ) ) return random_weights , random_bias , np . float32 ( design_matrix ) , labels", "nl": "Generates synthetic data for binary classification ."}}
{"translation": {"code": "def plot_weight_posteriors ( names , qm_vals , qs_vals , fname ) : fig = figure . Figure ( figsize = ( 6 , 3 ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) ax = fig . add_subplot ( 1 , 2 , 1 ) for n , qm in zip ( names , qm_vals ) : sns . distplot ( qm . flatten ( ) , ax = ax , label = n ) ax . set_title ( \"weight means\" ) ax . set_xlim ( [ - 1.5 , 1.5 ] ) ax . legend ( ) ax = fig . add_subplot ( 1 , 2 , 2 ) for n , qs in zip ( names , qs_vals ) : sns . distplot ( qs . flatten ( ) , ax = ax ) ax . set_title ( \"weight stddevs\" ) ax . set_xlim ( [ 0 , 1. ] ) fig . tight_layout ( ) canvas . print_figure ( fname , format = \"png\" ) print ( \"saved {}\" . format ( fname ) )", "nl": "Save a PNG plot with histograms of weight means and stddevs ."}}
{"translation": {"code": "def visualize_decision ( features , labels , true_w_b , candidate_w_bs , fname ) : fig = figure . Figure ( figsize = ( 6 , 6 ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . scatter ( features [ : , 0 ] , features [ : , 1 ] , c = np . float32 ( labels [ : , 0 ] ) , cmap = cm . get_cmap ( \"binary\" ) , edgecolors = \"k\" ) def plot_weights ( w , b , * * kwargs ) : w1 , w2 = w x1s = np . linspace ( - 1 , 1 , 100 ) x2s = - ( w1 * x1s + b ) / w2 ax . plot ( x1s , x2s , * * kwargs ) for w , b in candidate_w_bs : plot_weights ( w , b , alpha = 1. / np . sqrt ( len ( candidate_w_bs ) ) , lw = 1 , color = \"blue\" ) if true_w_b is not None : plot_weights ( * true_w_b , lw = 4 , color = \"green\" , label = \"true separator\" ) ax . set_xlim ( [ - 1.5 , 1.5 ] ) ax . set_ylim ( [ - 1.5 , 1.5 ] ) ax . legend ( ) canvas . print_figure ( fname , format = \"png\" ) print ( \"saved {}\" . format ( fname ) )", "nl": "Utility method to visualize decision boundaries in R^2 ."}}
{"translation": {"code": "def build_input_pipeline ( x , y , batch_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( ( x , y ) ) training_batches = training_dataset . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) batch_features , batch_labels = training_iterator . get_next ( ) return batch_features , batch_labels", "nl": "Build a Dataset iterator for supervised classification ."}}
{"translation": {"code": "def deserialize_function ( serial , function_type ) : if function_type == 'function' : # Simple lookup in custom objects function = tf . keras . utils . deserialize_keras_object ( serial ) elif function_type == 'lambda' : # Unsafe deserialization from bytecode function = generic_utils . func_load ( serial ) else : raise TypeError ( 'Unknown function type:' , function_type ) return function", "nl": "Deserializes the Keras - serialized function ."}}
{"translation": {"code": "def serialize_function ( func ) : if isinstance ( func , types . LambdaType ) : return generic_utils . func_dump ( func ) , 'lambda' return func . __name__ , 'function'", "nl": "Serializes function for Keras ."}}
{"translation": {"code": "def from_config ( cls , config ) : config = config . copy ( ) function_keys = [ 'kernel_posterior_fn' , 'kernel_posterior_tensor_fn' , 'kernel_prior_fn' , 'kernel_divergence_fn' , 'bias_posterior_fn' , 'bias_posterior_tensor_fn' , 'bias_prior_fn' , 'bias_divergence_fn' , ] for function_key in function_keys : serial = config [ function_key ] function_type = config . pop ( function_key + '_type' ) if serial is not None : config [ function_key ] = tfp_layers_util . deserialize_function ( serial , function_type = function_type ) return cls ( * * config )", "nl": "Creates a layer from its config ."}}
{"translation": {"code": "def pinv ( a , rcond = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : a = tf . convert_to_tensor ( value = a , name = 'a' ) assertions = _maybe_validate_matrix ( a , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : a = tf . identity ( a ) dtype = a . dtype . as_numpy_dtype if rcond is None : def get_dim_size ( dim ) : if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : return tf . compat . dimension_value ( a . shape [ dim ] ) return tf . shape ( input = a ) [ dim ] num_rows = get_dim_size ( - 2 ) num_cols = get_dim_size ( - 1 ) if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : max_rows_cols = float ( max ( num_rows , num_cols ) ) else : max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) # Calculate pseudo inverse via SVD. # Note: if a is symmetric then u == v. (We might observe additional # performance by explicitly setting `v = u` in such cases.) [ singular_values , # Sigma left_singular_vectors , # U right_singular_vectors , # V ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) # Saturate small singular values to inf. This has the effect of make # `1. / s = 0.` while not resulting in `NaN` gradients. cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) singular_values = tf . where ( singular_values > cutoff [ ... , tf . newaxis ] , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) # Although `a == tf.matmul(u, s * v, transpose_b=True)` we swap # `u` and `v` here so that `tf.matmul(pinv(A), A) = tf.eye()`, i.e., # a matrix inverse has \"transposed\" semantics. a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) if a . shape . ndims is not None : a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2 ] ] ) ) return a_pinv", "nl": "Compute the Moore - Penrose pseudo - inverse of a matrix ."}}
{"translation": {"code": "def default_multivariate_normal_fn ( dtype , shape , name , trainable , add_variable_fn ) : del name , trainable , add_variable_fn # unused dist = tfd . Normal ( loc = tf . zeros ( shape , dtype ) , scale = dtype . as_numpy_dtype ( 1 ) ) batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims )", "nl": "Creates multivariate standard Normal distribution ."}}
{"translation": {"code": "def build_fake_input_fns ( batch_size ) : random_sample = np . random . rand ( batch_size , * IMAGE_SHAPE ) . astype ( \"float32\" ) def train_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch_size ) . repeat ( ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn", "nl": "Builds fake MNIST - style data for unit testing ."}}
{"translation": {"code": "def make_mixture_prior ( latent_size , mixture_components ) : if mixture_components == 1 : # See the module docstring for why we don't learn the parameters here. return tfd . MultivariateNormalDiag ( loc = tf . zeros ( [ latent_size ] ) , scale_identity_multiplier = 1.0 ) loc = tf . compat . v1 . get_variable ( name = \"loc\" , shape = [ mixture_components , latent_size ] ) raw_scale_diag = tf . compat . v1 . get_variable ( name = \"raw_scale_diag\" , shape = [ mixture_components , latent_size ] ) mixture_logits = tf . compat . v1 . get_variable ( name = \"mixture_logits\" , shape = [ mixture_components ] ) return tfd . MixtureSameFamily ( components_distribution = tfd . MultivariateNormalDiag ( loc = loc , scale_diag = tf . nn . softplus ( raw_scale_diag ) ) , mixture_distribution = tfd . Categorical ( logits = mixture_logits ) , name = \"prior\" )", "nl": "Creates the mixture of Gaussians prior distribution ."}}
{"translation": {"code": "def default_exchange_proposed_fn ( prob_exchange ) : def default_exchange_proposed_fn_ ( num_replica , seed = None ) : \"\"\"Default function for `exchange_proposed_fn` of `kernel`.\"\"\" seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) zero_start = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) > 0.5 if num_replica % 2 == 0 : def _exchange ( ) : flat_exchange = tf . range ( num_replica ) if num_replica > 2 : start = tf . cast ( ~ zero_start , dtype = tf . int32 ) end = num_replica - start flat_exchange = flat_exchange [ start : end ] return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) else : def _exchange ( ) : start = tf . cast ( zero_start , dtype = tf . int32 ) end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) flat_exchange = tf . range ( num_replica ) [ start : end ] return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) def _null_exchange ( ) : return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ 0 , 2 ] ) return tf . cond ( pred = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) < prob_exchange , true_fn = _exchange , false_fn = _null_exchange ) return default_exchange_proposed_fn_", "nl": "Default exchange proposal function for replica exchange MC ."}}
{"translation": {"code": "def _get_drift ( step_size_parts , volatility_parts , grads_volatility , grads_target_log_prob , name = None ) : with tf . compat . v1 . name_scope ( name , 'mala_get_drift' , [ step_size_parts , volatility_parts , grads_volatility , grads_target_log_prob ] ) : drift_parts = [ ] for step_size , volatility , grad_volatility , grad_target_log_prob in ( zip ( step_size_parts , volatility_parts , grads_volatility , grads_target_log_prob ) ) : volatility_squared = tf . square ( volatility ) drift = 0.5 * step_size * ( volatility_squared * grad_target_log_prob + grad_volatility ) drift_parts . append ( drift ) return drift_parts", "nl": "Compute diffusion drift at the current location current_state ."}}
{"translation": {"code": "def _compute_log_acceptance_correction ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims , name = None ) : with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims ] ) : proposed_log_density_parts = [ ] dual_log_density_parts = [ ] for [ current_state , proposed_state , current_volatility , proposed_volatility , current_drift , proposed_drift , step_size , ] in zip ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , ) : axis = tf . range ( independent_chain_ndims , tf . rank ( current_state ) ) state_diff = proposed_state - current_state current_volatility *= tf . sqrt ( step_size ) proposed_energy = ( state_diff - current_drift ) / current_volatility proposed_volatility *= tf . sqrt ( step_size ) # Compute part of `q(proposed_state | current_state)` proposed_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( current_volatility ) , 0.5 * ( proposed_energy ** 2 ) ] ) , axis = axis ) ) proposed_log_density_parts . append ( - proposed_energy ) # Compute part of `q(current_state | proposed_state)` dual_energy = ( state_diff + proposed_drift ) / proposed_volatility dual_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( proposed_volatility ) , 0.5 * ( dual_energy ** 2 ) ] ) , axis = axis ) ) dual_log_density_parts . append ( - dual_energy ) # Compute `q(proposed_state | current_state)` proposed_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( proposed_log_density_parts , axis = - 1 ) , axis = - 1 ) # Compute `q(current_state | proposed_state)` dual_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( dual_log_density_parts , axis = - 1 ) , axis = - 1 ) return mcmc_util . safe_sum ( [ dual_log_density_reduce , - proposed_log_density_reduce ] )", "nl": "r Helper to kernel which computes the log acceptance - correction ."}}
{"translation": {"code": "def _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] needs_volatility_fn_gradients = grads_volatility_fn is None # Convert `volatility_fn_results` to a list if volatility_fn_results is None : volatility_fn_results = volatility_fn ( * state_parts ) volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) if len ( volatility_fn_results ) == 1 : volatility_fn_results *= len ( state_parts ) if len ( state_parts ) != len ( volatility_fn_results ) : raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) # The shape of 'volatility_parts' needs to have the number of chains as a # leading dimension. For determinism we broadcast 'volatility_parts' to the # shape of `state_parts` since each dimension of `state_parts` could have a # different volatility value. volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) if grads_volatility_fn is None : [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) # Compute gradient of `volatility_parts**2` if needs_volatility_fn_gradients : grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] return volatility_fn_results , grads_volatility_fn", "nl": "Helper which computes volatility_fn results and grads if needed ."}}
{"translation": {"code": "def _maybe_broadcast_volatility ( volatility_parts , state_parts ) : return [ v + tf . zeros_like ( sp , dtype = sp . dtype . base_dtype ) for v , sp in zip ( volatility_parts , state_parts ) ]", "nl": "Helper to broadcast volatility_parts to the shape of state_parts ."}}
{"translation": {"code": "def _euler_method ( random_draw_parts , state_parts , drift_parts , step_size_parts , volatility_parts , name = None ) : with tf . compat . v1 . name_scope ( name , 'mala_euler_method' , [ random_draw_parts , state_parts , drift_parts , step_size_parts , volatility_parts ] ) : proposed_state_parts = [ ] for random_draw , state , drift , step_size , volatility in zip ( random_draw_parts , state_parts , drift_parts , step_size_parts , volatility_parts ) : proposal = state + drift + volatility * tf . sqrt ( step_size ) * random_draw proposed_state_parts . append ( proposal ) return proposed_state_parts", "nl": "Applies one step of Euler - Maruyama method ."}}
{"translation": {"code": "def _value_and_gradients ( fn , fn_arg_list , result = None , grads = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'value_and_gradients' , [ fn_arg_list , result , grads ] ) : def _convert_to_tensor ( x , name ) : ctt = lambda x_ : x_ if x_ is None else tf . convert_to_tensor ( value = x_ , name = name ) return [ ctt ( x_ ) for x_ in x ] if is_list_like ( x ) else ctt ( x ) fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) fn_arg_list = _convert_to_tensor ( fn_arg_list , 'fn_arg' ) if result is None : result = fn ( * fn_arg_list ) if grads is None and tf . executing_eagerly ( ) : # Ensure we disable bijector cacheing in eager mode. # TODO(b/72831017): Remove this once bijector cacheing is fixed for # eager mode. fn_arg_list = [ 0 + x for x in fn_arg_list ] result = _convert_to_tensor ( result , 'fn_result' ) if grads is not None : grads = _convert_to_tensor ( grads , 'fn_grad' ) return result , grads if is_list_like ( result ) and len ( result ) == len ( fn_arg_list ) : # Compute the block diagonal of Jacobian. # TODO(b/79158574): Guard this calculation by an arg which explicitly # requests block diagonal Jacobian calculation. def fn_slice ( i ) : \"\"\"Needed to prevent `cell-var-from-loop` pylint warning.\"\"\" return lambda x : fn ( * ( fn_arg_list [ : i ] + [ x ] + fn_arg_list [ i + 1 : ] ) ) grads = [ tfp_math_value_and_gradients ( fn_slice ( i ) , fn_arg_list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] else : _ , grads = tfp_math_value_and_gradients ( fn , fn_arg_list ) return result , grads", "nl": "Helper to maybe_call_fn_and_grads ."}}
{"translation": {"code": "def maybe_call_fn_and_grads ( fn , fn_arg_list , result = None , grads = None , check_non_none_grads = True , name = None ) : with tf . compat . v1 . name_scope ( name , 'maybe_call_fn_and_grads' , [ fn_arg_list , result , grads ] ) : fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) result , grads = _value_and_gradients ( fn , fn_arg_list , result , grads ) if not all ( r . dtype . is_floating for r in ( result if is_list_like ( result ) else [ result ] ) ) : # pylint: disable=superfluous-parens raise TypeError ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) if len ( fn_arg_list ) != len ( grads ) : raise ValueError ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) if check_non_none_grads and any ( g is None for g in grads ) : raise ValueError ( 'Encountered `None` gradient.\\n' '  fn_arg_list: {}\\n' '  grads: {}' . format ( fn_arg_list , grads ) ) return result , grads", "nl": "Calls fn and computes the gradient of the result wrt args_list ."}}
{"translation": {"code": "def num_cols ( x ) : if tf . compat . dimension_value ( x . shape [ - 1 ] ) is not None : return tf . compat . dimension_value ( x . shape [ - 1 ] ) return tf . shape ( input = x ) [ - 1 ]", "nl": "Returns number of cols in a given Tensor ."}}
{"translation": {"code": "def fit ( model_matrix , response , model , model_coefficients_start = None , predicted_linear_response_start = None , l2_regularizer = None , dispersion = None , offset = None , convergence_criteria_fn = None , learning_rate = None , fast_unsafe_numerics = True , maximum_iterations = None , name = None ) : graph_deps = [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , dispersion , offset , learning_rate , maximum_iterations ] with tf . compat . v1 . name_scope ( name , 'fit' , graph_deps ) : [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset , ] = prepare_args ( model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset ) if convergence_criteria_fn is None : convergence_criteria_fn = ( convergence_criteria_small_relative_norm_weights_change ( ) ) def _body ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous ) : \"\"\"`tf.while_loop` body.\"\"\" model_coefficients_next , predicted_linear_response_next = fit_one_step ( model_matrix , response , model , model_coefficients_previous , predicted_linear_response_previous , l2_regularizer , dispersion , offset , learning_rate , fast_unsafe_numerics ) is_converged_next = convergence_criteria_fn ( is_converged_previous = is_converged_previous , iter_ = iter_ , model_coefficients_previous = model_coefficients_previous , predicted_linear_response_previous = predicted_linear_response_previous , model_coefficients_next = model_coefficients_next , predicted_linear_response_next = predicted_linear_response_next , response = response , model = model , dispersion = dispersion ) return [ is_converged_next , iter_ + 1 , model_coefficients_next , predicted_linear_response_next , ] # while not converged: #   fit_one_step [ is_converged , iter_ , model_coefficients , predicted_linear_response , ] = tf . while_loop ( cond = lambda is_converged , * args : tf . logical_not ( is_converged ) , body = _body , loop_vars = [ tf . zeros ( [ ] , np . bool ) , # is_converged tf . zeros ( [ ] , np . int32 ) , # iter_ model_coefficients_start , predicted_linear_response_start , ] , maximum_iterations = maximum_iterations ) return [ model_coefficients , predicted_linear_response , is_converged , iter_ ]", "nl": "Runs multiple Fisher scoring steps ."}}
{"translation": {"code": "def convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : def convergence_criteria_fn ( is_converged_previous , # pylint: disable=unused-argument iter_ , model_coefficients_previous , predicted_linear_response_previous , # pylint: disable=unused-argument model_coefficients_next , predicted_linear_response_next , # pylint: disable=unused-argument response , # pylint: disable=unused-argument model , # pylint: disable=unused-argument dispersion ) : # pylint: disable=unused-argument \"\"\"Returns `bool` `Tensor` indicating if fitting procedure has converged.\n\n    Args:\n      is_converged_previous: \"old\" convergence results.\n      iter_: Iteration number.\n      model_coefficients_previous: \"old\" `model_coefficients`.\n      predicted_linear_response_previous: \"old\" `predicted_linear_response`.\n      model_coefficients_next: \"new\" `model_coefficients`.\n      predicted_linear_response_next: \"new: `predicted_linear_response`.\n      response: (Batch of) vector-shaped `Tensor` where each element represents\n        a sample's observed response (to the corresponding row of features).\n        Must have same `dtype` as `model_matrix`.\n      model: `tfp.glm.ExponentialFamily`-like instance used to construct the\n        negative log-likelihood loss, gradient, and expected Hessian (i.e., the\n        Fisher information matrix).\n      dispersion: `Tensor` representing `response` dispersion, i.e., as in:\n        `p(y|theta) := exp((y theta - A(theta)) / dispersion)`. Must broadcast\n        with rows of `model_matrix`.\n        Default value: `None` (i.e., \"no dispersion\").\n\n    Returns:\n      is_converged: `bool` `Tensor`.\n    \"\"\" relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous - model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) return ( iter_ > 0 ) & tf . reduce_all ( input_tensor = relative_euclidean_norm < tolerance ) return convergence_criteria_fn", "nl": "Returns Python callable which indicates fitting procedure has converged ."}}
{"translation": {"code": "def prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] with tf . compat . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) if offset is not None : offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) use_default_model_coefficients = model_coefficients is None if use_default_model_coefficients : # User did not supply model coefficients; assume they're all zero. batch_shape = tf . shape ( input = model_matrix ) [ : - 2 ] num_columns = tf . shape ( input = model_matrix ) [ - 1 ] model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = 0 ) , dtype = dtype , name = 'model_coefficients' ) else : # User did supply model coefficients; convert to Tensor in case it's # numpy or literal. model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) if predicted_linear_response is None : if use_default_model_coefficients : # Since we're using zeros for model_coefficients, we know the predicted # linear response will also be all zeros. if offset is None : predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) else : predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( input = response ) , name = 'predicted_linear_response' ) else : # We were given model_coefficients but not the predicted linear # response. predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) else : predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) return [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ]", "nl": "Helper to fit which sanitizes input args ."}}
{"translation": {"code": "def _name_scope ( self , name = None , default_name = None , values = None ) : with tf . compat . v1 . name_scope ( self . name ) : with tf . compat . v1 . name_scope ( name , default_name , values = values or [ ] ) as scope : yield scope", "nl": "Helper function to standardize op scope ."}}
{"translation": {"code": "def random_rademacher ( shape , dtype = tf . float32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'random_rademacher' , [ shape , seed ] ) : # Choose the dtype to cause `2 * random_bernoulli - 1` to run in the same # memory (host or device) as the downstream cast will want to put it.  The # convention on GPU is that int32 are in host memory and int64 are in device # memory. generation_dtype = tf . int64 if tf . as_dtype ( dtype ) != tf . int32 else tf . int32 random_bernoulli = tf . random . uniform ( shape , minval = 0 , maxval = 2 , dtype = generation_dtype , seed = seed ) return tf . cast ( 2 * random_bernoulli - 1 , dtype )", "nl": "Generates Tensor consisting of - 1 or + 1 chosen uniformly at random ."}}
{"translation": {"code": "def as_random_variable ( distribution , sample_shape = ( ) , value = None ) : return _build_custom_rv ( distribution = distribution , sample_shape = sample_shape , value = value , name = _simple_name ( distribution ) )", "nl": "Wrap an existing distribution as a traceable random variable ."}}
{"translation": {"code": "def _build_custom_rv ( distribution , sample_shape , value , name ) : # Program transformations (e.g., `make_log_joint_fn`) assume that # the traced constructor has `name` and `value` kwargs, enabling # them to override the value of an RV according to its name. # User-defined RVs inherit their name from the provided # distribution; this helper method exposes the name as a dummy kwarg # so that it's visible to program transformations. del name # unused return RandomVariable ( distribution = distribution , sample_shape = sample_shape , value = value )", "nl": "RandomVariable constructor with a dummy name argument ."}}
{"translation": {"code": "def _simple_name ( distribution ) : simple_name = distribution . name # turn 'scope/x/' into 'x' if simple_name . endswith ( '/' ) : simple_name = simple_name . split ( '/' ) [ - 2 ] # turn 'x_3' into 'x' parts = simple_name . split ( '_' ) if parts [ - 1 ] . isdigit ( ) : simple_name = '_' . join ( parts [ : - 1 ] ) return simple_name", "nl": "Infer the original name passed into a distribution constructor ."}}
{"translation": {"code": "def sample_shape_tensor ( self , name = \"sample_shape_tensor\" ) : with tf . compat . v1 . name_scope ( name ) : if isinstance ( self . _sample_shape , tf . Tensor ) : return self . _sample_shape return tf . convert_to_tensor ( value = self . sample_shape . as_list ( ) , dtype = tf . int32 )", "nl": "Sample shape of random variable as a 1 - D Tensor ."}}
{"translation": {"code": "def forward_log_det_jacobian_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( transformed_state_parts , event_ndims ) : return sum ( [ b . forward_log_det_jacobian ( sp , event_ndims = e ) for b , e , sp in zip ( bijector , event_ndims , transformed_state_parts ) ] ) return fn", "nl": "Makes a function which applies a list of Bijectors log_det_jacobian s ."}}
{"translation": {"code": "def forward_transform_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( transformed_state_parts ) : return [ b . forward ( sp ) for b , sp in zip ( bijector , transformed_state_parts ) ] return fn", "nl": "Makes a function which applies a list of Bijectors forward s ."}}
{"translation": {"code": "def inverse_transform_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( state_parts ) : return [ b . inverse ( sp ) for b , sp in zip ( bijector , state_parts ) ] return fn", "nl": "Makes a function which applies a list of Bijectors inverse s ."}}
{"translation": {"code": "def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'transformed_kernel' , 'one_step' ) , values = [ previous_kernel_results ] ) : transformed_next_state , kernel_results = self . _inner_kernel . one_step ( previous_kernel_results . transformed_state , previous_kernel_results . inner_results ) transformed_next_state_parts = ( transformed_next_state if mcmc_util . is_list_like ( transformed_next_state ) else [ transformed_next_state ] ) next_state_parts = self . _forward_transform ( transformed_next_state_parts ) next_state = ( next_state_parts if mcmc_util . is_list_like ( transformed_next_state ) else next_state_parts [ 0 ] ) kernel_results = TransformedTransitionKernelResults ( transformed_state = transformed_next_state , inner_results = kernel_results ) return next_state , kernel_results", "nl": "Runs one iteration of the Transformed Kernel ."}}
{"translation": {"code": "def hager_zhang ( value_and_gradients_function , initial_step_size = None , value_at_initial_step = None , value_at_zero = None , converged = None , threshold_use_approximate_wolfe_condition = 1e-6 , shrinkage_param = 0.66 , expansion_param = 5.0 , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , step_size_shrink_param = 0.1 , max_iterations = 50 , name = None ) : with tf . compat . v1 . name_scope ( name , 'hager_zhang' , [ initial_step_size , value_at_initial_step , value_at_zero , converged , threshold_use_approximate_wolfe_condition , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ] ) : val_0 , val_initial , f_lim , prepare_evals = _prepare_args ( value_and_gradients_function , initial_step_size , value_at_initial_step , value_at_zero , threshold_use_approximate_wolfe_condition ) valid_inputs = ( hzl . is_finite ( val_0 ) & ( val_0 . df < 0 ) & tf . math . is_finite ( val_initial . x ) & ( val_initial . x > 0 ) ) if converged is None : init_converged = tf . zeros_like ( valid_inputs ) # i.e. all false. else : init_converged = tf . convert_to_tensor ( value = converged ) failed = ~ init_converged & ~ valid_inputs active = ~ init_converged & valid_inputs # Note: _fix_step_size returns immediately if either all inputs are invalid # or none of the active ones need fixing. fix_step_evals , val_c , fix_failed = _fix_step_size ( value_and_gradients_function , val_initial , active , step_size_shrink_param ) init_interval = HagerZhangLineSearchResult ( converged = init_converged , failed = failed | fix_failed , func_evals = prepare_evals + fix_step_evals , iterations = tf . convert_to_tensor ( value = 0 ) , left = val_0 , right = hzl . val_where ( init_converged , val_0 , val_c ) ) def _apply_bracket_and_search ( ) : \"\"\"Bracketing and searching to do for valid inputs.\"\"\" return _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) init_active = ~ init_interval . failed & ~ init_interval . converged return prefer_static . cond ( tf . reduce_any ( input_tensor = init_active ) , _apply_bracket_and_search , lambda : init_interval )", "nl": "The Hager Zhang line search algorithm ."}}
{"translation": {"code": "def _line_search_after_bracketing ( value_and_gradients_function , search_interval , val_0 , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param ) : def _loop_cond ( curr_interval ) : \"\"\"Loop condition.\"\"\" active = ~ ( curr_interval . converged | curr_interval . failed ) return ( curr_interval . iterations < max_iterations ) & tf . reduce_any ( input_tensor = active ) def _loop_body ( curr_interval ) : \"\"\"The loop body.\"\"\" secant2_raw_result = hzl . secant2 ( value_and_gradients_function , val_0 , curr_interval , f_lim , sufficient_decrease_param , curvature_param ) secant2_result = HagerZhangLineSearchResult ( converged = secant2_raw_result . converged , failed = secant2_raw_result . failed , iterations = curr_interval . iterations + 1 , func_evals = secant2_raw_result . num_evals , left = secant2_raw_result . left , right = secant2_raw_result . right ) should_check_shrinkage = ~ ( secant2_result . converged | secant2_result . failed ) def _do_check_shrinkage ( ) : \"\"\"Check if interval has shrinked enough.\"\"\" old_width = curr_interval . right . x - curr_interval . left . x new_width = secant2_result . right . x - secant2_result . left . x sufficient_shrinkage = new_width < old_width * shrinkage_param func_is_flat = ( _very_close ( curr_interval . left . f , curr_interval . right . f ) & _very_close ( secant2_result . left . f , secant2_result . right . f ) ) new_converged = ( should_check_shrinkage & sufficient_shrinkage & func_is_flat ) needs_inner_bisect = should_check_shrinkage & ~ sufficient_shrinkage inner_bisect_args = secant2_result . _replace ( converged = secant2_result . converged | new_converged ) def _apply_inner_bisect ( ) : return _line_search_inner_bisection ( value_and_gradients_function , inner_bisect_args , needs_inner_bisect , f_lim ) return prefer_static . cond ( tf . reduce_any ( input_tensor = needs_inner_bisect ) , _apply_inner_bisect , lambda : inner_bisect_args ) next_args = prefer_static . cond ( tf . reduce_any ( input_tensor = should_check_shrinkage ) , _do_check_shrinkage , lambda : secant2_result ) interval_shrunk = ( ~ next_args . failed & _very_close ( next_args . left . x , next_args . right . x ) ) return [ next_args . _replace ( converged = next_args . converged | interval_shrunk ) ] return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ search_interval ] , parallel_iterations = 1 ) [ 0 ]", "nl": "The main loop of line search after the minimum has been bracketed ."}}
{"translation": {"code": "def _prepare_args ( value_and_gradients_function , initial_step_size , val_initial , val_0 , approximate_wolfe_threshold ) : eval_count = 0 if val_initial is None : if initial_step_size is not None : initial_step_size = tf . convert_to_tensor ( value = initial_step_size ) else : initial_step_size = tf . convert_to_tensor ( value = 1.0 , dtype = tf . float32 ) val_initial = value_and_gradients_function ( initial_step_size ) eval_count += 1 if val_0 is None : x_0 = tf . zeros_like ( val_initial . x ) val_0 = value_and_gradients_function ( x_0 ) eval_count += 1 f_lim = val_0 . f + ( approximate_wolfe_threshold * tf . abs ( val_0 . f ) ) return val_0 , val_initial , f_lim , tf . convert_to_tensor ( value = eval_count )", "nl": "Prepares the arguments for the line search initialization ."}}
{"translation": {"code": "def _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) : bracket_result = hzl . bracket ( value_and_gradients_function , init_interval , f_lim , max_iterations , expansion_param ) converged = init_interval . converged | _very_close ( bracket_result . left . x , bracket_result . right . x ) # We fail if we have not yet converged but already exhausted all iterations. exhausted_iterations = ~ converged & tf . greater_equal ( bracket_result . iteration , max_iterations ) line_search_args = HagerZhangLineSearchResult ( converged = converged , failed = bracket_result . failed | exhausted_iterations , iterations = bracket_result . iteration , func_evals = bracket_result . num_evals , left = bracket_result . left , right = bracket_result . right ) return _line_search_after_bracketing ( value_and_gradients_function , line_search_args , init_interval . left , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param )", "nl": "Brackets the minimum and performs a line search ."}}
{"translation": {"code": "def pad_shape_right_with_ones ( x , ndims ) : if not ( isinstance ( ndims , int ) and ndims >= 0 ) : raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) if ndims == 0 : return x x = tf . convert_to_tensor ( value = x ) original_shape = x . shape new_shape = distribution_util . pad ( tf . shape ( input = x ) , axis = 0 , back = True , value = 1 , count = ndims ) x = tf . reshape ( x , new_shape ) x . set_shape ( original_shape . concatenate ( [ 1 ] * ndims ) ) return x", "nl": "Maybe add ndims ones to x . shape on the right ."}}
{"translation": {"code": "def _call_reshape_input_output ( self , fn , x , extra_kwargs = None ) : # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs` # because it is possible the user provided extra kwargs would itself # have `fn` and/or `x` as a key. with tf . control_dependencies ( self . _runtime_assertions + self . _validate_sample_arg ( x ) ) : sample_shape , static_sample_shape = self . _sample_shape ( x ) old_shape = tf . concat ( [ sample_shape , self . distribution . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = 0 ) x_reshape = tf . reshape ( x , old_shape ) result = fn ( x_reshape , * * extra_kwargs ) if extra_kwargs else fn ( x_reshape ) new_shape = tf . concat ( [ sample_shape , self . _batch_shape_unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new_shape ) if ( tensorshape_util . rank ( static_sample_shape ) is not None and tensorshape_util . rank ( self . batch_shape ) is not None ) : new_shape = tensorshape_util . concatenate ( static_sample_shape , self . batch_shape ) tensorshape_util . set_shape ( result , new_shape ) return result", "nl": "Calls fn appropriately reshaping its input x and output ."}}
{"translation": {"code": "def _sample_shape ( self , x ) : x_ndims = ( tf . rank ( x ) if tensorshape_util . rank ( x . shape ) is None else tensorshape_util . rank ( x . shape ) ) event_ndims = ( tf . size ( input = self . event_shape_tensor ( ) ) if tensorshape_util . rank ( self . event_shape ) is None else tensorshape_util . rank ( self . event_shape ) ) batch_ndims = ( tf . size ( input = self . _batch_shape_unexpanded ) if tensorshape_util . rank ( self . batch_shape ) is None else tensorshape_util . rank ( self . batch_shape ) ) sample_ndims = x_ndims - batch_ndims - event_ndims if isinstance ( sample_ndims , int ) : static_sample_shape = x . shape [ : sample_ndims ] else : static_sample_shape = tf . TensorShape ( None ) if tensorshape_util . is_fully_defined ( static_sample_shape ) : sample_shape = np . int32 ( static_sample_shape ) else : sample_shape = tf . shape ( input = x ) [ : sample_ndims ] return sample_shape , static_sample_shape", "nl": "Computes graph and static sample_shape ."}}
{"translation": {"code": "def interpolate_loc ( grid , loc ) : if len ( loc ) != 2 : raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( loc ) ) ) deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) with tf . name_scope ( \"interpolate_loc\" ) : if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : return [ None ] * deg # shape: [B, 1, k, deg] w = grid [ ... , tf . newaxis , : , : ] loc = [ x [ ... , tf . newaxis ] # shape: [B, e, 1] if x is not None else None for x in loc ] if loc [ 0 ] is None : x = w [ ... , 1 , : ] * loc [ 1 ] # shape: [B, e, deg] elif loc [ 1 ] is None : x = w [ ... , 0 , : ] * loc [ 0 ] # shape: [B, e, deg] else : delta = loc [ 0 ] - loc [ 1 ] x = w [ ... , 0 , : ] * delta + loc [ 1 ] # shape: [B, e, deg] return [ x [ ... , k ] for k in range ( deg ) ]", "nl": "Helper which interpolates between two locs ."}}
{"translation": {"code": "def determine_batch_event_shapes ( grid , endpoint_affine ) : with tf . name_scope ( \"determine_batch_event_shapes\" ) : # grid  # shape: [B, k, q] # endpoint_affine     # len=k, shape: [B, d, d] batch_shape = grid . shape [ : - 2 ] batch_shape_tensor = tf . shape ( input = grid ) [ : - 2 ] event_shape = None event_shape_tensor = None def _set_event_shape ( shape , shape_tensor ) : if event_shape is None : return shape , shape_tensor return ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) for aff in endpoint_affine : if aff . shift is not None : batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - 1 ] ) batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) if aff . scale is not None : batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor", "nl": "Helper to infer batch_shape and event_shape ."}}
{"translation": {"code": "def maybe_check_quadrature_param ( param , name , validate_args ) : with tf . name_scope ( \"check_\" + name ) : assertions = [ ] if tensorshape_util . rank ( param . shape ) is not None : if tensorshape_util . rank ( param . shape ) == 0 : raise ValueError ( \"Mixing params must be a (batch of) vector; \" \"{}.rank={} is not at least one.\" . format ( name , tensorshape_util . rank ( param . shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_rank_at_least ( param , 1 , message = ( \"Mixing params must be a (batch of) vector; \" \"{}.rank is not at least one.\" . format ( name ) ) ) ) # TODO(jvdillon): Remove once we support k-mixtures. if tensorshape_util . with_rank_at_least ( param . shape , 1 ) [ - 1 ] is not None : if tf . compat . dimension_value ( param . shape [ - 1 ] ) != 1 : raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"{}.shape[-1]={} is not 1.\" . format ( name , tf . compat . dimension_value ( param . shape [ - 1 ] ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( \"Currently only bimixtures are supported; \" \"{}.shape[-1] is not 1.\" . format ( name ) ) ) ) if assertions : return distribution_util . with_dependencies ( assertions , param ) return param", "nl": "Helper which checks validity of loc and scale init args ."}}
{"translation": {"code": "def _harmonic_number ( x ) : one = tf . ones ( [ ] , dtype = x . dtype ) return tf . math . digamma ( x + one ) - tf . math . digamma ( one )", "nl": "Compute the harmonic number from its analytic continuation ."}}
{"translation": {"code": "def _compute_min_event_ndims ( bijector_list , compute_forward = True ) : min_event_ndims = 0 # This is a mouthful, but what this encapsulates is that if not for rank # changing bijectors, we'd only need to compute the largest of the min # required ndims. Hence \"max_min\". Due to rank changing bijectors, we need to # account for synthetic rank growth / synthetic rank decrease from a rank # changing bijector. rank_changed_adjusted_max_min_event_ndims = 0 if compute_forward : bijector_list = reversed ( bijector_list ) for b in bijector_list : if compute_forward : current_min_event_ndims = b . forward_min_event_ndims current_inverse_min_event_ndims = b . inverse_min_event_ndims else : current_min_event_ndims = b . inverse_min_event_ndims current_inverse_min_event_ndims = b . forward_min_event_ndims # New dimensions were touched. if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims : min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) # If the number of dimensions has increased via forward, then # inverse_min_event_ndims > forward_min_event_ndims, and hence the # dimensions we computed on, have moved left (so we have operated # on additional dimensions). # Conversely, if the number of dimensions has decreased via forward, # then we have inverse_min_event_ndims < forward_min_event_ndims, # and so we will have operated on fewer right most dimensions. number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions return min_event_ndims", "nl": "Computes the min_event_ndims associated with the give list of bijectors ."}}
{"translation": {"code": "def vector_size_to_square_matrix_size ( d , validate_args , name = None ) : if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. if float ( int ( n ) ) != n : raise ValueError ( \"Vector length is not a triangular number.\" ) return int ( n ) else : with tf . name_scope ( name or \"vector_size_to_square_matrix_size\" ) as name : n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. if validate_args : with tf . control_dependencies ( [ assert_util . assert_equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = \"Vector length is not a triangular number\" ) ] ) : n = tf . identity ( n ) return tf . cast ( n , d . dtype )", "nl": "Convert a vector size to a matrix size ."}}
{"translation": {"code": "def _as_tensor ( x , name , dtype ) : return None if x is None else tf . convert_to_tensor ( value = x , name = name , dtype = dtype )", "nl": "Convenience to convert to Tensor or leave as None ."}}
{"translation": {"code": "def entropy_lower_bound ( self , name = \"entropy_lower_bound\" ) : with self . _name_scope ( name ) : with tf . control_dependencies ( self . _assertions ) : distribution_entropies = [ d . entropy ( ) for d in self . components ] cat_probs = self . _cat_probs ( log_probs = False ) partial_entropies = [ c_p * m for ( c_p , m ) in zip ( cat_probs , distribution_entropies ) ] # These are all the same shape by virtue of matching batch_shape return tf . add_n ( partial_entropies )", "nl": "r A lower bound on the entropy of this mixture model ."}}
{"translation": {"code": "def _call_and_reshape_output ( self , fn , event_shape_list = None , static_event_shape_list = None , extra_kwargs = None ) : # Note: we take `extra_kwargs` as a dict rather than `**extra_kwargs` # because it is possible the user provided extra kwargs would itself # have `fn`, `event_shape_list`, `static_event_shape_list` and/or # `extra_kwargs` as keys. with tf . control_dependencies ( self . _runtime_assertions ) : if event_shape_list is None : event_shape_list = [ self . _event_shape_tensor ( ) ] if static_event_shape_list is None : static_event_shape_list = [ self . event_shape ] new_shape = tf . concat ( [ self . _batch_shape_unexpanded ] + event_shape_list , axis = 0 ) result = tf . reshape ( fn ( * * extra_kwargs ) if extra_kwargs else fn ( ) , new_shape ) if ( tensorshape_util . rank ( self . batch_shape ) is not None and tensorshape_util . rank ( self . event_shape ) is not None ) : event_shape = tf . TensorShape ( [ ] ) for rss in static_event_shape_list : event_shape = tensorshape_util . concatenate ( event_shape , rss ) static_shape = tensorshape_util . concatenate ( self . batch_shape , event_shape ) tensorshape_util . set_shape ( result , static_shape ) return result", "nl": "Calls fn and appropriately reshapes its output ."}}
{"translation": {"code": "def _cat_probs ( self , log_probs ) : which_softmax = tf . nn . log_softmax if log_probs else tf . nn . softmax cat_probs = which_softmax ( self . cat . logits ) cat_probs = tf . unstack ( cat_probs , num = self . num_components , axis = - 1 ) return cat_probs", "nl": "Get a list of num_components batchwise probabilities ."}}
{"translation": {"code": "def quadrature_scheme_lognormal_gauss_hermite ( loc , scale , quadrature_size , validate_args = False , name = None ) : # pylint: disable=unused-argument with tf . name_scope ( name or \"vector_diffeomixture_quadrature_gauss_hermite\" ) : grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) npdt = dtype_util . as_numpy_dtype ( loc . dtype ) grid = grid . astype ( npdt ) probs = probs . astype ( npdt ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = loc . dtype ) # The following maps the broadcast of `loc` and `scale` to each grid # point, i.e., we are creating several log-rates that correspond to the # different Gauss-Hermite quadrature points and (possible) batches of # `loc` and `scale`. grid = ( loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * scale [ ... , tf . newaxis ] * grid ) return grid , probs", "nl": "Use Gauss - Hermite quadrature to form quadrature on positive - reals ."}}
{"translation": {"code": "def quadrature_scheme_lognormal_quantiles ( loc , scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or \"quadrature_scheme_lognormal_quantiles\" ) : # Create a LogNormal distribution. dist = transformed_distribution . TransformedDistribution ( distribution = normal . Normal ( loc = loc , scale = scale ) , bijector = exp_bijector . Exp ( ) , validate_args = validate_args ) batch_ndims = tensorshape_util . rank ( dist . batch_shape ) if batch_ndims is None : batch_ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] def _compute_quantiles ( ) : \"\"\"Helper to build quantiles.\"\"\" # Omit {0, 1} since they might lead to Inf/NaN. zero = tf . zeros ( [ ] , dtype = dist . dtype ) edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] # Expand edges so its broadcast across batch dims. edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) quantiles = dist . quantile ( edges ) # Cyclically permute left by one. perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) quantiles = tf . transpose ( a = quantiles , perm = perm ) return quantiles quantiles = _compute_quantiles ( ) # Compute grid as quantile midpoints. grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. # Set shape hints. new_shape = tensorshape_util . concatenate ( dist . batch_shape , [ quadrature_size ] ) tensorshape_util . set_shape ( grid , new_shape ) # By construction probs is constant, i.e., `1 / quadrature_size`. This is # important, because non-constant probs leads to non-reparameterizable # samples. probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) return grid , probs", "nl": "Use LogNormal quantiles to form quadrature on positive - reals ."}}
{"translation": {"code": "def quadrature_scheme_softmaxnormal_gauss_hermite ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or \"quadrature_scheme_softmaxnormal_gauss_hermite\" ) : normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) npdt = dtype_util . as_numpy_dtype ( normal_loc . dtype ) normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = npdt , name = \"normal_scale\" ) normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) grid = grid . astype ( npdt ) probs = probs . astype ( npdt ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = npdt ) grid = softmax ( - distribution_util . pad ( ( normal_loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * normal_scale [ ... , tf . newaxis ] * grid ) , axis = - 2 , front = True ) , axis = - 2 ) # shape: [B, components, deg] return grid , probs", "nl": "Use Gauss - Hermite quadrature to form quadrature on K - 1 simplex ."}}
{"translation": {"code": "def quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or \"softmax_normal_grid_and_probs\" ) : normal_loc = tf . convert_to_tensor ( value = normal_loc , name = \"normal_loc\" ) dt = dtype_util . base_dtype ( normal_loc . dtype ) normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = \"normal_scale\" ) normal_scale = maybe_check_quadrature_param ( normal_scale , \"normal_scale\" , validate_args ) dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) def _get_batch_ndims ( ) : \"\"\"Helper to get rank(dist.batch_shape), statically if possible.\"\"\" ndims = tensorshape_util . rank ( dist . batch_shape ) if ndims is None : ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] return ndims batch_ndims = _get_batch_ndims ( ) def _get_final_shape ( qs ) : \"\"\"Helper to build `TensorShape`.\"\"\" bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , 1 ) num_components = tf . compat . dimension_value ( bs [ - 1 ] ) if num_components is not None : num_components += 1 tail = tf . TensorShape ( [ num_components , qs ] ) return bs [ : - 1 ] . concatenate ( tail ) def _compute_quantiles ( ) : \"\"\"Helper to build quantiles.\"\"\" # Omit {0, 1} since they might lead to Inf/NaN. zero = tf . zeros ( [ ] , dtype = dist . dtype ) edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] # Expand edges so its broadcast across batch dims. edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) quantiles = dist . quantile ( edges ) quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) # Cyclically permute left by one. perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) quantiles = tf . transpose ( a = quantiles , perm = perm ) tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + 1 ) ) return quantiles quantiles = _compute_quantiles ( ) # Compute grid as quantile midpoints. grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. # Set shape hints. tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) # By construction probs is constant, i.e., `1 / quadrature_size`. This is # important, because non-constant probs leads to non-reparameterizable # samples. probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) return grid , probs", "nl": "Use SoftmaxNormal quantiles to form quadrature on K - 1 simplex ."}}
{"translation": {"code": "def interpolate_scale ( grid , scale ) : if len ( scale ) != 2 : raise NotImplementedError ( \"Currently only bimixtures are supported; \" \"len(scale)={} is not 2.\" . format ( len ( scale ) ) ) deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise ValueError ( \"Num quadrature grid points must be known prior \" \"to graph execution.\" ) with tf . name_scope ( \"interpolate_scale\" ) : return [ linop_add_lib . add_operators ( [ linop_scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ]", "nl": "Helper which interpolates between two scales ."}}
{"translation": {"code": "def real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , # pylint: disable=keyword-arg-before-vararg * * kwargs ) : with tf . compat . v2 . name_scope ( name or \"real_nvp_default_template\" ) : def _fn ( x , output_units , * * condition_kwargs ) : \"\"\"Fully connected MLP parameterized via `real_nvp_template`.\"\"\" if condition_kwargs : raise NotImplementedError ( \"Conditioning not implemented in the default template.\" ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] reshape_output = lambda x : x [ 0 ] else : reshape_output = lambda x : x for units in hidden_layers : x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , # pylint: disable=keyword-arg-before-vararg * * kwargs ) x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2 ) * output_units , activation = None , * args , # pylint: disable=keyword-arg-before-vararg * * kwargs ) if shift_only : return reshape_output ( x ) , None shift , log_scale = tf . split ( x , 2 , axis = - 1 ) return reshape_output ( shift ) , reshape_output ( log_scale ) return tf . compat . v1 . make_template ( \"real_nvp_default_template\" , _fn )", "nl": "Build a scale - and - shift function using a multi - layer neural network ."}}
{"translation": {"code": "def move_dimension ( x , source_idx , dest_idx ) : ndims = prefer_static_rank ( x ) dtype = dtype_util . common_dtype ( [ source_idx , dest_idx ] , preferred_dtype = tf . int32 ) source_idx = tf . convert_to_tensor ( value = source_idx , dtype = dtype ) dest_idx = tf . convert_to_tensor ( value = dest_idx , dtype = dtype ) # Handle negative indexing. source_idx = pick_scalar_condition ( source_idx < 0 , ndims + source_idx , source_idx ) dest_idx = pick_scalar_condition ( dest_idx < 0 , ndims + dest_idx , dest_idx ) # Construct the appropriate permutation of dimensions, depending # whether the source is before or after the destination. def move_left_permutation ( ) : return prefer_static_value ( tf . concat ( [ tf . range ( 0 , dest_idx , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) def move_right_permutation ( ) : return prefer_static_value ( tf . concat ( [ tf . range ( 0 , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , dest_idx + 1 , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) def x_permuted ( ) : return tf . transpose ( a = x , perm = prefer_static . cond ( source_idx < dest_idx , move_right_permutation , move_left_permutation ) ) # One final conditional to handle the special case where source # and destination indices are equal. return prefer_static . cond ( tf . equal ( source_idx , dest_idx ) , lambda : x , x_permuted )", "nl": "Move a single tensor dimension within its shape ."}}
{"translation": {"code": "def pad_mixture_dimensions ( x , mixture_distribution , categorical_distribution , event_ndims ) : with tf . name_scope ( \"pad_mix_dims\" ) : def _get_ndims ( d ) : if tensorshape_util . rank ( d . batch_shape ) is not None : return tensorshape_util . rank ( d . batch_shape ) return tf . shape ( input = d . batch_shape_tensor ( ) ) [ 0 ] dist_batch_ndims = _get_ndims ( mixture_distribution ) cat_batch_ndims = _get_ndims ( categorical_distribution ) pad_ndims = tf . where ( categorical_distribution . is_scalar_batch ( ) , dist_batch_ndims , dist_batch_ndims - cat_batch_ndims ) s = tf . shape ( input = x ) x = tf . reshape ( x , shape = tf . concat ( [ s [ : - 1 ] , tf . ones ( [ pad_ndims ] , dtype = tf . int32 ) , s [ - 1 : ] , tf . ones ( [ event_ndims ] , dtype = tf . int32 ) , ] , axis = 0 ) ) return x", "nl": "Pad dimensions of event tensors for mixture distributions ."}}
{"translation": {"code": "def maybe_check_scalar_distribution ( distribution , expected_base_dtype , validate_args ) : if distribution . dtype != expected_base_dtype : raise TypeError ( \"dtype mismatch; \" \"distribution.dtype=\\\"{}\\\" is not \\\"{}\\\"\" . format ( dtype_util . name ( distribution . dtype ) , dtype_util . name ( expected_base_dtype ) ) ) # Although `reparameterization_type` is a static property, we guard it by # `validate_args`. This allows users to use a `distribution` which is not # reparameterized itself. However, we tacitly assume that although the # distribution is not reparameterized, it only depends on non-trainable # variables. if validate_args and ( distribution . reparameterization_type != reparameterization . FULLY_REPARAMETERIZED ) : raise ValueError ( \"Base distribution should be reparameterized or be \" \"a function of non-trainable variables; \" \"distribution.reparameterization_type = \\\"{}\\\" \" \"!= \\\"FULLY_REPARAMETERIZED\\\".\" . format ( distribution . reparameterization_type ) ) with tf . name_scope ( \"check_distribution\" ) : assertions = [ ] def check_is_scalar ( is_scalar , name ) : is_scalar_ = tf . get_static_value ( is_scalar ) if is_scalar_ is not None : if not is_scalar_ : raise ValueError ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( is_scalar , True , message = ( \"distribution must be scalar; \" \"distribution.{}=False is not True\" . format ( name ) ) ) ) check_is_scalar ( distribution . is_scalar_event ( ) , \"is_scalar_event\" ) check_is_scalar ( distribution . is_scalar_batch ( ) , \"is_scalar_batch\" ) return assertions", "nl": "Helper which checks validity of a scalar distribution init arg ."}}
{"translation": {"code": "def is_diagonal_scale ( scale ) : if not isinstance ( scale , tf . linalg . LinearOperator ) : raise TypeError ( \"Expected argument 'scale' to be instance of LinearOperator\" \". Found: %s\" % scale ) return ( isinstance ( scale , tf . linalg . LinearOperatorIdentity ) or isinstance ( scale , tf . linalg . LinearOperatorScaledIdentity ) or isinstance ( scale , tf . linalg . LinearOperatorDiag ) )", "nl": "Returns True if scale is a LinearOperator that is known to be diag ."}}
{"translation": {"code": "def shapes_from_loc_and_scale ( loc , scale , name = \"shapes_from_loc_and_scale\" ) : if loc is not None and tensorshape_util . rank ( loc . shape ) == 0 : loc = None # scalar loc is irrelevant to determining batch/event shape. with tf . name_scope ( name ) : # Get event shape. event_size = scale . range_dimension_tensor ( ) event_size_ = tf . get_static_value ( event_size ) loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - 1 ] ) ) if event_size_ is not None and loc_event_size_ is not None : # Static check that event shapes match. if loc_event_size_ != 1 and loc_event_size_ != event_size_ : raise ValueError ( \"Event size of 'scale' ({}) could not be broadcast up to that \" \"of 'loc' ({}).\" . format ( event_size_ , loc_event_size_ ) ) elif loc_event_size_ is not None and loc_event_size_ != 1 : event_size_ = loc_event_size_ if event_size_ is None : event_shape = event_size [ tf . newaxis ] else : event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ 1 ] ) , dtype = tf . int32 , name = \"event_shape\" ) # Get batch shape. batch_shape = scale . batch_shape_tensor ( ) if loc is not None : loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , 1 ) [ : - 1 ] if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : loc_batch_shape = tf . shape ( input = loc ) [ : - 1 ] else : loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = \"loc_batch_shape\" ) # This is defined in the core util module. batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) # pylint: disable=undefined-variable batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = \"batch_shape\" ) return batch_shape , event_shape", "nl": "Infer distribution batch and event shapes from a location and scale ."}}
{"translation": {"code": "def make_diag_scale ( loc = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None , dtype = None ) : def _maybe_attach_assertion ( x ) : if not validate_args : return x if assert_positive : return with_dependencies ( [ assert_util . assert_positive ( x , message = \"diagonal part must be positive\" ) , ] , x ) return with_dependencies ( [ assert_util . assert_none_equal ( x , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) ] , x ) with tf . name_scope ( name or \"make_diag_scale\" ) : if dtype is None : dtype = dtype_util . common_dtype ( [ loc , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) if scale_diag is not None : if scale_identity_multiplier is not None : scale_diag += scale_identity_multiplier [ ... , tf . newaxis ] return tf . linalg . LinearOperatorDiag ( diag = _maybe_attach_assertion ( scale_diag ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive ) if loc is None and shape_hint is None : raise ValueError ( \"Cannot infer `event_shape` unless `loc` or \" \"`shape_hint` is specified.\" ) num_rows = shape_hint del shape_hint if num_rows is None : num_rows = tf . compat . dimension_value ( loc . shape [ - 1 ] ) if num_rows is None : num_rows = tf . shape ( input = loc ) [ - 1 ] if scale_identity_multiplier is None : return tf . linalg . LinearOperatorIdentity ( num_rows = num_rows , dtype = dtype , is_self_adjoint = True , is_positive_definite = True , assert_proper_shapes = validate_args ) return tf . linalg . LinearOperatorScaledIdentity ( num_rows = num_rows , multiplier = _maybe_attach_assertion ( scale_identity_multiplier ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive , assert_proper_shapes = validate_args )", "nl": "Creates a LinearOperator representing a diagonal matrix ."}}
{"translation": {"code": "def make_tril_scale ( loc = None , scale_tril = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None ) : def _maybe_attach_assertion ( x ) : if not validate_args : return x if assert_positive : return with_dependencies ( [ assert_util . assert_positive ( tf . linalg . diag_part ( x ) , message = \"diagonal part must be positive\" ) , ] , x ) return with_dependencies ( [ assert_util . assert_none_equal ( tf . linalg . diag_part ( x ) , tf . zeros ( [ ] , x . dtype ) , message = \"diagonal part must be non-zero\" ) , ] , x ) with tf . name_scope ( name or \"make_tril_scale\" ) : dtype = dtype_util . common_dtype ( [ loc , scale_tril , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) loc = _convert_to_tensor ( loc , name = \"loc\" , dtype = dtype ) scale_tril = _convert_to_tensor ( scale_tril , name = \"scale_tril\" , dtype = dtype ) scale_diag = _convert_to_tensor ( scale_diag , name = \"scale_diag\" , dtype = dtype ) scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = \"scale_identity_multiplier\" , dtype = dtype ) if scale_tril is not None : scale_tril = tf . linalg . band_part ( scale_tril , - 1 , 0 ) # Zero out TriU. tril_diag = tf . linalg . diag_part ( scale_tril ) if scale_diag is not None : tril_diag += scale_diag if scale_identity_multiplier is not None : tril_diag += scale_identity_multiplier [ ... , tf . newaxis ] scale_tril = tf . linalg . set_diag ( scale_tril , tril_diag ) return tf . linalg . LinearOperatorLowerTriangular ( tril = _maybe_attach_assertion ( scale_tril ) , is_non_singular = True , is_self_adjoint = False , is_positive_definite = assert_positive ) return make_diag_scale ( loc = loc , scale_diag = scale_diag , scale_identity_multiplier = scale_identity_multiplier , shape_hint = shape_hint , validate_args = validate_args , assert_positive = assert_positive , name = name )", "nl": "Creates a LinearOperator representing a lower triangular matrix ."}}
{"translation": {"code": "def mixture_stddev ( mixture_weight_vector , mean_vector , stddev_vector ) : tensorshape_util . assert_has_rank ( mixture_weight_vector . shape , 2 ) if not tensorshape_util . is_compatible_with ( mean_vector . shape , mixture_weight_vector . shape ) : raise ValueError ( \"Expecting means to have same shape as mixture weights.\" ) if not tensorshape_util . is_compatible_with ( stddev_vector . shape , mixture_weight_vector . shape ) : raise ValueError ( \"Expecting stddevs to have same shape as mixture weights.\" ) # Reshape the distribution parameters for batched vectorized dot products. pi_for_dot_prod = tf . expand_dims ( mixture_weight_vector , axis = 1 ) mu_for_dot_prod = tf . expand_dims ( mean_vector , axis = 2 ) sigma_for_dot_prod = tf . expand_dims ( stddev_vector , axis = 2 ) # weighted average of component means under mixture distribution. mean_wa = tf . matmul ( pi_for_dot_prod , mu_for_dot_prod ) mean_wa = tf . reshape ( mean_wa , ( - 1 , ) ) # weighted average of component variances under mixture distribution. var_wa = tf . matmul ( pi_for_dot_prod , tf . square ( sigma_for_dot_prod ) ) var_wa = tf . reshape ( var_wa , ( - 1 , ) ) # weighted average of component squared means under mixture distribution. sq_mean_wa = tf . matmul ( pi_for_dot_prod , tf . square ( mu_for_dot_prod ) ) sq_mean_wa = tf . reshape ( sq_mean_wa , ( - 1 , ) ) mixture_variance = var_wa + sq_mean_wa - tf . square ( mean_wa ) return tf . sqrt ( mixture_variance )", "nl": "Computes the standard deviation of a mixture distribution ."}}
{"translation": {"code": "def normal_conjugates_known_scale_posterior ( prior , scale , s , n ) : if not isinstance ( prior , normal . Normal ) : raise TypeError ( \"Expected prior to be an instance of type Normal\" ) if s . dtype != prior . dtype : raise TypeError ( \"Observation sum s.dtype does not match prior dtype: %s vs. %s\" % ( s . dtype , prior . dtype ) ) n = tf . cast ( n , prior . dtype ) scale0_2 = tf . square ( prior . scale ) scale_2 = tf . square ( scale ) scalep_2 = 1.0 / ( 1 / scale0_2 + n / scale_2 ) return normal . Normal ( loc = ( prior . loc / scale0_2 + s / scale_2 ) * scalep_2 , scale = tf . sqrt ( scalep_2 ) )", "nl": "Posterior Normal distribution with conjugate prior on the mean ."}}
{"translation": {"code": "def _bdtr ( k , n , p ) : # Trick for getting safe backprop/gradients into n, k when #   betainc(a = 0, ..) = nan # Write: #   where(unsafe, safe_output, betainc(where(unsafe, safe_input, input))) ones = tf . ones_like ( n - k ) k_eq_n = tf . equal ( k , n ) safe_dn = tf . where ( k_eq_n , ones , n - k ) dk = tf . math . betainc ( a = safe_dn , b = k + 1 , x = 1 - p ) return tf . where ( k_eq_n , ones , dk )", "nl": "The binomial cumulative distribution function ."}}
{"translation": {"code": "def _create_scale_operator ( self , identity_multiplier , diag , tril , perturb_diag , perturb_factor , shift , validate_args , dtype ) : identity_multiplier = _as_tensor ( identity_multiplier , \"identity_multiplier\" , dtype ) diag = _as_tensor ( diag , \"diag\" , dtype ) tril = _as_tensor ( tril , \"tril\" , dtype ) perturb_diag = _as_tensor ( perturb_diag , \"perturb_diag\" , dtype ) perturb_factor = _as_tensor ( perturb_factor , \"perturb_factor\" , dtype ) # If possible, use the low rank update to infer the shape of # the identity matrix, when scale represents a scaled identity matrix # with a low rank update. shape_hint = None if perturb_factor is not None : shape_hint = distribution_util . dimension_size ( perturb_factor , axis = - 2 ) if self . _is_only_identity_multiplier : if validate_args : return distribution_util . with_dependencies ( [ assert_util . assert_none_equal ( identity_multiplier , tf . zeros ( [ ] , identity_multiplier . dtype ) , [ \"identity_multiplier should be non-zero.\" ] ) ] , identity_multiplier ) return identity_multiplier scale = distribution_util . make_tril_scale ( loc = shift , scale_tril = tril , scale_diag = diag , scale_identity_multiplier = identity_multiplier , validate_args = validate_args , assert_positive = False , shape_hint = shape_hint ) if perturb_factor is not None : return tf . linalg . LinearOperatorLowRankUpdate ( scale , u = perturb_factor , diag_update = perturb_diag , is_diag_update_positive = perturb_diag is None , is_non_singular = True , # Implied by is_positive_definite=True. is_self_adjoint = True , is_positive_definite = True , is_square = True ) return scale", "nl": "Construct scale from various components ."}}
{"translation": {"code": "def _validate_bn_layer ( self , layer ) : if ( not isinstance ( layer , tf . keras . layers . BatchNormalization ) and not isinstance ( layer , tf . compat . v1 . layers . BatchNormalization ) ) : raise ValueError ( \"batchnorm_layer must be an instance of BatchNormalization layer.\" ) if layer . renorm : raise ValueError ( \"BatchNorm Bijector does not support renormalization.\" ) if layer . virtual_batch_size : raise ValueError ( \"BatchNorm Bijector does not support virtual batch sizes.\" )", "nl": "Check for valid BatchNormalization layer ."}}
{"translation": {"code": "def _undo_batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , name = None ) : with tf . compat . v2 . name_scope ( name or \"undo_batchnorm\" ) : # inv = tf.rsqrt(variance + variance_epsilon) # if scale is not None: #   inv *= scale # return x * inv + ( #     offset - mean * inv if offset is not None else -mean * inv) rescale = tf . sqrt ( variance + variance_epsilon ) if scale is not None : rescale /= scale batch_unnormalized = x * rescale + ( mean - offset * rescale if offset is not None else mean ) return batch_unnormalized", "nl": "r Inverse of tf . nn . batch_normalization ."}}
{"translation": {"code": "def _make_columnar ( self , x ) : if tensorshape_util . rank ( x . shape ) is not None : if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , : ] return x shape = tf . shape ( input = x ) maybe_expanded_shape = tf . concat ( [ shape [ : - 1 ] , distribution_util . pick_vector ( tf . equal ( tf . rank ( x ) , 1 ) , [ 1 ] , np . array ( [ ] , dtype = np . int32 ) ) , shape [ - 1 : ] , ] , 0 ) return tf . reshape ( x , maybe_expanded_shape )", "nl": "Ensures non - scalar input has at least one column ."}}
{"translation": {"code": "def masked_dense ( inputs , units , num_blocks = None , exclusive = False , kernel_initializer = None , reuse = None , name = None , * args , # pylint: disable=keyword-arg-before-vararg * * kwargs ) : # TODO(b/67594795): Better support of dynamic shape. input_depth = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( inputs . shape , 1 ) [ - 1 ] ) if input_depth is None : raise NotImplementedError ( \"Rightmost dimension must be known prior to graph execution.\" ) mask = _gen_mask ( num_blocks , input_depth , units , MASK_EXCLUSIVE if exclusive else MASK_INCLUSIVE ) . T if kernel_initializer is None : kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) def masked_initializer ( shape , dtype = None , partition_info = None ) : return mask * kernel_initializer ( shape , dtype , partition_info ) with tf . compat . v2 . name_scope ( name or \"masked_dense\" ) : layer = tf . compat . v1 . layers . Dense ( units , kernel_initializer = masked_initializer , kernel_constraint = lambda x : mask * x , name = name , dtype = dtype_util . base_dtype ( inputs . dtype ) , _scope = name , _reuse = reuse , * args , # pylint: disable=keyword-arg-before-vararg * * kwargs ) return layer . apply ( inputs )", "nl": "A autoregressively masked dense layer . Analogous to tf . layers . dense ."}}
{"translation": {"code": "def _gen_mask ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE , dtype = tf . float32 ) : # TODO(b/67594795): Better support of dynamic shape. mask = np . zeros ( [ n_out , n_in ] , dtype = dtype . as_numpy_dtype ( ) ) slices = _gen_slices ( num_blocks , n_in , n_out , mask_type = mask_type ) for [ row_slice , col_slice ] in slices : mask [ row_slice , col_slice ] = 1 return mask", "nl": "Generate the mask for building an autoregressive dense layer ."}}
{"translation": {"code": "def _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : # TODO(b/67594795): Better support of dynamic shape. slices = [ ] col = 0 d_in = n_in // num_blocks d_out = n_out // num_blocks row = d_out if mask_type == MASK_EXCLUSIVE else 0 for _ in range ( num_blocks ) : row_slice = slice ( row , None ) col_slice = slice ( col , col + d_in ) slices . append ( [ row_slice , col_slice ] ) col += d_in row += d_out return slices", "nl": "Generate the slices for building an autoregressive mask ."}}
{"translation": {"code": "def assign_log_moving_mean_exp ( log_mean_exp_var , log_value , decay , name = None ) : with tf . compat . v1 . name_scope ( name , \"assign_log_moving_mean_exp\" , [ log_mean_exp_var , log_value , decay ] ) : # We want to update the variable in a numerically stable and lock-free way. # To do this, observe that variable `x` updated by `v` is: # x = log(w exp(x) + (1-w) exp(v)) #   = log(exp(x + log(w)) + exp(v + log1p(-w))) #   = x + log(exp(x - x + log(w)) + exp(v - x + log1p(-w))) #   = x + lse([log(w), v - x + log1p(-w)]) with tf . compat . v1 . colocate_with ( log_mean_exp_var ) : base_dtype = log_mean_exp_var . dtype . base_dtype if not base_dtype . is_floating : raise TypeError ( \"log_mean_exp_var.base_dtype({}) does not have float type \" \"`dtype`.\" . format ( base_dtype . name ) ) log_value = tf . convert_to_tensor ( value = log_value , dtype = base_dtype , name = \"log_value\" ) decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = \"decay\" ) delta = ( log_value - log_mean_exp_var ) [ tf . newaxis , ... ] x = tf . concat ( [ tf . math . log ( decay ) * tf . ones_like ( delta ) , delta + tf . math . log1p ( - decay ) ] , axis = 0 ) x = tf . reduce_logsumexp ( input_tensor = x , axis = 0 ) return log_mean_exp_var . assign_add ( x )", "nl": "Compute the log of the exponentially weighted moving mean of the exp ."}}
{"translation": {"code": "def concat_vectors ( * args ) : args_ = [ tf . get_static_value ( x ) for x in args ] if any ( vec is None for vec in args_ ) : return tf . concat ( args , axis = 0 ) return [ val for vec in args_ for val in vec ]", "nl": "Concatenates input vectors statically if possible ."}}
{"translation": {"code": "def linop_scale ( w , op ) : # We assume w > 0. (This assumption only relates to the is_* attributes.) with tf . name_scope ( \"linop_scale\" ) : # TODO(b/35301104): LinearOperatorComposition doesn't combine operators, so # special case combinations here. Once it does, this function can be # replaced by: #     return linop_composition_lib.LinearOperatorComposition([ #         scaled_identity(w), op]) def scaled_identity ( w ) : return tf . linalg . LinearOperatorScaledIdentity ( num_rows = op . range_dimension_tensor ( ) , multiplier = w , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) if isinstance ( op , tf . linalg . LinearOperatorIdentity ) : return scaled_identity ( w ) if isinstance ( op , tf . linalg . LinearOperatorScaledIdentity ) : return scaled_identity ( w * op . multiplier ) if isinstance ( op , tf . linalg . LinearOperatorDiag ) : return tf . linalg . LinearOperatorDiag ( diag = w [ ... , tf . newaxis ] * op . diag_part ( ) , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) if isinstance ( op , tf . linalg . LinearOperatorLowerTriangular ) : return tf . linalg . LinearOperatorLowerTriangular ( tril = w [ ... , tf . newaxis , tf . newaxis ] * op . to_dense ( ) , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) raise NotImplementedError ( \"Unsupported Linop type ({})\" . format ( type ( op ) . __name__ ) )", "nl": "Creates weighted LinOp from existing LinOp ."}}
{"translation": {"code": "def _outer_squared_difference ( x , y ) : z = x - y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]", "nl": "Convenience function analogous to tf . squared_difference ."}}
{"translation": {"code": "def _expand_to_event_rank ( self , x ) : expanded_x = x for _ in range ( tensorshape_util . rank ( self . event_shape ) ) : expanded_x = tf . expand_dims ( expanded_x , - 1 ) return expanded_x", "nl": "Expand the rank of x up to static_event_rank times for broadcasting ."}}
{"translation": {"code": "def build_kalman_filter_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def kalman_filter_step ( state , elems_t ) : \"\"\"Run a single step of Kalman filtering.\n\n    Args:\n      state: A `KalmanFilterState` object representing the previous\n        filter state at time `t-1`.\n      elems_t: A tuple of Tensors `(x_t, mask_t)`, or a `Tensor` `x_t`.\n        `x_t` is a `Tensor` with rightmost shape dimensions\n        `[observation_size, 1]` representing the vector observed at time `t`,\n        and `mask_t` is a `Tensor` with rightmost dimensions`[1, 1]`\n        representing the observation mask at time `t`. Both `x_t` and `mask_t`\n        may have batch dimensions, which must be compatible with the batch\n        dimensions of `state.predicted_mean` and `state.predictived_cov`\n        respectively. If `mask_t` is not provided, it is assumed to be `None`.\n\n    Returns:\n      new_state: A `KalmanFilterState` object representing the new\n        filter state at time `t`.\n    \"\"\" if isinstance ( elems_t , tuple ) : x_t , mask_t = elems_t else : x_t = elems_t mask_t = None observation_matrix = get_observation_matrix_for_timestep ( state . timestep ) observation_noise = get_observation_noise_for_timestep ( state . timestep ) if mask_t is not None : # Before running the update, fill in masked observations using the prior # expectation. The precise filled value shouldn't matter since updates # from masked elements will not be selected below, but we need to ensure # that any results we incidently compute on masked values are at least # finite (not inf or NaN) so that they don't screw up gradient propagation # through `tf.where`, as described in #  https://github.com/tensorflow/tensorflow/issues/2540. # We fill with the prior expectation because any fixed value such as zero # might be arbitrarily unlikely under the prior, leading to overflow in # the updates, but the prior expectation should always be a # 'reasonable' observation. x_expected = _propagate_mean ( state . predicted_mean , observation_matrix , observation_noise ) * tf . ones_like ( x_t ) x_t = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = x_expected ) ) , x_expected , tf . broadcast_to ( x_t , tf . shape ( input = x_expected ) ) ) # Given predicted mean u_{t|t-1} and covariance P_{t|t-1} from the # previous step, incorporate the observation x_t, producing the # filtered mean u_t and covariance P_t. ( filtered_mean , filtered_cov , observation_dist ) = linear_gaussian_update ( state . predicted_mean , state . predicted_cov , observation_matrix , observation_noise , x_t ) # Compute the marginal likelihood p(x_{t} | x_{:t-1}) for this # observation. log_marginal_likelihood = observation_dist . log_prob ( x_t [ ... , 0 ] ) if mask_t is not None : filtered_mean = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = filtered_mean ) ) , state . predicted_mean , filtered_mean ) filtered_cov = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = filtered_cov ) ) , state . predicted_cov , filtered_cov ) log_marginal_likelihood = tf . where ( tf . broadcast_to ( mask_t [ ... , 0 , 0 ] , tf . shape ( input = log_marginal_likelihood ) ) , tf . zeros_like ( log_marginal_likelihood ) , log_marginal_likelihood ) # Run the filtered posterior through the transition # model to predict the next time step: #  u_{t|t-1} = F_t u_{t-1} + b_t #  P_{t|t-1} = F_t P_{t-1} F_t' + Q_t predicted_mean , predicted_cov = kalman_transition ( filtered_mean , filtered_cov , get_transition_matrix_for_timestep ( state . timestep ) , get_transition_noise_for_timestep ( state . timestep ) ) return KalmanFilterState ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , observation_dist . mean ( ) [ ... , tf . newaxis ] , observation_dist . covariance ( ) , log_marginal_likelihood , state . timestep + 1 ) return kalman_filter_step", "nl": "Build a callable that performs one step of Kalman filtering ."}}
{"translation": {"code": "def _joint_mean ( self ) : with tf . name_scope ( \"mean_joint\" ) : # The initial timestep is a special case, since we sample the # latent state from the prior rather than the transition model. with tf . control_dependencies ( self . runtime_assertions ) : # Broadcast to ensure we represent the full batch shape. initial_latent_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , 1 ] ] , axis = 0 ) ) initial_observation_mean = _propagate_mean ( initial_latent_mean , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) mean_step = build_kalman_mean_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) # Scan over all timesteps following the initial step. ( latent_means , observation_means ) = tf . scan ( mean_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_mean , initial_observation_mean ) ) # Squish the initial step back on top of the other (scanned) timesteps latent_means = tf . concat ( [ initial_latent_mean [ tf . newaxis , ... ] , latent_means ] , axis = 0 ) observation_means = tf . concat ( [ initial_observation_mean [ tf . newaxis , ... ] , observation_means ] , axis = 0 ) # Put dimensions back in order. The samples we've computed have # shape `[num_timesteps, batch_shape, size, 1]`, where `size` # is the dimension of the latent or observation spaces # respectively, but we want to return values with shape # `[batch_shape, num_timesteps, size]`. latent_means = tf . squeeze ( latent_means , - 1 ) latent_means = distribution_util . move_dimension ( latent_means , 0 , - 2 ) observation_means = tf . squeeze ( observation_means , - 1 ) observation_means = distribution_util . move_dimension ( observation_means , 0 , - 2 ) return latent_means , observation_means", "nl": "Compute prior means for all variables via dynamic programming ."}}
{"translation": {"code": "def _augment_sample_shape ( partial_batch_dist , full_sample_and_batch_shape , validate_args = False ) : full_ndims = distribution_util . prefer_static_shape ( full_sample_and_batch_shape ) [ 0 ] partial_batch_ndims = ( tensorshape_util . rank ( partial_batch_dist . batch_shape ) # pylint: disable=g-long-ternary if tensorshape_util . rank ( partial_batch_dist . batch_shape ) is not None else distribution_util . prefer_static_shape ( partial_batch_dist . batch_shape_tensor ( ) ) [ 0 ] ) num_broadcast_dims = full_ndims - partial_batch_ndims expected_partial_batch_shape = ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) expected_partial_batch_shape_static = tf . get_static_value ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) # Raise errors statically if possible. num_broadcast_dims_static = tf . get_static_value ( num_broadcast_dims ) if num_broadcast_dims_static is not None : if num_broadcast_dims_static < 0 : raise ValueError ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions\" . format ( partial_batch_dist ) ) if ( expected_partial_batch_shape_static is not None and tensorshape_util . is_fully_defined ( partial_batch_dist . batch_shape ) ) : if ( partial_batch_dist . batch_shape and any ( expected_partial_batch_shape_static != tensorshape_util . as_list ( partial_batch_dist . batch_shape ) ) ) : raise NotImplementedError ( \"Broadcasting is not supported; \" \"unexpected batch shape \" \"(expected {}, saw {}).\" . format ( expected_partial_batch_shape_static , partial_batch_dist . batch_shape ) ) runtime_assertions = [ ] if validate_args : runtime_assertions . append ( assert_util . assert_greater_equal ( tf . convert_to_tensor ( value = num_broadcast_dims , dtype = tf . int32 ) , tf . zeros ( ( ) , dtype = tf . int32 ) , message = ( \"Cannot broadcast distribution {} batch shape to \" \"target batch shape with fewer dimensions.\" . format ( partial_batch_dist ) ) ) ) runtime_assertions . append ( assert_util . assert_equal ( expected_partial_batch_shape , partial_batch_dist . batch_shape_tensor ( ) , message = ( \"Broadcasting is not supported; \" \"unexpected batch shape.\" ) , name = \"assert_batch_shape_same\" ) ) with tf . control_dependencies ( runtime_assertions ) : return full_sample_and_batch_shape [ : num_broadcast_dims ]", "nl": "Augment a sample shape to broadcast batch dimensions ."}}
{"translation": {"code": "def _check_equal_shape ( name , static_shape , dynamic_shape , static_target_shape , dynamic_target_shape = None ) : static_target_shape = tf . TensorShape ( static_target_shape ) if tensorshape_util . is_fully_defined ( static_shape ) and tensorshape_util . is_fully_defined ( static_target_shape ) : if static_shape != static_target_shape : raise ValueError ( \"{}: required shape {} but found {}\" . format ( name , static_target_shape , static_shape ) ) return None else : if dynamic_target_shape is None : if tensorshape_util . is_fully_defined ( static_target_shape ) : dynamic_target_shape = tensorshape_util . as_list ( static_target_shape ) else : raise ValueError ( \"{}: cannot infer target shape: no dynamic shape \" \"specified and static shape {} is not fully defined\" . format ( name , static_target_shape ) ) return assert_util . assert_equal ( dynamic_shape , dynamic_target_shape , message = ( \"{}: required shape {}\" . format ( name , static_target_shape ) ) )", "nl": "Check that source and target shape match statically if possible ."}}
{"translation": {"code": "def kalman_transition ( filtered_mean , filtered_cov , transition_matrix , transition_noise ) : predicted_mean = _propagate_mean ( filtered_mean , transition_matrix , transition_noise ) predicted_cov = _propagate_cov ( filtered_cov , transition_matrix , transition_noise ) return predicted_mean , predicted_cov", "nl": "Propagate a filtered distribution through a transition model ."}}
{"translation": {"code": "def build_kalman_mean_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def mean_step ( previous_means , t ) : \"\"\"Single step of prior mean recursion.\"\"\" previous_latent_mean , _ = previous_means latent_mean = _propagate_mean ( previous_latent_mean , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) observation_mean = _propagate_mean ( latent_mean , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) return ( latent_mean , observation_mean ) return mean_step", "nl": "Build a callable that performs one step of Kalman mean recursion ."}}
{"translation": {"code": "def build_kalman_cov_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def cov_step ( previous_covs , t ) : \"\"\"Single step of prior covariance recursion.\"\"\" previous_latent_cov , _ = previous_covs latent_cov = _propagate_cov ( previous_latent_cov , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) observation_cov = _propagate_cov ( latent_cov , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) return ( latent_cov , observation_cov ) return cov_step", "nl": "Build a callable for one step of Kalman covariance recursion ."}}
{"translation": {"code": "def _propagate_mean ( mean , linop , dist ) : return linop . matmul ( mean ) + dist . mean ( ) [ ... , tf . newaxis ]", "nl": "Propagate a mean through linear Gaussian transformation ."}}
{"translation": {"code": "def _joint_covariances ( self ) : with tf . name_scope ( \"covariance_joint\" ) : with tf . control_dependencies ( self . runtime_assertions ) : initial_latent_cov = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , self . latent_size ] ] , axis = 0 ) ) initial_observation_cov = _propagate_cov ( initial_latent_cov , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) cov_step = build_kalman_cov_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) # Scan over all timesteps following the initial step. ( latent_covs , observation_covs ) = tf . scan ( cov_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_cov , initial_observation_cov ) ) # Squish the initial step back on top of the other (scanned) timesteps latent_covs = tf . concat ( [ initial_latent_cov [ tf . newaxis , ... ] , latent_covs ] , axis = 0 ) observation_covs = tf . concat ( [ initial_observation_cov [ tf . newaxis , ... ] , observation_covs ] , axis = 0 ) # Put dimensions back in order. The samples we've computed have # shape `[num_timesteps, batch_shape, size, size]`, where `size` # is the dimension of the state or observation spaces # respectively, but we want to return values with shape # `[batch_shape, num_timesteps, size, size]`. latent_covs = distribution_util . move_dimension ( latent_covs , 0 , - 3 ) observation_covs = distribution_util . move_dimension ( observation_covs , 0 , - 3 ) return latent_covs , observation_covs", "nl": "Compute prior covariances for all variables via dynamic programming ."}}
{"translation": {"code": "def build_kalman_sample_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep , full_sample_and_batch_shape , stream , validate_args = False ) : def sample_step ( sampled_prev , t ) : \"\"\"Sample values for a single timestep.\"\"\" latent_prev , _ = sampled_prev transition_matrix = get_transition_matrix_for_timestep ( t - 1 ) transition_noise = get_transition_noise_for_timestep ( t - 1 ) latent_pred = transition_matrix . matmul ( latent_prev ) latent_sampled = latent_pred + transition_noise . sample ( sample_shape = _augment_sample_shape ( transition_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] observation_matrix = get_observation_matrix_for_timestep ( t ) observation_noise = get_observation_noise_for_timestep ( t ) observation_pred = observation_matrix . matmul ( latent_sampled ) observation_sampled = observation_pred + observation_noise . sample ( sample_shape = _augment_sample_shape ( observation_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] return ( latent_sampled , observation_sampled ) return sample_step", "nl": "Build a callable for one step of Kalman sampling recursion ."}}
{"translation": {"code": "def linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : # If observations are scalar, we can avoid some matrix ops. observation_size_is_static_and_scalar = ( tf . compat . dimension_value ( observation_matrix . shape [ - 2 ] ) == 1 ) # Push the predicted mean for the latent state through the # observation model x_expected = _propagate_mean ( prior_mean , observation_matrix , observation_noise ) # Push the predictive covariance of the latent state through the # observation model: #  S = R + H * P * H'. # We use a temporary variable for H * P, # reused below to compute Kalman gain. tmp_obs_cov = observation_matrix . matmul ( prior_cov ) predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = True ) + observation_noise . covariance ( ) ) # Compute optimal Kalman gain: #  K = P * H' * S^{-1} # Since both S and P are cov matrices, thus symmetric, # we can take the transpose and reuse our previous # computation: #      = (S^{-1} * H * P)' #      = (S^{-1} * tmp_obs_cov) ' #      = (S \\ tmp_obs_cov)' if observation_size_is_static_and_scalar : gain_transpose = tmp_obs_cov / predicted_obs_cov else : predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) # Compute the posterior mean, incorporating the observation. #  u* = u + K (x_observed - x_expected) posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = True ) ) # For the posterior covariance, we could use the simple update #  P* = P - K * H * P # but this is prone to numerical issues because it subtracts a # value from a PSD matrix.  We choose instead to use the more # expensive Jordan form update #  P* = (I - K H) * P * (I - K H)' + K R K' # which always produces a PSD result. This uses #  tmp_term = (I - K * H)' # as an intermediate quantity. tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = True ) # -K * H tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + 1 ) posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = True ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = True ) ) if observation_size_is_static_and_scalar : # A plain Normal would have event shape `[]`; wrapping with Independent # ensures `event_shape=[1]` as required. predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , 0 ] , scale = tf . sqrt ( predicted_obs_cov [ ... , 0 ] ) ) , reinterpreted_batch_ndims = 1 ) # Minor hack to define the covariance, so that `predictive_dist` can pass as # an MVNTriL-like object. predictive_dist . covariance = lambda : predicted_obs_cov else : predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , 0 ] , scale_tril = predicted_obs_cov_chol ) return posterior_mean , posterior_cov , predictive_dist", "nl": "Conjugate update for a linear Gaussian model ."}}
{"translation": {"code": "def _propagate_cov ( cov , linop , dist ) : # For linop A and input cov P, returns `A P A' + dist.cov()` return linop . matmul ( linop . matmul ( cov ) , adjoint_arg = True ) + dist . covariance ( )", "nl": "Propagate covariance through linear Gaussian transformation ."}}
{"translation": {"code": "def _joint_sample_n ( self , n , seed = None ) : with tf . name_scope ( \"sample_n_joint\" ) : stream = seed_stream . SeedStream ( seed , salt = \"LinearGaussianStateSpaceModel_sample_n_joint\" ) sample_and_batch_shape = distribution_util . prefer_static_value ( tf . concat ( [ [ n ] , self . batch_shape_tensor ( ) ] , axis = 0 ) ) # Sample the initial timestep from the prior.  Since we want # this sample to have full batch shape (not just the batch shape # of the self.initial_state_prior object which might in general be # smaller), we augment the sample shape to include whatever # extra batch dimensions are required. with tf . control_dependencies ( self . runtime_assertions ) : initial_latent = self . initial_state_prior . sample ( sample_shape = _augment_sample_shape ( self . initial_state_prior , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) # Add a dummy dimension so that matmul() does matrix-vector # multiplication. initial_latent = initial_latent [ ... , tf . newaxis ] initial_observation_matrix = ( self . get_observation_matrix_for_timestep ( self . initial_step ) ) initial_observation_noise = ( self . get_observation_noise_for_timestep ( self . initial_step ) ) initial_observation_pred = initial_observation_matrix . matmul ( initial_latent ) initial_observation = ( initial_observation_pred + initial_observation_noise . sample ( sample_shape = _augment_sample_shape ( initial_observation_noise , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) sample_step = build_kalman_sample_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep , full_sample_and_batch_shape = sample_and_batch_shape , stream = stream , validate_args = self . validate_args ) # Scan over all timesteps to sample latents and observations. ( latents , observations ) = tf . scan ( sample_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent , initial_observation ) ) # Combine the initial sampled timestep with the remaining timesteps. latents = tf . concat ( [ initial_latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) observations = tf . concat ( [ initial_observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) # Put dimensions back in order. The samples we've computed are # ordered by timestep, with shape `[num_timesteps, num_samples, # batch_shape, size, 1]` where `size` represents `latent_size` # or `observation_size` respectively. But timesteps are really # part of each probabilistic event, so we need to return a Tensor # of shape `[num_samples, batch_shape, num_timesteps, size]`. latents = tf . squeeze ( latents , - 1 ) latents = distribution_util . move_dimension ( latents , 0 , - 2 ) observations = tf . squeeze ( observations , - 1 ) observations = distribution_util . move_dimension ( observations , 0 , - 2 ) return latents , observations", "nl": "Draw a joint sample from the prior over latents and observations ."}}
{"translation": {"code": "def sum_rightmost_ndims_preserving_shape ( x , ndims ) : x = tf . convert_to_tensor ( value = x ) if x . shape . ndims is not None : axes = tf . range ( x . shape . ndims - ndims , x . shape . ndims ) else : axes = tf . range ( tf . rank ( x ) - ndims , tf . rank ( x ) ) return tf . reduce_sum ( input_tensor = x , axis = axes )", "nl": "Return Tensor with right - most ndims summed ."}}
{"translation": {"code": "def _flatten_summand_list ( kernels ) : flattened = [ ] for k in kernels : if isinstance ( k , _SumKernel ) : flattened += k . kernels else : flattened . append ( k ) return flattened", "nl": "Flatten a list of kernels which may contain _SumKernel instances ."}}
{"translation": {"code": "def _flatten_multiplicand_list ( kernels ) : flattened = [ ] for k in kernels : if isinstance ( k , _ProductKernel ) : flattened += k . kernels else : flattened . append ( k ) return flattened", "nl": "Flatten a list of kernels which may contain _ProductKernel instances ."}}
{"translation": {"code": "def download ( directory , filename ) : filepath = os . path . join ( directory , filename ) if tf . io . gfile . exists ( filepath ) : return filepath if not tf . io . gfile . exists ( directory ) : tf . io . gfile . makedirs ( directory ) url = os . path . join ( ROOT_PATH , filename ) print ( \"Downloading %s to %s\" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) return filepath", "nl": "Downloads a file ."}}
{"translation": {"code": "def pack_images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . minimum ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images", "nl": "Helper utility to make a field of images ."}}
{"translation": {"code": "def add_ema_control_dependencies ( vector_quantizer , one_hot_assignments , codes , commitment_loss , decay ) : # Use an exponential moving average to update the codebook. updated_ema_count = moving_averages . assign_moving_average ( vector_quantizer . ema_count , tf . reduce_sum ( input_tensor = one_hot_assignments , axis = [ 0 , 1 ] ) , decay , zero_debias = False ) updated_ema_means = moving_averages . assign_moving_average ( vector_quantizer . ema_means , tf . reduce_sum ( input_tensor = tf . expand_dims ( codes , 2 ) * tf . expand_dims ( one_hot_assignments , 3 ) , axis = [ 0 , 1 ] ) , decay , zero_debias = False ) # Add small value to avoid dividing by zero. perturbed_ema_count = updated_ema_count + 1e-5 with tf . control_dependencies ( [ commitment_loss ] ) : update_means = tf . compat . v1 . assign ( vector_quantizer . codebook , updated_ema_means / perturbed_ema_count [ ... , tf . newaxis ] ) with tf . control_dependencies ( [ update_means ] ) : return tf . identity ( commitment_loss )", "nl": "Add control dependencies to the commmitment loss to update the codebook ."}}
{"translation": {"code": "def save_imgs ( x , fname ) : n = x . shape [ 0 ] fig = figure . Figure ( figsize = ( n , 1 ) , frameon = False ) canvas = backend_agg . FigureCanvasAgg ( fig ) for i in range ( n ) : ax = fig . add_subplot ( 1 , n , i + 1 ) ax . imshow ( x [ i ] . squeeze ( ) , interpolation = \"none\" , cmap = cm . get_cmap ( \"binary\" ) ) ax . axis ( \"off\" ) canvas . print_figure ( fname , format = \"png\" ) print ( \"saved %s\" % fname )", "nl": "Helper method to save a grid of images to a PNG file ."}}
{"translation": {"code": "def visualize_training ( images_val , reconstructed_images_val , random_images_val , log_dir , prefix , viz_n = 10 ) : save_imgs ( images_val [ : viz_n ] , os . path . join ( log_dir , \"{}_inputs.png\" . format ( prefix ) ) ) save_imgs ( reconstructed_images_val [ : viz_n ] , os . path . join ( log_dir , \"{}_reconstructions.png\" . format ( prefix ) ) ) if random_images_val is not None : save_imgs ( random_images_val [ : viz_n ] , os . path . join ( log_dir , \"{}_prior_samples.png\" . format ( prefix ) ) )", "nl": "Helper method to save images visualizing model reconstructions ."}}
{"translation": {"code": "def _batch_transpose ( mat ) : n = distribution_util . prefer_static_rank ( mat ) perm = tf . range ( n ) perm = tf . concat ( [ perm [ : - 2 ] , [ perm [ - 1 ] , perm [ - 2 ] ] ] , axis = 0 ) return tf . transpose ( a = mat , perm = perm )", "nl": "Transpose a possibly batched matrix ."}}
{"translation": {"code": "def minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) dtype = initial_position . dtype . base_dtype tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) input_shape = distribution_util . prefer_static_shape ( initial_position ) batch_shape , domain_size = input_shape [ : - 1 ] , input_shape [ - 1 ] if stopping_condition is None : stopping_condition = bfgs_utils . converged_all # Control inputs are an optional list of tensors to evaluate before # the start of the search procedure. These can be used to assert the # validity of inputs to the search procedure. control_inputs = None if initial_inverse_hessian_estimate is None : # Create a default initial inverse Hessian. initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) else : # If an initial inverse Hessian is supplied, compute some control inputs # to ensure that it is positive definite and symmetric. initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , 0 ) initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) # The `state` here is a `BfgsOptimizerResults` tuple with values for the # current state of the algorithm computation. def _cond ( state ) : \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\" return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) def _body ( state ) : \"\"\"Main optimization loop.\"\"\" search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) # If the derivative at the start point is not negative, recompute the # search direction with the initial inverse Hessian. needs_reset = ( ~ state . failed & ~ state . converged & ( derivative_at_start_pt >= 0 ) ) search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) # Replace the hessian estimate in the state, in case it had to be reset. current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) # Update the inverse Hessian if needed and continue. return [ _update_inv_hessian ( current_state , next_state ) ] kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian initial_state = BfgsOptimizerResults ( * * kwargs ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ]", "nl": "Applies the BFGS algorithm to minimize a differentiable function ."}}
{"translation": {"code": "def _bfgs_inv_hessian_update ( grad_delta , position_delta , normalization_factor , inv_hessian_estimate ) : # The quadratic form: y^T.H.y; where H is the inverse Hessian and y is the # gradient change. conditioned_grad_delta = _mul_right ( inv_hessian_estimate , grad_delta ) conditioned_grad_delta_norm = tf . reduce_sum ( input_tensor = conditioned_grad_delta * grad_delta , axis = - 1 ) # The first rank 1 update term requires the outer product: s.y^T. cross_term = _tensor_product ( position_delta , conditioned_grad_delta ) def _expand_scalar ( s ) : # Expand dimensions of a batch of scalars to multiply or divide a matrix. return s [ ... , tf . newaxis , tf . newaxis ] # Symmetrize cross_term += _tensor_product ( conditioned_grad_delta , position_delta ) position_term = _tensor_product ( position_delta , position_delta ) with tf . control_dependencies ( [ position_term ] ) : position_term *= _expand_scalar ( 1 + conditioned_grad_delta_norm / normalization_factor ) return ( inv_hessian_estimate + ( position_term - cross_term ) / _expand_scalar ( normalization_factor ) )", "nl": "Applies the BFGS update to the inverse Hessian estimate ."}}
{"translation": {"code": "def _tensor_product ( t1 , t2 ) : return tf . matmul ( tf . expand_dims ( t1 , axis = - 1 ) , tf . expand_dims ( t2 , axis = - 2 ) )", "nl": "Computes the outer product of two possibly batched vectors ."}}
{"translation": {"code": "def _sample_next ( target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'sample_next' , [ current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank ] ) : # First step: Choose a random direction. # Direction is a list of tensors. The i'th tensor should have the same shape # as the i'th state part. direction = _choose_random_direction ( current_state_parts , batch_rank = batch_rank , seed = seed ) # Interpolates the step sizes for the chosen direction. # Applies an ellipsoidal interpolation to compute the step direction for # the chosen direction. Suppose we are given step sizes for each direction. # Label these s_1, s_2, ... s_k. These are the step sizes to use if moving # in a direction parallel to one of the axes. Consider an ellipsoid which # intercepts the i'th axis at s_i. The step size for a direction specified # by the unit vector (n_1, n_2 ...n_k) is then defined as the intersection # of the line through this vector with this ellipsoid. # # One can show that the length of the vector from the origin to the # intersection point is given by: # 1 / sqrt(n_1^2 / s_1^2  + n_2^2 / s_2^2  + ...). # # Proof: # The equation of the ellipsoid is: # Sum_i [x_i^2 / s_i^2 ] = 1. Let n be a unit direction vector. Points # along the line given by n may be parameterized as alpha*n where alpha is # the distance along the vector. Plugging this into the equation for the # ellipsoid, we get: # alpha^2 ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) = 1 # so alpha = \\sqrt { \\frac{1} { ( n_1^2 / s_1^2 + n_2^2 / s_2^2 + ...) } } reduce_axes = [ tf . range ( batch_rank , tf . rank ( dirn_part ) ) for dirn_part in direction ] components = [ tf . reduce_sum ( input_tensor = ( dirn_part / step_size ) ** 2 , axis = reduce_axes [ i ] ) for i , ( step_size , dirn_part ) in enumerate ( zip ( step_sizes , direction ) ) ] step_size = tf . math . rsqrt ( tf . add_n ( components ) ) # Computes the rank of a tensor. Uses the static rank if possible. def _get_rank ( x ) : return ( len ( x . shape . as_list ( ) ) if x . shape . dims is not None else tf . rank ( x ) ) state_part_ranks = [ _get_rank ( part ) for part in current_state_parts ] def _step_along_direction ( alpha ) : \"\"\"Converts the scalar alpha into an n-dim vector with full state info.\n\n      Computes x_0 + alpha * direction where x_0 is the current state and\n      direction is the direction chosen above.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        state_parts: Tensor or Python list of `Tensor`s representing the\n          state(s) of the Markov chain(s) for a given alpha and a given chosen\n          direction. Has the same shape as `current_state_parts`.\n      \"\"\" padded_alphas = [ _right_pad ( alpha , final_rank = part_rank ) for part_rank in state_part_ranks ] state_parts = [ state_part + padded_alpha * direction_part for state_part , direction_part , padded_alpha in zip ( current_state_parts , direction , padded_alphas ) ] return state_parts def projected_target_log_prob_fn ( alpha ) : \"\"\"The target log density projected along the chosen direction.\n\n      Args:\n        alpha: A tensor of shape equal to the batch dimensions of\n          `current_state_parts`.\n\n      Returns:\n        Target log density evaluated at x_0 + alpha * direction where x_0 is the\n        current state and direction is the direction chosen above. Has the same\n        shape as `alpha`.\n      \"\"\" return target_log_prob_fn ( * _step_along_direction ( alpha ) ) alpha_init = tf . zeros_like ( current_target_log_prob , dtype = current_state_parts [ 0 ] . dtype . base_dtype ) [ next_alpha , next_target_log_prob , bounds_satisfied , upper_bounds , lower_bounds ] = ssu . slice_sampler_one_dim ( projected_target_log_prob_fn , x_initial = alpha_init , max_doublings = max_doublings , step_size = step_size , seed = seed ) return [ _step_along_direction ( next_alpha ) , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ]", "nl": "Applies a single iteration of slice sampling update ."}}
{"translation": {"code": "def slice_sampler_one_dim ( target_log_prob , x_initial , step_size = 0.01 , max_doublings = 30 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'slice_sampler_one_dim' , [ x_initial , step_size , max_doublings ] ) : x_initial = tf . convert_to_tensor ( value = x_initial ) # Obtain the input dtype of the array. dtype = x_initial . dtype . base_dtype # Select the height of the slice. Tensor of shape x_initial.shape. log_slice_heights = target_log_prob ( x_initial ) - tf . random . gamma ( tf . shape ( input = x_initial ) , alpha = 1 , dtype = dtype , seed = seed ) # Given the above x and slice heights, compute the bounds of the slice for # each chain. upper_bounds , lower_bounds , bounds_satisfied = slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = seed ) retval = _sample_with_shrinkage ( x_initial , target_log_prob = target_log_prob , log_slice_heights = log_slice_heights , step_size = step_size , lower_bounds = lower_bounds , upper_bounds = upper_bounds , seed = seed ) return ( retval , target_log_prob ( retval ) , bounds_satisfied , upper_bounds , lower_bounds )", "nl": "For a given x position in each Markov chain returns the next x ."}}
{"translation": {"code": "def _sample_with_shrinkage ( x_initial , target_log_prob , log_slice_heights , step_size , lower_bounds , upper_bounds , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'sample_with_shrinkage' , [ x_initial , log_slice_heights , step_size , lower_bounds , upper_bounds ] ) : seed_gen = distributions . SeedStream ( seed , salt = '_sample_with_shrinkage' ) # Keeps track of whether an acceptable sample has been found for the chain. found = tf . zeros_like ( x_initial , dtype = tf . bool ) cond = lambda found , * ignored_args : ~ tf . reduce_all ( input_tensor = found ) x_next = tf . identity ( x_initial ) x_initial_shape = tf . shape ( input = x_initial ) x_initial_dtype = x_initial . dtype . base_dtype def _body ( found , left , right , x_next ) : \"\"\"Iterates until every chain has found a suitable next state.\"\"\" proportions = tf . random . uniform ( x_initial_shape , dtype = x_initial_dtype , seed = seed_gen ( ) ) x_proposed = tf . where ( ~ found , left + proportions * ( right - left ) , x_next ) accept_res = _test_acceptance ( x_initial , target_log_prob = target_log_prob , decided = found , log_slice_heights = log_slice_heights , x_proposed = x_proposed , step_size = step_size , lower_bounds = left , upper_bounds = right ) boundary_test = log_slice_heights < target_log_prob ( x_proposed ) can_accept = boundary_test & accept_res next_found = found | can_accept # Note that it might seem that we are moving the left and right end points # even if the point has been accepted (which is contrary to the stated # algorithm in Neal). However, this does not matter because the endpoints # for points that have been already accepted are not used again so it # doesn't matter what we do with them. next_left = tf . where ( x_proposed < x_initial , x_proposed , left ) next_right = tf . where ( x_proposed >= x_initial , x_proposed , right ) return next_found , next_left , next_right , x_proposed return tf . while_loop ( cond = cond , body = _body , loop_vars = ( found , lower_bounds , upper_bounds , x_next ) ) [ - 1 ]", "nl": "Samples from the slice by applying shrinkage for rejected points ."}}
{"translation": {"code": "def _choose_random_direction ( current_state_parts , batch_rank , seed = None ) : seed_gen = distributions . SeedStream ( seed , salt = '_choose_random_direction' ) # Chooses the random directions across each of the input components. rnd_direction_parts = [ tf . random . normal ( tf . shape ( input = current_state_part ) , dtype = tf . float32 , seed = seed_gen ( ) ) for current_state_part in current_state_parts ] # Sum squares over all of the input components. Note this takes all # components into account. sum_squares = sum ( tf . reduce_sum ( input_tensor = rnd_direction ** 2. , axis = tf . range ( batch_rank , tf . rank ( rnd_direction ) ) , keepdims = True ) for rnd_direction in rnd_direction_parts ) # Normalizes the random direction fragments. rnd_direction_parts = [ rnd_direction / tf . sqrt ( sum_squares ) for rnd_direction in rnd_direction_parts ] return rnd_direction_parts", "nl": "Chooses a random direction in the event space ."}}
{"translation": {"code": "def _right_pad ( x , final_rank ) : padded_shape = tf . concat ( [ tf . shape ( input = x ) , tf . ones ( final_rank - tf . rank ( x ) , dtype = tf . int32 ) ] , axis = 0 ) static_padded_shape = None if x . shape . is_fully_defined ( ) and isinstance ( final_rank , int ) : static_padded_shape = x . shape . as_list ( ) extra_dims = final_rank - len ( static_padded_shape ) static_padded_shape . extend ( [ 1 ] * extra_dims ) padded_x = tf . reshape ( x , static_padded_shape or padded_shape ) return padded_x", "nl": "Pads the shape of x to the right to be of rank final_rank ."}}
{"translation": {"code": "def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'slice' , 'one_step' ) , values = [ self . step_size , self . max_doublings , self . _seed_stream , current_state , previous_kernel_results . target_log_prob ] ) : with tf . compat . v1 . name_scope ( 'initialize' ) : [ current_state_parts , step_sizes , current_target_log_prob ] = _prepare_args ( self . target_log_prob_fn , current_state , self . step_size , previous_kernel_results . target_log_prob , maybe_expand = True ) max_doublings = tf . convert_to_tensor ( value = self . max_doublings , dtype = tf . int32 , name = 'max_doublings' ) independent_chain_ndims = distribution_util . prefer_static_rank ( current_target_log_prob ) [ next_state_parts , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] = _sample_next ( self . target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , independent_chain_ndims , seed = self . _seed_stream ( ) ) def maybe_flatten ( x ) : return x if mcmc_util . is_list_like ( current_state ) else x [ 0 ] return [ maybe_flatten ( next_state_parts ) , SliceSamplerKernelResults ( target_log_prob = next_target_log_prob , bounds_satisfied = bounds_satisfied , direction = direction , upper_bounds = upper_bounds , lower_bounds = lower_bounds ) , ]", "nl": "Runs one iteration of Slice Sampler ."}}
{"translation": {"code": "def slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) x_initial = tf . convert_to_tensor ( value = x_initial ) batch_shape = tf . shape ( input = x_initial ) dtype = step_size . dtype . base_dtype left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) # Compute the increments by which we need to step the upper and lower bounds # part of the doubling procedure. left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) # The left and right end points. Shape (max_doublings+1,) + batch_shape. left_endpoints -= left_increments right_endpoints = left_endpoints + widths # Test if these end points lie outside of the slice. # Checks if the end points of the slice are outside the graph of the pdf. left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) left_ok = left_ep_values < log_slice_heights right_ok = right_ep_values < log_slice_heights both_ok = left_ok & right_ok both_ok_f = tf . reshape ( both_ok , [ max_doublings + 1 , - 1 ] ) best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) # Formats the above index as required to use with gather_nd. point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( input = best_interval_idx ) ) ] , axis = 1 , name = 'point_index_gather' ) left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + 1 , - 1 ] ) right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + 1 , - 1 ] ) # The x values of the uppper and lower bounds of the slices for each chain. lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) both_ok = tf . reduce_any ( input_tensor = both_ok , axis = 0 ) return upper_bounds , lower_bounds , both_ok", "nl": "Returns the bounds of the slice at each stage of doubling procedure ."}}
{"translation": {"code": "def _find_best_interval_idx ( x , name = None ) : with tf . compat . v1 . name_scope ( name , 'find_best_interval_idx' , [ x ] ) : # Returns max_doublings + 1. Positive int32. k = tf . shape ( input = x ) [ 0 ] dtype = x . dtype . base_dtype # Factors by which to multiply the flag. Corresponds to (2 * k - i) above. mults = tf . range ( 2 * k , k , - 1 , dtype = dtype ) [ : , tf . newaxis ] # Factors by which to shift the flag. Corresponds to i above. Ensures the # widest bounds are selected if there are no bounds outside the slice. shifts = tf . range ( k , dtype = dtype ) [ : , tf . newaxis ] indices = tf . argmax ( input = mults * x + shifts , axis = 0 , output_type = dtype ) return indices", "nl": "Finds the index of the optimal set of bounds for each chain ."}}
{"translation": {"code": "def _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : step_size = tf . convert_to_tensor ( value = step_size ) dtype = step_size . dtype . base_dtype # Output shape of the left increments tensor. output_shape = tf . concat ( ( [ max_doublings + 1 ] , batch_shape ) , axis = 0 ) # A sample realization of X_k. expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . sample ( sample_shape = output_shape , seed = seed ) # The widths of the successive intervals. Starts with 1.0 and ends with # 2^max_doublings. width_multipliers = tf . cast ( 2 ** tf . range ( 0 , max_doublings + 1 ) , dtype = dtype ) # Output shape of the `widths` tensor. widths_shape = tf . concat ( ( [ max_doublings + 1 ] , tf . ones_like ( batch_shape ) ) , axis = 0 ) width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) # Widths shape is [max_doublings + 1, 1, 1, 1...]. widths = width_multipliers * step_size # Take the cumulative sum of the left side increments in slice width to give # the resulting distance from the inital lower bound. left_increments = tf . cumsum ( widths * expand_left , exclusive = True , axis = 0 ) return left_increments , widths", "nl": "Computes the doubling increments for the left end point ."}}
{"translation": {"code": "def _maybe_call_fn ( fn , fn_arg_list , fn_result = None , description = 'target_log_prob' ) : fn_arg_list = ( list ( fn_arg_list ) if mcmc_util . is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) if fn_result is None : fn_result = fn ( * fn_arg_list ) if not fn_result . dtype . is_floating : raise TypeError ( '`{}` must be a `Tensor` with `float` `dtype`.' . format ( description ) ) return fn_result", "nl": "Helper which computes fn_result if needed ."}}
{"translation": {"code": "def random_rayleigh ( shape , scale = None , dtype = tf . float32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'random_rayleigh' , [ shape , scale , seed ] ) : if scale is not None : # Its important to expand the shape to match scale's, otherwise we won't # have independent draws. scale = tf . convert_to_tensor ( value = scale , dtype = dtype , name = 'scale' ) shape = tf . broadcast_dynamic_shape ( shape , tf . shape ( input = scale ) ) x = tf . sqrt ( - 2. * tf . math . log ( tf . random . uniform ( shape , minval = 0 , maxval = 1 , dtype = dtype , seed = seed ) ) ) if scale is None : return x return x * scale", "nl": "Generates Tensor of positive reals drawn from a Rayleigh distributions ."}}
{"translation": {"code": "def _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : assertions = [ ] if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != 0 : raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) elif validate_args : assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) msg = '`rightmost_transposed_ndims` must be non-negative.' if rightmost_transposed_ndims_ is not None : if rightmost_transposed_ndims_ < 0 : raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) elif validate_args : assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] return assertions", "nl": "Checks that rightmost_transposed_ndims is valid ."}}
{"translation": {"code": "def _maybe_validate_perm ( perm , validate_args , name = None ) : with tf . name_scope ( name or 'maybe_validate_perm' ) : assertions = [ ] if not dtype_util . is_integer ( perm . dtype ) : raise TypeError ( '`perm` must be integer type' ) msg = '`perm` must be a vector.' if tensorshape_util . rank ( perm . shape ) is not None : if tensorshape_util . rank ( perm . shape ) != 1 : raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) elif validate_args : assertions += [ assert_util . assert_rank ( perm , 1 , message = msg ) ] perm_ = tf . get_static_value ( perm ) msg = '`perm` must be a valid permutation vector.' if perm_ is not None : if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) elif validate_args : assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] return assertions", "nl": "Checks that perm is valid ."}}
{"translation": {"code": "def _replace_event_shape_in_tensorshape ( input_tensorshape , event_shape_in , event_shape_out ) : event_shape_in_ndims = tensorshape_util . num_elements ( event_shape_in . shape ) if tensorshape_util . rank ( input_tensorshape ) is None or event_shape_in_ndims is None : return tf . TensorShape ( None ) , False # Not is_validated. input_non_event_ndims = tensorshape_util . rank ( input_tensorshape ) - event_shape_in_ndims if input_non_event_ndims < 0 : raise ValueError ( 'Input has fewer ndims ({}) than event shape ndims ({}).' . format ( tensorshape_util . rank ( input_tensorshape ) , event_shape_in_ndims ) ) input_non_event_tensorshape = input_tensorshape [ : input_non_event_ndims ] input_event_tensorshape = input_tensorshape [ input_non_event_ndims : ] # Check that `input_event_shape_` and `event_shape_in` are compatible in the # sense that they have equal entries in any position that isn't a `-1` in # `event_shape_in`. Note that our validations at construction time ensure # there is at most one such entry in `event_shape_in`. event_shape_in_ = tf . get_static_value ( event_shape_in ) is_validated = ( tensorshape_util . is_fully_defined ( input_event_tensorshape ) and event_shape_in_ is not None ) if is_validated : input_event_shape_ = np . int32 ( input_event_tensorshape ) mask = event_shape_in_ >= 0 explicit_input_event_shape_ = input_event_shape_ [ mask ] explicit_event_shape_in_ = event_shape_in_ [ mask ] if not all ( explicit_input_event_shape_ == explicit_event_shape_in_ ) : raise ValueError ( 'Input `event_shape` does not match `event_shape_in`. ' '({} vs {}).' . format ( input_event_shape_ , event_shape_in_ ) ) event_tensorshape_out = tensorshape_util . constant_value_as_shape ( event_shape_out ) if tensorshape_util . rank ( event_tensorshape_out ) is None : output_tensorshape = tf . TensorShape ( None ) else : output_tensorshape = tensorshape_util . concatenate ( input_non_event_tensorshape , event_tensorshape_out ) return output_tensorshape , is_validated", "nl": "Replaces the event shape dims of a TensorShape ."}}
{"translation": {"code": "def make_encoder ( activation , num_topics , layer_sizes ) : encoder_net = tf . keras . Sequential ( ) for num_hidden_units in layer_sizes : encoder_net . add ( tf . keras . layers . Dense ( num_hidden_units , activation = activation , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) encoder_net . add ( tf . keras . layers . Dense ( num_topics , activation = tf . nn . softplus , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) def encoder ( bag_of_words ) : net = _clip_dirichlet_parameters ( encoder_net ( bag_of_words ) ) return tfd . Dirichlet ( concentration = net , name = \"topics_posterior\" ) return encoder", "nl": "Create the encoder function ."}}
{"translation": {"code": "def make_lda_variational ( activation , num_topics , layer_sizes ) : encoder_net = tf . keras . Sequential ( ) for num_hidden_units in layer_sizes : encoder_net . add ( tf . keras . layers . Dense ( num_hidden_units , activation = activation , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) encoder_net . add ( tf . keras . layers . Dense ( num_topics , activation = tf . nn . softplus , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) def lda_variational ( bag_of_words ) : concentration = _clip_dirichlet_parameters ( encoder_net ( bag_of_words ) ) return ed . Dirichlet ( concentration = concentration , name = \"topics_posterior\" ) return lda_variational", "nl": "Creates the variational distribution for LDA ."}}
{"translation": {"code": "def get_topics_strings ( topics_words , alpha , vocabulary , topics_to_print = 10 , words_per_topic = 10 ) : alpha = np . squeeze ( alpha , axis = 0 ) # Use a stable sorting algorithm so that when alpha is fixed # we always get the same topics. highest_weight_topics = np . argsort ( - alpha , kind = \"mergesort\" ) top_words = np . argsort ( - topics_words , axis = 1 ) res = [ ] for topic_idx in highest_weight_topics [ : topics_to_print ] : l = [ \"index={} alpha={:.2f}\" . format ( topic_idx , alpha [ topic_idx ] ) ] l += [ vocabulary [ word ] for word in top_words [ topic_idx , : words_per_topic ] ] res . append ( \" \" . join ( l ) ) return np . array ( res )", "nl": "Returns the summary of the learned topics ."}}
{"translation": {"code": "def newsgroups_dataset ( directory , split_name , num_words , shuffle_and_repeat ) : data = np . load ( download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) ) # The last row is empty in both train and test. data = data [ : - 1 ] # Each row is a list of word ids in the document. We first convert this to # sparse COO matrix (which automatically sums the repeating words). Then, # we convert this COO matrix to CSR format which allows for fast querying of # documents. num_documents = data . shape [ 0 ] indices = np . array ( [ ( row_idx , column_idx ) for row_idx , row in enumerate ( data ) for column_idx in row ] ) sparse_matrix = scipy . sparse . coo_matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num_documents , num_words ) , dtype = np . float32 ) sparse_matrix = sparse_matrix . tocsr ( ) dataset = tf . data . Dataset . range ( num_documents ) # For training, we shuffle each epoch and repeat the epochs. if shuffle_and_repeat : dataset = dataset . shuffle ( num_documents ) . repeat ( ) # Returns a single document as a dense TensorFlow tensor. The dataset is # stored as a sparse matrix outside of the graph. def get_row_py_func ( idx ) : def get_row_python ( idx_py ) : return np . squeeze ( np . array ( sparse_matrix [ idx_py ] . todense ( ) ) , axis = 0 ) py_func = tf . compat . v1 . py_func ( get_row_python , [ idx ] , tf . float32 , stateful = False ) py_func . set_shape ( ( num_words , ) ) return py_func dataset = dataset . map ( get_row_py_func ) return dataset", "nl": "20 newsgroups as a tf . data . Dataset ."}}
{"translation": {"code": "def make_decoder ( num_topics , num_words ) : topics_words_logits = tf . compat . v1 . get_variable ( \"topics_words_logits\" , shape = [ num_topics , num_words ] , initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) topics_words = tf . nn . softmax ( topics_words_logits , axis = - 1 ) def decoder ( topics ) : word_probs = tf . matmul ( topics , topics_words ) # The observations are bag of words and therefore not one-hot. However, # log_prob of OneHotCategorical computes the probability correctly in # this case. return tfd . OneHotCategorical ( probs = word_probs , name = \"bag_of_words\" ) return decoder , topics_words", "nl": "Create the decoder function ."}}
{"translation": {"code": "def build_input_fns ( data_dir , batch_size ) : with open ( download ( data_dir , \"vocab.pkl\" ) , \"r\" ) as f : words_to_idx = pickle . load ( f ) num_words = len ( words_to_idx ) vocabulary = [ None ] * num_words for word , idx in words_to_idx . items ( ) : vocabulary [ idx ] = word # Build an iterator over training batches. def train_input_fn ( ) : dataset = newsgroups_dataset ( data_dir , \"train\" , num_words , shuffle_and_repeat = True ) # Prefetching makes training about 1.5x faster. dataset = dataset . batch ( batch_size ) . prefetch ( 32 ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) # Build an iterator over the heldout set. def eval_input_fn ( ) : dataset = newsgroups_dataset ( data_dir , \"test\" , num_words , shuffle_and_repeat = False ) dataset = dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn , vocabulary", "nl": "Builds iterators for train and evaluation data ."}}
{"translation": {"code": "def build_fake_input_fns ( batch_size ) : num_words = 1000 vocabulary = [ str ( i ) for i in range ( num_words ) ] random_sample = np . random . randint ( 10 , size = ( batch_size , num_words ) ) . astype ( np . float32 ) def train_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) dataset = dataset . batch ( batch_size ) . repeat ( ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) dataset = dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn , vocabulary", "nl": "Builds fake data for unit testing ."}}
{"translation": {"code": "def make_prior ( num_topics , initial_value ) : def _softplus_inverse ( x ) : return np . log ( np . expm1 ( x ) ) logit_concentration = tf . compat . v1 . get_variable ( \"logit_concentration\" , shape = [ 1 , num_topics ] , initializer = tf . compat . v1 . initializers . constant ( _softplus_inverse ( initial_value ) ) ) concentration = _clip_dirichlet_parameters ( tf . nn . softplus ( logit_concentration ) ) def prior ( ) : return tfd . Dirichlet ( concentration = concentration , name = \"topics_prior\" ) prior_variables = [ logit_concentration ] return prior , prior_variables", "nl": "Create the prior distribution ."}}
{"translation": {"code": "def _replace_at_index ( x , index , replacement ) : x_new = tf . concat ( [ x [ : index ] , tf . expand_dims ( replacement , axis = 0 ) , x [ ( index + 1 ) : ] ] , axis = 0 ) return x_new", "nl": "Replaces an element at supplied index ."}}
{"translation": {"code": "def _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : dim = tf . size ( input = initial_vertex ) num_vertices = dim + 1 unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial_vertex ) ] , axis = 0 ) ) # If step_sizes does not broadcast to initial_vertex, the multiplication # in the second term will fail. simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = 0 ) , simplex_face ] , axis = 0 ) num_evaluations = 0 # Evaluate the objective function at the simplex vertices. if objective_at_initial_vertex is None : objective_at_initial_vertex = objective_function ( initial_vertex ) num_evaluations += 1 objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) num_evaluations += num_evals objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = 0 ) , objective_at_simplex_face ] , axis = 0 ) return ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations )", "nl": "Constructs a standard axes aligned simplex ."}}
{"translation": {"code": "def _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) # If d is the dimension of the problem, the number of vertices in the # simplex should be d+1. From this, we can infer the number of dimensions # as n - 1 where n is the number of vertices specified. num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] dim = num_vertices - 1 num_evaluations = 0 if objective_at_initial_simplex is None : objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) num_evaluations += n_evals objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations )", "nl": "Evaluates the objective function at the specified initial simplex ."}}
{"translation": {"code": "def _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) : if objective_at_initial_simplex is not None and initial_simplex is None : raise ValueError ( '`objective_at_initial_simplex` specified but the' '`initial_simplex` was not.' ) if objective_at_initial_vertex is not None and initial_vertex is None : raise ValueError ( '`objective_at_initial_vertex` specified but the' '`initial_vertex` was not.' ) # The full simplex was specified. if initial_simplex is not None : if initial_vertex is not None : raise ValueError ( 'Both `initial_simplex` and `initial_vertex` specified.' ' Only one of the two should be specified.' ) if step_sizes is not None : raise ValueError ( '`step_sizes` must not be specified when an' ' `initial_simplex` has been specified.' ) return _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) if initial_vertex is None : raise ValueError ( 'One of `initial_simplex` or `initial_vertex`' ' must be supplied' ) if step_sizes is None : step_sizes = _default_step_sizes ( initial_vertex ) return _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective )", "nl": "Computes the initial simplex and the objective values at the simplex ."}}
{"translation": {"code": "def _check_convergence ( simplex , best_vertex , best_objective , worst_objective , func_tolerance , position_tolerance ) : objective_convergence = tf . abs ( worst_objective - best_objective ) < func_tolerance simplex_degeneracy = tf . reduce_max ( input_tensor = tf . abs ( simplex - best_vertex ) ) < position_tolerance return objective_convergence | simplex_degeneracy", "nl": "Returns True if the simplex has converged ."}}
{"translation": {"code": "def minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = False , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = 1 , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) domain_dtype = simplex . dtype ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , * * closure_kwargs ) return ( converged , iterations + 1 , next_simplex , next_objective , num_evaluations + evaluations ) initial_args = ( False , 0 , simplex , objective_at_simplex , num_evaluations ) # Loop until either we have converged or if the max iterations are supplied # then until we have converged or exhausted the available iteration budget. def _is_converged ( converged , num_iterations , * ignored_args ) : # pylint:disable=unused-argument # It is important to ensure that not_converged is a tensor. If # converged is not a tensor but a Python bool, then the overloaded # op '~' acts as bitwise complement so ~True = -2 and ~False = -1. # In that case, the loop will never terminate. not_converged = tf . logical_not ( converged ) return ( not_converged if max_iterations is None else ( not_converged & ( num_iterations < max_iterations ) ) ) ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = True ) best_index = order [ 0 ] # The explicit cast to Tensor below is done to avoid returning a mixture # of Python types and Tensors which cause problems with session.run. # In the eager mode, converged may remain a Python bool. Trying to evaluate # the whole tuple in one evaluate call will raise an exception because # of the presence of non-tensors. This is very annoying so we explicitly # cast those arguments to Tensors. return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex )", "nl": "Minimum of the objective function using the Nelder Mead simplex algorithm ."}}
{"translation": {"code": "def nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : domain_dtype = current_simplex . dtype . base_dtype order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] worst_vertex = current_simplex [ worst_index ] ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) # Compute the centroid of the face opposite the worst vertex. face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex face_centroid /= tf . cast ( dim , domain_dtype ) # Reflect the worst vertex through the opposite face. reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) objective_at_reflected = objective_function ( reflected ) num_evaluations = 1 has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) def _converged_fn ( ) : return ( True , current_simplex , current_objective_values , 0 ) case0 = has_converged , _converged_fn accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) case1 = accept_reflected , accept_reflected_fn do_expansion = objective_at_reflected < best_objective_value expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) case2 = do_expansion , expansion_fn do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) case3 = do_outside_contraction , outside_contraction_fn default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) next_simplex . set_shape ( current_simplex . shape ) next_objective_at_simplex . set_shape ( current_objective_values . shape ) return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals )", "nl": "A single iteration of the Nelder Mead algorithm ."}}
{"translation": {"code": "def _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , objective_at_reflected ) : def _replace_worst_with_reflected ( ) : next_simplex = _replace_at_index ( simplex , worst_index , reflected ) next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) return False , next_simplex , next_objective_values , 0 return _replace_worst_with_reflected", "nl": "Creates the condition function pair for a reflection to be accepted ."}}
{"translation": {"code": "def _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : def _expand_and_maybe_replace ( ) : \"\"\"Performs the expansion step.\"\"\" expanded = face_centroid + expansion * ( reflected - face_centroid ) expanded_objective_value = objective_function ( expanded ) expanded_is_better = ( expanded_objective_value < objective_at_reflected ) accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) return False , next_simplex , next_objective_at_simplex , 1 return _expand_and_maybe_replace", "nl": "Creates the condition function pair for an expansion ."}}
{"translation": {"code": "def _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : # If the contraction step fails to improve the average objective enough, # the simplex is shrunk towards the best vertex. best_vertex = simplex [ best_index ] shrunk_simplex = best_vertex + shrinkage * ( simplex - best_vertex ) objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) return ( False , shrunk_simplex , objective_at_shrunk_simplex , evals )", "nl": "Shrinks the simplex around the best vertex ."}}
{"translation": {"code": "def _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : def _contraction ( ) : \"\"\"Performs a contraction.\"\"\" contracted = face_centroid + contraction * ( reflected - face_centroid ) objective_at_contracted = objective_function ( contracted ) is_contracted_acceptable = objective_at_contracted <= objective_at_reflected def _accept_contraction ( ) : next_simplex = _replace_at_index ( simplex , worst_index , contracted ) objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) return ( False , next_simplex , objective_at_next_simplex , 1 ) def _reject_contraction ( ) : return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) return _contraction", "nl": "Creates the condition function pair for an outside contraction ."}}
{"translation": {"code": "def _evaluate_objective_multiple ( objective_function , arg_batch , batch_evaluate_objective ) : n_points = tf . shape ( input = arg_batch ) [ 0 ] if batch_evaluate_objective : return objective_function ( arg_batch ) , n_points return tf . map_fn ( objective_function , arg_batch ) , n_points", "nl": "Evaluates the objective function on a batch of points ."}}
{"translation": {"code": "def get_marginal_distribution ( self , index_points = None ) : with self . _name_scope ( 'get_marginal_distribution' ) : # TODO(cgs): consider caching the result here, keyed on `index_points`. index_points = self . _get_index_points ( index_points ) covariance = self . _compute_covariance ( index_points ) loc = self . _mean_fn ( index_points ) # If we're sure the number of index points is 1, we can just construct a # scalar Normal. This has computational benefits and supports things like # CDF that aren't otherwise straightforward to provide. if self . _is_univariate_marginal ( index_points ) : scale = tf . sqrt ( covariance ) # `loc` has a trailing 1 in the shape; squeeze it. loc = tf . squeeze ( loc , axis = - 1 ) return normal . Normal ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' ) else : scale = tf . linalg . LinearOperatorLowerTriangular ( tf . linalg . cholesky ( _add_diagonal_shift ( covariance , self . jitter ) ) , is_non_singular = True , name = 'GaussianProcessScaleLinearOperator' ) return mvn_linear_operator . MultivariateNormalLinearOperator ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' )", "nl": "Compute the marginal of this GP over function values at index_points ."}}
{"translation": {"code": "def _is_univariate_marginal ( self , index_points ) : num_index_points = tf . compat . dimension_value ( index_points . shape [ - ( self . kernel . feature_ndims + 1 ) ] ) if num_index_points is None : warnings . warn ( 'Unable to detect statically whether the number of index_points is ' '1. As a result, defaulting to treating the marginal GP at ' '`index_points` as a multivariate Gaussian. This makes some methods, ' 'like `cdf` unavailable.' ) return num_index_points == 1", "nl": "True if the given index_points would yield a univariate marginal ."}}
{"translation": {"code": "def benchmark_eight_schools_hmc ( num_results = int ( 5e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 , step_size = 0.4 ) : num_schools = 8 treatment_effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ] , dtype = np . float32 , name = 'treatment_effects' ) treatment_stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment_stddevs' ) def unnormalized_posterior_log_prob ( avg_effect , avg_stddev , school_effects_standard ) : \"\"\"Eight-schools unnormalized log posterior.\"\"\" return eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) if tf . executing_eagerly ( ) : sample_chain = tf . function ( tfp . mcmc . sample_chain ) else : sample_chain = tfp . mcmc . sample_chain def computation ( ) : \"\"\"The benchmark computation.\"\"\" _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = ( tf . zeros ( [ ] , name = 'init_avg_effect' ) , tf . zeros ( [ ] , name = 'init_avg_stddev' ) , tf . ones ( [ num_schools ] , name = 'init_school_effects_standard' ) , ) , kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_posterior_log_prob , step_size = step_size , num_leapfrog_steps = num_leapfrog_steps ) ) return kernel_results . is_accepted # Let's force evaluation of graph to ensure build time is not part of our time # trial. is_accepted_tensor = computation ( ) if not tf . executing_eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( is_accepted_tensor ) start_time = time . time ( ) if tf . executing_eagerly ( ) : is_accepted = computation ( ) else : is_accepted = session . run ( is_accepted_tensor ) wall_time = time . time ( ) - start_time num_accepted = np . sum ( is_accepted ) acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time )", "nl": "Runs HMC on the eight - schools unnormalized posterior ."}}
{"translation": {"code": "def mvn ( * args , * * kwargs ) : # Faster than using `tfd.MultivariateNormalDiag`. return tfd . Independent ( tfd . Normal ( * args , * * kwargs ) , reinterpreted_batch_ndims = 1 )", "nl": "Convenience function to efficiently construct a MultivariateNormalDiag ."}}
{"translation": {"code": "def eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) : rv_avg_effect = tfd . Normal ( loc = 0. , scale = 10. ) rv_avg_stddev = tfd . Normal ( loc = 5. , scale = 1. ) rv_school_effects_standard = mvn ( loc = tf . zeros_like ( school_effects_standard ) , scale = tf . ones_like ( school_effects_standard ) ) rv_treatment_effects = mvn ( loc = ( avg_effect + tf . exp ( avg_stddev ) * school_effects_standard ) , scale = treatment_stddevs ) return ( rv_avg_effect . log_prob ( avg_effect ) + rv_avg_stddev . log_prob ( avg_stddev ) + rv_school_effects_standard . log_prob ( school_effects_standard ) + rv_treatment_effects . log_prob ( treatment_effects ) )", "nl": "Eight - schools joint log - prob ."}}
{"translation": {"code": "def _mode ( self ) : return ( self . mean_direction + tf . zeros_like ( self . concentration ) [ ... , tf . newaxis ] )", "nl": "The mode of the von Mises - Fisher distribution is the mean direction ."}}
{"translation": {"code": "def _log_normalization ( self ) : event_dim = tf . compat . dimension_value ( self . event_shape [ 0 ] ) if event_dim is None : raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_lognorm = ( ( event_dim / 2 - 1 ) * tf . math . log ( safe_conc ) - ( event_dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2 - 1 , safe_conc ) ) - tf . abs ( safe_conc ) ) log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2 , self . dtype ) ) ) return tf . where ( self . concentration > 0 , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) )", "nl": "Computes the log - normalizer of the distribution ."}}
{"translation": {"code": "def _sample_3d ( self , n , seed = None ) : seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) # TODO(bjp): Higher-order odd dim analytic CDFs are available in [1], could # be bisected for bounded sampling runtime (i.e. not rejection sampling). # [1]: Inversion sampler via: https://ieeexplore.ieee.org/document/7347705/ # The inversion is: u = 1 + log(z + (1-z)*exp(-2*kappa)) / kappa # We must protect against both kappa and z being zero. safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_z = tf . where ( z > 0 , z , tf . ones_like ( z ) ) safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = 0 ) / safe_conc # Limit of the above expression as kappa->0 is 2*z-1 u = tf . where ( self . concentration > tf . zeros_like ( safe_u ) , safe_u , 2 * z - 1 ) # Limit of the expression as z->0 is -1. u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) if not self . _allow_nan_stats : u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) return u [ ... , tf . newaxis ]", "nl": "Specialized inversion sampler for 3D ."}}
{"translation": {"code": "def _rotate ( self , samples ) : event_dim = ( tf . compat . dimension_value ( self . event_shape [ 0 ] ) or self . _event_shape_tensor ( ) [ 0 ] ) basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event_dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , u = tf . nn . l2_normalize ( basis - self . mean_direction , axis = - 1 ) return samples - 2 * tf . reduce_sum ( input_tensor = samples * u , axis = - 1 , keepdims = True ) * u", "nl": "Applies a Householder rotation to samples ."}}
{"translation": {"code": "def make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : if step_counter is None and num_adaptation_steps is not None : step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - 1 , dtype = np . int32 ) , # Specify the dtype for variable sharing to work correctly # (b/120599991). dtype = tf . int32 , trainable = False , use_resource = True ) def step_size_simple_update_fn ( step_size_var , kernel_results ) : \"\"\"Updates (list of) `step_size` using a standard adaptive MCMC procedure.\n\n    Args:\n      step_size_var: (List of) `tf.Variable`s representing the per `state_part`\n        HMC `step_size`.\n      kernel_results: `collections.namedtuple` containing `Tensor`s\n        representing values from most recent call to `one_step`.\n\n    Returns:\n      step_size_assign: (List of) `Tensor`(s) representing updated\n        `step_size_var`(s).\n    \"\"\" if kernel_results is None : if mcmc_util . is_list_like ( step_size_var ) : return [ tf . identity ( ss ) for ss in step_size_var ] return tf . identity ( step_size_var ) log_n = tf . math . log ( tf . cast ( tf . size ( input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n adjustment = tf . where ( log_mean_accept_ratio < tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) def build_assign_op ( ) : if mcmc_util . is_list_like ( step_size_var ) : return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) if num_adaptation_steps is None : return build_assign_op ( ) else : with tf . control_dependencies ( [ step_counter . assign_add ( 1 ) ] ) : return tf . cond ( pred = step_counter < num_adaptation_steps , true_fn = build_assign_op , false_fn = lambda : step_size_var ) return step_size_simple_update_fn", "nl": "Create a function implementing a step - size update policy ."}}
{"translation": {"code": "def _get_field ( kernel_results , field_name ) : if hasattr ( kernel_results , field_name ) : return getattr ( kernel_results , field_name ) if hasattr ( kernel_results , 'accepted_results' ) : return getattr ( kernel_results . accepted_results , field_name ) raise TypeError ( 'Cannot extract %s from %s' % ( field_name , kernel_results ) )", "nl": "field_name from kernel_results or kernel_results . accepted_results ."}}
{"translation": {"code": "def _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : target_log_probs = [ ] for replica in range ( self . num_replica ) : replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) inverse_temp = self . inverse_temperatures [ replica ] target_log_probs . append ( replica_log_prob / inverse_temp ) target_log_probs = tf . stack ( target_log_probs , axis = 0 ) dtype = target_log_probs . dtype num_state_parts = len ( sampled_replica_states [ 0 ] ) # exchanged_states[k][i] is Tensor of (new) state part k, for replica i. # The `k` will be known statically, and `i` is a Tensor. # We will insert values into indices `i` for every replica with a proposed # exchange. exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , # State part k has same shape, regardless of replica.  So use 0. element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] # Draw random variables here, to avoid sampling in the loop (and losing # reproducibility).  This may mean we sample too many, but we will always # have enough. sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) def _swap ( is_exchange_accepted , x , y ) : \"\"\"Swap batches of x, y where accepted.\"\"\" with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) return new_x , new_y def cond ( i , unused_exchanged_states ) : return i < exchange_proposed_n def body ( i , exchanged_states ) : \"\"\"Body of while loop for exchanging states.\"\"\" # Propose exchange between replicas indexed by m and n. m , n = tf . unstack ( exchange_proposed [ i ] ) # Construct log_accept_ratio:  -temp_diff * target_log_prob_diff. # Note target_log_prob_diff = -EnergyDiff (common definition is in terms # of energy). temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] # Difference of target log probs may be +- Inf or NaN.  We want the # product of this with the temperature difference to have \"alt value\" of # -Inf. log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio for k in range ( num_state_parts ) : new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) return i + 1 , exchanged_states # At this point, exchanged_states[k] is a length num_replicas TensorArray. return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ]", "nl": "Get list of TensorArrays holding exchanged states and zeros ."}}
{"translation": {"code": "def _log_normalization ( self , name = 'log_normalization' ) : # The formula is from D. Lewandowski et al [1], p. 1999, from the # proof that eqs 16 and 17 are equivalent. with tf . name_scope ( name or 'log_normalization_lkj' ) : logpi = np . log ( np . pi ) ans = tf . zeros_like ( self . concentration ) for k in range ( 1 , self . dimension ) : ans += logpi * ( k / 2. ) ans += tf . math . lgamma ( self . concentration + ( self . dimension - 1 - k ) / 2. ) ans -= tf . math . lgamma ( self . concentration + ( self . dimension - 1 ) / 2. ) return ans", "nl": "Returns the log normalization of an LKJ distribution ."}}
{"translation": {"code": "def _log_unnorm_prob ( self , x , name = None ) : with tf . name_scope ( name or 'log_unnorm_prob_lkj' ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) # The density is det(matrix) ** (concentration - 1). # Computing the determinant with `logdet` is usually fine, since # correlation matrices are Hermitian and PSD. But in some cases, for a # PSD matrix whose eigenvalues are close to zero, `logdet` raises an error # complaining that it is not PSD. The root cause is the computation of the # cholesky decomposition in `logdet`. Hence, we use the less efficient but # more robust `slogdet` which does not use `cholesky`. # # An alternative would have been to check allow_nan_stats and use #   eigenvalues = tf.linalg.self_adjoint_eigvals(x) #   psd_mask = tf.cast( #     tf.reduce_min(eigenvalues, axis=-1) >= 0, dtype=x.dtype) #   tf.where(psd_mask, answer, float('-inf')) # to emit probability 0 for inputs that are not PSD, without ever raising # an error. More care must be taken, as due to numerical stability issues, # self_adjoint_eigvals can return slightly negative eigenvalues even for # a PSD matrix. if self . input_output_cholesky : logdet = 2.0 * tf . reduce_sum ( input_tensor = tf . math . log ( tf . linalg . diag_part ( x ) ) , axis = [ - 1 ] ) else : _ , logdet = tf . linalg . slogdet ( x ) answer = ( self . concentration - 1. ) * logdet return answer", "nl": "Returns the unnormalized log density of an LKJ distribution ."}}
{"translation": {"code": "def _uniform_unit_norm ( dimension , shape , dtype , seed ) : # This works because the Gaussian distribution is spherically symmetric. # raw shape: shape + [dimension] raw = normal . Normal ( loc = dtype_util . as_numpy_dtype ( dtype ) ( 0 ) , scale = dtype_util . as_numpy_dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) unit_norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - 1 ) [ ... , tf . newaxis ] return unit_norm", "nl": "Returns a batch of points chosen uniformly from the unit hypersphere ."}}
{"translation": {"code": "def _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : # TODO(b/111926503): Determine whether there are some practical cases where it # is computationally favorable to compute the full FIM. mean , variance , grad_mean = model ( linear_response ) is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) def _mask_if_invalid ( x , mask ) : mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) return tf . where ( is_valid , x , mask ) # TODO(b/111923449): Link to derivation once it's available. v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) return - grad_log_likelihood , fim_middle", "nl": "Computes the neg - log - likelihood gradient and Fisher information for a GLM ."}}
{"translation": {"code": "def fit_sparse ( model_matrix , response , model , model_coefficients_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = None , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : graph_deps = [ model_matrix , response , model_coefficients_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , # TODO(b/111925792): Replace `tolerance` arg with something like # `convergence_criteria_fn`. tolerance , learning_rate , ] with tf . compat . v1 . name_scope ( name , 'fit_sparse' , graph_deps ) : # TODO(b/111922388): Include dispersion and offset parameters. def _grad_neg_log_likelihood_and_fim_fn ( x ) : predicted_linear_response = sparse_or_dense_matvecmul ( model_matrix , x ) g , h_middle = _grad_neg_log_likelihood_and_fim ( model_matrix , predicted_linear_response , response , model ) return g , model_matrix , h_middle return tfp . optimizer . proximal_hessian_sparse_minimize ( _grad_neg_log_likelihood_and_fim_fn , x_start = model_coefficients_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_iterations = maximum_iterations , maximum_full_sweeps_per_iteration = maximum_full_sweeps_per_iteration , learning_rate = learning_rate , tolerance = tolerance , name = name )", "nl": "r Fits a GLM using coordinate - wise FIM - informed proximal gradient descent ."}}
{"translation": {"code": "def tape ( ) : tape_data = collections . OrderedDict ( { } ) def record ( f , * args , * * kwargs ) : \"\"\"Records execution to a tape.\"\"\" name = kwargs . get ( \"name\" ) output = interceptable ( f ) ( * args , * * kwargs ) if name : tape_data [ name ] = output return output with interception ( record ) : yield tape_data", "nl": "Context manager for recording interceptable executions onto a tape ."}}
{"translation": {"code": "def _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : # Keep the number of terms as a float. It should be a small integer, so # exactly representable as a float. num_terms = tf . cast ( num_terms , dtype = dtype ) def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : \"\"\"One iteration of the series loop.\"\"\" denominator = 2. * n / concentration + rn ddenominator_dk = - 2. * n / concentration ** 2 + drn_dconcentration rn = 1. / denominator drn_dconcentration = - ddenominator_dk / denominator ** 2 multiplier = tf . sin ( n * x ) / n + vn vn = rn * multiplier dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) n -= 1. return n , rn , drn_dconcentration , vn , dvn_dconcentration ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : n > 0. , body = loop_body , loop_vars = ( num_terms , # n tf . zeros_like ( x , name = \"rn\" ) , tf . zeros_like ( x , name = \"drn_dconcentration\" ) , tf . zeros_like ( x , name = \"vn\" ) , tf . zeros_like ( x , name = \"dvn_dconcentration\" ) , ) , ) cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi dcdf_dconcentration = dvn_dconcentration / np . pi # Clip the result to [0, 1]. cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) # The clipped values do not depend on concentration anymore, so set their # derivative to zero. dcdf_dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) return cdf_clipped , dcdf_dconcentration", "nl": "Computes the von Mises CDF and its derivative via series expansion ."}}
{"translation": {"code": "def _von_mises_cdf_normal ( x , concentration , dtype ) : def cdf_func ( concentration ) : \"\"\"A helper function that is passed to value_and_gradient.\"\"\" # z is an \"almost Normally distributed\" random variable. z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel_i0e ( concentration ) ) * tf . sin ( .5 * x ) ) # This is the correction described in [1] which reduces the error # of the Normal approximation. z2 = z ** 2 z3 = z2 * z z4 = z2 ** 2 c = 24. * concentration c1 = 56. xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2 distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) return distrib . cdf ( xi ) return value_and_gradient ( cdf_func , concentration )", "nl": "Computes the von Mises CDF and its derivative via Normal approximation ."}}
{"translation": {"code": "def prior_sample ( self , num_timesteps , initial_step = 0 , params_sample_shape = ( ) , trajectories_sample_shape = ( ) , seed = None ) : seed = distributions . SeedStream ( seed , salt = 'StructuralTimeSeries_prior_sample' ) with tf . compat . v1 . name_scope ( 'prior_sample' , values = [ num_timesteps , params_sample_shape , trajectories_sample_shape ] ) : param_samples = [ p . prior . sample ( params_sample_shape , seed = seed ( ) , name = p . name ) for p in self . parameters ] model = self . make_state_space_model ( num_timesteps = num_timesteps , initial_step = initial_step , param_vals = param_samples ) return model . sample ( trajectories_sample_shape , seed = seed ( ) ) , param_samples", "nl": "Sample from the joint prior over model parameters and trajectories ."}}
{"translation": {"code": "def empirical_statistics ( observed_time_series ) : with tf . compat . v1 . name_scope ( 'empirical_statistics' , values = [ observed_time_series ] ) : [ observed_time_series , mask ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) squeezed_series = observed_time_series [ ... , 0 ] if mask is None : observed_mean , observed_variance = tf . nn . moments ( x = squeezed_series , axes = - 1 ) observed_initial = squeezed_series [ ... , 0 ] else : broadcast_mask = tf . broadcast_to ( tf . cast ( mask , tf . bool ) , tf . shape ( input = squeezed_series ) ) observed_mean , observed_variance = ( missing_values_util . moments_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) try : observed_initial = ( missing_values_util . initial_value_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) except NotImplementedError : tf . compat . v1 . logging . warn ( 'Cannot compute initial values for a masked time series' 'with dynamic shape; using the mean instead. This will' 'affect heuristic priors and may change the results of' 'inference.' ) observed_initial = observed_mean observed_stddev = tf . sqrt ( observed_variance ) observed_initial_centered = observed_initial - observed_mean return observed_mean , observed_stddev , observed_initial_centered", "nl": "Compute statistics of a provided time series as heuristic initialization ."}}
{"translation": {"code": "def make_state_space_model ( self , num_timesteps , param_vals = None , initial_state_prior = None , initial_step = 0 ) : return self . _make_state_space_model ( num_timesteps = num_timesteps , param_map = self . _canonicalize_param_vals_as_map ( param_vals ) , initial_state_prior = initial_state_prior , initial_step = initial_step )", "nl": "Instantiate this model as a Distribution over specified num_timesteps ."}}
{"translation": {"code": "def _maybe_expand_trailing_dim ( observed_time_series_tensor ) : with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : if ( observed_time_series_tensor . shape . ndims is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - 1 ] ) is not None ) : expanded_time_series = ( observed_time_series_tensor if observed_time_series_tensor . shape [ - 1 ] == 1 else observed_time_series_tensor [ ... , tf . newaxis ] ) else : expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( input = observed_time_series_tensor ) [ - 1 ] , 1 ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) return expanded_time_series", "nl": "Ensures observed_time_series_tensor has a trailing dimension of size 1 ."}}
{"translation": {"code": "def latent_dirichlet_allocation ( concentration , topics_words ) : topics = ed . Dirichlet ( concentration = concentration , name = \"topics\" ) word_probs = tf . matmul ( topics , topics_words ) # The observations are bags of words and therefore not one-hot. However, # log_prob of OneHotCategorical computes the probability correctly in # this case. bag_of_words = ed . OneHotCategorical ( probs = word_probs , name = \"bag_of_words\" ) return bag_of_words", "nl": "Latent Dirichlet Allocation in terms of its generative process ."}}
{"translation": {"code": "def deep_exponential_family ( data_size , feature_size , units , shape ) : w2 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 2 ] , units [ 1 ] ] , name = \"w2\" ) w1 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 1 ] , units [ 0 ] ] , name = \"w1\" ) w0 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 0 ] , feature_size ] , name = \"w0\" ) z2 = ed . Gamma ( 0.1 , 0.1 , sample_shape = [ data_size , units [ 2 ] ] , name = \"z2\" ) z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = \"z1\" ) z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = \"z0\" ) x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = \"x\" ) return x", "nl": "A multi - layered topic model over a documents - by - terms matrix ."}}
{"translation": {"code": "def trainable_positive_deterministic ( shape , min_loc = 1e-3 , name = None ) : with tf . compat . v1 . variable_scope ( None , default_name = \"trainable_positive_deterministic\" ) : unconstrained_loc = tf . compat . v1 . get_variable ( \"unconstrained_loc\" , shape ) loc = tf . maximum ( tf . nn . softplus ( unconstrained_loc ) , min_loc ) rv = ed . Deterministic ( loc = loc , name = name ) return rv", "nl": "Learnable Deterministic distribution over positive reals ."}}
{"translation": {"code": "def trainable_gamma ( shape , min_concentration = 1e-3 , min_scale = 1e-5 , name = None ) : with tf . compat . v1 . variable_scope ( None , default_name = \"trainable_gamma\" ) : unconstrained_concentration = tf . compat . v1 . get_variable ( \"unconstrained_concentration\" , shape , initializer = tf . compat . v1 . initializers . random_normal ( mean = 0.5 , stddev = 0.1 ) ) unconstrained_scale = tf . compat . v1 . get_variable ( \"unconstrained_scale\" , shape , initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) ) concentration = tf . maximum ( tf . nn . softplus ( unconstrained_concentration ) , min_concentration ) rate = tf . maximum ( 1. / tf . nn . softplus ( unconstrained_scale ) , 1. / min_scale ) rv = ed . Gamma ( concentration = concentration , rate = rate , name = name ) return rv", "nl": "Learnable Gamma via concentration and scale parameterization ."}}
{"translation": {"code": "def load_nips2011_papers ( path ) : path = os . path . expanduser ( path ) filename = \"NIPS_1987-2015.csv\" filepath = os . path . join ( path , filename ) if not os . path . exists ( filepath ) : url = ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \"00371/NIPS_1987-2015.csv\" ) if not tf . io . gfile . exists ( path ) : tf . io . gfile . makedirs ( path ) print ( \"Downloading %s to %s\" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) with open ( filepath ) as f : iterator = csv . reader ( f ) documents = next ( iterator ) [ 1 : ] words = [ ] x_train = [ ] for row in iterator : words . append ( row [ 0 ] ) x_train . append ( row [ 1 : ] ) x_train = np . array ( x_train , dtype = np . int ) # Subset to documents in 2011 and words appearing in at least two documents # and have a total word count of at least 10. doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( \"2011\" ) ] documents = [ documents [ doc ] for doc in doc_idx ] x_train = x_train [ : , doc_idx ] word_idx = np . logical_and ( np . sum ( x_train != 0 , 1 ) >= 2 , np . sum ( x_train , 1 ) >= 10 ) words = [ word for word , idx in zip ( words , word_idx ) if idx ] bag_of_words = x_train [ word_idx , : ] . T return bag_of_words , words", "nl": "Loads NIPS 2011 conference papers ."}}
{"translation": {"code": "def load_bernoulli_mnist_dataset ( directory , split_name ) : amat_file = download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) dataset = tf . data . TextLineDataset ( amat_file ) str_to_arr = lambda string : np . array ( [ c == b\"1\" for c in string . split ( ) ] ) def _parser ( s ) : booltensor = tf . compat . v1 . py_func ( str_to_arr , [ s ] , tf . bool ) reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) return dataset . map ( _parser )", "nl": "Returns Hugo Larochelle s binary static MNIST tf . data . Dataset ."}}
{"translation": {"code": "def convert_to_string ( self , productions ) : symbols = [ ] for production in tf . unstack ( productions , axis = 1 ) : lhs , rhs = self . production_rules [ tf . argmax ( input = production , axis = - 1 ) ] if not symbols : # first iteration if lhs != self . start_symbol : raise ValueError ( \"`productions` must begin with `self.start_symbol`.\" ) symbols = rhs else : # Greedily unroll the nonterminal symbols based on the first occurrence # in a linear sequence. index = symbols . index ( lhs ) symbols = symbols [ : index ] + rhs + symbols [ index + 1 : ] string = \"\" . join ( symbols ) return string", "nl": "Converts a sequence of productions into a string of terminal symbols ."}}
{"translation": {"code": "def call ( self , inputs ) : del inputs # unused latent_code = ed . MultivariateNormalDiag ( loc = tf . zeros ( self . latent_size ) , sample_shape = 1 , name = \"latent_code\" ) state = self . lstm . zero_state ( 1 , dtype = tf . float32 ) t = 0 productions = [ ] stack = [ self . grammar . start_symbol ] while stack : symbol = stack . pop ( ) net , state = self . lstm ( latent_code , state ) logits = ( self . output_layer ( net ) + self . grammar . mask ( symbol , on_value = 0. , off_value = - 1e9 ) ) production = ed . OneHotCategorical ( logits = logits , name = \"production_\" + str ( t ) ) _ , rhs = self . grammar . production_rules [ tf . argmax ( input = production , axis = - 1 ) ] for symbol in rhs : if symbol in self . grammar . nonterminal_symbols : stack . append ( symbol ) productions . append ( production ) t += 1 return tf . stack ( productions , axis = 1 )", "nl": "Runs the model forward to generate a sequence of productions ."}}
{"translation": {"code": "def call ( self , inputs ) : net = self . encoder_net ( tf . cast ( inputs , tf . float32 ) ) return ed . MultivariateNormalDiag ( loc = net [ ... , : self . latent_size ] , scale_diag = tf . nn . softplus ( net [ ... , self . latent_size : ] ) , name = \"latent_code_posterior\" )", "nl": "Runs the model forward to return a stochastic encoding ."}}
{"translation": {"code": "def _has_no_u_turn ( state_one , state_two , momentum ) : dot_product = sum ( [ tf . reduce_sum ( input_tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state_one , state_two , momentum ) ] ) return dot_product > 0", "nl": "If two given states and momentum do not exhibit a U - turn pattern ."}}
{"translation": {"code": "def _leapfrog ( value_and_gradients_fn , current_state , current_grads_target_log_prob , current_momentum , step_size ) : mid_momentum = [ m + 0.5 * step * g for m , step , g in zip ( current_momentum , step_size , current_grads_target_log_prob ) ] next_state = [ s + step * m for s , step , m in zip ( current_state , step_size , mid_momentum ) ] next_target_log_prob , next_grads_target_log_prob = value_and_gradients_fn ( * next_state ) next_momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid_momentum , step_size , next_grads_target_log_prob ) ] return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ]", "nl": "Runs one step of leapfrog integration ."}}
{"translation": {"code": "def _log_joint ( current_target_log_prob , current_momentum ) : momentum_log_prob = - sum ( [ tf . reduce_sum ( input_tensor = 0.5 * ( m ** 2. ) ) for m in current_momentum ] ) return current_target_log_prob + momentum_log_prob", "nl": "Log - joint probability given a state s log - probability and momentum ."}}
{"translation": {"code": "def _random_bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , \"random_bernoulli\" , [ shape , probs ] ) : probs = tf . convert_to_tensor ( value = probs ) random_uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random_uniform , probs ) , dtype )", "nl": "Returns samples from a Bernoulli distribution ."}}
{"translation": {"code": "def covertype ( ) : import sklearn . datasets # pylint: disable=g-import-not-at-top data = sklearn . datasets . covtype . fetch_covtype ( ) features = data . data labels = data . target # Normalize features and append a column of ones for the intercept. features -= features . mean ( 0 ) features /= features . std ( 0 ) features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) features = tf . cast ( features , dtype = tf . float32 ) # Binarize outcomes on whether it is a specific category. _ , counts = np . unique ( labels , return_counts = True ) specific_category = np . argmax ( counts ) labels = ( labels == specific_category ) labels = tf . cast ( labels , dtype = tf . int32 ) return features , labels", "nl": "Builds the Covertype data set ."}}
{"translation": {"code": "def _embed_no_none_gradient_check ( value_and_gradients_fn ) : @ functools . wraps ( value_and_gradients_fn ) def func_wrapped ( * args , * * kwargs ) : \"\"\"Wrapped function which checks for None gradients.\"\"\" value , grads = value_and_gradients_fn ( * args , * * kwargs ) if any ( grad is None for grad in grads ) : raise ValueError ( \"Gradient is None for a state.\" ) return value , grads return func_wrapped", "nl": "Wraps value and gradients function to assist with None gradients ."}}
{"translation": {"code": "def logistic_regression ( features ) : coeffs = ed . MultivariateNormalDiag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = \"coeffs\" ) labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = \"labels\" ) return labels", "nl": "Bayesian logistic regression which returns labels given features ."}}
{"translation": {"code": "def _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : if depth == 0 : # base case # Take a leapfrog step. Terminate the tree-building if the simulation # error from the leapfrog integrator is too large. States discovered by # continuing the simulation are likely to have very low probability. [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) num_states = tf . cast ( next_log_joint > log_slice_sample , dtype = tf . int32 ) continue_trajectory = ( next_log_joint > log_slice_sample - max_simulation_error ) return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] # Build a tree at the current state. seed_stream = tfd . SeedStream ( seed , \"build_tree\" ) [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) if continue_trajectory : # If the just-built subtree did not terminate, build a second subtree at # the forward or reverse state, as appropriate. if direction < 0 : [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) else : [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) # Propose either `next_state` (which came from the first subtree and so is # nearby) or the new forward/reverse state (which came from the second # subtree and so is far away). num_states += far_num_states accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) if accept_far_state : next_state = far_state next_target_log_prob = far_target_log_prob next_grads_target_log_prob = far_grads_target_log_prob # Continue the NUTS trajectory if the far subtree did not terminate either, # and if the reverse-most and forward-most states do not exhibit a U-turn. has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) continue_trajectory = far_continue_trajectory and has_no_u_turn return [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ]", "nl": "Builds a tree at a given tree depth and at a given state ."}}
{"translation": {"code": "def sqrt_with_finite_grads ( x , name = None ) : with tf . compat . v1 . name_scope ( name , 'sqrt_with_finite_grads' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if not x . dtype . is_floating : raise TypeError ( 'Input `x` must be floating type.' ) def grad ( grad_ys ) : large_float_like_x = np . sqrt ( np . finfo ( x . dtype . as_numpy_dtype ( ) ) . max ) safe_grads = tf . where ( tf . equal ( x , 0 ) , tf . fill ( tf . shape ( input = x ) , large_float_like_x ) , 0.5 * tf . math . rsqrt ( x ) ) return grad_ys * safe_grads return tf . sqrt ( x ) , grad", "nl": "A sqrt function whose gradient at zero is very large but finite ."}}
{"translation": {"code": "def bayesian_resnet ( input_shape , num_classes = 10 , kernel_posterior_scale_mean = - 9.0 , kernel_posterior_scale_stddev = 0.1 , kernel_posterior_scale_constraint = 0.2 ) : filters = [ 64 , 128 , 256 , 512 ] kernels = [ 3 , 3 , 3 , 3 ] strides = [ 1 , 2 , 2 , 2 ] def _untransformed_scale_constraint ( t ) : return tf . clip_by_value ( t , - 1000 , tf . math . log ( kernel_posterior_scale_constraint ) ) kernel_posterior_fn = tfp . layers . default_mean_field_normal_fn ( untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = kernel_posterior_scale_mean , stddev = kernel_posterior_scale_stddev ) , untransformed_scale_constraint = _untransformed_scale_constraint ) image = tf . keras . layers . Input ( shape = input_shape , dtype = 'float32' ) x = tfp . layers . Convolution2DFlipout ( 64 , 3 , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( image ) for i in range ( len ( kernels ) ) : x = _resnet_block ( x , filters [ i ] , kernels [ i ] , strides [ i ] , kernel_posterior_fn ) x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tf . keras . layers . AveragePooling2D ( 4 , 1 ) ( x ) x = tf . keras . layers . Flatten ( ) ( x ) x = tfp . layers . DenseFlipout ( num_classes , kernel_posterior_fn = kernel_posterior_fn ) ( x ) model = tf . keras . Model ( inputs = image , outputs = x , name = 'resnet18' ) return model", "nl": "Constructs a ResNet18 model ."}}
{"translation": {"code": "def _vggconv_block ( x , filters , kernel , stride , kernel_posterior_fn ) : out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) out = tf . keras . layers . BatchNormalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( out ) out = tf . keras . layers . BatchNormalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tf . keras . layers . MaxPooling2D ( pool_size = ( 2 , 2 ) , strides = stride ) ( out ) return out", "nl": "Network block for VGG ."}}
{"translation": {"code": "def _resnet_block ( x , filters , kernel , stride , kernel_posterior_fn ) : x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = _projection_shortcut ( x , filters , stride , kernel_posterior_fn ) else : shortcut = x x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = stride , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x", "nl": "Network block for ResNet ."}}
{"translation": {"code": "def build_fake_data ( ) : num_examples = 10 x_train = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) y_train = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) x_test = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) y_test = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) return ( x_train , y_train ) , ( x_test , y_test )", "nl": "Build fake CIFAR10 - style data for unit testing ."}}
{"translation": {"code": "def factored_joint_mvn ( distributions ) : graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] # pylint: disable=protected-access with tf . compat . v1 . name_scope ( 'factored_joint_mvn' , values = graph_parents ) : # We explicitly broadcast the `locs` so that we can concatenate them. # We don't have direct numerical access to the `scales`, which are arbitrary # linear operators, but `LinearOperatorBlockDiag` appears to do the right # thing without further intervention. dtype = tf . debugging . assert_same_float_dtype ( distributions ) broadcast_ones = tf . ones ( broadcast_batch_shape ( distributions ) , dtype = dtype ) [ ... , tf . newaxis ] return MultivariateNormalLinearOperator ( loc = tf . concat ( [ mvn . mean ( ) * broadcast_ones for mvn in distributions ] , axis = - 1 ) , scale = tfl . LinearOperatorBlockDiag ( [ mvn . scale for mvn in distributions ] , is_square = True ) )", "nl": "Combine MultivariateNormals into a factored joint distribution ."}}
{"translation": {"code": "def sum_mvns ( distributions ) : graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] # pylint: disable=protected-access with tf . compat . v1 . name_scope ( 'sum_mvns' , values = graph_parents ) : if all ( [ isinstance ( mvn , tfd . MultivariateNormalDiag ) for mvn in distributions ] ) : return tfd . MultivariateNormalDiag ( loc = sum ( [ mvn . mean ( ) for mvn in distributions ] ) , scale_diag = tf . sqrt ( sum ( [ mvn . scale . diag ** 2 for mvn in distributions ] ) ) ) else : raise NotImplementedError ( 'Sums of distributions other than MultivariateNormalDiag are not ' 'currently implemented. (given: {})' . format ( distributions ) )", "nl": "Attempt to sum MultivariateNormal distributions ."}}
{"translation": {"code": "def broadcast_batch_shape ( distributions ) : # Static case batch_shape = distributions [ 0 ] . batch_shape for distribution in distributions : batch_shape = tf . broadcast_static_shape ( batch_shape , distribution . batch_shape ) if batch_shape . is_fully_defined ( ) : return batch_shape . as_list ( ) # Fallback on dynamic. batch_shape = distributions [ 0 ] . batch_shape_tensor ( ) for distribution in distributions : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , distribution . batch_shape_tensor ( ) ) return tf . convert_to_tensor ( value = batch_shape )", "nl": "Get broadcast batch shape from distributions statically if possible ."}}
{"translation": {"code": "def create_sprites_dataset ( characters , actions , directions , channels = 3 , length = 8 , shuffle = False , fake_data = False ) : if fake_data : dummy_image = tf . random . normal ( [ HEIGHT , WIDTH , CHANNELS ] ) else : basedir = download_sprites ( ) action_names = [ action . name for action in actions ] action_metadata = [ ( action . start_row , action . frames ) for action in actions ] direction_rows = [ direction . row_offset for direction in directions ] chars = tf . data . Dataset . from_tensor_slices ( characters ) act_names = tf . data . Dataset . from_tensor_slices ( action_names ) . repeat ( ) acts_metadata = tf . data . Dataset . from_tensor_slices ( action_metadata ) . repeat ( ) dir_rows = tf . data . Dataset . from_tensor_slices ( direction_rows ) . repeat ( ) if shuffle : chars = chars . shuffle ( len ( characters ) ) dataset = tf . data . Dataset . zip ( ( chars , act_names , acts_metadata , dir_rows ) ) skin_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( SKIN_COLORS ) ) hair_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( HAIRSTYLES ) ) top_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( TOPS ) ) pants_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( PANTS ) ) action_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( action_names ) ) def process_example ( attrs , act_name , act_metadata , dir_row_offset ) : \"\"\"Processes a dataset row.\"\"\" skin_name = attrs [ 0 ] hair_name = attrs [ 1 ] top_name = attrs [ 2 ] pants_name = attrs [ 3 ] if fake_data : char = dummy_image else : skin = read_image ( basedir + os . sep + skin_name ) hair = read_image ( basedir + os . sep + hair_name ) top = read_image ( basedir + os . sep + top_name ) pants = read_image ( basedir + os . sep + pants_name ) char = create_character ( skin , hair , top , pants ) if shuffle : seq = create_random_seq ( char , act_metadata , dir_row_offset , length ) else : seq = create_seq ( char , act_metadata , dir_row_offset , length ) seq = seq [ ... , : channels ] # limit output channels skin_idx = skin_table . lookup ( skin_name ) hair_idx = hair_table . lookup ( hair_name ) top_idx = top_table . lookup ( top_name ) pants_idx = pants_table . lookup ( pants_name ) act_idx = action_table . lookup ( act_name ) return ( seq , skin_idx , hair_idx , top_idx , pants_idx , act_idx , skin_name , hair_name , top_name , pants_name , act_name ) dataset = dataset . map ( process_example ) return dataset", "nl": "Creates a tf . data pipeline for the sprites dataset ."}}
{"translation": {"code": "def create_seq ( character , action_metadata , direction , length = 8 , start = 0 ) : sprite_start = ( action_metadata [ 0 ] + direction ) * FRAME_SIZE sprite_end = ( action_metadata [ 0 ] + direction + 1 ) * FRAME_SIZE sprite_line = character [ sprite_start : sprite_end , ... ] # Extract 64x64 patches that are side-by-side in the sprite, and limit # to the actual number of frames for the given action. frames = tf . stack ( tf . split ( sprite_line , 13 , axis = 1 ) ) # 13 is a hack frames = frames [ 0 : action_metadata [ 1 ] ] # Extract a slice of the desired length. # NOTE: Length could be longer than the number of frames, so tile as needed. frames = tf . roll ( frames , shift = - start , axis = 0 ) frames = tf . tile ( frames , [ 2 , 1 , 1 , 1 ] ) # 2 is a hack frames = frames [ : length ] frames = tf . cast ( frames , dtype = tf . float32 ) frames . set_shape ( [ length , FRAME_SIZE , FRAME_SIZE , CHANNELS ] ) return frames", "nl": "Creates a sequence ."}}
{"translation": {"code": "def create_character ( skin , hair , top , pants ) : dtype = skin . dtype hair_mask = tf . cast ( hair [ ... , - 1 : ] <= 0 , dtype ) top_mask = tf . cast ( top [ ... , - 1 : ] <= 0 , dtype ) pants_mask = tf . cast ( pants [ ... , - 1 : ] <= 0 , dtype ) char = ( skin * hair_mask ) + hair char = ( char * top_mask ) + top char = ( char * pants_mask ) + pants return char", "nl": "Creates a character sprite from a set of attribute sprites ."}}
{"translation": {"code": "def download_sprites ( ) : filepath = os . path . join ( FLAGS . data_dir , DATA_SPRITES_DIR ) if not tf . io . gfile . exists ( filepath ) : if not tf . io . gfile . exists ( FLAGS . data_dir ) : tf . io . gfile . makedirs ( FLAGS . data_dir ) zip_name = \"{}.zip\" . format ( filepath ) urllib . request . urlretrieve ( DATA_SPRITES_URL , zip_name ) with zipfile . ZipFile ( zip_name , \"r\" ) as zip_file : zip_file . extractall ( FLAGS . data_dir ) tf . io . gfile . remove ( zip_name ) return filepath", "nl": "Downloads the sprites data and returns the saved filepath ."}}
{"translation": {"code": "def read_image ( filepath ) : im_bytes = tf . io . read_file ( filepath ) im = tf . image . decode_image ( im_bytes , channels = CHANNELS ) im = tf . image . convert_image_dtype ( im , tf . float32 ) return im", "nl": "Returns an image tensor ."}}
{"translation": {"code": "def create_random_seq ( character , action_metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , maxval = action_metadata [ 1 ] , dtype = tf . int32 ) return create_seq ( character , action_metadata , direction , length , start )", "nl": "Creates a random sequence ."}}
{"translation": {"code": "def visualize_qualitative_analysis ( inputs , model , samples = 1 , batch_size = 3 , length = 8 ) : average = lambda dist : tf . reduce_mean ( input_tensor = dist . mean ( ) , axis = 0 ) # avg over samples with tf . compat . v1 . name_scope ( \"val_reconstruction\" ) : reconstruct = functools . partial ( model . reconstruct , inputs = inputs , samples = samples ) visualize_reconstruction ( inputs , average ( reconstruct ( ) ) ) visualize_reconstruction ( inputs , average ( reconstruct ( sample_static = True ) ) , name = \"static_prior\" ) visualize_reconstruction ( inputs , average ( reconstruct ( sample_dynamic = True ) ) , name = \"dynamic_prior\" ) visualize_reconstruction ( inputs , average ( reconstruct ( swap_static = True ) ) , name = \"swap_static\" ) visualize_reconstruction ( inputs , average ( reconstruct ( swap_dynamic = True ) ) , name = \"swap_dynamic\" ) with tf . compat . v1 . name_scope ( \"generation\" ) : generate = functools . partial ( model . generate , batch_size = batch_size , length = length , samples = samples ) image_summary ( average ( generate ( fix_static = True ) ) , \"fix_static\" ) image_summary ( average ( generate ( fix_dynamic = True ) ) , \"fix_dynamic\" )", "nl": "Visualizes a qualitative analysis of a given model ."}}
{"translation": {"code": "def visualize_reconstruction ( inputs , reconstruct , num = 3 , name = \"reconstruction\" ) : reconstruct = tf . clip_by_value ( reconstruct , 0. , 1. ) inputs_and_reconstruct = tf . concat ( ( inputs [ : num ] , reconstruct [ : num ] ) , axis = 0 ) image_summary ( inputs_and_reconstruct , name )", "nl": "Visualizes the reconstruction of inputs in TensorBoard ."}}
{"translation": {"code": "def call ( self , inputs , state ) : # In order to allow the user to pass in a single example without a batch # dimension, we always expand the input to at least two dimensions, then # fix the output shape to remove the batch dimension if necessary. original_shape = inputs . shape if len ( original_shape ) < 2 : inputs = tf . reshape ( inputs , [ 1 , - 1 ] ) out , state = self . lstm_cell ( inputs , state ) out = self . output_layer ( out ) correct_shape = tf . concat ( ( original_shape [ : - 1 ] , tf . shape ( input = out ) [ - 1 : ] ) , 0 ) out = tf . reshape ( out , correct_shape ) loc = out [ ... , : self . dimensions ] scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 # keep > 0 return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state", "nl": "Runs the model to generate a distribution for a single timestep ."}}
{"translation": {"code": "def zero_state ( self , sample_batch_shape = ( ) ) : h0 = tf . zeros ( [ 1 , self . hidden_size ] ) c0 = tf . zeros ( [ 1 , self . hidden_size ] ) combined_shape = tf . concat ( ( tf . convert_to_tensor ( value = sample_batch_shape , dtype = tf . int32 ) , [ self . dimensions ] ) , axis = - 1 ) previous_output = tf . zeros ( combined_shape ) return previous_output , ( h0 , c0 )", "nl": "Returns an initial state for the LSTM cell ."}}
{"translation": {"code": "def call ( self , inputs ) : del inputs # unused with tf . compat . v1 . name_scope ( self . _name ) : return tfd . MultivariateNormalDiag ( self . loc , self . scale_diag )", "nl": "Runs the model to generate multivariate normal distribution ."}}
{"translation": {"code": "def sample_dynamic_prior ( self , samples , batch_size , length , fixed = False ) : if fixed : sample_batch_size = 1 else : sample_batch_size = batch_size sample , state = self . dynamic_prior . zero_state ( [ samples , sample_batch_size ] ) locs = [ ] scale_diags = [ ] sample_list = [ ] for _ in range ( length ) : dist , state = self . dynamic_prior ( sample , state ) sample = dist . sample ( ) locs . append ( dist . parameters [ \"loc\" ] ) scale_diags . append ( dist . parameters [ \"scale_diag\" ] ) sample_list . append ( sample ) sample = tf . stack ( sample_list , axis = 2 ) loc = tf . stack ( locs , axis = 2 ) scale_diag = tf . stack ( scale_diags , axis = 2 ) if fixed : # tile along the batch axis sample = sample + tf . zeros ( [ batch_size , 1 , 1 ] ) return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag )", "nl": "Sample the dynamic latent prior ."}}
{"translation": {"code": "def sample_static_prior ( self , samples , batch_size , fixed = False ) : dist = self . static_prior ( ) if fixed : # in either case, shape is (samples, batch, latent) sample = dist . sample ( ( samples , 1 ) ) + tf . zeros ( [ batch_size , 1 ] ) else : sample = dist . sample ( ( samples , batch_size ) ) return sample , dist", "nl": "Sample the static latent prior ."}}
{"translation": {"code": "def image_summary ( seqs , name , num = None ) : seqs = tf . clip_by_value ( seqs , 0. , 1. ) seqs = tf . unstack ( seqs [ : num ] ) joined_seqs = [ tf . concat ( tf . unstack ( seq ) , 1 ) for seq in seqs ] joined_seqs = tf . expand_dims ( tf . concat ( joined_seqs , 0 ) , 0 ) tf . compat . v2 . summary . image ( name , joined_seqs , max_outputs = 1 , step = tf . compat . v1 . train . get_or_create_global_step ( ) )", "nl": "Visualizes sequences as TensorBoard summaries ."}}
{"translation": {"code": "def call ( self , inputs ) : image_shape = tf . shape ( input = inputs ) [ - 3 : ] collapsed_shape = tf . concat ( ( [ - 1 ] , image_shape ) , axis = 0 ) out = tf . reshape ( inputs , collapsed_shape ) # (sample*batch*T, h, w, c) out = self . conv1 ( out ) out = self . conv2 ( out ) out = self . conv3 ( out ) out = self . conv4 ( out ) expanded_shape = tf . concat ( ( tf . shape ( input = inputs ) [ : - 3 ] , [ - 1 ] ) , axis = 0 ) return tf . reshape ( out , expanded_shape )", "nl": "Runs the model to generate an intermediate representation of x_t ."}}
{"translation": {"code": "def generate ( self , batch_size , length , samples = 1 , fix_static = False , fix_dynamic = False ) : static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) return likelihood", "nl": "Generate new sequences ."}}
{"translation": {"code": "def reconstruct ( self , inputs , samples = 1 , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : batch_size = tf . shape ( input = inputs ) [ - 5 ] length = len ( tf . unstack ( inputs , axis = - 4 ) ) # hack for graph mode features = self . compressor ( inputs ) # (..., batch, timesteps, hidden) if sample_static : static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) else : static_sample , _ = self . sample_static_posterior ( features , samples ) if swap_static : static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) if sample_dynamic : dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) else : dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) if swap_dynamic : dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) return likelihood", "nl": "Reconstruct the given input sequences ."}}
{"translation": {"code": "def summarize_dist_params ( dist , name , name_scope = \"dist_params\" ) : with tf . compat . v1 . name_scope ( name_scope ) : tf . compat . v2 . summary . histogram ( name = \"{}/{}\" . format ( name , \"mean\" ) , data = dist . mean ( ) , step = tf . compat . v1 . train . get_or_create_global_step ( ) ) tf . compat . v2 . summary . histogram ( name = \"{}/{}\" . format ( name , \"stddev\" ) , data = dist . stddev ( ) , step = tf . compat . v1 . train . get_or_create_global_step ( ) )", "nl": "Summarize the parameters of a distribution ."}}
{"translation": {"code": "def summarize_mean_in_nats_and_bits ( inputs , units , name , nats_name_scope = \"nats\" , bits_name_scope = \"bits_per_dim\" ) : mean = tf . reduce_mean ( input_tensor = inputs ) with tf . compat . v1 . name_scope ( nats_name_scope ) : tf . compat . v2 . summary . scalar ( name , mean , step = tf . compat . v1 . train . get_or_create_global_step ( ) ) with tf . compat . v1 . name_scope ( bits_name_scope ) : tf . compat . v2 . summary . scalar ( name , mean / units / tf . math . log ( 2. ) , step = tf . compat . v1 . train . get_or_create_global_step ( ) )", "nl": "Summarize the mean of a tensor in nats and bits per unit ."}}
{"translation": {"code": "def check_arg_in_support ( f ) : @ functools . wraps ( f ) def _check_arg_and_apply_f ( * args , * * kwargs ) : dist = args [ 0 ] x = args [ 1 ] with tf . control_dependencies ( [ assert_util . assert_greater_equal ( x , dist . loc , message = \"x is not in the support of the distribution\" ) ] if dist . validate_args else [ ] ) : return f ( * args , * * kwargs ) return _check_arg_and_apply_f", "nl": "Decorator function for argument bounds checking ."}}
{"translation": {"code": "def _merge ( self , old , new , use_equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use_equals else ( old is new ) : return old raise ValueError ( \"Incompatible values: %s != %s\" % ( old , new ) )", "nl": "Helper to merge which handles merging one value ."}}
{"translation": {"code": "def merge ( self , x = None , y = None , ildj = None , kwargs = None , mapping = None ) : if mapping is None : mapping = _Mapping ( x = x , y = y , ildj = ildj , kwargs = kwargs ) elif any ( arg is not None for arg in [ x , y , ildj , kwargs ] ) : raise ValueError ( \"Cannot simultaneously specify mapping and individual \" \"arguments.\" ) return _Mapping ( x = self . _merge ( self . x , mapping . x ) , y = self . _merge ( self . y , mapping . y ) , ildj = self . _merge ( self . ildj , mapping . ildj ) , kwargs = self . _merge ( self . kwargs , mapping . kwargs , use_equals = True ) )", "nl": "Returns new _Mapping with args merged with self ."}}
{"translation": {"code": "def _init_params ( self , amplitude , length_scale , validate_args ) : dtype = util . maybe_get_common_dtype ( [ amplitude , length_scale ] ) if amplitude is not None : amplitude = tf . convert_to_tensor ( value = amplitude , name = 'amplitude' , dtype = dtype ) self . _amplitude = _validate_arg_if_not_none ( amplitude , tf . compat . v1 . assert_positive , validate_args ) if length_scale is not None : length_scale = tf . convert_to_tensor ( value = length_scale , name = 'length_scale' , dtype = dtype ) self . _length_scale = _validate_arg_if_not_none ( length_scale , tf . compat . v1 . assert_positive , validate_args ) return dtype", "nl": "Shared init logic for amplitude and length_scale params ."}}
{"translation": {"code": "def batch_shape_tensor ( self ) : batch_shape = tf . constant ( [ ] , dtype = tf . int32 ) for param in self . parameters : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , param . prior . batch_shape_tensor ( ) ) return batch_shape", "nl": "Runtime batch shape of models represented by this component ."}}
{"translation": {"code": "def batch_shape ( self ) : batch_shape = tf . TensorShape ( [ ] ) for param in self . parameters : batch_shape = tf . broadcast_static_shape ( batch_shape , param . prior . batch_shape ) return batch_shape", "nl": "Static batch shape of models represented by this component ."}}
{"translation": {"code": "def pick_scalar_condition ( pred , true_value , false_value , name = None ) : with tf . name_scope ( name or \"pick_scalar_condition\" ) : pred = tf . convert_to_tensor ( value = pred , dtype_hint = tf . bool , name = \"pred\" ) true_value = tf . convert_to_tensor ( value = true_value , name = \"true_value\" ) false_value = tf . convert_to_tensor ( value = false_value , name = \"false_value\" ) pred_ = tf . get_static_value ( pred ) if pred_ is None : return tf . where ( pred , true_value , false_value ) return true_value if pred_ else false_value", "nl": "Convenience function that chooses one of two values based on the predicate ."}}
{"translation": {"code": "def _psd_mask ( x ) : # Allegedly # https://scicomp.stackexchange.com/questions/12979/testing-if-a-matrix-is-positive-semi-definite # it is more efficient to test for positive semi-definiteness by # trying to compute the Cholesky decomposition -- the matrix is PSD # if you succeed and not PSD if you fail.  However, TensorFlow's # Cholesky raises an exception if _any_ of the input matrices are # not PSD, from which I don't know how to extract _which ones_, so I # proceed by explicitly computing all the eigenvalues and checking # whether they are all positive or not. # # Also, as was discussed in the answer, it is somewhat dangerous to # treat SPD-ness as binary in floating-point arithmetic. Cholesky # factorization can complete and 'look' like everything is fine # (e.g., O(1) entries and a diagonal of all ones) but the matrix can # have an exponential condition number. eigenvalues , _ = tf . linalg . eigh ( x ) return tf . cast ( tf . reduce_min ( input_tensor = eigenvalues , axis = - 1 ) >= 0 , dtype = x . dtype )", "nl": "Computes whether each square matrix in the input is positive semi - definite ."}}
{"translation": {"code": "def _det_large_enough_mask ( x , det_bounds ) : # For the curious: I wonder whether it is possible and desirable to # use a Cholesky decomposition-based algorithm for this, since the # only matrices whose determinant this code cares about will be PSD. # Didn't figure out how to code that in TensorFlow. # # Expert opinion is that it would be about twice as fast since # Cholesky is roughly half the cost of Gaussian Elimination with # Partial Pivoting. But this is less of an impact than the switch in # _psd_mask. return tf . cast ( tf . linalg . det ( x ) > det_bounds , dtype = x . dtype )", "nl": "Returns whether the input matches the given determinant limit ."}}
{"translation": {"code": "def _uniform_correlation_like_matrix ( num_rows , batch_shape , dtype , seed ) : num_entries = num_rows * ( num_rows + 1 ) / 2 ones = tf . ones ( shape = [ num_entries ] , dtype = dtype ) # It seems wasteful to generate random values for the diagonal since # I am going to throw them away, but `fill_triangular` fills the # diagonal, so I probably need them. # It's not impossible that it would be more efficient to just fill # the whole matrix with random values instead of messing with # `fill_triangular`.  Then would need to filter almost half out with # `matrix_band_part`. unifs = uniform . Uniform ( - ones , ones ) . sample ( batch_shape , seed = seed ) tril = util . fill_triangular ( unifs ) symmetric = tril + tf . linalg . matrix_transpose ( tril ) diagonal_ones = tf . ones ( shape = util . pad ( batch_shape , axis = 0 , back = True , value = num_rows ) , dtype = dtype ) return tf . linalg . set_diag ( symmetric , diagonal_ones )", "nl": "Returns a uniformly random Tensor of correlation - like matrices ."}}
{"translation": {"code": "def correlation_matrix_volume_rejection_samples ( det_bounds , dim , sample_shape , dtype , seed ) : with tf . compat . v1 . name_scope ( \"rejection_sampler\" ) : rej_proposals = _uniform_correlation_like_matrix ( dim , sample_shape , dtype , seed = seed ) rej_proposal_volume = 2. ** ( dim * ( dim - 1 ) / 2. ) # The density of proposing any given point is 1 / rej_proposal_volume; # The weight of that point should be scaled by # 1 / density = rej_proposal_volume. rej_weights = rej_proposal_volume * _psd_mask ( rej_proposals ) * _det_large_enough_mask ( rej_proposals , det_bounds ) return rej_weights , rej_proposal_volume", "nl": "Returns rejection samples from trying to get good correlation matrices ."}}
{"translation": {"code": "def _clopper_pearson_confidence_interval ( samples , error_rate ) : # TODO(b/78025336) Migrate this confidence interval function # to statistical_testing.py.  In order to do that # - Get the binomial CDF from the Binomial distribution # - Implement scalar root finding in TF.  Batch bisection search #   shouldn't be too hard, and is definitely good enough for this #   problem.  Batching the Brent algorithm (from scipy) that is used #   here may be more involved, but may also not be necessary---it's #   only used here because scipy made it convenient.  In particular, #   robustness is more important than speed here, which may make #   bisection search actively better. # - The rest is just a matter of rewriting in the appropriate style. if optimize is None or stats is None : raise ValueError ( \"Scipy is required for computing Clopper-Pearson confidence intervals\" ) if len ( samples . shape ) != 1 : raise ValueError ( \"Batch semantics not implemented\" ) n = len ( samples ) low = np . amin ( samples ) high = np . amax ( samples ) successes = np . count_nonzero ( samples - low ) failures = np . count_nonzero ( samples - high ) if successes + failures != n : uniques = np . unique ( samples ) msg = ( \"Purportedly Bernoulli distribution had distinct samples\" \" {}, {}, and {}\" . format ( uniques [ 0 ] , uniques [ 1 ] , uniques [ 2 ] ) ) raise ValueError ( msg ) def p_small_enough ( p ) : prob = stats . binom . logcdf ( successes , n , p ) return prob - np . log ( error_rate / 2. ) def p_big_enough ( p ) : prob = stats . binom . logsf ( successes , n , p ) return prob - np . log ( error_rate / 2. ) high_p = optimize . brentq ( p_small_enough , float ( successes ) / n , 1. , rtol = 1e-9 ) low_p = optimize . brentq ( p_big_enough , 0. , float ( successes ) / n , rtol = 1e-9 ) low_interval = low + ( high - low ) * low_p high_interval = low + ( high - low ) * high_p return ( low_interval , high_interval )", "nl": "Computes a confidence interval for the mean of the given 1 - D distribution ."}}
{"translation": {"code": "def compute_true_volumes ( det_bounds , dim , num_samples , error_rate = 1e-6 , seed = 42 ) : bounds = { } with tf . compat . v1 . Session ( ) as sess : rej_weights , _ = correlation_matrix_volume_rejection_samples ( det_bounds , dim , [ num_samples , len ( det_bounds ) ] , np . float32 , seed = seed ) rej_weights = sess . run ( rej_weights ) for rw , det in zip ( np . rollaxis ( rej_weights , 1 ) , det_bounds ) : template = ( \"Estimating volume of {}x{} correlation \" \"matrices with determinant >= {}.\" ) print ( template . format ( dim , dim , det ) ) sys . stdout . flush ( ) bounds [ det ] = _clopper_pearson_confidence_interval ( rw , error_rate = error_rate ) return bounds", "nl": "Returns confidence intervals for the desired correlation matrix volumes ."}}
{"translation": {"code": "def _inv_hessian_control_inputs ( inv_hessian ) : # The easiest way to validate if the inverse Hessian is positive definite is # to compute its Cholesky decomposition. is_positive_definite = tf . reduce_all ( input_tensor = tf . math . is_finite ( tf . linalg . cholesky ( inv_hessian ) ) , axis = [ - 1 , - 2 ] ) # Then check that the supplied inverse Hessian is symmetric. is_symmetric = tf . equal ( bfgs_utils . norm ( inv_hessian - _batch_transpose ( inv_hessian ) , dims = 2 ) , 0 ) # Simply adding a control dependencies on these results is not enough to # trigger them, we need to add asserts on the results. return [ tf . Assert ( is_positive_definite , [ 'Initial inverse Hessian is not positive definite.' , inv_hessian ] ) , tf . Assert ( is_symmetric , [ 'Initial inverse Hessian is not symmetric' , inv_hessian ] ) ]", "nl": "Computes control inputs to validate a provided inverse Hessian ."}}
{"translation": {"code": "def _print ( pass_through_tensor , values ) : flat_values = [ ] for value in values : # Checks if it is a namedtuple. if hasattr ( value , '_fields' ) : for field in value . _fields : flat_values . extend ( [ field , _to_str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat_values . append ( _to_str ( v ) ) continue flat_values . append ( _to_str ( value ) ) return tf . compat . v1 . Print ( pass_through_tensor , flat_values )", "nl": "Wrapper for tf . Print which supports lists and namedtuples for printing ."}}
{"translation": {"code": "def _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : midpoint = ( search_interval . left . x + search_interval . right . x ) / 2 val_mid = value_and_gradients_function ( midpoint ) is_valid_mid = hzl . is_finite ( val_mid ) still_active = active & is_valid_mid new_failed = active & ~ is_valid_mid next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , func_evals = search_interval . func_evals + 1 ) def _apply_update ( ) : update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval )", "nl": "Performs bisection and updates the interval ."}}
{"translation": {"code": "def _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : # The maximum iterations permitted are determined as the number of halvings # it takes to reduce 1 to 0 in the given dtype. iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) def _cond ( i , val_c , to_fix ) : del val_c # Unused. return ( i < iter_max ) & tf . reduce_any ( input_tensor = to_fix ) def _body ( i , val_c , to_fix ) : next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) next_val_c = value_and_gradients_function ( next_c ) still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) return ( i + 1 , next_val_c , still_to_fix ) to_fix = active & ~ hzl . is_finite ( val_c_input ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( 0 , val_c_input , to_fix ) )", "nl": "Shrinks the input step size until the value and grad become finite ."}}
{"translation": {"code": "def _machine_eps ( dtype ) : if isinstance ( dtype , tf . DType ) : dtype = dtype . as_numpy_dtype ( ) return np . finfo ( dtype ) . eps", "nl": "Returns the machine epsilon for the supplied dtype ."}}
{"translation": {"code": "def _broadcast_event_and_samples ( event , samples , event_ndims ) : # This is the shape of self.samples, without the samples axis, i.e. the shape # of the result of a call to dist.sample(). This way we can broadcast it with # event to get a properly-sized event, then add the singleton dim back at # -event_ndims - 1. samples_shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event_ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event_ndims : ] ] , axis = 0 ) event *= tf . ones ( samples_shape , dtype = event . dtype ) event = tf . expand_dims ( event , axis = - event_ndims - 1 ) samples *= tf . ones_like ( event , dtype = samples . dtype ) return event , samples", "nl": "Broadcasts the event or samples ."}}
{"translation": {"code": "def smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10 , name = None ) : with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : loop_num_iter_ = tf . get_static_value ( loop_num_iter ) if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : # Cast to int32 to run the comparison against i in host memory, # where while/LoopCond needs it. loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) return tf . while_loop ( cond = lambda i , * args : i < loop_num_iter , body = lambda i , * args : [ i + 1 ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( 0 ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ 1 : ] result = initial_loop_vars for _ in range ( loop_num_iter_ ) : result = body_fn ( * result ) return result", "nl": "Construct a for loop preferring a python loop if n is staticaly known ."}}
{"translation": {"code": "def _is_known_unsigned_by_dtype ( dt ) : return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base_dtype , False )", "nl": "Helper returning True if dtype is known to be unsigned ."}}
{"translation": {"code": "def maybe_get_static_value ( x , dtype = None ) : if x is None : return x try : # This returns an np.ndarray. x_ = tf . get_static_value ( x ) except TypeError : x_ = x if x_ is None or dtype is None : return x_ return np . array ( x_ , dtype )", "nl": "Helper which tries to return a static value ."}}
{"translation": {"code": "def _is_known_signed_by_dtype ( dt ) : return { tf . float16 : True , tf . float32 : True , tf . float64 : True , tf . int8 : True , tf . int16 : True , tf . int32 : True , tf . int64 : True , } . get ( dt . base_dtype , False )", "nl": "Helper returning True if dtype is known to be signed ."}}
{"translation": {"code": "def _largest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) if dt . is_floating : return int ( 2 ** ( np . finfo ( dt . as_numpy_dtype ) . nmant + 1 ) ) if dt . is_integer : return np . iinfo ( dt . as_numpy_dtype ) . max if dt . base_dtype == tf . bool : return int ( 1 ) # We actually can't land here but keep the case for completeness. raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) )", "nl": "Helper returning the largest integer exactly representable by dtype ."}}
{"translation": {"code": "def rotate_transpose ( x , shift , name = \"rotate_transpose\" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) shift = tf . convert_to_tensor ( value = shift , name = \"shift\" ) # We do not assign back to preserve constant-ness. assert_util . assert_integer ( shift ) shift_value_static = tf . get_static_value ( shift ) ndims = tensorshape_util . rank ( x . shape ) if ndims is not None and shift_value_static is not None : if ndims < 2 : return x shift_value_static = np . sign ( shift_value_static ) * ( abs ( shift_value_static ) % ndims ) if shift_value_static == 0 : return x perm = np . roll ( np . arange ( ndims ) , shift_value_static ) return tf . transpose ( a = x , perm = perm ) else : # Consider if we always had a positive shift, and some specified # direction. # When shifting left we want the new array: #   last(x, n-shift) + first(x, shift) # and if shifting right then we want: #   last(x, shift) + first(x, n-shift) # Observe that last(a) == slice(a, n) and first(a) == slice(0, a). # Also, we can encode direction and shift as one: direction * shift. # Combining these facts, we have: #   a = cond(shift<0, -shift, n-shift) #   last(x, n-a) + first(x, a) == x[a:n] + x[0:a] # Finally, we transform shift by modulo length so it can be specified # independently from the array upon which it operates (like python). ndims = tf . rank ( x ) shift = tf . where ( tf . less ( shift , 0 ) , - shift % ndims , ndims - shift % ndims ) first = tf . range ( 0 , shift ) last = tf . range ( shift , ndims ) perm = tf . concat ( [ last , first ] , 0 ) return tf . transpose ( a = x , perm = perm )", "nl": "Circularly moves dims left or right ."}}
{"translation": {"code": "def _is_integer_like_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) return dt . is_integer or dt . base_dtype == tf . bool", "nl": "Helper returning True if dtype . is_integer or is bool ."}}
{"translation": {"code": "def embed_check_categorical_event_shape ( categorical_param , name = \"embed_check_categorical_event_shape\" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = categorical_param , name = \"categorical_param\" ) # The size must not exceed both of: # - The largest possible int32 (since categorical values are presumed to be #   indexes into a Tensor). # - The largest possible integer exactly representable under the given #   floating-point dtype (since we need to cast to/from). # # The chosen floating-point thresholds are 2**(1 + mantissa_bits). # For more details, see: # https://en.wikipedia.org/wiki/Floating-point_arithmetic#Internal_representation x_dtype = dtype_util . base_dtype ( x . dtype ) max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else 0 ) if max_event_size is 0 : raise TypeError ( \"Unable to validate size of unrecognized dtype \" \"({}).\" . format ( dtype_util . name ( x_dtype ) ) ) try : x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) except ValueError : raise ValueError ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) if event_size is not None : if event_size < 2 : raise ValueError ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) if event_size > max_event_size : raise ValueError ( \"Number of classes exceeds `dtype` precision, i.e., \" \"{} implies shape ({}) cannot exceed {}.\" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) return x else : event_size = tf . shape ( input = x , out_type = tf . int64 , name = \"x_shape\" ) [ - 1 ] return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( \"A categorical-distribution parameter must have \" \"at least 1 dimension.\" ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2 , message = ( \"A categorical-distribution parameter must have at \" \"least 2 events.\" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = \"Number of classes exceeds `dtype` precision, \" \"i.e., {} dtype cannot exceed {} shape.\" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x )", "nl": "Embeds checks that categorical distributions don t have too many classes ."}}
{"translation": {"code": "def log_combinations ( n , counts , name = \"log_combinations\" ) : # First a bit about the number of ways counts could have come in: # E.g. if counts = [1, 2], then this is 3 choose 2. # In general, this is (sum counts)! / sum(counts!) # The sum should be along the last dimension of counts. This is the # \"distribution\" dimension. Here n a priori represents the sum of counts. with tf . name_scope ( name ) : n = tf . convert_to_tensor ( value = n , name = \"n\" ) counts = tf . convert_to_tensor ( value = counts , name = \"counts\" ) total_permutations = tf . math . lgamma ( n + 1 ) counts_factorial = tf . math . lgamma ( counts + 1 ) redundant_permutations = tf . reduce_sum ( input_tensor = counts_factorial , axis = [ - 1 ] ) return total_permutations - redundant_permutations", "nl": "Multinomial coefficient ."}}
{"translation": {"code": "def pick_vector ( cond , true_vector , false_vector , name = \"pick_vector\" ) : with tf . name_scope ( name ) : cond = tf . convert_to_tensor ( value = cond , dtype_hint = tf . bool , name = \"cond\" ) if cond . dtype != tf . bool : raise TypeError ( \"{}.dtype={} which is not {}\" . format ( cond , cond . dtype , tf . bool ) ) true_vector = tf . convert_to_tensor ( value = true_vector , name = \"true_vector\" ) false_vector = tf . convert_to_tensor ( value = false_vector , name = \"false_vector\" ) if true_vector . dtype != false_vector . dtype : raise TypeError ( \"{}.dtype={} does not match {}.dtype={}\" . format ( true_vector , true_vector . dtype , false_vector , false_vector . dtype ) ) cond_value_static = tf . get_static_value ( cond ) if cond_value_static is not None : return true_vector if cond_value_static else false_vector n = tf . shape ( input = true_vector ) [ 0 ] return tf . slice ( tf . concat ( [ true_vector , false_vector ] , 0 ) , [ tf . where ( cond , 0 , n ) ] , [ tf . where ( cond , n , - 1 ) ] )", "nl": "Picks possibly different length row Tensor s based on condition ."}}
{"translation": {"code": "def prefer_static_broadcast_shape ( shape1 , shape2 , name = \"prefer_static_broadcast_shape\" ) : with tf . name_scope ( name ) : def make_shape_tensor ( x ) : return tf . convert_to_tensor ( value = x , name = \"shape\" , dtype = tf . int32 ) def get_tensor_shape ( s ) : if isinstance ( s , tf . TensorShape ) : return s s_ = tf . get_static_value ( make_shape_tensor ( s ) ) if s_ is not None : return tf . TensorShape ( s_ ) return None def get_shape_tensor ( s ) : if not isinstance ( s , tf . TensorShape ) : return make_shape_tensor ( s ) if tensorshape_util . is_fully_defined ( s ) : return make_shape_tensor ( tensorshape_util . as_list ( s ) ) raise ValueError ( \"Cannot broadcast from partially \" \"defined `TensorShape`.\" ) shape1_ = get_tensor_shape ( shape1 ) shape2_ = get_tensor_shape ( shape2 ) if shape1_ is not None and shape2_ is not None : return tf . broadcast_static_shape ( shape1_ , shape2_ ) shape1_ = get_shape_tensor ( shape1 ) shape2_ = get_shape_tensor ( shape2 ) return tf . broadcast_dynamic_shape ( shape1_ , shape2_ )", "nl": "Convenience function which statically broadcasts shape when possible ."}}
{"translation": {"code": "def gen_new_seed ( seed , salt ) : if seed is None : return None string = ( str ( seed ) + salt ) . encode ( \"utf-8\" ) return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8 ] , 16 ) & 0x7FFFFFFF", "nl": "Generate a new seed from the given seed and salt ."}}
{"translation": {"code": "def dimension_size ( x , axis ) : # Since tf.gather isn't \"constant-in, constant-out\", we must first check the # static shape or fallback to dynamic shape. s = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , np . abs ( axis ) ) [ axis ] ) if s is not None : return s return tf . shape ( input = x ) [ axis ]", "nl": "Returns the size of a specific dimension ."}}
{"translation": {"code": "def process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : with tf . name_scope ( name or \"process_quadrature_grid_and_probs\" ) : if quadrature_grid_and_probs is None : grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = \"probs\" , dtype = dtype ) return grid , probs grid , probs = tuple ( quadrature_grid_and_probs ) grid = tf . convert_to_tensor ( value = grid , name = \"grid\" , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = \"unnormalized_probs\" , dtype = dtype ) probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = \"probs\" ) def _static_event_size ( x ) : \"\"\"Returns the static size of a specific dimension or `None`.\"\"\" return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) m , n = _static_event_size ( probs ) , _static_event_size ( grid ) if m is not None and n is not None : if m != n : raise ValueError ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s \" \"(saw lengths {}, {})\" . format ( m , n ) ) elif validate_args : assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( \"`quadrature_grid_and_probs` must be a `tuple` of \" \"same-length zero-th-dimension `Tensor`s\" ) ) , ] with tf . control_dependencies ( assertions ) : grid = tf . identity ( grid ) probs = tf . identity ( probs ) return grid , probs", "nl": "Validates quadrature grid probs or computes them as necessary ."}}
{"translation": {"code": "def parent_frame_arguments ( ) : # All arguments and the names used for *varargs, and **kwargs arg_names , variable_arg_name , keyword_arg_name , local_vars = ( tf_inspect . _inspect . getargvalues ( # pylint: disable=protected-access # Get the first frame of the caller of this method. tf_inspect . _inspect . stack ( ) [ 1 ] [ 0 ] ) ) # pylint: disable=protected-access # Remove the *varargs, and flatten the **kwargs. Both are # nested lists. local_vars . pop ( variable_arg_name , { } ) keyword_args = local_vars . pop ( keyword_arg_name , { } ) final_args = { } # Copy over arguments and their values. In general, local_vars # may contain more than just the arguments, since this method # can be called anywhere in a function. for arg_name in arg_names : final_args [ arg_name ] = local_vars . pop ( arg_name ) final_args . update ( keyword_args ) return final_args", "nl": "Returns parent frame arguments ."}}
{"translation": {"code": "def _variance_scale_term ( self ) : # Expand back the last dim so the shape of _variance_scale_term matches the # shape of self.concentration. c0 = self . total_concentration [ ... , tf . newaxis ] return tf . sqrt ( ( 1. + c0 / self . total_count [ ... , tf . newaxis ] ) / ( 1. + c0 ) )", "nl": "Helper to _covariance and _variance which computes a shared scale ."}}
{"translation": {"code": "def _copy_fn ( fn ) : if not callable ( fn ) : raise TypeError ( \"fn is not callable: {}\" . format ( fn ) ) # The blessed way to copy a function. copy.deepcopy fails to create a # non-reference copy. Since: #   types.FunctionType == type(lambda: None), # and the docstring for the function type states: # #   function(code, globals[, name[, argdefs[, closure]]]) # #   Create a function object from a code object and a dictionary. #   ... # # Here we can use this to create a new function with the old function's # code, globals, closure, etc. return types . FunctionType ( code = fn . __code__ , globals = fn . __globals__ , name = fn . __name__ , argdefs = fn . __defaults__ , closure = fn . __closure__ )", "nl": "Create a deep copy of fn ."}}
{"translation": {"code": "def _inv_z ( self , z ) : with tf . name_scope ( \"reconstruct\" ) : return z * self . scale + self . loc", "nl": "Reconstruct input x from a its normalized version ."}}
{"translation": {"code": "def _z ( self , x ) : with tf . name_scope ( \"standardize\" ) : return ( x - self . loc ) / self . scale", "nl": "Standardize input x to a unit normal ."}}
{"translation": {"code": "def _smallest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( \"Unrecognized dtype: {}\" . format ( dt . name ) ) if _is_known_unsigned_by_dtype ( dt ) : return 0 return - 1 * _largest_integer_by_dtype ( dt )", "nl": "Helper returning the smallest integer exactly representable by dtype ."}}
{"translation": {"code": "def same_dynamic_shape ( a , b ) : a = tf . convert_to_tensor ( value = a , name = \"a\" ) b = tf . convert_to_tensor ( value = b , name = \"b\" ) # Here we can't just do tf.equal(a.shape, b.shape), since # static shape inference may break the equality comparison between # shape(a) and shape(b) in tf.equal. def all_shapes_equal ( ) : return tf . reduce_all ( input_tensor = tf . equal ( tf . concat ( [ tf . shape ( input = a ) , tf . shape ( input = b ) ] , 0 ) , tf . concat ( [ tf . shape ( input = b ) , tf . shape ( input = a ) ] , 0 ) ) ) # One of the shapes isn't fully defined, so we need to use the dynamic # shape. return tf . cond ( pred = tf . equal ( tf . rank ( a ) , tf . rank ( b ) ) , true_fn = all_shapes_equal , false_fn = lambda : tf . constant ( False ) )", "nl": "Returns whether a and b have the same dynamic shape ."}}
{"translation": {"code": "def tridiag ( below = None , diag = None , above = None , name = None ) : def _pad ( x ) : \"\"\"Prepends and appends a zero to every vector in a batch of vectors.\"\"\" shape = tf . concat ( [ tf . shape ( input = x ) [ : - 1 ] , [ 1 ] ] , axis = 0 ) z = tf . zeros ( shape , dtype = x . dtype ) return tf . concat ( [ z , x , z ] , axis = - 1 ) def _add ( * x ) : \"\"\"Adds list of Tensors, ignoring `None`.\"\"\" s = None for y in x : if y is None : continue elif s is None : s = y else : s += y if s is None : raise ValueError ( \"Must specify at least one of `below`, `diag`, `above`.\" ) return s with tf . name_scope ( name or \"tridiag\" ) : if below is not None : below = tf . convert_to_tensor ( value = below , name = \"below\" ) below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - 1 , 1 : ] if diag is not None : diag = tf . convert_to_tensor ( value = diag , name = \"diag\" ) diag = tf . linalg . diag ( diag ) if above is not None : above = tf . convert_to_tensor ( value = above , name = \"above\" ) above = tf . linalg . diag ( _pad ( above ) ) [ ... , 1 : , : - 1 ] # TODO(jvdillon): Consider using scatter_nd instead of creating three full # matrices. return _add ( below , diag , above )", "nl": "Creates a matrix with values set above below and on the diagonal ."}}
{"translation": {"code": "def _broadcast_cat_event_and_params ( event , params , base_dtype ) : if dtype_util . is_integer ( event . dtype ) : pass elif dtype_util . is_floating ( event . dtype ) : # When `validate_args=True` we've already ensured int/float casting # is closed. event = tf . cast ( event , dtype = tf . int32 ) else : raise TypeError ( \"`value` should have integer `dtype` or \" \"`self.dtype` ({})\" . format ( base_dtype ) ) shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) if not shape_known_statically or params . shape [ : - 1 ] != event . shape : params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) params_shape = tf . shape ( input = params ) [ : - 1 ] event *= tf . ones ( params_shape , dtype = event . dtype ) if tensorshape_util . rank ( params . shape ) is not None : tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) return event , params", "nl": "Broadcasts the event or distribution parameters ."}}
{"translation": {"code": "def range ( self , name = \"range\" ) : with self . _name_scope ( name ) : return self . high - self . low", "nl": "high - low ."}}
{"translation": {"code": "def _pick_scalar_condition ( pred , cond_true , cond_false ) : # Note: This function is only valid if all of pred, cond_true, and cond_false # are scalars. This means its semantics are arguably more like tf.cond than # tf.where even though we use tf.where to implement it. pred_ = tf . get_static_value ( tf . convert_to_tensor ( value = pred ) ) if pred_ is None : return tf . where ( pred , cond_true , cond_false ) return cond_true if pred_ else cond_false", "nl": "Convenience function which chooses the condition based on the predicate ."}}
{"translation": {"code": "def _finish_prob_for_one_fiber ( self , y , x , ildj , event_ndims , * * distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) prob = self . distribution . prob ( x , * * distribution_kwargs ) if self . _is_maybe_event_override : prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return prob", "nl": "Finish computation of prob on one element of the inverse image ."}}
{"translation": {"code": "def _maybe_rotate_dims ( self , x , rotate_right = False ) : needs_rotation_const = tf . get_static_value ( self . _needs_rotation ) if needs_rotation_const is not None and not needs_rotation_const : return x ndims = prefer_static . rank ( x ) n = ( ndims - self . _rotate_ndims ) if rotate_right else self . _rotate_ndims perm = prefer_static . concat ( [ prefer_static . range ( n , ndims ) , prefer_static . range ( 0 , n ) ] , axis = 0 ) return tf . transpose ( a = x , perm = perm )", "nl": "Helper which rolls left event_dims left or right event_dims right ."}}
{"translation": {"code": "def embed_check_nonnegative_integer_form ( x , name = \"embed_check_nonnegative_integer_form\" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) assertions = [ assert_util . assert_non_negative ( x , message = \"'{}' must be non-negative.\" . format ( x ) ) , ] if not dtype_util . is_integer ( x . dtype ) : assertions += [ assert_integer_form ( x , message = \"'{}' cannot contain fractional components.\" . format ( x ) ) , ] return with_dependencies ( assertions , x )", "nl": "Assert x is a non - negative tensor and optionally of integers ."}}
{"translation": {"code": "def _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , * * distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) log_prob = self . distribution . log_prob ( x , * * distribution_kwargs ) if self . _is_maybe_event_override : log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) log_prob += tf . cast ( ildj , log_prob . dtype ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return log_prob", "nl": "Finish computation of log_prob on one element of the inverse image ."}}
{"translation": {"code": "def _registered_kl ( type_a , type_b ) : hierarchy_a = tf_inspect . getmro ( type_a ) hierarchy_b = tf_inspect . getmro ( type_b ) dist_to_children = None kl_fn = None for mro_to_a , parent_a in enumerate ( hierarchy_a ) : for mro_to_b , parent_b in enumerate ( hierarchy_b ) : candidate_dist = mro_to_a + mro_to_b candidate_kl_fn = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) if not kl_fn or ( candidate_kl_fn and candidate_dist < dist_to_children ) : dist_to_children = candidate_dist kl_fn = candidate_kl_fn return kl_fn", "nl": "Get the KL function registered for classes a and b ."}}
{"translation": {"code": "def log_cdf_laplace ( x , name = \"log_cdf_laplace\" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) # For x < 0, L(x) = 0.5 * exp{x} exactly, so Log[L(x)] = log(0.5) + x. lower_solution = - np . log ( 2. ) + x # safe_exp_neg_x = exp{-x} for x > 0, but is # bounded above by 1, which avoids #   log[1 - 1] = -inf for x = log(1/2), AND #   exp{-x} --> inf, for x << -1 safe_exp_neg_x = tf . exp ( - tf . abs ( x ) ) # log1p(z) = log(1 + z) approx z for |z| << 1. This approxmation is used # internally by log1p, rather than being done explicitly here. upper_solution = tf . math . log1p ( - 0.5 * safe_exp_neg_x ) return tf . where ( x < 0. , lower_solution , upper_solution )", "nl": "Log Laplace distribution function ."}}
{"translation": {"code": "def erfinv ( x , name = \"erfinv\" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) if dtype_util . as_numpy_dtype ( x . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( \"x.dtype={} is not handled, see docstring for supported \" \"types.\" . format ( dtype_util . name ( x . dtype ) ) ) return ndtri ( ( x + 1. ) / 2. ) / np . sqrt ( 2. )", "nl": "The inverse function for erf the error function ."}}
{"translation": {"code": "def _ndtr ( x ) : half_sqrt_2 = tf . constant ( 0.5 * np . sqrt ( 2. ) , dtype = x . dtype , name = \"half_sqrt_2\" ) w = x * half_sqrt_2 z = tf . abs ( w ) y = tf . where ( tf . less ( z , half_sqrt_2 ) , 1. + tf . math . erf ( w ) , tf . where ( tf . greater ( w , 0. ) , 2. - tf . math . erfc ( z ) , tf . math . erfc ( z ) ) ) return 0.5 * y", "nl": "Implements ndtr core logic ."}}
{"translation": {"code": "def ndtr ( x , name = \"ndtr\" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) if dtype_util . as_numpy_dtype ( x . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( \"x.dtype=%s is not handled, see docstring for supported types.\" % x . dtype ) return _ndtr ( x )", "nl": "Normal distribution function ."}}
{"translation": {"code": "def ndtri ( p , name = \"ndtri\" ) : with tf . name_scope ( name ) : p = tf . convert_to_tensor ( value = p , name = \"p\" ) if dtype_util . as_numpy_dtype ( p . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( \"p.dtype=%s is not handled, see docstring for supported types.\" % p . dtype ) return _ndtri ( p )", "nl": "The inverse of the CDF of the Normal distribution function ."}}
{"translation": {"code": "def log_ndtr ( x , series_order = 3 , name = \"log_ndtr\" ) : if not isinstance ( series_order , int ) : raise TypeError ( \"series_order must be a Python integer.\" ) if series_order < 0 : raise ValueError ( \"series_order must be non-negative.\" ) if series_order > 30 : raise ValueError ( \"series_order must be <= 30.\" ) with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) if dtype_util . base_equal ( x . dtype , tf . float64 ) : lower_segment = LOGNDTR_FLOAT64_LOWER upper_segment = LOGNDTR_FLOAT64_UPPER elif dtype_util . base_equal ( x . dtype , tf . float32 ) : lower_segment = LOGNDTR_FLOAT32_LOWER upper_segment = LOGNDTR_FLOAT32_UPPER else : raise TypeError ( \"x.dtype=%s is not supported.\" % x . dtype ) # The basic idea here was ported from: #   https://root.cern.ch/doc/v608/SpecFuncCephesInv_8cxx_source.html # We copy the main idea, with a few changes # * For x >> 1, and X ~ Normal(0, 1), #     Log[P[X < x]] = Log[1 - P[X < -x]] approx -P[X < -x], #     which extends the range of validity of this function. # * We use one fixed series_order for all of 'x', rather than adaptive. # * Our docstring properly reflects that this is an asymptotic series, not a #   Taylor series. We also provided a correct bound on the remainder. # * We need to use the max/min in the _log_ndtr_lower arg to avoid nan when #   x=0. This happens even though the branch is unchosen because when x=0 #   the gradient of a select involves the calculation 1*dy+0*(-inf)=nan #   regardless of whether dy is finite. Note that the minimum is a NOP if #   the branch is chosen. return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , # log(1-x) ~= -x, x << 1 tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) )", "nl": "Log Normal distribution function ."}}
{"translation": {"code": "def _log_ndtr_asymptotic_series ( x , series_order ) : npdt = dtype_util . as_numpy_dtype ( x . dtype ) if series_order <= 0 : return npdt ( 1 ) x_2 = tf . square ( x ) even_sum = tf . zeros_like ( x ) odd_sum = tf . zeros_like ( x ) x_2n = x_2 # Start with x^{2*1} = x^{2*n} with n = 1. for n in range ( 1 , series_order + 1 ) : y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n if n % 2 : odd_sum += y else : even_sum += y x_2n *= x_2 return 1. + even_sum - odd_sum", "nl": "Calculates the asymptotic series used in log_ndtr ."}}
{"translation": {"code": "def _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) # We need to put the tf.where inside the outer tf.where to ensure we never # hit a NaN in the gradient. denom = tf . where ( df > 2. , df - 2. , tf . ones_like ( df ) ) statistic = statistic * df_factor_fn ( df / denom ) # When 1 < df <= 2, stddev/variance are infinite. inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) result_where_defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = \"inf\" ) ) if self . allow_nan_stats : nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = \"nan\" ) ) else : with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + \" not defined for components of df <= 1\" ) , ] ) : return tf . identity ( result_where_defined )", "nl": "Helper to compute stddev covariance and variance ."}}
{"translation": {"code": "def _hat_integral_inverse ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . math . expm1 ( - ( tf . math . log ( t ) + tf . math . log ( x ) ) / t )", "nl": "Inverse function of _hat_integral ."}}
{"translation": {"code": "def _hat_integral ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . exp ( ( - t ) * tf . math . log1p ( x ) - tf . math . log ( t ) )", "nl": "Integral of the hat function used for sampling ."}}
{"translation": {"code": "def _pdf_at_peak ( self ) : return ( self . peak - self . low ) / ( self . high - self . low )", "nl": "Pdf evaluated at the peak ."}}
{"translation": {"code": "def _broadcast_to ( tensor_to_broadcast , target_tensors ) : output = tensor_to_broadcast for tensor in target_tensors : output += tf . zeros_like ( tensor ) return output", "nl": "Helper to broadcast a tensor using a list of target tensors ."}}
{"translation": {"code": "def _squeeze ( x , axis ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if axis is None : return tf . squeeze ( x , axis = None ) axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) # Make axis at least 1d. keep_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep_axis ) )", "nl": "A version of squeeze that works with dynamic axis ."}}
{"translation": {"code": "def _make_positive_axis ( axis , ndims ) : axis = _make_list_or_1d_tensor ( axis ) ndims = tf . convert_to_tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) ndims_ = tf . get_static_value ( ndims ) if _is_list_like ( axis ) and ndims_ is not None : # Static case positive_axis = [ ] for a in axis : if a < 0 : a = ndims_ + a positive_axis . append ( a ) else : # Dynamic case axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) positive_axis = tf . where ( axis >= 0 , axis , axis + ndims ) return positive_axis", "nl": "Rectify possibly negatively axis . Prefer return Python list ."}}
{"translation": {"code": "def variance ( x , sample_axis = 0 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'variance' , values = [ x , sample_axis ] ) : return covariance ( x , y = None , sample_axis = sample_axis , event_axis = None , keepdims = keepdims )", "nl": "Estimate variance using samples ."}}
{"translation": {"code": "def _build_trainable_posterior ( param , initial_loc_fn ) : loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) # Ensure the `event_shape` of the variational distribution matches the # parameter. if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) # Transform to constrained parameter space. return tfd . TransformedDistribution ( q , param . bijector )", "nl": "Built a transformed - normal variational dist over a parameter s support ."}}
{"translation": {"code": "def build_factored_variational_loss ( model , observed_time_series , init_batch_shape = ( ) , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'build_factored_variational_loss' , values = [ observed_time_series ] ) as name : seed = tfd . SeedStream ( seed , salt = 'StructuralTimeSeries_build_factored_variational_loss' ) variational_distributions = collections . OrderedDict ( ) variational_samples = [ ] for param in model . parameters : def initial_loc_fn ( param ) : return sample_uniform_initial_state ( param , return_constrained = True , init_sample_shape = init_batch_shape , seed = seed ( ) ) q = _build_trainable_posterior ( param , initial_loc_fn = initial_loc_fn ) variational_distributions [ param . name ] = q variational_samples . append ( q . sample ( seed = seed ( ) ) ) # Multiple initializations (similar to HMC chains) manifest as an extra # param batch dimension, so we need to add corresponding batch dimension(s) # to `observed_time_series`. observed_time_series = sts_util . pad_batch_dimension_for_multiple_chains ( observed_time_series , model , chain_batch_shape = init_batch_shape ) # Construct the variational bound. log_prob_fn = model . joint_log_prob ( observed_time_series ) expected_log_joint = log_prob_fn ( * variational_samples ) entropy = tf . reduce_sum ( input_tensor = [ - q . log_prob ( sample ) for ( q , sample ) in zip ( variational_distributions . values ( ) , variational_samples ) ] , axis = 0 ) variational_loss = - ( expected_log_joint + entropy ) # -ELBO return variational_loss , variational_distributions", "nl": "Build a loss function for variational inference in STS models ."}}
{"translation": {"code": "def _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer def train_loop_body ( step ) : train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : step < num_steps , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = True ) [ 0 ] # Always return a single op. return minimize_op", "nl": "Run an optimizer within the graph to minimize a loss function ."}}
{"translation": {"code": "def stddev ( x , sample_axis = 0 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'stddev' , values = [ x , sample_axis ] ) : return tf . sqrt ( variance ( x , sample_axis = sample_axis , keepdims = keepdims ) )", "nl": "Estimate standard deviation using samples ."}}
{"translation": {"code": "def cholesky_covariance ( x , sample_axis = 0 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'cholesky_covariance' , values = [ x , sample_axis ] ) : sample_axis = tf . convert_to_tensor ( value = sample_axis , dtype = tf . int32 ) cov = covariance ( x , sample_axis = sample_axis , event_axis = - 1 , keepdims = keepdims ) return tf . linalg . cholesky ( cov )", "nl": "Cholesky factor of the covariance matrix of vector - variate random samples ."}}
{"translation": {"code": "def _ensure_tf_install ( ) : # pylint: disable=g-statement-before-imports try : import tensorflow as tf except ImportError : # Print more informative error message, then reraise. print ( \"\\n\\nFailed to import TensorFlow. Please note that TensorFlow is not \" \"installed by default when you install TensorFlow Probability. This \" \"is so that users can decide whether to install the GPU-enabled \" \"TensorFlow package. To use TensorFlow Probability, please install \" \"the most recent version of TensorFlow, by following instructions at \" \"https://tensorflow.org/install.\\n\\n\" ) raise import distutils . version # # Update this whenever we need to depend on a newer TensorFlow release. # required_tensorflow_version = \"1.13\" if ( distutils . version . LooseVersion ( tf . __version__ ) < distutils . version . LooseVersion ( required_tensorflow_version ) ) : raise ImportError ( \"This version of TensorFlow Probability requires TensorFlow \" \"version >= {required}; Detected an installation of version {present}. \" \"Please upgrade TensorFlow to proceed.\" . format ( required = required_tensorflow_version , present = tf . __version__ ) )", "nl": "Attempt to import tensorflow and ensure its version is sufficient ."}}
{"translation": {"code": "def _log_vector_matrix ( vs , ms ) : return tf . reduce_logsumexp ( input_tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )", "nl": "Multiply tensor of vectors by matrices assuming values stored are logs ."}}
{"translation": {"code": "def _extract_log_probs ( num_states , dist ) : states = tf . reshape ( tf . range ( num_states ) , tf . concat ( [ [ num_states ] , tf . ones_like ( dist . batch_shape_tensor ( ) ) ] , axis = 0 ) ) return distribution_util . move_dimension ( dist . log_prob ( states ) , 0 , - 1 )", "nl": "Tabulate log probabilities from a batch of distributions ."}}
{"translation": {"code": "def _vector_matrix ( vs , ms ) : return tf . reduce_sum ( input_tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2 )", "nl": "Multiply tensor of vectors by matrices ."}}
{"translation": {"code": "def _marginal_hidden_probs ( self ) : initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) # initial_log_probs :: batch_shape num_states if self . _num_steps > 1 : transition_log_probs = self . _log_trans def forward_step ( log_probs , _ ) : return _log_vector_matrix ( log_probs , transition_log_probs ) dummy_index = tf . zeros ( self . _num_steps - 1 , dtype = tf . float32 ) forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = \"forward_log_probs\" ) forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) else : forward_log_probs = initial_log_probs [ tf . newaxis , ... ] # returns :: num_steps batch_shape num_states return tf . exp ( forward_log_probs )", "nl": "Compute marginal pdf for each individual observable ."}}
{"translation": {"code": "def _mul_right ( mat , vec ) : return tf . squeeze ( tf . matmul ( mat , tf . expand_dims ( vec , axis = - 1 ) ) , axis = - 1 )", "nl": "Computes the product of a matrix with a vector on the right ."}}
{"translation": {"code": "def _assert_ndims_statically ( x , expect_ndims = None , expect_ndims_at_least = None , expect_static = False ) : ndims = x . shape . ndims if ndims is None : if expect_static : raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect_ndims is not None and ndims != expect_ndims : raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) if expect_ndims_at_least is not None and ndims < expect_ndims_at_least : raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) )", "nl": "Assert that Tensor x has expected number of dimensions ."}}
{"translation": {"code": "def _update_inv_hessian ( prev_state , next_state ) : # Only update the inverse Hessian if not already failed or converged. should_update = ~ next_state . converged & ~ next_state . failed # Compute the normalization term (y^T . s), should not update if is singular. gradient_delta = next_state . objective_gradient - prev_state . objective_gradient position_delta = next_state . position - prev_state . position normalization_factor = tf . reduce_sum ( input_tensor = gradient_delta * position_delta , axis = - 1 ) should_update = should_update & ~ tf . equal ( normalization_factor , 0 ) def _do_update_inv_hessian ( ) : next_inv_hessian = _bfgs_inv_hessian_update ( gradient_delta , position_delta , normalization_factor , prev_state . inverse_hessian_estimate ) return bfgs_utils . update_fields ( next_state , inverse_hessian_estimate = tf . where ( should_update , next_inv_hessian , prev_state . inverse_hessian_estimate ) ) return prefer_static . cond ( tf . reduce_any ( input_tensor = should_update ) , _do_update_inv_hessian , lambda : next_state )", "nl": "Update the BGFS state by computing the next inverse hessian estimate ."}}
{"translation": {"code": "def _get_static_ndims ( x , expect_static = False , expect_ndims = None , expect_ndims_no_more_than = None , expect_ndims_at_least = None ) : ndims = x . shape . ndims if ndims is None : shape_const = tf . get_static_value ( tf . shape ( input = x ) ) if shape_const is not None : ndims = shape_const . ndim if ndims is None : if expect_static : raise ValueError ( 'Expected argument `x` to have statically defined `ndims`.  Found: ' % x ) return if expect_ndims is not None : ndims_message = ( 'Expected argument `x` to have ndims %s.  Found tensor %s' % ( expect_ndims , x ) ) if ndims != expect_ndims : raise ValueError ( ndims_message ) if expect_ndims_at_least is not None : ndims_at_least_message = ( 'Expected argument `x` to have ndims >= %d.  Found tensor %s' % ( expect_ndims_at_least , x ) ) if ndims < expect_ndims_at_least : raise ValueError ( ndims_at_least_message ) if expect_ndims_no_more_than is not None : ndims_no_more_than_message = ( 'Expected argument `x` to have ndims <= %d.  Found tensor %s' % ( expect_ndims_no_more_than , x ) ) if ndims > expect_ndims_no_more_than : raise ValueError ( ndims_no_more_than_message ) return ndims", "nl": "Get static number of dimensions and assert that some expectations are met ."}}
{"translation": {"code": "def _insert_back_keep_dims ( x , axis ) : for i in sorted ( axis ) : x = tf . expand_dims ( x , axis = i ) return x", "nl": "Insert the dims in axis back as singletons after being removed ."}}
{"translation": {"code": "def _make_static_axis_non_negative_list ( axis , ndims ) : axis = distribution_util . make_non_negative_axis ( axis , ndims ) axis_const = tf . get_static_value ( axis ) if axis_const is None : raise ValueError ( 'Expected argument `axis` to be statically available.  Found: %s' % axis ) # Make at least 1-D. axis = axis_const + np . zeros ( [ 1 ] , dtype = axis_const . dtype ) return list ( int ( dim ) for dim in axis )", "nl": "Convert possibly negatively indexed axis to non - negative list of ints ."}}
{"translation": {"code": "def _move_dims_to_flat_end ( x , axis , x_ndims , right_end = True ) : if not axis : return x # Suppose x.shape = [a, b, c, d] # Suppose axis = [1, 3] # other_dims = [0, 2] in example above. other_dims = sorted ( set ( range ( x_ndims ) ) . difference ( axis ) ) # x_permed.shape = [a, c, b, d] perm = other_dims + list ( axis ) if right_end else list ( axis ) + other_dims x_permed = tf . transpose ( a = x , perm = perm ) if x . shape . is_fully_defined ( ) : x_shape = x . shape . as_list ( ) # other_shape = [a, c], end_shape = [b * d] other_shape = [ x_shape [ i ] for i in other_dims ] end_shape = [ np . prod ( [ x_shape [ i ] for i in axis ] ) ] full_shape = ( other_shape + end_shape if right_end else end_shape + other_shape ) else : other_shape = tf . gather ( tf . shape ( input = x ) , other_dims ) full_shape = tf . concat ( [ other_shape , [ - 1 ] ] if right_end else [ [ - 1 ] , other_shape ] , axis = 0 ) return tf . reshape ( x_permed , shape = full_shape )", "nl": "Move dims corresponding to axis in x to the end then flatten ."}}
{"translation": {"code": "def _sort_tensor ( tensor ) : sorted_ , _ = tf . nn . top_k ( tensor , k = tf . shape ( input = tensor ) [ - 1 ] ) sorted_ . set_shape ( tensor . shape ) return sorted_", "nl": "Use top_k to sort a Tensor along the last dimension ."}}
{"translation": {"code": "def _restrict_along_direction ( value_and_gradients_function , position , direction ) : def _restricted_func ( t ) : t = _broadcast ( t , position ) pt = position + tf . expand_dims ( t , axis = - 1 ) * direction objective_value , gradient = value_and_gradients_function ( pt ) return ValueAndGradient ( x = t , f = objective_value , df = tf . reduce_sum ( input_tensor = gradient * direction , axis = - 1 ) , full_gradient = gradient ) return _restricted_func", "nl": "Restricts a function in n - dimensions to a given direction ."}}
{"translation": {"code": "def _broadcast ( value , target ) : return tf . broadcast_to ( tf . convert_to_tensor ( value = value , dtype = target . dtype ) , distribution_util . prefer_static_shape ( target ) [ : - 1 ] )", "nl": "Broadcast a value to match the batching dimensions of a target ."}}
{"translation": {"code": "def get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : if control_inputs : with tf . control_dependencies ( control_inputs ) : f0 , df0 = value_and_gradients_function ( initial_position ) else : f0 , df0 = value_and_gradients_function ( initial_position ) converged = norm ( df0 , dims = 1 ) < grad_tolerance return dict ( converged = converged , failed = tf . zeros_like ( converged ) , # i.e. False. num_iterations = tf . convert_to_tensor ( value = 0 ) , num_objective_evaluations = tf . convert_to_tensor ( value = 1 ) , position = initial_position , objective_value = f0 , objective_gradient = df0 )", "nl": "Returns a dictionary to populate the initial state of the search procedure ."}}
{"translation": {"code": "def _check_convergence ( current_position , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : grad_converged = norm ( next_gradient , dims = 1 ) <= grad_tolerance x_converged = norm ( next_position - current_position , dims = 1 ) <= x_tolerance f_converged = ( norm ( next_objective - current_objective , dims = 0 ) <= f_relative_tolerance * current_objective ) return grad_converged | x_converged | f_converged", "nl": "Checks if the algorithm satisfies the convergence criteria ."}}
{"translation": {"code": "def line_search_step ( state , value_and_gradients_function , search_direction , grad_tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) : line_search_value_grad_func = _restrict_along_direction ( value_and_gradients_function , state . position , search_direction ) derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) val_0 = ValueAndGradient ( x = _broadcast ( 0 , state . position ) , f = state . objective_value , df = derivative_at_start_pt , full_gradient = state . objective_gradient ) inactive = state . failed | state . converged ls_result = linesearch . hager_zhang ( line_search_value_grad_func , initial_step_size = _broadcast ( 1 , state . position ) , value_at_zero = val_0 , converged = inactive ) # No search needed for these. state_after_ls = update_fields ( state , failed = state . failed | ~ ls_result . converged , num_iterations = state . num_iterations + 1 , num_objective_evaluations = ( state . num_objective_evaluations + ls_result . func_evals ) ) def _do_update_position ( ) : # For inactive batch members `left.x` is zero. However, their # `search_direction` might also be undefined, so we can't rely on # multiplication by zero to produce a `position_delta` of zero. position_delta = tf . where ( inactive , tf . zeros_like ( search_direction ) , search_direction * tf . expand_dims ( ls_result . left . x , axis = - 1 ) ) return _update_position ( state_after_ls , position_delta , ls_result . left . f , ls_result . left . full_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) return prefer_static . cond ( stopping_condition ( state . converged , state . failed ) , true_fn = lambda : state_after_ls , false_fn = _do_update_position )", "nl": "Performs the line search step of the BFGS search procedure ."}}
{"translation": {"code": "def _update_position ( state , position_delta , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : failed = state . failed | ~ tf . math . is_finite ( next_objective ) | ~ tf . reduce_all ( input_tensor = tf . math . is_finite ( next_gradient ) , axis = - 1 ) next_position = state . position + position_delta converged = ~ failed & _check_convergence ( state . position , next_position , state . objective_value , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) return update_fields ( state , converged = state . converged | converged , failed = failed , position = next_position , objective_value = next_objective , objective_gradient = next_gradient )", "nl": "Updates the state advancing its position by a given position_delta ."}}
{"translation": {"code": "def _batch_gather_with_broadcast ( params , indices , axis ) : # batch_gather assumes... #   params.shape =  [A1,...,AN, B1,...,BM] #   indices.shape = [A1,...,AN, C] # which gives output of shape #                   [A1,...,AN, C, B1,...,BM] # Here we broadcast dims of each to the left of `axis` in params, and left of # the rightmost dim in indices, e.g. we can # have #   params.shape =  [A1,...,AN, B1,...,BM] #   indices.shape = [a1,...,aN, C], # where ai broadcasts with Ai. # leading_bcast_shape is the broadcast of [A1,...,AN] and [a1,...,aN]. leading_bcast_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) params += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) indices += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) return tf . compat . v1 . batch_gather ( params , indices )", "nl": "Like batch_gather but broadcasts to the left of axis ."}}
{"translation": {"code": "def _get_tensor_like_attributes ( ) : # Enable \"Tensor semantics\" for distributions. # See tensorflow/python/framework/ops.py `class Tensor` for details. attrs = dict ( ) # Setup overloadable operators and white-listed members / properties. attrs . update ( ( attr , _wrap_method ( tf . Tensor , attr ) ) for attr in tf . Tensor . OVERLOADABLE_OPERATORS . union ( { '__iter__' } ) ) # Copy some members straight-through. attrs . update ( ( attr , getattr ( tf . Tensor , attr ) ) for attr in { '__nonzero__' , '__bool__' , '__array_priority__' } ) return attrs", "nl": "Returns Tensor attributes related to shape and Python builtins ."}}
{"translation": {"code": "def _eval_all_one_hot ( fn , dist , name = None ) : with tf . compat . v1 . name_scope ( name , 'eval_all_one_hot' ) : event_size = dist . event_shape_tensor ( ) [ - 1 ] batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) # Reshape `eye(d)` to: `[d] + [1]*batch_ndims + [d]`. x = tf . reshape ( tf . eye ( event_size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch_ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant_values = event_size ) ) # Compute `fn(x)` then cyclically left-transpose one dim. perm = tf . pad ( tensor = tf . range ( 1 , batch_ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )", "nl": "OneHotCategorical helper computing probs cdf etc over its support ."}}
{"translation": {"code": "def benchmark_text_messages_hmc ( num_results = int ( 3e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 ) : if not tf . executing_eagerly ( ) : tf . compat . v1 . reset_default_graph ( ) # Build a static, pretend dataset. count_data = tf . cast ( tf . concat ( [ tfd . Poisson ( rate = 15. ) . sample ( 43 ) , tfd . Poisson ( rate = 25. ) . sample ( 31 ) ] , axis = 0 ) , dtype = tf . float32 ) if tf . executing_eagerly ( ) : count_data = count_data . numpy ( ) else : with tf . compat . v1 . Session ( ) : count_data = count_data . eval ( ) # Define a closure over our joint_log_prob. def unnormalized_log_posterior ( lambda1 , lambda2 , tau ) : return text_messages_joint_log_prob ( count_data , lambda1 , lambda2 , tau ) if tf . executing_eagerly ( ) : sample_chain = tf . function ( tfp . mcmc . sample_chain ) else : sample_chain = tfp . mcmc . sample_chain # Initialize the step_size. (It will be automatically adapted.) step_size = tf . compat . v2 . Variable ( name = 'step_size' , initial_value = tf . constant ( 0.05 , dtype = tf . float32 ) , trainable = False ) def computation ( ) : \"\"\"The benchmark computation.\"\"\" initial_chain_state = [ tf . constant ( count_data . mean ( ) , name = 'init_lambda1' ) , tf . constant ( count_data . mean ( ) , name = 'init_lambda2' ) , tf . constant ( 0.5 , name = 'init_tau' ) , ] unconstraining_bijectors = [ tfp . bijectors . Exp ( ) , # Maps a positive real to R. tfp . bijectors . Exp ( ) , # Maps a positive real to R. tfp . bijectors . Sigmoid ( ) , # Maps [0,1] to R. ] _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = initial_chain_state , kernel = tfp . mcmc . TransformedTransitionKernel ( inner_kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_log_posterior , num_leapfrog_steps = num_leapfrog_steps , step_size = step_size , step_size_update_fn = tfp . mcmc . make_simple_step_size_update_policy ( num_burnin_steps ) , state_gradients_are_stopped = True ) , bijector = unconstraining_bijectors ) ) return kernel_results . inner_results . is_accepted # Let's force evaluation of graph to ensure build time is not part of our time # trial. is_accepted_tensor = computation ( ) if not tf . executing_eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( tf . compat . v1 . global_variables_initializer ( ) ) session . run ( is_accepted_tensor ) start_time = time . time ( ) if tf . executing_eagerly ( ) : is_accepted = computation ( ) else : is_accepted = session . run ( is_accepted_tensor ) wall_time = time . time ( ) - start_time num_accepted = np . sum ( is_accepted ) acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time )", "nl": "Runs HMC on the text - messages unnormalized posterior ."}}
{"translation": {"code": "def text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) rv_lambda = tfd . Exponential ( rate = alpha ) rv_tau = tfd . Uniform ( ) lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv_observation = tfd . Poisson ( rate = lambda_ ) return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) )", "nl": "Joint log probability function ."}}
{"translation": {"code": "def _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) : init_args = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance ) empty_queue = _make_empty_queue_for ( num_correction_pairs , initial_position ) init_args . update ( position_deltas = empty_queue , gradient_deltas = empty_queue ) return LBfgsOptimizerResults ( * * init_args )", "nl": "Create LBfgsOptimizerResults with initial state of search procedure ."}}
{"translation": {"code": "def _get_search_direction ( state ) : # The number of correction pairs that have been collected so far. num_elements = tf . minimum ( state . num_iterations , distribution_util . prefer_static_shape ( state . position_deltas ) [ 0 ] ) def _two_loop_algorithm ( ) : \"\"\"L-BFGS two-loop algorithm.\"\"\" # Correction pairs are always appended to the end, so only the latest # `num_elements` vectors have valid position/gradient deltas. position_deltas = state . position_deltas [ - num_elements : ] gradient_deltas = state . gradient_deltas [ - num_elements : ] # Pre-compute all `inv_rho[i]`s. inv_rhos = tf . reduce_sum ( input_tensor = gradient_deltas * position_deltas , axis = - 1 ) def first_loop ( acc , args ) : _ , q_direction = acc position_delta , gradient_delta , inv_rho = args alpha = tf . reduce_sum ( input_tensor = position_delta * q_direction , axis = - 1 ) / inv_rho direction_delta = tf . expand_dims ( alpha , axis = - 1 ) * gradient_delta return ( alpha , q_direction - direction_delta ) # Run first loop body computing and collecting `alpha[i]`s, while also # computing the updated `q_direction` at each step. zero = tf . zeros_like ( inv_rhos [ 0 ] ) alphas , q_directions = tf . scan ( first_loop , [ position_deltas , gradient_deltas , inv_rhos ] , initializer = ( zero , state . objective_gradient ) , reverse = True ) # We use `H^0_k = gamma_k * I` as an estimate for the initial inverse # hessian for the k-th iteration; then `r_direction = H^0_k * q_direction`. gamma_k = inv_rhos [ - 1 ] / tf . reduce_sum ( input_tensor = gradient_deltas [ - 1 ] * gradient_deltas [ - 1 ] , axis = - 1 ) r_direction = tf . expand_dims ( gamma_k , axis = - 1 ) * q_directions [ 0 ] def second_loop ( r_direction , args ) : alpha , position_delta , gradient_delta , inv_rho = args beta = tf . reduce_sum ( input_tensor = gradient_delta * r_direction , axis = - 1 ) / inv_rho direction_delta = tf . expand_dims ( alpha - beta , axis = - 1 ) * position_delta return r_direction + direction_delta # Finally, run second loop body computing the updated `r_direction` at each # step. r_directions = tf . scan ( second_loop , [ alphas , position_deltas , gradient_deltas , inv_rhos ] , initializer = r_direction ) return - r_directions [ - 1 ] return prefer_static . cond ( tf . equal ( num_elements , 0 ) , ( lambda : - state . objective_gradient ) , _two_loop_algorithm )", "nl": "Computes the search direction to follow at the current state ."}}
{"translation": {"code": "def _make_empty_queue_for ( k , element ) : queue_shape = tf . concat ( [ [ k ] , distribution_util . prefer_static_shape ( element ) ] , axis = 0 ) return tf . zeros ( queue_shape , dtype = element . dtype . base_dtype )", "nl": "Creates a tf . Tensor suitable to hold k element - shaped tensors ."}}
{"translation": {"code": "def _queue_push ( queue , should_update , new_vecs ) : new_queue = tf . concat ( [ queue [ 1 : ] , [ new_vecs ] ] , axis = 0 ) update_pattern = tf . broadcast_to ( should_update [ tf . newaxis , ... , tf . newaxis ] , distribution_util . prefer_static_shape ( queue ) ) return tf . where ( update_pattern , new_queue , queue )", "nl": "Conditionally push new vectors into a batch of first - in - first - out queues ."}}
{"translation": {"code": "def minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10 , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : if initial_inverse_hessian_estimate is not None : raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) if stopping_condition is None : stopping_condition = bfgs_utils . converged_all with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) dtype = initial_position . dtype . base_dtype tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) # The `state` here is a `LBfgsOptimizerResults` tuple with values for the # current state of the algorithm computation. def _cond ( state ) : \"\"\"Continue if iterations remain and stopping condition is not met.\"\"\" return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) def _body ( current_state ) : \"\"\"Main optimization loop.\"\"\" search_direction = _get_search_direction ( current_state ) # TODO(b/120134934): Check if the derivative at the start point is not # negative, if so then reset position/gradient deltas and recompute # search direction. next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) # If not failed or converged, update the Hessian estimate. should_update = ~ ( next_state . converged | next_state . failed ) state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) return [ state_after_inv_hessian_update ] initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ]", "nl": "Applies the L - BFGS algorithm to minimize a differentiable function ."}}
{"translation": {"code": "def _log_matrix_vector ( ms , vs ) : return tf . reduce_logsumexp ( input_tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - 1 )", "nl": "Multiply tensor of matrices by vectors assuming values stored are logs ."}}
{"translation": {"code": "def posterior_marginals ( self , observations , name = None ) : with tf . name_scope ( name or \"posterior_marginals\" ) : with tf . control_dependencies ( self . _runtime_assertions ) : observation_tensor_shape = tf . shape ( input = observations ) with self . _observation_shape_preconditions ( observation_tensor_shape ) : observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) log_transition = self . _log_trans observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) observation_rank = tf . rank ( observations ) underlying_event_rank = self . _underlying_event_rank observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) observation_log_probs = self . _observation_distribution . log_prob ( observations ) log_adjoint_prob = tf . zeros_like ( log_init ) def forward_step ( log_previous_step , log_prob_observation ) : return _log_vector_matrix ( log_previous_step , log_transition ) + log_prob_observation log_prob = log_init + observation_log_probs [ 0 ] forward_log_probs = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = log_prob , name = \"forward_log_probs\" ) forward_log_probs = tf . concat ( [ [ log_prob ] , forward_log_probs ] , axis = 0 ) def backward_step ( log_previous_step , log_prob_observation ) : return _log_matrix_vector ( log_transition , log_prob_observation + log_previous_step ) backward_log_adjoint_probs = tf . scan ( backward_step , observation_log_probs [ 1 : ] , initializer = log_adjoint_prob , reverse = True , name = \"backward_log_adjoint_probs\" ) total_log_prob = tf . reduce_logsumexp ( input_tensor = forward_log_probs [ - 1 ] , axis = - 1 ) backward_log_adjoint_probs = tf . concat ( [ backward_log_adjoint_probs , [ log_adjoint_prob ] ] , axis = 0 ) log_likelihoods = forward_log_probs + backward_log_adjoint_probs marginal_log_probs = distribution_util . move_dimension ( log_likelihoods - total_log_prob [ ... , tf . newaxis ] , 0 , - 2 ) return categorical . Categorical ( logits = marginal_log_probs )", "nl": "Compute marginal posterior distribution for each state ."}}
{"translation": {"code": "def _deep_tuple ( self , x ) : if isinstance ( x , dict ) : return self . _deep_tuple ( tuple ( sorted ( x . items ( ) ) ) ) elif isinstance ( x , ( list , tuple ) ) : return tuple ( map ( self . _deep_tuple , x ) ) return x", "nl": "Converts nested tuple list or dict to nested tuple ."}}
{"translation": {"code": "def minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : if initial_population is None and initial_position is None : raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) if initial_population is not None and initial_position is not None : raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) def evolve_body ( loop_vars ) : \"\"\"Performs one step of the evolution.\"\"\" next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) failed = _check_failure ( next_population_values ) return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] def evolve_cond ( loop_vars ) : should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and loop_vars . num_iterations >= max_iterations ) ) return ~ should_stop initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) # Ensure we return a similar structure to what the user supplied. final_population = final_state . population if not was_iterable : final_population = final_population [ 0 ] best_position = best_position [ 0 ] return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations )", "nl": "Applies the Differential evolution algorithm to minimize a function ."}}
{"translation": {"code": "def one_step ( objective_function , population , population_values = None , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'one_step' , [ population , population_values , differential_weight , crossover_prob ] ) : population , _ = _ensure_list ( population ) if population_values is None : population_values = objective_function ( * population ) population_size = tf . shape ( input = population [ 0 ] ) [ 0 ] seed_stream = distributions . SeedStream ( seed , salt = 'one_step' ) mixing_indices = _get_mixing_indices ( population_size , seed = seed_stream ( ) ) # Construct the mutated solution vectors. There is one for each member of # the population. mutants = _get_mutants ( population , population_size , mixing_indices , differential_weight ) # Perform recombination between the parents and the mutants. candidates = _binary_crossover ( population , population_size , mutants , crossover_prob , seed = seed_stream ( ) ) candidate_values = objective_function ( * candidates ) if population_values is None : population_values = objective_function ( * population ) infinity = tf . zeros_like ( population_values ) + np . inf population_values = tf . where ( tf . math . is_nan ( population_values ) , x = infinity , y = population_values ) to_replace = candidate_values < population_values next_population = [ tf . where ( to_replace , x = candidates_part , y = population_part ) for candidates_part , population_part in zip ( candidates , population ) ] next_values = tf . where ( to_replace , x = candidate_values , y = population_values ) return next_population , next_values", "nl": "Performs one step of the differential evolution algorithm ."}}
{"translation": {"code": "def _ensure_list ( tensor_or_list ) : if isinstance ( tensor_or_list , ( list , tuple ) ) : return list ( tensor_or_list ) , True return [ tensor_or_list ] , False", "nl": "Converts the input arg to a list if it is not a list already ."}}
{"translation": {"code": "def _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : sizes = [ tf . cast ( tf . size ( input = x ) , dtype = tf . float64 ) for x in population ] seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , 1 ] , seed = seed_stream ( ) ) recombinants = [ ] for i , population_part in enumerate ( population ) : pop_part_flat = tf . reshape ( population_part , [ population_size , - 1 ] ) mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - 1 ] ) part_size = tf . size ( input = population_part ) // population_size force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = 0 , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = True , off_value = False , dtype = tf . bool ) # Tensor of shape [population_size, size] group_mask = tf . math . equal ( force_crossover_group , i ) force_crossovers &= group_mask do_binary_crossover = tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) < crossover_prob do_binary_crossover |= force_crossovers recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) recombinant = tf . reshape ( recombinant_flat , tf . shape ( input = population_part ) ) recombinants . append ( recombinant ) return recombinants", "nl": "Performs recombination by binary crossover for the current population ."}}
{"translation": {"code": "def _get_mixing_indices ( size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , default_name = 'get_mixing_indices' , values = [ size ] ) : size = tf . convert_to_tensor ( value = size ) dtype = size . dtype seed_stream = distributions . SeedStream ( seed , salt = 'get_mixing_indices' ) first = tf . random . uniform ( [ size ] , maxval = size - 1 , dtype = dtype , seed = seed_stream ( ) ) second = tf . random . uniform ( [ size ] , maxval = size - 2 , dtype = dtype , seed = seed_stream ( ) ) third = tf . random . uniform ( [ size ] , maxval = size - 3 , dtype = dtype , seed = seed_stream ( ) ) # Shift second if it is on top of or to the right of first second = tf . where ( first < second , x = second , y = second + 1 ) smaller = tf . math . minimum ( first , second ) larger = tf . math . maximum ( first , second ) # Shift the third one so it does not coincide with either the first or the # second number. Assuming first < second, shift by 1 if the number is in # [first, second) and by 2 if the number is greater than or equal to the # second. third = tf . where ( third < smaller , x = third , y = third + 1 ) third = tf . where ( third < larger , x = third , y = third + 1 ) sample = tf . stack ( [ first , second , third ] , axis = 1 ) to_avoid = tf . expand_dims ( tf . range ( size ) , axis = - 1 ) sample = tf . where ( sample < to_avoid , x = sample , y = sample + 1 ) return sample", "nl": "Generates an array of indices suitable for mutation operation ."}}
{"translation": {"code": "def _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : # Check func tolerance value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) value_converged = value_range <= func_tolerance # Ideally, we would compute the position convergence by computing the # pairwise distance between every member of the population and checking if # the maximum of those is less than the supplied tolerance. However, this is # completely infeasible in terms of performance. We adopt a more conservative # approach which checks the distance between the first population member # with the rest of the population. If the largest such distance is less than # half the supplied tolerance, we stop. The reason why this is sufficient is # as follows. For any pair of distinct points (a, b) in the population, we # have the relation:  |a - b| <= |x0 - a| + |x0 - b|, where x0 is any # other point. In particular, let x0 be the first element of the population # and suppose that the largest distance between this point and any other # member is epsilon. Then, for any pair of points (a, b), # |a - b| <= 2 * epsilon and hence, the maximum distance between any pair of # points in the population is bounded above by twice the distance between # the first point and other points. half_tol = position_tolerance / 2 def part_converged ( part ) : return tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half_tol x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) return value_converged | x_converged", "nl": "Checks whether the convergence criteria have been met ."}}
{"translation": {"code": "def _find_best_in_population ( population , values ) : best_value = tf . math . reduce_min ( input_tensor = values ) best_index = tf . where ( tf . math . equal ( values , best_value ) ) [ 0 , 0 ] return ( [ population_part [ best_index ] for population_part in population ] , best_value )", "nl": "Finds the population member with the lowest value ."}}
{"translation": {"code": "def _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) : was_iterable = False if initial_position is not None : initial_position , was_iterable = _ensure_list ( initial_position ) if initial_population is not None : initial_population , was_iterable = _ensure_list ( initial_population ) population = _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed = seed ) differential_weight = tf . convert_to_tensor ( value = differential_weight , dtype = population [ 0 ] . dtype . base_dtype ) crossover_prob = tf . convert_to_tensor ( value = crossover_prob ) population_values = objective_function ( * population ) if max_iterations is not None : max_iterations = tf . convert_to_tensor ( value = max_iterations ) func_tolerance = tf . convert_to_tensor ( value = func_tolerance , dtype = population_values . dtype . base_dtype ) position_tolerance = tf . convert_to_tensor ( value = position_tolerance , dtype = population [ 0 ] . dtype . base_dtype ) return ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob )", "nl": "Processes initial args ."}}
{"translation": {"code": "def _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed ) : if initial_population is not None : return [ tf . convert_to_tensor ( value = part ) for part in initial_population ] # Constructs the population by adding normal noise to the initial position. seed_stream = distributions . SeedStream ( seed , salt = 'get_starting_population' ) population = [ ] for part in initial_position : part = tf . convert_to_tensor ( value = part ) part_event_shape = tf . shape ( input = part ) # We only draw population_size-1 random vectors because we want to ensure # that the supplied position is part of the population. The first member # is set to be the initial_position. population_part_shape = tf . concat ( [ [ population_size - 1 ] , part_event_shape ] , axis = 0 ) population_part = tf . random . normal ( population_part_shape , stddev = population_stddev , dtype = part . dtype . base_dtype , seed = seed_stream ( ) ) population_part += part population_part = tf . concat ( [ [ part ] , population_part ] , axis = 0 ) population . append ( population_part ) return population", "nl": "Constructs the initial population ."}}
{"translation": {"code": "def _get_mutants ( population , population_size , mixing_indices , differential_weight ) : mixing_indices = tf . reshape ( mixing_indices , [ - 1 ] ) weights = tf . stack ( [ 1.0 , differential_weight , - differential_weight ] ) def _mutant_part ( population_part ) : donors = tf . gather ( population_part , mixing_indices ) donors = tf . transpose ( a = tf . reshape ( donors , [ population_size , 3 , - 1 ] ) , perm = [ 0 , 2 , 1 ] ) return tf . math . reduce_sum ( input_tensor = donors * weights , axis = - 1 ) return [ _mutant_part ( population_part ) for population_part in population ]", "nl": "Computes the mutatated vectors for each population member ."}}
{"translation": {"code": "def _event_size ( event_shape , name = None ) : with tf . compat . v1 . name_scope ( name , 'event_size' , [ event_shape ] ) : event_shape = tf . convert_to_tensor ( value = event_shape , dtype = tf . int32 , name = 'event_shape' ) event_shape_const = tf . get_static_value ( event_shape ) if event_shape_const is not None : return np . prod ( event_shape_const ) else : return tf . reduce_prod ( input_tensor = event_shape )", "nl": "Computes the number of elements in a tensor with shape event_shape ."}}
{"translation": {"code": "def params_size ( num_components , component_params_size , name = None ) : with tf . compat . v1 . name_scope ( name , 'MixtureSameFamily_params_size' , [ num_components , component_params_size ] ) : num_components = tf . convert_to_tensor ( value = num_components , name = 'num_components' , dtype_hint = tf . int32 ) component_params_size = tf . convert_to_tensor ( value = component_params_size , name = 'component_params_size' ) num_components = dist_util . prefer_static_value ( num_components ) component_params_size = dist_util . prefer_static_value ( component_params_size ) return num_components + num_components * component_params_size", "nl": "Number of params needed to create a MixtureSameFamily distribution ."}}
{"translation": {"code": "def one_step_predictive ( model , observed_time_series , parameter_samples ) : with tf . compat . v1 . name_scope ( 'one_step_predictive' , values = [ observed_time_series , parameter_samples ] ) : [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) # Run filtering over the training timesteps to extract the # predictive means and variances. num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] lgssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) ( _ , _ , _ , _ , _ , observation_means , observation_covs ) = lgssm . forward_filter ( observed_time_series , mask = is_missing ) # Squeeze dims to convert from LGSSM's event shape `[num_timesteps, 1]` # to a scalar time series. return sts_util . mix_over_posterior_draws ( means = observation_means [ ... , 0 ] , variances = observation_covs [ ... , 0 , 0 ] )", "nl": "Compute one - step - ahead predictive distributions for all timesteps ."}}
{"translation": {"code": "def forecast ( model , observed_time_series , parameter_samples , num_steps_forecast ) : with tf . compat . v1 . name_scope ( 'forecast' , values = [ observed_time_series , parameter_samples , num_steps_forecast ] ) : [ observed_time_series , mask ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) # Run filtering over the observed timesteps to extract the # latent state posterior at timestep T+1 (i.e., the final # filtering distribution, pushed through the transition model). # This is the prior for the forecast model (\"today's prior # is yesterday's posterior\"). num_observed_steps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] observed_data_ssm = model . make_state_space_model ( num_timesteps = num_observed_steps , param_vals = parameter_samples ) ( _ , _ , _ , predictive_means , predictive_covs , _ , _ ) = observed_data_ssm . forward_filter ( observed_time_series , mask = mask ) # Build a batch of state-space models over the forecast period. Because # we'll use MixtureSameFamily to mix over the posterior draws, we need to # do some shenanigans to move the `[num_posterior_draws]` batch dimension # from the leftmost to the rightmost side of the model's batch shape. # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an # arbitrary axis, and eliminate `move_dimension` calls here. parameter_samples = model . _canonicalize_param_vals_as_map ( parameter_samples ) # pylint: disable=protected-access parameter_samples_with_reordered_batch_dimension = { param . name : dist_util . move_dimension ( parameter_samples [ param . name ] , 0 , - ( 1 + _prefer_static_event_ndims ( param . prior ) ) ) for param in model . parameters } forecast_prior = tfd . MultivariateNormalFullCovariance ( loc = dist_util . move_dimension ( predictive_means [ ... , - 1 , : ] , 0 , - 2 ) , covariance_matrix = dist_util . move_dimension ( predictive_covs [ ... , - 1 , : , : ] , 0 , - 3 ) ) # Ugly hack: because we moved `num_posterior_draws` to the trailing (rather # than leading) dimension of parameters, the parameter batch shapes no # longer broadcast against the `constant_offset` attribute used in `sts.Sum` # models. We fix this by manually adding an extra broadcasting dim to # `constant_offset` if present. # The root cause of this hack is that we mucked with param dimensions above # and are now passing params that are 'invalid' in the sense that they don't # match the shapes of the model's param priors. The fix (as above) will be # to update MixtureSameFamily so we can avoid changing param dimensions # altogether. # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an # arbitrary axis, and eliminate this hack. kwargs = { } if hasattr ( model , 'constant_offset' ) : kwargs [ 'constant_offset' ] = tf . convert_to_tensor ( value = model . constant_offset , dtype = forecast_prior . dtype ) [ ... , tf . newaxis ] # We assume that any STS model that has a `constant_offset` attribute # will allow it to be overridden as a kwarg. This is currently just # `sts.Sum`. # TODO(b/120245392): when kwargs hack is removed, switch back to calling # the public version of `_make_state_space_model`. forecast_ssm = model . _make_state_space_model ( # pylint: disable=protected-access num_timesteps = num_steps_forecast , param_map = parameter_samples_with_reordered_batch_dimension , initial_state_prior = forecast_prior , initial_step = num_observed_steps , * * kwargs ) num_posterior_draws = dist_util . prefer_static_value ( forecast_ssm . batch_shape_tensor ( ) ) [ - 1 ] return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = forecast_ssm . dtype ) ) , components_distribution = forecast_ssm )", "nl": "Construct predictive distribution over future observations ."}}
{"translation": {"code": "def quantiles ( x , num_quantiles , axis = None , interpolation = None , keep_dims = False , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'quantiles' , values = [ x , num_quantiles , axis ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) return percentile ( x , q = tf . linspace ( # percentile casts q to float64 before using it...so may as well use # float64 here. Note that  using x.dtype won't work with linspace # if x is integral type (which is anothe motivation for hard-coding # float64). tf . convert_to_tensor ( value = 0 , dtype = tf . float64 ) , tf . convert_to_tensor ( value = 100 , dtype = tf . float64 ) , num = num_quantiles + 1 ) , axis = axis , interpolation = interpolation , keep_dims = keep_dims , validate_args = validate_args , preserve_gradients = False )", "nl": "Compute quantiles of x along axis ."}}
{"translation": {"code": "def _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : assertions = [ ] message = 'Input `lower_upper` must have at least 2 dimensions.' if lower_upper . shape . ndims is not None : if lower_upper . shape . ndims < 2 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) message = '`rank(lower_upper)` must equal `rank(perm) + 1`' if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : if lower_upper . shape . ndims != perm . shape . ndims + 1 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) message = '`lower_upper` must be square.' if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : if lower_upper . shape [ - 2 ] != lower_upper . shape [ - 1 ] : raise ValueError ( message ) elif validate_args : m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) return assertions", "nl": "Returns list of assertions related to lu_reconstruct assumptions ."}}
{"translation": {"code": "def lu_solve ( lower_upper , perm , rhs , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : lower_upper = tf . identity ( lower_upper ) perm = tf . identity ( perm ) rhs = tf . identity ( rhs ) if rhs . shape . ndims == 2 and perm . shape . ndims == 1 : # Both rhs and perm have scalar batch_shape. permuted_rhs = tf . gather ( rhs , perm , axis = - 2 ) else : # Either rhs or perm have non-scalar batch_shape or we can't determine # this information statically. rhs_shape = tf . shape ( input = rhs ) broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2 ] , tf . shape ( input = perm ) [ : - 1 ] ) d , m = rhs_shape [ - 2 ] , rhs_shape [ - 1 ] rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = 0 ) # Tile out rhs. broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) broadcast_rhs = tf . reshape ( broadcast_rhs , [ - 1 , d , m ] ) # Tile out perm and add batch indices. broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - 1 ] ) broadcast_perm = tf . reshape ( broadcast_perm , [ - 1 , d ] ) broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - 1 ) permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( tf . shape ( input = lower_upper ) [ : - 1 ] , dtype = lower_upper . dtype ) ) return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , # Only upper is accessed. linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = False )", "nl": "Solves systems of linear eqns A X = RHS given LU factorizations ."}}
{"translation": {"code": "def expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : with tf . name_scope ( op_name or \"expand_to_vector\" ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) ndims = tensorshape_util . rank ( x . shape ) if ndims is None : # Maybe expand ndims from 0 to 1. if validate_args : x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = \"Input is neither scalar nor vector.\" ) ] , x ) ndims = tf . rank ( x ) expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) return tf . reshape ( x , expanded_shape ) elif ndims == 0 : # Definitely expand ndims from 0 to 1. x_const = tf . get_static_value ( x ) if x_const is not None : return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) else : return tf . reshape ( x , [ 1 ] ) elif ndims != 1 : raise ValueError ( \"Input is neither scalar nor vector.\" ) # ndims == 1 return x", "nl": "Transform a 0 - D or 1 - D Tensor to be 1 - D ."}}
{"translation": {"code": "def lu_matrix_inverse ( lower_upper , perm , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'lu_matrix_inverse' , [ lower_upper , perm ] ) : lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : lower_upper = tf . identity ( lower_upper ) perm = tf . identity ( perm ) shape = tf . shape ( input = lower_upper ) return lu_solve ( lower_upper , perm , rhs = tf . eye ( shape [ - 1 ] , batch_shape = shape [ : - 2 ] , dtype = lower_upper . dtype ) , validate_args = False )", "nl": "Computes a matrix inverse given the matrix s LU decomposition ."}}
{"translation": {"code": "def remove ( self , field ) : return _Mapping ( x = None if field == \"x\" else self . x , y = None if field == \"y\" else self . y , ildj = self . ildj , kwargs = self . kwargs )", "nl": "To support weak referencing removes cache key from the cache value ."}}
{"translation": {"code": "def backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : with tf . name_scope ( \"backward_pass\" ) : filtered_means = tf . convert_to_tensor ( value = filtered_means , name = \"filtered_means\" ) filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = \"filtered_covs\" ) predicted_means = tf . convert_to_tensor ( value = predicted_means , name = \"predicted_means\" ) predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = \"predicted_covs\" ) # To scan over time dimension, we need to move 'num_timesteps' from the # event shape to the initial dimension of the tensor. filtered_means = distribution_util . move_dimension ( filtered_means , - 2 , 0 ) filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3 , 0 ) predicted_means = distribution_util . move_dimension ( predicted_means , - 2 , 0 ) predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3 , 0 ) # The means are assumed to be vectors. Adding a dummy index to # ensure the `matmul` op working smoothly. filtered_means = filtered_means [ ... , tf . newaxis ] predicted_means = predicted_means [ ... , tf . newaxis ] initial_backward_mean = predicted_means [ - 1 , ... ] initial_backward_cov = predicted_covs [ - 1 , ... ] num_timesteps = tf . shape ( input = filtered_means ) [ 0 ] initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - 1 ) update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) # For backward pass, it scans the `elems` from last to first. posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = True ) # Move the time dimension back into the event shape. posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , 0 ] , 0 , - 2 ) posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , 0 , - 3 ) return ( posterior_means , posterior_covs )", "nl": "Run the backward pass in Kalman smoother ."}}
{"translation": {"code": "def backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) : # Compute backward Kalman gain: # J = F * T' * P^{-1} # Since both F(iltered) and P(redictive) are cov matrices, # thus self-adjoint, we can take the transpose. # computation: #      = (P^{-1} * T * F)' #      = (P^{-1} * tmp_gain_cov) ' #      = (P \\ tmp_gain_cov)' tmp_gain_cov = transition_matrix . matmul ( filtered_cov ) predicted_cov_chol = tf . linalg . cholesky ( predicted_cov ) gain_transpose = tf . linalg . cholesky_solve ( predicted_cov_chol , tmp_gain_cov ) posterior_mean = ( filtered_mean + tf . linalg . matmul ( gain_transpose , next_posterior_mean - predicted_mean , adjoint_a = True ) ) posterior_cov = ( filtered_cov + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( next_posterior_cov - predicted_cov , gain_transpose ) , adjoint_a = True ) ) return ( posterior_mean , posterior_cov )", "nl": "Backward update for a Kalman smoother ."}}
{"translation": {"code": "def build_backward_pass_step ( get_transition_matrix_for_timestep ) : def backward_pass_step ( state , filtered_parameters ) : \"\"\"Run a single step of backward smoothing.\"\"\" ( filtered_mean , filtered_cov , predicted_mean , predicted_cov ) = filtered_parameters transition_matrix = get_transition_matrix_for_timestep ( state . timestep ) next_posterior_mean = state . backward_mean next_posterior_cov = state . backward_cov posterior_mean , posterior_cov = backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) return BackwardPassState ( backward_mean = posterior_mean , backward_cov = posterior_cov , timestep = state . timestep - 1 ) return backward_pass_step", "nl": "Build a callable that perform one step for backward smoothing ."}}
{"translation": {"code": "def posterior_marginals ( self , x , mask = None ) : with tf . name_scope ( \"smooth\" ) : x = tf . convert_to_tensor ( value = x , name = \"x\" ) ( _ , filtered_means , filtered_covs , predicted_means , predicted_covs , _ , _ ) = self . forward_filter ( x , mask = mask ) ( smoothed_means , smoothed_covs ) = self . backward_smoothing_pass ( filtered_means , filtered_covs , predicted_means , predicted_covs ) return ( smoothed_means , smoothed_covs )", "nl": "Run a Kalman smoother to return posterior mean and cov ."}}
{"translation": {"code": "def _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) message = 'Input `rhs` must have at least 2 dimensions.' if rhs . shape . ndims is not None : if rhs . shape . ndims < 2 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : if lower_upper . shape [ - 1 ] != rhs . shape [ - 2 ] : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) return assertions", "nl": "Returns list of assertions related to lu_solve assumptions ."}}
{"translation": {"code": "def make_value_setter ( * * model_kwargs ) : def set_values ( f , * args , * * kwargs ) : \"\"\"Sets random variable values to its aligned value.\"\"\" name = kwargs . get ( \"name\" ) if name in model_kwargs : kwargs [ \"value\" ] = model_kwargs [ name ] return interceptable ( f ) ( * args , * * kwargs ) return set_values", "nl": "Creates a value - setting interceptor ."}}
{"translation": {"code": "def bisect ( value_and_gradients_function , initial_left , initial_right , f_lim ) : failed = ~ is_finite ( initial_left , initial_right ) needs_bisect = ( initial_right . df < 0 ) & ( initial_right . f > f_lim ) bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = failed | ~ needs_bisect , failed = failed , num_evals = tf . convert_to_tensor ( value = 0 ) , left = initial_left , right = initial_right ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )", "nl": "Bisects an interval and updates to satisfy opposite slope conditions ."}}
{"translation": {"code": "def _bisect ( value_and_gradients_function , initial_args , f_lim ) : def _loop_cond ( curr ) : # TODO(b/112524024): Also take into account max_iterations. return ~ tf . reduce_all ( input_tensor = curr . stopped ) def _loop_body ( curr ) : \"\"\"Narrow down interval to satisfy opposite slope conditions.\"\"\" mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) # Fail if function values at mid point are no longer finite; or left/right # points are so close to it that we can't distinguish them any more. failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) # If mid point has a negative slope and the function value at that point is # small enough, we can use it as a new left end point to narrow down the # interval. If mid point has a positive slope, then we have found a suitable # right end point to bracket a minima within opposite slopes. Otherwise, the # mid point has a negative slope but the function value at that point is too # high to work as left end point, we are in the same situation in which we # started the loop so we just update the right end point and continue. to_update = ~ ( curr . stopped | failed ) update_left = ( mid . df < 0 ) & ( mid . f <= f_lim ) left = val_where ( to_update & update_left , mid , curr . left ) right = val_where ( to_update & ~ update_left , mid , curr . right ) # We're done when the right end point has a positive slope. stopped = curr . stopped | failed | ( right . df >= 0 ) return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] # The interval needs updating if the right end point has a negative slope and # the value of the function at that point is too high. It is not a valid left # end point but along with the current left end point, it encloses another # minima. The loop above tries to narrow the interval so that it satisfies the # opposite slope conditions. return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ]", "nl": "Actual implementation of bisect given initial_args in a _BracketResult ."}}
{"translation": {"code": "def is_finite ( val_1 , val_2 = None ) : val_1_finite = tf . math . is_finite ( val_1 . f ) & tf . math . is_finite ( val_1 . df ) if val_2 is not None : return val_1_finite & tf . math . is_finite ( val_2 . f ) & tf . math . is_finite ( val_2 . df ) return val_1_finite", "nl": "Checks if the supplied values are finite ."}}
{"translation": {"code": "def find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : # TFP users may be surprised to see the \"action\" in the leftmost dim of # edges, rather than the rightmost (event) dim.  Why? # 1. Most likely you created edges by getting quantiles over samples, and #    quantile/percentile return these edges in the leftmost (sample) dim. # 2. Say you have event_shape = [5], then we expect the bin will be different #    for all 5 events, so the index of the bin should not be in the event dim. with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and tf . compat . dimension_value ( edges . shape [ 0 ] ) < 2 ) : raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) flattening_x = edges . shape . ndims == 1 and x . shape . ndims > 1 if flattening_x : x_orig_shape = tf . shape ( input = x ) x = tf . reshape ( x , [ - 1 ] ) if dtype is None : dtype = in_type dtype = tf . as_dtype ( dtype ) # Move first dims into the rightmost. x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) # If... #   x_permed = [0, 1, 6., 10] #   edges = [0, 5, 10.] #   ==> almost_output = [0, 1, 2, 2] searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) # Move the rightmost dims back to the leftmost. almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) # In above example, we want [0, 0, 1, 1], so correct this here. bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( input = edges ) [ 0 ] - 2 , dtype ) ) if not extend_lower_interval : low_fill = np . nan if dtype . is_floating else - 1 bins = tf . where ( x < tf . expand_dims ( edges [ 0 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) if not extend_upper_interval : up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ 0 ] - 1 bins = tf . where ( x > tf . expand_dims ( edges [ - 1 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) if flattening_x : bins = tf . reshape ( bins , x_orig_shape ) return bins", "nl": "Bin values into discrete intervals ."}}
{"translation": {"code": "def soft_threshold ( x , threshold , name = None ) : # https://math.stackexchange.com/questions/471339/derivation-of-soft-thresholding-operator with tf . compat . v1 . name_scope ( name , 'soft_threshold' , [ x , threshold ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) threshold = tf . convert_to_tensor ( value = threshold , dtype = x . dtype , name = 'threshold' ) return tf . sign ( x ) * tf . maximum ( tf . abs ( x ) - threshold , 0. )", "nl": "Soft Thresholding operator ."}}
{"translation": {"code": "def minimize ( grad_and_hessian_loss_fn , x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = 1 , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : graph_deps = [ x_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] , with tf . compat . v1 . name_scope ( name , 'minimize' , graph_deps ) : def _loop_cond ( x_start , converged , iter_ ) : del x_start return tf . logical_and ( iter_ < maximum_iterations , tf . logical_not ( converged ) ) def _loop_body ( x_start , converged , iter_ ) : # pylint: disable=missing-docstring g , h_outer , h_middle = grad_and_hessian_loss_fn ( x_start ) x_start , converged , _ = minimize_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = h_outer , hessian_unregularized_loss_middle = h_middle , x_start = x_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps_per_iteration , tolerance = tolerance , learning_rate = learning_rate ) return x_start , converged , iter_ + 1 return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ x_start , tf . zeros ( [ ] , np . bool , name = 'converged' ) , tf . zeros ( [ ] , np . int32 , name = 'iter' ) , ] )", "nl": "Minimize using Hessian - informed proximal gradient descent ."}}
{"translation": {"code": "def _sparse_block_diag ( sp_a ) : # Construct the matrix [[M, N], [1, 0], [0, 1]] which would map the index # (b, i, j) to (Mb + i, Nb + j). This effectively creates a block-diagonal # matrix of dense shape [B * M, B * N]. # Note that this transformation doesn't increase the number of non-zero # entries in the SparseTensor. sp_a_shape = tf . convert_to_tensor ( value = _get_shape ( sp_a , tf . int64 ) ) ind_mat = tf . concat ( [ [ sp_a_shape [ - 2 : ] ] , tf . eye ( 2 , dtype = tf . int64 ) ] , axis = 0 ) indices = tf . matmul ( sp_a . indices , ind_mat ) dense_shape = sp_a_shape [ 0 ] * sp_a_shape [ 1 : ] return tf . SparseTensor ( indices = indices , values = sp_a . values , dense_shape = dense_shape )", "nl": "Returns a block diagonal rank 2 SparseTensor from a batch of SparseTensors ."}}
{"translation": {"code": "def semilocal_linear_trend_transition_noise ( level_scale , slope_mean , slope_scale , autoregressive_coef ) : # At each timestep, the stochasticity of `level` and `slope` are given # by `level_scale` and `slope_scale` respectively. broadcast_batch_shape = dist_util . get_broadcast_shape ( level_scale , slope_mean , slope_scale , autoregressive_coef ) broadcast_ones = tf . ones ( broadcast_batch_shape , dtype = level_scale . dtype ) scale_diag = tf . stack ( [ level_scale * broadcast_ones , slope_scale * broadcast_ones ] , axis = - 1 ) # We additionally fold in a bias term implementing the nonzero `slope_mean`. # The overall `slope` update is (from `SemiLocalLinearTrend` docstring) #   slope[t] = (slope_mean + #               autoregressive_coef * (slope[t-1] - slope_mean) + #               Normal(0., slope_scale)) # which we rewrite as #   slope[t] = ( #    autoregressive_coef * slope[t-1] +                  # linear transition #    Normal(loc=slope_mean - autoregressive_coef * slope_mean,  # noise bias #           scale=slope_scale))                                 # noise scale bias = tf . stack ( [ tf . zeros_like ( broadcast_ones ) , slope_mean * ( 1 - autoregressive_coef ) * broadcast_ones ] , axis = - 1 ) return tfd . MultivariateNormalDiag ( loc = bias , scale_diag = scale_diag )", "nl": "Build the transition noise model for a semi - local linear trend model ."}}
{"translation": {"code": "def semilocal_linear_trend_transition_matrix ( autoregressive_coef ) : # We want to write the following 2 x 2 matrix: #  [[1., 1., ],    # level(t+1) = level(t) + slope(t) #   [0., ar_coef], # slope(t+1) = ar_coef * slope(t) # but it's slightly tricky to properly incorporate the batch shape of # autoregressive_coef. E.g., if autoregressive_coef has shape [4,6], we want # to return shape [4, 6, 2, 2]. We do this by breaking the matrix into its # fixed entries, written explicitly, and then the autoregressive_coef part # which we add in after using a mask to broadcast to the correct matrix shape. fixed_entries = tf . constant ( [ [ 1. , 1. ] , [ 0. , 0. ] ] , dtype = autoregressive_coef . dtype ) autoregressive_coef_mask = tf . constant ( [ [ 0. , 0. ] , [ 0. , 1. ] ] , dtype = autoregressive_coef . dtype ) bottom_right_entry = ( autoregressive_coef [ ... , tf . newaxis , tf . newaxis ] * autoregressive_coef_mask ) return tf . linalg . LinearOperatorFullMatrix ( fixed_entries + bottom_right_entry )", "nl": "Build the transition matrix for a semi - local linear trend model ."}}
{"translation": {"code": "def bracket ( value_and_gradients_function , search_interval , f_lim , max_iterations , expansion_param = 5.0 ) : already_stopped = search_interval . failed | search_interval . converged # If the slope at right end point is positive, step B1 in [2], then the given # initial points already bracket a minimum. bracketed = search_interval . right . df >= 0 # Bisection is needed, step B2, if right end point almost works as a new left # end point but the objective value is too high. needs_bisect = ( search_interval . right . df < 0 ) & ( search_interval . right . f > f_lim ) # In these three cases bracketing is already `stopped` and there is no need # to perform further evaluations. Otherwise the bracketing loop is needed to # expand the interval, step B3, until the conditions are met. initial_args = _IntermediateResult ( iteration = search_interval . iterations , stopped = already_stopped | bracketed | needs_bisect , failed = search_interval . failed , num_evals = search_interval . func_evals , left = search_interval . left , right = search_interval . right ) def _loop_cond ( curr ) : return ( curr . iteration < max_iterations ) & ~ tf . reduce_all ( input_tensor = curr . stopped ) def _loop_body ( curr ) : \"\"\"Main body of bracketing loop.\"\"\" # The loop maintains the invariant that curr.stopped is true if we have # either: failed, successfully bracketed, or not yet bracketed but needs # bisect. On the only remaining case, step B3 in [2]. case we need to # expand and update the left/right values appropriately. new_right = value_and_gradients_function ( expansion_param * curr . right . x ) left = val_where ( curr . stopped , curr . left , curr . right ) right = val_where ( curr . stopped , curr . right , new_right ) # Updated the failed, bracketed, and needs_bisect conditions. failed = curr . failed | ~ is_finite ( right ) bracketed = right . df >= 0 needs_bisect = ( right . df < 0 ) & ( right . f > f_lim ) return [ _IntermediateResult ( iteration = curr . iteration + 1 , stopped = curr . stopped | failed | bracketed | needs_bisect , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] bracket_result = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] # For entries where bisect is still needed, mark them as not yet stopped, # reset the left end point, and run `_bisect` on them. needs_bisect = ( ( bracket_result . right . df < 0 ) & ( bracket_result . right . f > f_lim ) ) stopped = already_stopped | bracket_result . failed | ~ needs_bisect left = val_where ( stopped , bracket_result . left , search_interval . left ) bisect_args = bracket_result . _replace ( stopped = stopped , left = left ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )", "nl": "Brackets the minimum given an initial starting point ."}}
{"translation": {"code": "def clip_by_value_preserve_gradient ( t , clip_value_min , clip_value_max , name = None ) : with tf . compat . v1 . name_scope ( name , 'clip_by_value_preserve_gradient' , [ t , clip_value_min , clip_value_max ] ) : t = tf . convert_to_tensor ( value = t , name = 't' ) clip_t = tf . clip_by_value ( t , clip_value_min , clip_value_max ) return t + tf . stop_gradient ( clip_t - t )", "nl": "Clips values to a specified min and max while leaving gradient unaltered ."}}
{"translation": {"code": "def _value_and_batch_jacobian ( f , x ) : if tf . executing_eagerly ( ) : with tf . GradientTape ( ) as tape : tape . watch ( x ) value = f ( x ) batch_jacobian = tape . batch_jacobian ( value , x ) else : value = f ( x ) batch_jacobian = gradients . batch_jacobian ( value , x ) return value , batch_jacobian", "nl": "Enables uniform interface to value and batch jacobian calculation ."}}
{"translation": {"code": "def _prevent_2nd_derivative ( x ) : def grad ( dy ) : return array_ops . prevent_gradient ( dy , message = \"Second derivative is not implemented.\" ) return tf . identity ( x ) , grad", "nl": "Disables computation of the second derivatives for a tensor ."}}
{"translation": {"code": "def _distributional_transform ( self , x ) : if tensorshape_util . rank ( x . shape ) is None : # tf.nn.softmax raises an error when applied to inputs of undefined rank. raise ValueError ( \"Distributional transform does not support inputs of \" \"undefined rank.\" ) # Obtain factorized components distribution and assert that it's # a scalar distribution. if isinstance ( self . _components_distribution , independent . Independent ) : univariate_components = self . _components_distribution . distribution else : univariate_components = self . _components_distribution with tf . control_dependencies ( [ assert_util . assert_equal ( univariate_components . is_scalar_event ( ) , True , message = \"`univariate_components` must have scalar event\" ) ] ) : x_padded = self . _pad_sample_dims ( x ) # [S, B, 1, E] log_prob_x = univariate_components . log_prob ( x_padded ) # [S, B, k, E] cdf_x = univariate_components . cdf ( x_padded ) # [S, B, k, E] # log prob_k (x_1, ..., x_i-1) cumsum_log_prob_x = tf . reshape ( tf . math . cumsum ( # [S*prod(B)*k, prod(E)] tf . reshape ( log_prob_x , [ - 1 , self . _event_size ] ) , exclusive = True , axis = - 1 ) , tf . shape ( input = log_prob_x ) ) # [S, B, k, E] logits_mix_prob = distribution_utils . pad_mixture_dimensions ( self . mixture_distribution . logits , self , self . mixture_distribution , self . _event_ndims ) # [B, k, 1] # Logits of the posterior weights: log w_k + log prob_k (x_1, ..., x_i-1) log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x component_axis = tensorshape_util . rank ( x . shape ) - self . _event_ndims posterior_weights_x = tf . nn . softmax ( log_posterior_weights_x , axis = component_axis ) return tf . reduce_sum ( input_tensor = posterior_weights_x * cdf_x , axis = component_axis )", "nl": "Performs distributional transform of the mixture samples ."}}
{"translation": {"code": "def draw_sample ( num_samples , num_classes , logits , num_trials , dtype , seed ) : with tf . name_scope ( \"multinomial.draw_sample\" ) : # broadcast the num_trials and logits to same shape num_trials = tf . ones_like ( logits [ ... , 0 ] , dtype = num_trials . dtype ) * num_trials logits = tf . ones_like ( num_trials [ ... , tf . newaxis ] , dtype = logits . dtype ) * logits # flatten the total_count and logits # flat_logits has shape [B1B2...Bm, num_classes] flat_logits = tf . reshape ( logits , [ - 1 , num_classes ] ) flat_num_trials = num_samples * tf . reshape ( num_trials , [ - 1 ] ) # [B1B2...Bm] # Computes each logits and num_trials situation by map_fn. # Using just one batch tf.random.categorical call doesn't work because that # requires num_trials to be the same across all members of the batch of # logits.  This restriction makes sense for tf.random.categorical because # for it, num_trials is part of the returned shape.  However, the # multinomial sampler does not need that restriction, because it sums out # exactly that dimension. # One possibility would be to draw a batch categorical whose sample count is # max(num_trials) and mask out the excess ones.  However, if the elements of # num_trials vary widely, this can be wasteful of memory. # TODO(b/123763054, b/112152209): Revisit the possibility of writing this # with a batch categorical followed by batch unsorted_segment_sum, once both # of those work and are memory-efficient enough. def _sample_one_batch_member ( args ) : logits , num_cat_samples = args [ 0 ] , args [ 1 ] # [K], [] # x has shape [1, num_cat_samples = num_samples * num_trials] x = tf . random . categorical ( logits [ tf . newaxis , ... ] , num_cat_samples , seed = seed ) x = tf . reshape ( x , shape = [ num_samples , - 1 ] ) # [num_samples, num_trials] x = tf . one_hot ( x , depth = num_classes ) # [num_samples, num_trials, num_classes] x = tf . reduce_sum ( input_tensor = x , axis = - 2 ) # [num_samples, num_classes] return tf . cast ( x , dtype = dtype ) x = tf . map_fn ( _sample_one_batch_member , [ flat_logits , flat_num_trials ] , dtype = dtype ) # [B1B2...Bm, num_samples, num_classes] # reshape the results to proper shape x = tf . transpose ( a = x , perm = [ 1 , 0 , 2 ] ) final_shape = tf . concat ( [ [ num_samples ] , tf . shape ( input = num_trials ) , [ num_classes ] ] , axis = 0 ) x = tf . reshape ( x , final_shape ) return x", "nl": "Sample a multinomial ."}}
{"translation": {"code": "def _validate_block_sizes ( block_sizes , bijectors , validate_args ) : block_sizes_shape = block_sizes . shape if tensorshape_util . is_fully_defined ( block_sizes_shape ) : if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) return block_sizes elif validate_args : message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : return tf . identity ( block_sizes ) else : return block_sizes", "nl": "Helper to validate block sizes ."}}
{"translation": {"code": "def val_where ( cond , tval , fval ) : if isinstance ( tval , tf . Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val_where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( TypeError )", "nl": "Like tf . where but works on namedtuples ."}}
{"translation": {"code": "def update ( value_and_gradients_function , val_left , val_right , val_trial , f_lim , active = None ) : # We should only update if the trial point is within the interval. within_range = ( val_left . x < val_trial . x ) & ( val_trial . x < val_right . x ) if active is not None : within_range = within_range & active # The new point is a valid left end point if it has negative slope # and the value at the point is not too large. valid_left = ( val_trial . df < 0 ) & ( val_trial . f <= f_lim ) # If the trial point has a negative slope but the value at that point # is too high, bisect can narrow down an interval between the current left # and the trial point. needs_bisect = within_range & ( val_trial . df < 0 ) & ( val_trial . f > f_lim ) # Note that if `~valid_left` it is because either: # - the slope at the trial point is positive, so it is a valid right #   point, or # - the needs_bisect condition is true. # In both cases we want to keep the current left and replace right # with the trial point. left = val_where ( within_range & valid_left , val_trial , val_left ) right = val_where ( within_range & ~ valid_left , val_trial , val_right ) bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = ~ needs_bisect , failed = tf . zeros_like ( within_range ) , # i.e. all false. num_evals = tf . convert_to_tensor ( value = 0 ) , left = left , right = right ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )", "nl": "Squeezes a bracketing interval containing the minimum ."}}
{"translation": {"code": "def expectation_importance_sampler_logspace ( log_f , log_p , sampling_dist_q , z = None , n = None , seed = None , name = 'expectation_importance_sampler_logspace' ) : q = sampling_dist_q with tf . name_scope ( name ) : z = _get_samples ( q , z , n , seed ) log_values = log_f ( z ) + log_p ( z ) - q . log_prob ( z ) return _logspace_mean ( log_values )", "nl": "r Importance sampling with a positive function in log - space ."}}
{"translation": {"code": "def _maybe_check_valid_map_values ( map_values , validate_args ) : assertions = [ ] message = 'Rank of map_values must be 1.' if tensorshape_util . rank ( map_values . shape ) is not None : if tensorshape_util . rank ( map_values . shape ) != 1 : raise ValueError ( message ) elif validate_args : assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) message = 'Size of map_values must be greater than 0.' if tensorshape_util . num_elements ( map_values . shape ) is not None : if tensorshape_util . num_elements ( map_values . shape ) == 0 : raise ValueError ( message ) elif validate_args : assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , 0 , message = message ) ) if validate_args : assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) return assertions", "nl": "Validate map_values if validate_args == True ."}}
{"translation": {"code": "def trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10 , name = None ) : with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : if vs . caching_device is None and not tf . executing_eagerly ( ) : vs . set_caching_device ( lambda op : op . device ) initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) static_length = elems . shape [ 0 ] if tf . compat . dimension_value ( static_length ) is None : length = tf . shape ( input = elems ) [ 0 ] else : length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) # This is an TensorArray in part because of XLA, which had trouble with # non-statically known indices. I.e. elems[i] errored, but # elems_array.read(i) worked. elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ 1 : ] ) elems_array = elems_array . unstack ( elems ) trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) def _body ( i , state , trace_arrays ) : state = loop_fn ( state , elems_array . read ( i ) ) trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) return i + 1 , state , trace_arrays _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : i < length , body = _body , loop_vars = ( 0 , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) # Restore the static length if we know it. def _merge_static_length ( x ) : x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ 1 : ] ) ) return x stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) return final_state , stacked_trace", "nl": "A simplified version of tf . scan that has configurable tracing ."}}
{"translation": {"code": "def with_dependencies ( dependencies , output_tensor , name = None ) : if tf . executing_eagerly ( ) : return output_tensor with tf . name_scope ( name or \"control_dependency\" ) as name : with tf . control_dependencies ( d for d in dependencies if d is not None ) : output_tensor = tf . convert_to_tensor ( value = output_tensor ) if isinstance ( output_tensor , tf . Tensor ) : return tf . identity ( output_tensor , name = name ) else : return tf . IndexedSlices ( tf . identity ( output_tensor . values , name = name ) , output_tensor . indices , output_tensor . dense_shape )", "nl": "Produces the content of output_tensor only after dependencies ."}}
{"translation": {"code": "def case ( pred_fn_pairs , default = None , exclusive = False , name = 'smart_case' ) : return control_flow_ops . _case_helper ( # pylint: disable=protected-access cond , pred_fn_pairs , default , exclusive , name , allow_python_preds = True )", "nl": "Like tf . case except attempts to statically evaluate predicates ."}}
{"translation": {"code": "def _get_static_predicate ( pred ) : if pred in { 0 , 1 } : # Accept 1/0 as valid boolean values pred_value = bool ( pred ) elif isinstance ( pred , bool ) : pred_value = pred elif isinstance ( pred , tf . Tensor ) : pred_value = tf . get_static_value ( pred ) # TODO(jamieas): remove the dependency on `pywrap_tensorflow`. # pylint: disable=protected-access if pred_value is None : pred_value = c_api . TF_TryEvaluateConstant_wrapper ( pred . graph . _c_graph , pred . _as_tf_output ( ) ) # pylint: enable=protected-access else : raise TypeError ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) return pred_value", "nl": "Helper function for statically evaluating predicates in cond ."}}
{"translation": {"code": "def dense_to_sparse ( x , ignore_value = None , name = None ) : # Copied (with modifications) from: # tensorflow/contrib/layers/python/ops/sparse_ops.py. with tf . compat . v1 . name_scope ( name , 'dense_to_sparse' , [ x , ignore_value ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if ignore_value is None : if x . dtype . base_dtype == tf . string : # Exception due to TF strings are converted to numpy objects by default. ignore_value = '' else : ignore_value = x . dtype . as_numpy_dtype ( 0 ) ignore_value = tf . cast ( ignore_value , x . dtype , name = 'ignore_value' ) indices = tf . where ( tf . not_equal ( x , ignore_value ) , name = 'indices' ) return tf . SparseTensor ( indices = indices , values = tf . gather_nd ( x , indices , name = 'values' ) , dense_shape = tf . shape ( input = x , out_type = tf . int64 , name = 'dense_shape' ) )", "nl": "Converts dense Tensor to SparseTensor dropping ignore_value cells ."}}
{"translation": {"code": "def _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) # TODO(b/124240153): Remove map(tf.identity, deps) once tf.function # correctly supports control_dependencies. validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : with tf . control_dependencies ( validation_dependencies ) : output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) return output_shape , output_tensorshape with tf . control_dependencies ( validation_dependencies ) : event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) additional_assertions = [ ] if is_validated : pass elif validate_args : # Check that `input_event_shape` and `event_shape_in` are compatible in the # sense that they have equal entries in any position that isn't a `-1` in # `event_shape_in`. Note that our validations at construction time ensure # there is at most one such entry in `event_shape_in`. mask = event_shape_in >= 0 explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) # We don't explicitly additionally verify # `tf.size(input_shape) > tf.size(event_shape_in)` since `tf.split` # already makes this assertion. with tf . control_dependencies ( additional_assertions ) : output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) return output_shape , output_tensorshape", "nl": "Replaces the rightmost dims in a Tensor representing a shape ."}}
{"translation": {"code": "def _maybe_check_valid_shape ( shape , validate_args ) : if not dtype_util . is_integer ( shape . dtype ) : raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) assertions = [ ] message = '`{}` rank should be <= 1.' if tensorshape_util . rank ( shape . shape ) is not None : if tensorshape_util . rank ( shape . shape ) > 1 : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) shape_ = tf . get_static_value ( shape ) message = '`{}` elements must have at most one `-1`.' if shape_ is not None : if sum ( shape_ == - 1 ) > 1 : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) message = '`{}` elements must be either positive integers or `-1`.' if shape_ is not None : if np . any ( shape_ < - 1 ) : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) return assertions", "nl": "Check that a shape Tensor is int - type and otherwise sane ."}}
{"translation": {"code": "def _remove_dict_keys_with_value ( dict_ , val ) : return { k : v for k , v in dict_ . items ( ) if v is not val }", "nl": "Removes dict keys which have have self as value ."}}
{"translation": {"code": "def _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : exact_wolfe_suff_dec = ( sufficient_decrease_param * val_0 . df >= ( val_c . f - val_0 . f ) / val_c . x ) wolfe_curvature = val_c . df >= curvature_param * val_0 . df exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature approx_wolfe_applies = val_c . f <= f_lim approx_wolfe_suff_dec = ( ( 2 * sufficient_decrease_param - 1 ) * val_0 . df >= val_c . df ) approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature is_satisfied = exact_wolfe | approx_wolfe return is_satisfied", "nl": "Checks whether the Wolfe or approx Wolfe conditions are satisfied ."}}
{"translation": {"code": "def secant2 ( value_and_gradients_function , val_0 , search_interval , f_lim , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , name = None ) : with tf . compat . v1 . name_scope ( name , 'secant2' , [ val_0 , search_interval , f_lim , sufficient_decrease_param , curvature_param ] ) : # This will always be s.t. left <= c <= right val_c = value_and_gradients_function ( _secant ( search_interval . left , search_interval . right ) ) failed = search_interval . failed | ~ is_finite ( val_c ) converged = search_interval . converged | ( ~ failed & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) ) new_converged = converged & ~ search_interval . converged val_left = val_where ( new_converged , val_c , search_interval . left ) val_right = val_where ( new_converged , val_c , search_interval . right ) initial_args = _Secant2Result ( active = ~ failed & ~ converged , converged = converged , failed = failed , num_evals = search_interval . func_evals + 1 , left = val_left , right = val_right ) def _apply_secant2_inner ( ) : return _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) return prefer_static . cond ( tf . reduce_any ( input_tensor = initial_args . active ) , _apply_secant2_inner , lambda : initial_args )", "nl": "Performs the secant square procedure of Hager Zhang ."}}
{"translation": {"code": "def _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : # Apply the `update` function on active branch members to squeeze their # bracketing interval. update_result = update ( value_and_gradients_function , initial_args . left , initial_args . right , val_c , f_lim , active = initial_args . active ) # Update active and failed flags, update left/right on non-failed entries. active = initial_args . active & ~ update_result . failed failed = initial_args . failed | update_result . failed val_left = val_where ( active , update_result . left , initial_args . left ) val_right = val_where ( active , update_result . right , initial_args . right ) # Check if new `c` points should be generated. updated_left = active & tf . equal ( val_left . x , val_c . x ) updated_right = active & tf . equal ( val_right . x , val_c . x ) is_new = updated_left | updated_right next_c = tf . where ( updated_left , _secant ( initial_args . left , val_left ) , val_c . x ) next_c = tf . where ( updated_right , _secant ( initial_args . right , val_right ) , next_c ) in_range = ( val_left . x <= next_c ) & ( next_c <= val_right . x ) # Figure out if an extra function evaluation is needed for new `c` points. needs_extra_eval = tf . reduce_any ( input_tensor = in_range & is_new ) num_evals = initial_args . num_evals + update_result . num_evals num_evals = num_evals + tf . cast ( needs_extra_eval , num_evals . dtype ) next_args = _Secant2Result ( active = active & in_range , # No longer active if `c` is out of range. converged = initial_args . converged , failed = failed , num_evals = num_evals , left = val_left , right = val_right ) def _apply_inner_update ( ) : next_val_c = prefer_static . cond ( needs_extra_eval , ( lambda : value_and_gradients_function ( next_c ) ) , ( lambda : val_c ) ) return _secant2_inner_update ( value_and_gradients_function , next_args , val_0 , next_val_c , f_lim , sufficient_decrease_param , curvature_param ) return prefer_static . cond ( tf . reduce_any ( input_tensor = next_args . active ) , _apply_inner_update , lambda : next_args )", "nl": "Helper function for secant square ."}}
{"translation": {"code": "def _secant2_inner_update ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : # Fail if `val_c` is no longer finite. new_failed = initial_args . active & ~ is_finite ( val_c ) active = initial_args . active & ~ new_failed failed = initial_args . failed | new_failed # We converge when we find a point satisfying the Wolfe conditions, in those # cases we set `val_left = val_right = val_c`. found_wolfe = active & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) val_left = val_where ( found_wolfe , val_c , initial_args . left ) val_right = val_where ( found_wolfe , val_c , initial_args . right ) converged = initial_args . converged | found_wolfe active = active & ~ found_wolfe # If any active batch members remain, we apply the `update` function to # squeeze further their corresponding left/right bracketing interval. def _apply_update ( ) : update_result = update ( value_and_gradients_function , val_left , val_right , val_c , f_lim , active = active ) return _Secant2Result ( active = tf . zeros_like ( active ) , # End of secant2, no actives anymore. converged = converged , failed = failed | update_result . failed , num_evals = initial_args . num_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) # Otherwise just return the current results. def _default ( ) : return _Secant2Result ( active = active , converged = converged , failed = failed , num_evals = initial_args . num_evals , left = val_left , right = val_right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = active ) , _apply_update , _default )", "nl": "Helper function for secant - square step ."}}
{"translation": {"code": "def _secant ( val_a , val_b ) : return ( val_a . x * val_b . df - val_b . x * val_a . df ) / ( val_b . df - val_a . df )", "nl": "Returns the secant interpolation for the minimum ."}}
{"translation": {"code": "def _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : assertions = [ ] if not _is_iterable ( distributions ) or not distributions : raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype_override is None : dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) # Validate event_ndims. for d in distributions : if tensorshape_util . rank ( d . event_shape ) is not None : if tensorshape_util . rank ( d . event_shape ) != 1 : raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( 1 , tf . size ( input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch_shapes = [ d . batch_shape for d in distributions ] if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : if batch_shapes [ 1 : ] != batch_shapes [ : - 1 ] : raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) elif validate_args : batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) # pylint: disable=g-complex-comprehension if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] assertions . extend ( assert_util . assert_equal ( # pylint: disable=g-complex-comprehension b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) return assertions", "nl": "Checks that distributions satisfies all assumptions ."}}
{"translation": {"code": "def matrix_rank ( a , tol = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) assertions = _maybe_validate_matrix ( a , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : a = tf . identity ( a ) s = tf . linalg . svd ( a , compute_uv = False ) if tol is None : if a . shape [ - 2 : ] . is_fully_defined ( ) : m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) else : m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2 : ] ) eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = True ) ) return tf . reduce_sum ( input_tensor = tf . cast ( s > tol , tf . int32 ) , axis = - 1 )", "nl": "Compute the matrix rank ; the number of non - zero SVD singular values ."}}
{"translation": {"code": "def _maybe_validate_matrix ( a , validate_args ) : assertions = [ ] if not a . dtype . is_floating : raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) if a . shape . ndims is not None : if a . shape . ndims < 2 : raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) return assertions", "nl": "Checks that input is a float matrix ."}}