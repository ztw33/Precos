{"translation": {"code": "def from_config ( cls , config ) : return cls ( * * { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate_args' : config . get ( 'validate_args' , False ) , } )", "nl": "Instantiates an initializer from a configuration dictionary ."}}
{"translation": {"code": "def _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : # Extend param shape with ones on the left to match dist_batch_shape. param_shape = tf . shape ( input = param ) insert_ones = tf . ones ( [ tf . size ( input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = 0 ) full_batch_param = tf . reshape ( param , new_param_shape ) param_slices = [ ] # We separately track the batch axis from the parameter axis because we want # them to align for positive indexing, and be offset by param_event_ndims for # negative indexing. param_dim_idx = 0 batch_dim_idx = 0 for slc in slices : if slc is tf . newaxis : param_slices . append ( slc ) continue if slc is Ellipsis : if batch_dim_idx < 0 : raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) param_slices . append ( slc ) # Switch over to negative indexing for the broadcast check. num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + 1 : ] ] ) batch_dim_idx = - num_remaining_non_newaxis_slices param_dim_idx = batch_dim_idx - param_event_ndims continue # Find the batch dimension sizes for both parameter and distribution. param_dim_size = new_param_shape [ param_dim_idx ] batch_dim_size = dist_batch_shape [ batch_dim_idx ] is_broadcast = batch_dim_size > param_dim_size # Slices are denoted by start:stop:step. if isinstance ( slc , slice ) : start , stop , step = slc . start , slc . stop , slc . step if start is not None : start = tf . where ( is_broadcast , 0 , start ) if stop is not None : stop = tf . where ( is_broadcast , 1 , stop ) if step is not None : step = tf . where ( is_broadcast , 1 , step ) param_slices . append ( slice ( start , stop , step ) ) else : # int, or int Tensor, e.g. d[d.batch_shape_tensor()[0] // 2] param_slices . append ( tf . where ( is_broadcast , 0 , slc ) ) param_dim_idx += 1 batch_dim_idx += 1 param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) return full_batch_param . __getitem__ ( param_slices )", "nl": "Slices a single parameter of a distribution ."}}
{"translation": {"code": "def _apply_single_step ( dist , params_event_ndims , slices , params_overrides ) : if len ( slices ) == 1 and slices [ 0 ] == Ellipsis : # The path used by Distribution.copy: batch_slice(...args..., Ellipsis) override_dict = { } else : override_dict = _slice_params_to_dict ( dist , params_event_ndims , slices ) override_dict . update ( params_overrides ) parameters = dict ( dist . parameters , * * override_dict ) new_dist = type ( dist ) ( * * parameters ) return new_dist", "nl": "Applies a single slicing step to dist returning a new instance ."}}
{"translation": {"code": "def posterior_mode ( self , observations , name = None ) : with tf . name_scope ( name or \"posterior_mode\" ) : with tf . control_dependencies ( self . _runtime_assertions ) : observation_tensor_shape = tf . shape ( input = observations ) with self . _observation_shape_preconditions ( observation_tensor_shape ) : observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) observation_rank = tf . rank ( observations ) underlying_event_rank = self . _underlying_event_rank observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) # We need to compute the probability of each observation for # each possible state. # This requires inserting an extra index just before the # observation event indices that will be broadcast with the # last batch index in `observation_distribution`. observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) observation_log_probs = self . _observation_distribution . log_prob ( observations ) log_prob = log_init + observation_log_probs [ 0 ] if self . _num_steps == 1 : most_likely_end = tf . argmax ( input = log_prob , axis = - 1 ) return most_likely_end [ ... , tf . newaxis ] def forward_step ( previous_step_pair , log_prob_observation ) : log_prob_previous = previous_step_pair [ 0 ] log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2 ) max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) return ( max_log_p_given_successor , most_likely_given_successor ) forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = \"forward_log_probs\" ) most_likely_end = tf . argmax ( input = forward_log_probs [ - 1 ] , axis = - 1 ) # We require the operation that gives C from A and B where # C[i...j] = A[i...j, B[i...j]] # and A = most_likely_given_successor #     B = most_likely_successor. # tf.gather requires indices of known shape so instead we use # reduction with tf.one_hot(B) to pick out elements from B def backward_step ( most_likely_successor , most_likely_given_successor ) : return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 )", "nl": "Compute maximum likelihood sequence of hidden states ."}}
{"translation": {"code": "def optimal_variational_posterior ( kernel , inducing_index_points , observation_index_points , observations , observation_noise_variance , mean_fn = None , jitter = 1e-6 , name = None ) : with tf . name_scope ( name or 'optimal_variational_posterior' ) : dtype = dtype_util . common_dtype ( [ inducing_index_points , observation_index_points , observations , observation_noise_variance , jitter ] , tf . float32 ) inducing_index_points = tf . convert_to_tensor ( value = inducing_index_points , dtype = dtype , name = 'inducing_index_points' ) observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = dtype , name = 'observation_index_points' ) observations = tf . convert_to_tensor ( value = observations , dtype = dtype , name = 'observations' ) observation_noise_variance = tf . convert_to_tensor ( value = observation_noise_variance , dtype = dtype , name = 'observation_noise_variance' ) jitter = tf . convert_to_tensor ( value = jitter , dtype = dtype , name = 'jitter' ) # Default to a constant zero function. if mean_fn is None : mean_fn = lambda x : tf . zeros ( [ 1 ] , dtype = dtype ) else : if not callable ( mean_fn ) : raise ValueError ( '`mean_fn` must be a Python callable' ) # z are the inducing points and x are the observation index points. kzz = kernel . matrix ( inducing_index_points , inducing_index_points ) kzx = kernel . matrix ( inducing_index_points , observation_index_points ) noise_var_inv = tf . math . reciprocal ( observation_noise_variance ) sigma_inv = _add_diagonal_shift ( kzz + noise_var_inv * tf . matmul ( kzx , kzx , adjoint_b = True ) , jitter ) chol_sigma_inv = tf . linalg . cholesky ( sigma_inv ) kzx_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzx ) kzx_obs = kzx_lin_op . matvec ( observations - mean_fn ( observation_index_points ) ) kzz_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzz ) loc = ( mean_fn ( inducing_index_points ) + noise_var_inv * kzz_lin_op . matvec ( _solve_cholesky_factored_system_vec ( chol_sigma_inv , kzx_obs ) ) ) chol_sigma_inv_lin_op = tf . linalg . LinearOperatorLowerTriangular ( chol_sigma_inv ) scale = chol_sigma_inv_lin_op . solve ( kzz ) return loc , scale", "nl": "Model selection for optimal variational hyperparameters ."}}
{"translation": {"code": "def variational_loss ( self , observations , observation_index_points = None , kl_weight = 1. , name = 'variational_loss' ) : with tf . name_scope ( name or 'variational_gp_loss' ) : if observation_index_points is None : observation_index_points = self . _index_points observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = self . _dtype , name = 'observation_index_points' ) observations = tf . convert_to_tensor ( value = observations , dtype = self . _dtype , name = 'observations' ) kl_weight = tf . convert_to_tensor ( value = kl_weight , dtype = self . _dtype , name = 'kl_weight' ) # The variational loss is a negative ELBO. The ELBO can be broken down # into three terms: #  1. a likelihood term #  2. a trace term arising from the covariance of the posterior predictive kzx = self . kernel . matrix ( self . _inducing_index_points , observation_index_points ) kzx_linop = tf . linalg . LinearOperatorFullMatrix ( kzx ) loc = ( self . _mean_fn ( observation_index_points ) + kzx_linop . matvec ( self . _kzz_inv_varloc , adjoint = True ) ) likelihood = independent . Independent ( normal . Normal ( loc = loc , scale = tf . sqrt ( self . _observation_noise_variance + self . _jitter ) , name = 'NormalLikelihood' ) , reinterpreted_batch_ndims = 1 ) obs_ll = likelihood . log_prob ( observations ) chol_kzz_linop = tf . linalg . LinearOperatorLowerTriangular ( self . _chol_kzz ) chol_kzz_inv_kzx = chol_kzz_linop . solve ( kzx ) kzz_inv_kzx = chol_kzz_linop . solve ( chol_kzz_inv_kzx , adjoint = True ) kxx_diag = tf . linalg . diag_part ( self . kernel . matrix ( observation_index_points , observation_index_points ) ) ktilde_trace_term = ( tf . reduce_sum ( input_tensor = kxx_diag , axis = - 1 ) - tf . reduce_sum ( input_tensor = chol_kzz_inv_kzx ** 2 , axis = [ - 2 , - 1 ] ) ) # Tr(SB) # where S = A A.T, A = variational_inducing_observations_scale # and B = Kzz^-1 Kzx Kzx.T Kzz^-1 # # Now Tr(SB) = Tr(A A.T Kzz^-1 Kzx Kzx.T Kzz^-1) #            = Tr(A.T Kzz^-1 Kzx Kzx.T Kzz^-1 A) #            = sum_ij (A.T Kzz^-1 Kzx)_{ij}^2 other_trace_term = tf . reduce_sum ( input_tensor = ( self . _variational_inducing_observations_posterior . scale . matmul ( kzz_inv_kzx ) ** 2 ) , axis = [ - 2 , - 1 ] ) trace_term = ( .5 * ( ktilde_trace_term + other_trace_term ) / self . _observation_noise_variance ) inducing_prior = gaussian_process . GaussianProcess ( kernel = self . _kernel , mean_fn = self . _mean_fn , index_points = self . _inducing_index_points , observation_noise_variance = self . _observation_noise_variance ) kl_term = kl_weight * kullback_leibler . kl_divergence ( self . _variational_inducing_observations_posterior , inducing_prior ) lower_bound = ( obs_ll - trace_term - kl_term ) return - tf . reduce_mean ( input_tensor = lower_bound )", "nl": "Variational loss for the VGP ."}}
{"translation": {"code": "def histogram ( x , edges , axis = None , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : # Tensor conversions. in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) # Move dims in axis to the left end as one flattened dim. # After this, x.shape = [n_samples] + E. if axis is None : x = tf . reshape ( x , shape = [ - 1 ] ) else : x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = 1 ) axis = _make_static_axis_non_negative_list ( axis , x_ndims ) if not axis : raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = False ) # bins.shape = x.shape = [n_samples] + E, # and bins[i] is a shape E Tensor of the bins that sample `i` fell into. # E is the \"event shape\", which is [] if axis is None. bins = find_bins ( x , edges = edges , # If not extending intervals, then values outside the edges will return # -1, which gives an error when fed to bincount. extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) # TODO(b/124015136) Use standard tf.math.bincount once it supports `axis`. counts = count_integers ( bins , # Ensure we get correct output, even if x did not fall into every bin minlength = tf . shape ( input = edges ) [ 0 ] - 1 , maxlength = tf . shape ( input = edges ) [ 0 ] - 1 , axis = 0 , dtype = dtype or in_dtype ) n_edges = tf . compat . dimension_value ( edges . shape [ 0 ] ) if n_edges is not None : counts . set_shape ( tf . TensorShape ( [ n_edges - 1 ] ) . concatenate ( counts . shape [ 1 : ] ) ) return counts", "nl": "Count how often x falls in intervals defined by edges ."}}
{"translation": {"code": "def count_integers ( arr , weights = None , minlength = None , maxlength = None , axis = None , dtype = tf . int32 , name = None ) : with tf . compat . v1 . name_scope ( name , 'count_integers' , values = [ arr , weights , minlength , maxlength , axis ] ) : if axis is None : return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) arr = tf . convert_to_tensor ( value = arr , dtype = tf . int32 , name = 'arr' ) arr_ndims = _get_static_ndims ( arr , expect_static = True ) axis = _make_static_axis_non_negative_list ( axis , arr_ndims ) # ~axis from docstring.  Dims in arr that are not in axis. not_axis = sorted ( set ( range ( arr_ndims ) ) . difference ( axis ) ) # If we're reducing over everything, just use standard bincount. if not not_axis : return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) # Move dims in ~axis to the left, so we can tf.map_fn bincount over them, # Producing counts for every index I in ~axis. # Thus, flat_arr is not totally flat, it just has the dims in ~axis # flattened. flat_arr = _move_dims_to_flat_end ( arr , not_axis , arr_ndims , right_end = False ) # tf.map_fn over dim 0. if weights is None : def one_bincount ( arr_slice ) : return tf . math . bincount ( arr_slice , weights = None , minlength = minlength , maxlength = maxlength , dtype = dtype ) flat_counts = tf . map_fn ( one_bincount , elems = flat_arr , dtype = dtype ) else : weights = tf . convert_to_tensor ( value = weights , name = 'weights' ) _get_static_ndims ( weights , expect_static = True , expect_ndims = arr_ndims ) flat_weights = _move_dims_to_flat_end ( weights , not_axis , arr_ndims , right_end = False ) def one_bincount ( arr_and_weights_slices ) : arr_slice , weights_slice = arr_and_weights_slices return tf . math . bincount ( arr_slice , weights = weights_slice , minlength = minlength , maxlength = maxlength , dtype = dtype ) flat_counts = tf . map_fn ( one_bincount , elems = [ flat_arr , flat_weights ] , dtype = weights . dtype ) # flat_counts.shape = [prod(~axis), K], because map_fn stacked on axis 0. # bincount needs to have the K bins in axis 0, so transpose... flat_counts_t = tf . transpose ( a = flat_counts , perm = [ 1 , 0 ] ) # Throw in this assert, to ensure shape assumptions are correct. _get_static_ndims ( flat_counts_t , expect_ndims = 2 , expect_static = True ) # not_axis_shape = arr.shape[~axis] not_axis_shape = tf . gather ( tf . shape ( input = arr ) , indices = not_axis ) # The first index of flat_counts_t indexes bins 0,..,K-1, the rest are ~axis out_shape = tf . concat ( [ [ - 1 ] , not_axis_shape ] , axis = 0 ) return tf . reshape ( flat_counts_t , out_shape )", "nl": "Counts the number of occurrences of each value in an integer array arr ."}}
{"translation": {"code": "def _get_tol ( tol , dtype , validate_args ) : if tol is None : return tf . convert_to_tensor ( value = 0 , dtype = dtype ) tol = tf . convert_to_tensor ( value = tol , dtype = dtype ) if validate_args : tol = distribution_util . with_dependencies ( [ assert_util . assert_non_negative ( tol , message = \"Argument 'tol' must be non-negative\" ) ] , tol ) return tol", "nl": "Gets a Tensor of type dtype 0 if tol is None validation optional ."}}
{"translation": {"code": "def latents_to_observations ( self , latent_means , latent_covs ) : with tf . name_scope ( \"latents_to_observations\" ) : pushforward_latents_step = build_pushforward_latents_step ( self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) latent_means = distribution_util . move_dimension ( latent_means , source_idx = - 2 , dest_idx = 0 ) latent_means = latent_means [ ... , tf . newaxis ] # Make matmul happy. latent_covs = distribution_util . move_dimension ( latent_covs , source_idx = - 3 , dest_idx = 0 ) ( initial_observation_mean , initial_observation_cov ) = pushforward_latents_step ( _ = None , # Loop body ignores previous observations. latent_t_mean_cov = ( self . initial_step , latent_means [ self . initial_step ] , latent_covs [ self . initial_step ] ) ) # TODO(davmre) this loop is embarassingly parallel; replace with `pfor`. timesteps = tf . range ( self . initial_step , self . initial_step + self . num_timesteps ) observation_means , observation_covs = tf . scan ( pushforward_latents_step , elems = ( timesteps , latent_means , latent_covs ) , initializer = ( initial_observation_mean , initial_observation_cov ) , parallel_iterations = 10000 ) observation_means = distribution_util . move_dimension ( observation_means [ ... , 0 ] , source_idx = 0 , dest_idx = - 2 ) observation_covs = distribution_util . move_dimension ( observation_covs , source_idx = 0 , dest_idx = - 3 ) return observation_means , observation_covs", "nl": "Push latent means and covariances forward through the observation model ."}}
{"translation": {"code": "def mix_over_posterior_draws ( means , variances ) : # The inputs `means`, `variances` have shape #   `concat([ #      [num_posterior_draws], #      sample_shape, #      batch_shape, #      [num_timesteps]])` # Because MixtureSameFamily mixes over the rightmost batch dimension, # we need to move the `num_posterior_draws` dimension to be rightmost # in the batch shape. This requires use of `Independent` (to preserve # `num_timesteps` as part of the event shape) and `move_dimension`. # TODO(b/120245392): enhance `MixtureSameFamily` to reduce along an # arbitrary axis, and eliminate `move_dimension` calls here. with tf . compat . v1 . name_scope ( 'mix_over_posterior_draws' , values = [ means , variances ] ) : num_posterior_draws = dist_util . prefer_static_value ( tf . shape ( input = means ) ) [ 0 ] component_observations = tfd . Independent ( distribution = tfd . Normal ( loc = dist_util . move_dimension ( means , 0 , - 2 ) , scale = tf . sqrt ( dist_util . move_dimension ( variances , 0 , - 2 ) ) ) , reinterpreted_batch_ndims = 1 ) return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = component_observations . dtype ) ) , components_distribution = component_observations )", "nl": "Construct a predictive normal distribution that mixes over posterior draws ."}}
{"translation": {"code": "def decompose_by_component ( model , observed_time_series , parameter_samples ) : with tf . compat . v1 . name_scope ( 'decompose_by_component' , values = [ observed_time_series ] ) : [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) # Run smoothing over the training timesteps to extract the # posterior on latents. num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] ssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) posterior_means , posterior_covs = ssm . posterior_marginals ( observed_time_series , mask = is_missing ) return _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples )", "nl": "Decompose an observed time series into contributions from each component ."}}
{"translation": {"code": "def _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) : try : model . components except AttributeError : raise ValueError ( 'Model decomposed into components must be an instance of' '`tfp.sts.Sum` (passed model {})' . format ( model ) ) with tf . compat . v1 . name_scope ( 'decompose_from_posterior_marginals' ) : # Extract the component means/covs from the joint latent posterior. latent_sizes = [ component . latent_size for component in model . components ] component_means = tf . split ( posterior_means , latent_sizes , axis = - 1 ) component_covs = _split_covariance_into_marginals ( posterior_covs , latent_sizes ) # Instantiate per-component state space models, and use them to push the # posterior means/covs through the observation model for each component. num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = posterior_means ) ) [ - 2 ] component_ssms = model . make_component_state_space_models ( num_timesteps = num_timesteps , param_vals = parameter_samples ) component_predictive_dists = collections . OrderedDict ( ) for ( component , component_ssm , component_mean , component_cov ) in zip ( model . components , component_ssms , component_means , component_covs ) : component_obs_mean , component_obs_cov = ( component_ssm . latents_to_observations ( latent_means = component_mean , latent_covs = component_cov ) ) # Using the observation means and covs, build a mixture distribution # that integrates over the posterior draws. component_predictive_dists [ component ] = sts_util . mix_over_posterior_draws ( means = component_obs_mean [ ... , 0 ] , variances = component_obs_cov [ ... , 0 , 0 ] ) return component_predictive_dists", "nl": "Utility method to decompose a joint posterior into components ."}}
{"translation": {"code": "def build_constrained_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : # Conceptually, this method takes the noise covariance on effects L @ L' # computed by `build_seasonal_transition_noise`, with scale factor #       L = [ 0, 0, ..., 0 #             ... #             0, 0, ..., drift_scale], # and transforms it to act on the constrained-residual representation. # # The resulting noise covariance M @ M' is equivalent to #    M @ M' = effects_to_residuals @ LL' @ residuals_to_effects # where `@` is matrix multiplication. However because this matrix is # rank-deficient, we can't take its Cholesky decomposition directly, so we'll # construct its lower-triangular scale factor `M` by hand instead. # # Concretely, let `M = P @ R @ L` be the scale factor in the # transformed space, with matrices `R`, `P` applying the reparameterization # and zero-mean constraint respectively as defined in the # \"Mathematical Details\" section of `ConstrainedSeasonalStateSpaceModel`. It's # easy to see (*) that the implied covariance # `M @ M' = P @ R @ L @ L' @ R' @ P'` is just the constant matrix #  `M @ M' = [ 1, 1, ..., 1, 0 #              1, 1, ..., 1, 0 #              ... #              1, 1, ..., 1, 0 #              0, 0, ..., 0, 0] * (drift_scale / num_seasons)**2` # with zeros in the final row and column. So we can directly construct # the lower-triangular factor #  `Q = [ 1, 0, ...  0 #         1, 0, ..., 0 #         ... #         1, 0, ..., 0 #         0, 0, ..., 0 ] * drift_scale/num_seasons` # such that Q @ Q' = M @ M'. In practice, we don't reify the final row and # column full of zeroes, i.e., we construct # `Q[:num_seasons-1, :num_seasons-1]` as the scale-TriL covariance factor. # # (*) Argument: `L` is zero everywhere but the last column, so `R @ L` will be # too. Since the last column of `R` is the constant `-1/num_seasons`, `R @ L` # is simply the matrix with constant `-drift_scale/num_seasons` in the final # column (except the final row, which is negated) and zero in all other # columns, and `M = P @ R @ L` additionally zeroes out the final row. Then # M @ M' is just the outer product of that final column with itself (since all # other columns are zero), which gives the matrix shown above. drift_scale_tril_nonzeros = tf . concat ( [ tf . ones ( [ num_seasons - 1 , 1 ] , dtype = drift_scale . dtype ) , tf . zeros ( [ num_seasons - 1 , num_seasons - 2 ] , dtype = drift_scale . dtype ) ] , axis = - 1 ) drift_scale_tril = ( drift_scale_tril_nonzeros * drift_scale [ ... , tf . newaxis , tf . newaxis ] / num_seasons ) # Inject transition noise iff it is the last day of the season. def seasonal_transition_noise ( t ) : noise_scale_tril = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_tril , tf . zeros_like ( drift_scale_tril ) ) return tfd . MultivariateNormalTriL ( loc = tf . zeros ( num_seasons - 1 , dtype = drift_scale . dtype ) , scale_tril = noise_scale_tril ) return seasonal_transition_noise", "nl": "Build transition noise distribution for a ConstrainedSeasonalSSM ."}}
{"translation": {"code": "def build_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : # If the current season has just ended, increase the variance of its effect # following drift_scale. (the just-ended seasonal effect will always be the # bottom element of the vector). Otherwise, do nothing. drift_scale_diag = tf . stack ( [ tf . zeros_like ( drift_scale ) ] * ( num_seasons - 1 ) + [ drift_scale ] , axis = - 1 ) def seasonal_transition_noise ( t ) : noise_scale_diag = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_diag , tf . zeros_like ( drift_scale_diag ) ) return tfd . MultivariateNormalDiag ( loc = tf . zeros ( num_seasons , dtype = drift_scale . dtype ) , scale_diag = noise_scale_diag ) return seasonal_transition_noise", "nl": "Build the transition noise model for a SeasonalStateSpaceModel ."}}
{"translation": {"code": "def converged_any ( converged , failed ) : return ( tf . reduce_any ( input_tensor = converged ) | tf . reduce_all ( input_tensor = failed ) )", "nl": "Condition to stop when any batch member converges or all have failed ."}}
{"translation": {"code": "def enable_store_parameters_in_results ( kernel ) : kernel_stack = [ ] while hasattr ( kernel , 'parameters' ) and 'inner_kernel' in kernel . parameters : kernel_stack . append ( kernel ) kernel = kernel . parameters [ 'inner_kernel' ] def _recreate_kernel ( kernel , parameters ) : new_parameters = kernel . parameters . copy ( ) new_parameters . update ( parameters ) if 'store_parameters_in_results' in new_parameters : new_parameters [ 'store_parameters_in_results' ] = True with deprecation . silence ( ) : return type ( kernel ) ( * * new_parameters ) if hasattr ( kernel , 'parameters' ) : kernel = _recreate_kernel ( kernel , { } ) for outer_kernel in reversed ( kernel_stack ) : outer_kernel = _recreate_kernel ( outer_kernel , { 'inner_kernel' : kernel } ) kernel = outer_kernel return kernel", "nl": "Enables the store_parameters_in_results parameter in a chain of kernels ."}}
{"translation": {"code": "def make_innermost_setter ( setter ) : @ functools . wraps ( setter ) def _new_setter ( kernel_results , * args , * * kwargs ) : \"\"\"Wrapped setter.\"\"\" results_stack = [ ] while hasattr ( kernel_results , 'inner_results' ) : results_stack . append ( kernel_results ) kernel_results = kernel_results . inner_results new_kernel_results = setter ( kernel_results , * args , * * kwargs ) for outer_results in reversed ( results_stack ) : new_kernel_results = outer_results . _replace ( inner_results = new_kernel_results ) return new_kernel_results return _new_setter", "nl": "Wraps a setter so it applies to the inner - most results in kernel_results ."}}
{"translation": {"code": "def rank_from_shape ( shape_tensor_fn , tensorshape = None ) : if tensorshape is None : shape_tensor = ( shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) if ( hasattr ( shape_tensor , 'shape' ) and hasattr ( shape_tensor . shape , 'num_elements' ) ) : ndims_ = tensorshape_util . num_elements ( shape_tensor . shape ) else : ndims_ = len ( shape_tensor ) ndims_fn = lambda : tf . size ( input = shape_tensor ) else : ndims_ = tensorshape_util . rank ( tensorshape ) ndims_fn = lambda : tf . size ( input = shape_tensor_fn ( ) # pylint: disable=g-long-lambda if callable ( shape_tensor_fn ) else shape_tensor_fn ) return ndims_fn ( ) if ndims_ is None else ndims_", "nl": "Computes rank given a Tensor s shape ."}}
{"translation": {"code": "def _get_convert_to_tensor_fn ( identifier ) : if identifier is None : return None if isinstance ( identifier , six . string_types ) : identifier = str ( identifier ) return _deserialize ( identifier ) if isinstance ( identifier , dict ) : return _deserialize ( identifier ) if isinstance ( identifier , property ) : identifier = identifier . fget if callable ( identifier ) : return identifier raise ValueError ( 'Could not interpret ' 'convert-to-tensor function identifier:' , identifier )", "nl": "Return a convert - to - tensor func given a name config callable etc ."}}
{"translation": {"code": "def maybe_check_wont_broadcast ( flat_xs , validate_args ) : flat_xs = tuple ( flat_xs ) # So we can receive generators. if not validate_args : # Note: we don't try static validation because it is theoretically # possible that a user wants to take advantage of broadcasting. # Only when `validate_args` is `True` do we enforce the validation. return flat_xs msg = 'Broadcasting probably indicates an error in model specification.' s = tuple ( x . shape for x in flat_xs ) if all ( tensorshape_util . is_fully_defined ( s_ ) for s_ in s ) : if not all ( a == b for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : raise ValueError ( msg ) return flat_xs assertions = [ assert_util . assert_equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] with tf . control_dependencies ( assertions ) : return tuple ( tf . identity ( x ) for x in flat_xs )", "nl": "Verifies that parts don t broadcast ."}}
{"translation": {"code": "def _make_summary_statistic ( attr ) : def _fn ( self , * * kwargs ) : \"\"\"Implements summary statistic, eg, mean, stddev, mode.\"\"\" x = getattr ( self . distribution , attr ) ( * * kwargs ) shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , prefer_static . ones ( prefer_static . rank_from_shape ( self . sample_shape ) , dtype = self . sample_shape . dtype ) , self . distribution . event_shape_tensor ( ) , ] , axis = 0 ) x = tf . reshape ( x , shape = shape ) shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , self . sample_shape , self . distribution . event_shape_tensor ( ) , ] , axis = 0 ) return tf . broadcast_to ( x , shape ) return _fn", "nl": "Factory for implementing summary statistics eg mean stddev mode ."}}
{"translation": {"code": "def initial_value_of_masked_time_series ( time_series_tensor , broadcast_mask ) : num_timesteps = tf . shape ( input = time_series_tensor ) [ - 1 ] # Compute the index of the first unmasked entry for each series in the batch. unmasked_negindices = ( tf . cast ( ~ broadcast_mask , tf . int32 ) * tf . range ( num_timesteps , 0 , - 1 ) ) first_unmasked_indices = num_timesteps - tf . reduce_max ( input_tensor = unmasked_negindices , axis = - 1 ) if first_unmasked_indices . shape . ndims is None : raise NotImplementedError ( 'Cannot compute initial values of a masked time series with' 'dynamic rank.' ) # `batch_gather` requires static rank # Extract the initial value for each series in the batch. return tf . squeeze ( tf . compat . v1 . batch_gather ( params = time_series_tensor , indices = first_unmasked_indices [ ... , tf . newaxis ] ) , axis = - 1 )", "nl": "Get the first unmasked entry of each time series in the batch ."}}
{"translation": {"code": "def _create_masks ( degrees ) : return [ # Create input->hidden and hidden->hidden masks. inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ # Create hidden->output mask. degrees [ - 1 ] [ : , np . newaxis ] < degrees [ 0 ] ]", "nl": "Returns a list of binary mask matrices enforcing autoregressivity ."}}
{"translation": {"code": "def _make_masked_initializer ( mask , initializer ) : initializer = tf . keras . initializers . get ( initializer ) def masked_initializer ( shape , dtype = None , partition_info = None ) : # If no `partition_info` is given, then don't pass it to `initializer`, as # `initializer` may be a `tf.compat.v2.initializers.Initializer` (which # don't accept a `partition_info` argument). if partition_info is None : x = initializer ( shape , dtype ) else : x = initializer ( shape , dtype , partition_info ) return tf . cast ( mask , x . dtype ) * x return masked_initializer", "nl": "Returns a masked version of the given initializer ."}}
{"translation": {"code": "def build ( self , input_shape ) : if self . _event_shape is None : # `event_shape` wasn't specied at __init__, so infer from `input_shape`. self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - 1 ] ) ] self . _event_size = self . _event_shape [ - 1 ] self . _event_ndims = len ( self . _event_shape ) # Should we throw if input_shape has rank > 2? if input_shape [ - 1 ] != self . _event_shape [ - 1 ] : raise ValueError ( \"Invalid final dimension of `input_shape`. \" \"Expected `{!r}`, but got `{!r}`\" . format ( self . _event_shape [ - 1 ] , input_shape [ - 1 ] ) ) # Construct the masks. self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) # In the final layer, we will produce `self._params` outputs for each of the # `self._event_size` inputs to `AutoregressiveLayer`.  But `masks[-1]` has # shape `[self._hidden_units[-1], self._event_size]`.  Thus, we need to # expand the mask to `[hidden_units[-1], event_size * self._params]` such # that all units for the same input are masked identically.  In particular, # we tile the mask so the j-th element of `tf.unstack(output, axis=-1)` is a # tensor of the j-th parameter/unit for each input. # # NOTE: Other orderings of the output could be faster -- should benchmark. self . _masks [ - 1 ] = np . reshape ( np . tile ( self . _masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . _params ] ) , [ self . _masks [ - 1 ] . shape [ 0 ] , self . _event_size * self . _params ] ) self . _network = tf . keras . Sequential ( [ # Starting this model with an `InputLayer` ensures that Keras will build # and propagate our `dtype` to each layer we add. tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) # Input-to-hidden, hidden-to-hidden, and hidden-to-output layers: #  [..., self._event_size] -> [..., self._hidden_units[0]]. #  [..., self._hidden_units[k-1]] -> [..., self._hidden_units[k]]. #  [..., self._hidden_units[-1]] -> [..., event_size * self._params]. layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] for k in range ( len ( self . _masks ) ) : self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if k + 1 < len ( self . _masks ) else None , use_bias = self . _use_bias , * * self . _kwargs ) ) # Record that the layer has been built. super ( AutoregressiveLayer , self ) . build ( input_shape )", "nl": "See tfkl . Layer . build ."}}
{"translation": {"code": "def call ( self , x ) : with tf . compat . v2 . name_scope ( self . name or \"AutoregressiveLayer_call\" ) : x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = \"x\" ) input_shape = tf . shape ( input = x ) # TODO(b/67594795): Better support for dynamic shapes. if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] return tf . reshape ( self . _network ( x ) , tf . concat ( [ input_shape , [ self . _params ] ] , axis = 0 ) )", "nl": "See tfkl . Layer . call ."}}
{"translation": {"code": "def bootstrap_results ( self , state ) : def loss ( ) : q = self . _flattened_variational_distribution ( ) # TODO(siege): How to seed this? samples = q . sample ( self . train_batch_size ) return tf . reduce_mean ( input_tensor = q . log_prob ( samples ) - self . _flattened_target_log_prob ( samples ) , axis = - 1 ) lr = tf . convert_to_tensor ( value = self . learning_rate , dtype = self . _dtype ) dtype = lr . dtype learning_rate = tf . compat . v2 . optimizers . schedules . PiecewiseConstantDecay ( list ( self . num_train_steps * np . array ( [ 0.2 , 0.8 ] ) . astype ( dtype . as_numpy_dtype ( ) ) ) , [ lr , lr * 0.1 , lr * 0.01 ] ) opt = tf . compat . v2 . optimizers . Adam ( learning_rate ) @ tf . function ( autograph = False ) def train_step ( ) : with tf . GradientTape ( ) as tape : loss_val = loss ( ) vals = tape . watched_variables ( ) grads = tape . gradient ( loss_val , vals ) grads_and_vals = list ( zip ( grads , vals ) ) opt . apply_gradients ( grads_and_vals ) return loss_val for step in range ( self . num_train_steps ) : loss_val = train_step ( ) tf . debugging . assert_all_finite ( loss_val , 'NeuTra loss is NaN at step {}' . format ( step ) ) if self . train_debug_fn : # pylint: disable=not-callable self . train_debug_fn ( self , step , loss_val ) state_parts = tf . nest . flatten ( state ) flat_state_shapes = tf . nest . flatten ( self . state_shape ) batch_shape = tf . shape ( input = state_parts [ 0 ] ) [ : - flat_state_shapes [ 0 ] . ndims ] return self . _kernel . bootstrap_results ( self . _flattened_variational_distribution ( ) . sample ( batch_shape , seed = self . seed ) )", "nl": "Trains the bijector and creates initial previous_kernel_results ."}}
{"translation": {"code": "def make_iaf_stack ( total_event_size , num_hidden_layers = 2 , seed = None , dtype = tf . float32 ) : seed = tfd . SeedStream ( seed , 'make_iaf_stack' ) def make_iaf ( ) : \"\"\"Create an IAF.\"\"\" initializer = tf . compat . v2 . keras . initializers . VarianceScaling ( 2 * 0.01 , seed = seed ( ) % ( 2 ** 31 - 1 ) ) made = tfb . AutoregressiveLayer ( params = 2 , event_shape = [ total_event_size ] , hidden_units = [ total_event_size ] * num_hidden_layers , activation = tf . nn . elu , kernel_initializer = initializer , dtype = dtype ) def shift_and_scale ( x ) : # TODO(siege): Something is losing the static shape. x . set_shape ( x . shape . merge_with ( [ None ] * ( x . shape . ndims - 1 ) + [ total_event_size ] ) ) return tf . unstack ( made ( x ) , num = 2 , axis = - 1 ) return tfb . Invert ( tfb . MaskedAutoregressiveFlow ( shift_and_scale ) ) def make_swap ( ) : \"\"\"Create an swap.\"\"\" permutation = list ( reversed ( range ( total_event_size ) ) ) return tfb . Permute ( permutation ) bijector = make_iaf ( ) bijector = make_swap ( ) ( bijector ) bijector = make_iaf ( ) ( bijector ) bijector = make_swap ( ) ( bijector ) bijector = make_iaf ( ) ( bijector ) bijector = make_swap ( ) ( bijector ) return bijector", "nl": "Creates an stacked IAF bijector ."}}
{"translation": {"code": "def _entropy ( self ) : if any ( self . _dist_fn_args ) : raise ValueError ( 'Can only compute entropy when all distributions are independent.' ) return sum ( joint_distribution_lib . maybe_check_wont_broadcast ( ( d ( ) . entropy ( ) for d in self . _dist_fn_wrapped ) , self . validate_args ) )", "nl": "Shannon entropy in nats ."}}
{"translation": {"code": "def _resolve_graph ( self , distribution_names = None , leaf_name = 'x' ) : # This function additionally depends on: #   self._dist_fn_args #   self._dist_fn_wrapped # TODO(b/129008220): Robustify this procedure. Eg, handle collisions better, # ignore args prefixed with `_`. if distribution_names is None or any ( self . _dist_fn_args ) : distribution_names = _resolve_distribution_names ( self . _dist_fn_args , distribution_names , leaf_name ) if len ( set ( distribution_names ) ) != len ( distribution_names ) : raise ValueError ( 'Distribution names must be unique: {}' . format ( distribution_names ) ) if len ( distribution_names ) != len ( self . _dist_fn_wrapped ) : raise ValueError ( 'Distribution names must be 1:1 with `rvs`.' ) return tuple ( zip ( distribution_names , tuple ( ( ) if a is None else a for a in self . _dist_fn_args ) ) )", "nl": "Creates a tuple of tuple s of dependencies ."}}
{"translation": {"code": "def _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : if dist_names is None : dist_names = [ ] else : dist_names = dist_names . copy ( ) n = len ( dist_fn_args ) dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : if not args : continue # There's no args to analyze. i = n - i_ - 1 for j , arg_name in enumerate ( args ) : dist_names [ i - j - 1 ] = arg_name j = 0 for i_ in range ( len ( dist_names ) ) : i = n - i_ - 1 if dist_names [ i ] is None : dist_names [ i ] = leaf_name if j == 0 else leaf_name + str ( j ) j += 1 return tuple ( dist_names )", "nl": "Uses arg names to resolve distribution names ."}}
{"translation": {"code": "def trace ( state : State , fn : TransitionOperator , num_steps : IntTensor , trace_fn : Callable [ [ State , TensorNest ] , TensorNest ] ) -> Tuple [ State , TensorNest ] : def fn_wrapper ( args , _ ) : return tf . nest . map_structure ( tf . convert_to_tensor , call_fn ( fn , args [ 0 ] ) ) def trace_fn_wrapper ( args ) : return tf . nest . map_structure ( tf . convert_to_tensor , call_fn ( trace_fn , args ) ) state = call_fn ( fn , state ) first_trace = trace_fn_wrapper ( state ) state , full_trace = mcmc_util . trace_scan ( fn_wrapper , state , tf . ones ( num_steps - 1 ) , trace_fn = trace_fn_wrapper ) prepend = lambda x , y : tf . concat ( # pylint: disable=g-long-lambda [ tf . convert_to_tensor ( value = x ) [ tf . newaxis ] , y ] , 0 ) return state , tf . nest . map_structure ( prepend , first_trace , full_trace )", "nl": "TransitionOperator that runs fn repeatedly and traces its outputs ."}}
{"translation": {"code": "def metropolis_hastings_step ( current_state : State , proposed_state : State , energy_change : FloatTensor , seed = None ) -> Tuple [ State , tf . Tensor , tf . Tensor ] : flat_current = tf . nest . flatten ( current_state ) flat_proposed = nest . flatten_up_to ( current_state , proposed_state ) # Impute the None's in the current state. flat_current = [ p if c is None else c for p , c in zip ( flat_proposed , flat_current ) ] current_state = tf . nest . pack_sequence_as ( current_state , flat_current ) current_state = tf . nest . map_structure ( tf . convert_to_tensor , current_state ) proposed_state = tf . nest . map_structure ( tf . convert_to_tensor , proposed_state ) energy_change = tf . convert_to_tensor ( value = energy_change ) log_accept_ratio = - energy_change log_uniform = tf . math . log ( tf . random . uniform ( shape = tf . shape ( input = log_accept_ratio ) , dtype = log_accept_ratio . dtype . base_dtype , seed = seed ) ) is_accepted = log_uniform < log_accept_ratio next_state = mcmc_util . choose ( is_accepted , proposed_state , current_state , name = 'choose_next_state' ) return next_state , is_accepted , log_uniform", "nl": "Metropolis - Hastings step ."}}
{"translation": {"code": "def transform_log_prob_fn ( log_prob_fn : PotentialFn , bijector : BijectorNest , init_state : State = None ) -> Union [ PotentialFn , Tuple [ PotentialFn , State ] ] : def wrapper ( * args ) : \"\"\"Transformed wrapper.\"\"\" bijector_ = bijector args = tf . nest . map_structure ( lambda x : 0. + x , args ) if len ( args ) == 1 : args = args [ 0 ] elif isinstance ( bijector_ , list ) : bijector_ = tuple ( bijector_ ) original_space_args = tf . nest . map_structure ( lambda b , x : b . forward ( x ) , bijector_ , args ) original_space_args = original_space_args # type: Tuple[Any] original_space_log_prob , extra = call_fn ( log_prob_fn , original_space_args ) event_ndims = tf . nest . map_structure ( lambda x : tf . rank ( x ) - tf . rank ( original_space_log_prob ) , args ) return original_space_log_prob + sum ( tf . nest . flatten ( tf . nest . map_structure ( lambda b , x , e : b . forward_log_det_jacobian ( x , event_ndims = e ) , bijector_ , args , event_ndims ) ) ) , [ original_space_args , extra ] if init_state is None : return wrapper else : return wrapper , tf . nest . map_structure ( lambda b , s : b . inverse ( s ) , bijector , init_state )", "nl": "Transforms a log - prob function using a bijector ."}}
{"translation": {"code": "def call_fn ( fn : TransitionOperator , args : Union [ Tuple [ Any ] , Any ] ) -> Any : if isinstance ( args , ( list , tuple ) ) and not mcmc_util . is_namedtuple_like ( args ) : args = args # type: Tuple[Any] return fn ( * args ) else : return fn ( args )", "nl": "Calls a transition operator with args unpacking args if its a sequence ."}}
{"translation": {"code": "def _observe_timeseries_fn ( timeseries ) : def observation_noise_fn ( t ) : current_slice = timeseries [ ... , t , : ] return tfd . MultivariateNormalDiag ( loc = current_slice , scale_diag = tf . zeros_like ( current_slice ) ) return observation_noise_fn", "nl": "Build an observation_noise_fn that observes a Tensor timeseries ."}}
{"translation": {"code": "def base_dtype ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'base_dtype' ) : return dtype . base_dtype return dtype", "nl": "Returns a non - reference dtype based on this dtype ."}}
{"translation": {"code": "def as_numpy_dtype ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'as_numpy_dtype' ) : return dtype . as_numpy_dtype return dtype", "nl": "Returns a np . dtype based on this dtype ."}}
{"translation": {"code": "def is_bool ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_bool' ) : return dtype . is_bool # We use `kind` because: # np.issubdtype(np.uint8, np.bool) == True. return np . dtype ( dtype ) . kind == 'b'", "nl": "Returns whether this is a boolean data type ."}}
{"translation": {"code": "def merge_with ( x , other ) : return type ( x ) ( tf . TensorShape ( x ) . merge_with ( other ) )", "nl": "Returns a shape combining the information in x and other ."}}
{"translation": {"code": "def with_rank_at_least ( x , rank ) : # pylint: disable=redefined-outer-name return type ( x ) ( tf . TensorShape ( x ) . with_rank_at_least ( rank ) )", "nl": "Returns a shape based on x with at least the given rank ."}}
{"translation": {"code": "def assert_finite ( x , data = None , summarize = None , message = None , name = None ) : with tf . compat . v2 . name_scope ( name or 'assert_finite' ) : x_ = tf . get_static_value ( x ) if x_ is not None : if ~ np . all ( np . isfinite ( x_ ) ) : raise ValueError ( message ) return x assertion = tf . compat . v1 . assert_equal ( tf . math . is_finite ( x ) , tf . ones_like ( x , tf . bool ) , data = data , summarize = summarize , message = message ) with tf . control_dependencies ( [ assertion ] ) : return tf . identity ( x )", "nl": "Assert all elements of x are finite ."}}
{"translation": {"code": "def assert_same_float_dtype ( tensors = None , dtype = None ) : if tensors : dtype = _assert_same_base_type ( tensors , dtype ) if not dtype : dtype = tf . float32 elif not is_floating ( dtype ) : raise ValueError ( 'Expected floating point type, got {}.' . format ( dtype ) ) return dtype", "nl": "Validate and return float type based on tensors and dtype ."}}
{"translation": {"code": "def common_dtype ( args_list , preferred_dtype = None ) : dtype = None preferred_dtype = ( None if preferred_dtype is None else tf . as_dtype ( preferred_dtype ) ) for a in tf . nest . flatten ( args_list ) : if hasattr ( a , 'dtype' ) : dt = tf . as_dtype ( a . dtype ) else : continue if dtype is None : dtype = dt elif dtype != dt : raise TypeError ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) if dtype is None and preferred_dtype is None : return None return ( preferred_dtype if dtype is None else dtype ) . as_numpy_dtype", "nl": "Returns explict dtype from args_list if exists else preferred_dtype ."}}
{"translation": {"code": "def _prob_chain_rule_flatten ( named_makers ) : def _make ( dist_fn , args ) : if args is None : return lambda * _ : dist_fn if not args : return lambda * _ : dist_fn ( ) def _fn ( * xs ) : kwargs = dict ( zip ( args , reversed ( xs [ - len ( args ) : ] ) ) ) kwargs . pop ( '_' , None ) return dist_fn ( * * kwargs ) return _fn named_makers = _convert_to_dict ( named_makers ) g = { k : ( None if distribution_util . is_distribution_instance ( v ) else joint_distribution_sequential . _get_required_args ( v ) ) # pylint: disable=protected-access for k , v in named_makers . items ( ) } g = _best_order ( g ) dist_fn_name , dist_fn_args = zip ( * g ) dist_fn_args = tuple ( None if a is None else tuple ( a ) for a in dist_fn_args ) dist_fn_wrapped = tuple ( _make ( named_makers [ name ] , parents ) for ( name , parents ) in g ) dist_fn = tuple ( named_makers . get ( n ) for n in dist_fn_name ) return dist_fn , dist_fn_wrapped , dist_fn_args , dist_fn_name", "nl": "Creates lists of callables suitable for JDSeq ."}}
{"translation": {"code": "def _build ( self , model ) : if not isinstance ( model , collections . Sequence ) : raise TypeError ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . __name__ ) ) self . _dist_fn = model self . _dist_fn_wrapped , self . _dist_fn_args = zip ( * [ _unify_call_signature ( i , dist_fn ) for i , dist_fn in enumerate ( model ) ] )", "nl": "Creates dist_fn dist_fn_wrapped dist_fn_args ."}}
{"translation": {"code": "def _best_order ( g ) : def _explore ( u ) : \"\"\"Recursive function to ascend up through unvisited dependencies.\"\"\" if u . depth < 0 : return # Already visited. if not u . parents : result . append ( ( u . name , u . parents ) ) u . depth = - 1 # Mark visited. return b = ( u . name , [ ] ) result . append ( b ) u . depth = - 1 # Mark visited. d = 0 for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : n0 = len ( result ) _explore ( v ) n1 = len ( result ) b [ 1 ] . extend ( [ '_' ] * d + [ v . name ] ) d = n1 - n0 - 1 g = _depth ( g ) result = [ ] for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : _explore ( u ) return tuple ( reversed ( result ) )", "nl": "Creates tuple of str tuple - str pairs representing resolved & sorted DAG ."}}
{"translation": {"code": "def _nested_convert_to_tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf . nest . is_nested ( struct ) : return tf . convert_to_tensor ( struct , dtype = dtype ) if _maybe_convertible_to_tensor ( struct ) : try : # Try converting the structure wholesale. return tf . convert_to_tensor ( value = struct , name = name ) except ( ValueError , TypeError ) : # Unfortunately Eager/Graph mode don't agree on the error type. pass # Try converting all of its children. shallow_struct = _get_shallow_structure ( struct ) return nest . map_structure_up_to ( shallow_struct , lambda s : _nested_convert_to_tensor ( s , name = name ) , struct )", "nl": "Eagerly converts struct to Tensor recursing upon failure ."}}
{"translation": {"code": "def convert_args_to_tensor ( args , dtype = None , name = None ) : if dtype is None : if expand_as_args ( args ) or _expand_as_kwargs ( args ) : shallow_args = _get_shallow_structure ( args ) return nest . map_structure_up_to ( shallow_args , lambda s : _nested_convert_to_tensor ( s , name = name ) , args ) else : return _nested_convert_to_tensor ( args , name = name ) else : return nest . map_structure_up_to ( dtype , lambda s , dtype : _nested_convert_to_tensor ( s , dtype , name ) , args , dtype )", "nl": "Converts args to Tensor s ."}}
{"translation": {"code": "def call_fn ( fn , args ) : if expand_as_args ( args ) : return fn ( * args ) elif _expand_as_kwargs ( args ) : return fn ( * * args ) else : return fn ( args )", "nl": "Calls fn with args possibly expanding args ."}}
{"translation": {"code": "def _get_index_points ( self , index_points = None ) : if self . _index_points is None and index_points is None : raise ValueError ( 'This GaussianProcess instance was not instantiated with a value for ' 'index_points. One must therefore be provided when calling sample, ' 'log_prob, and other such methods. In particular, one can\\'t compute ' 'KL divergences to/from an instance of `GaussianProccess` with ' 'unspecified `index_points` directly. Instead, use the ' '`get_marginal_distribution` function, which takes `index_points` as ' 'an argument and returns a `Normal` or ' '`MultivariateNormalLinearOperator` instance, whose KL can be ' 'computed.' ) return index_points if index_points is not None else self . _index_points", "nl": "Return index_points if not None else self . _index_points ."}}
{"translation": {"code": "def _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : ds = [ ] values_out = [ ] seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) gen = self . _model ( ) index = 0 d = next ( gen ) try : while True : actual_distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual_distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next_value = value [ index ] else : next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values_out . append ( next_value ) index += 1 d = gen . send ( next_value ) except StopIteration : pass return ds , values_out", "nl": "Executes model creating both samples and distributions ."}}