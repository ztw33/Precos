{"translation": {"code": "def _scoop_single_run ( kwargs ) : try : try : is_origin = scoop . IS_ORIGIN except AttributeError : # scoop is not properly started, i.e. with `python -m scoop...` # in this case scoop uses default `map` function, i.e. # the main process is_origin = True if not is_origin : # configure logging and niceness if not the main process: _configure_niceness ( kwargs ) _configure_logging ( kwargs ) return _single_run ( kwargs ) except Exception : scoop . logger . exception ( 'ERROR occurred during a single run!' ) raise", "nl": "Wrapper function for scoop that does not configure logging"}}
{"translation": {"code": "def get_strings ( args ) : string_list = [ ] for elem in ast . walk ( ast . parse ( args ) ) : if isinstance ( elem , ast . Str ) : string_list . append ( elem . s ) return string_list", "nl": "Returns all valid python strings inside a given argument string ."}}
{"translation": {"code": "def extract_replacements ( self , trajectory ) : self . env_name = trajectory . v_environment_name self . traj_name = trajectory . v_name self . set_name = trajectory . f_wildcard ( '$set' ) self . run_name = trajectory . f_wildcard ( '$' )", "nl": "Extracts the wildcards and file replacements from the trajectory"}}
{"translation": {"code": "def _prm_load_into_dict ( self , full_name , load_dict , hdf5_group , instance , load_only , load_except , load_flags , _prefix = '' ) : for node in hdf5_group : load_type = self . _all_get_from_attrs ( node , HDF5StorageService . STORAGE_TYPE ) if _prefix : load_name = '%s.%s' % ( _prefix , node . _v_name ) else : load_name = node . _v_name if load_type == HDF5StorageService . NESTED_GROUP : self . _prm_load_into_dict ( full_name = full_name , load_dict = load_dict , hdf5_group = node , instance = instance , load_only = load_only , load_except = load_except , load_flags = load_flags , _prefix = load_name ) continue if load_only is not None : if load_name not in load_only : continue else : load_only . remove ( load_name ) elif load_except is not None : if load_name in load_except : load_except . remove ( load_name ) continue # Recall from the hdf5 node attributes how the data was stored and reload accordingly if load_name in load_flags : load_type = load_flags [ load_name ] if load_type == HDF5StorageService . DICT : to_load = self . _prm_read_dictionary ( node , full_name ) elif load_type == HDF5StorageService . TABLE : to_load = self . _prm_read_table ( node , full_name ) elif load_type in ( HDF5StorageService . ARRAY , HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : to_load = self . _prm_read_array ( node , full_name ) elif load_type in ( HDF5StorageService . FRAME , HDF5StorageService . SERIES , #  HDF5StorageService.PANEL ) : to_load = self . _prm_read_pandas ( node , full_name ) elif load_type . startswith ( HDF5StorageService . SHARED_DATA ) : to_load = self . _prm_read_shared_data ( node , instance ) else : raise pex . NoSuchServiceError ( 'Cannot load %s, do not understand the hdf5 file ' 'structure of %s [%s].' % ( full_name , str ( node ) , str ( load_type ) ) ) if to_load is None : raise RuntimeError ( 'You shall not pass!' ) load_dict [ load_name ] = to_load", "nl": "Loads into dictionary"}}
{"translation": {"code": "def download_file ( filename , session ) : print ( 'Downloading file %s' % filename ) infilesource = os . path . join ( 'sftp://' + ADDRESS + WORKING_DIR , filename ) infiletarget = os . path . join ( os . getcwd ( ) , filename ) incoming = saga . filesystem . File ( infilesource , session = session , flags = OVERWRITE ) incoming . copy ( infiletarget ) print ( 'Transfer of `%s` to `%s` successful' % ( filename , infiletarget ) )", "nl": "Downloads a file"}}
{"translation": {"code": "def start_jobs ( session ) : js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) batches = range ( 3 ) jobs = [ ] for batch in batches : print ( 'Starting batch %d' % batch ) jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'the_task.py --batch=' + str ( batch ) ] jd . output = \"mysagajob.stdout\" + str ( batch ) jd . error = \"mysagajob.stderr\" + str ( batch ) jd . working_directory = WORKING_DIR myjob = js . create_job ( jd ) print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...starting job...\\n\" ) myjob . run ( ) jobs . append ( myjob ) for myjob in jobs : print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...waiting for job...\\n\" ) # wait for the job to either finish or fail myjob . wait ( ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"Exitcode  : %s\" % ( myjob . exit_code ) )", "nl": "Starts all jobs and runs the_task . py in batches ."}}
{"translation": {"code": "def get_batch ( ) : optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) batch = 0 for o , a in optlist : if o == '--batch' : batch = int ( a ) print ( 'Found batch %d' % batch ) return batch", "nl": "Function that parses the batch id from the command line arguments"}}
{"translation": {"code": "def main ( ) : folder = os . getcwd ( ) print ( 'Merging all files' ) merge_all_in_folder ( folder , delete_other_files = True , # We will only keep one trajectory dynamic_imports = FunctionParameter , backup = False ) print ( 'Done' )", "nl": "Simply merge all trajectories in the working directory"}}
{"translation": {"code": "def format_time ( timestamp ) : format_string = '%Y_%m_%d_%Hh%Mm%Ss' formatted_time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( format_string ) return formatted_time", "nl": "Formats timestamp to human readable format"}}
{"translation": {"code": "def run ( self ) : try : while True : msg , args , kwargs = self . _receive_data ( ) stop = self . _handle_data ( msg , args , kwargs ) if stop : break finally : if self . _storage_service . is_open : self . _close_file ( ) self . _trajectory_name = ''", "nl": "Starts listening to the queue ."}}
{"translation": {"code": "def store ( self , msg , stuff_to_store , * args , * * kwargs ) : trajectory_name = kwargs [ 'trajectory_name' ] if trajectory_name not in self . references : self . references [ trajectory_name ] = [ ] self . references [ trajectory_name ] . append ( ( msg , cp . copy ( stuff_to_store ) , args , kwargs ) )", "nl": "Simply keeps a reference to the stored data"}}
{"translation": {"code": "def store_references ( self , references ) : for trajectory_name in references : self . _storage_service . store ( pypetconstants . LIST , references [ trajectory_name ] , trajectory_name = trajectory_name ) self . _check_and_collect_garbage ( )", "nl": "Stores references to disk and may collect garbage ."}}
{"translation": {"code": "def finalize ( self ) : if self . _context is not None : if self . _socket is not None : self . _close_socket ( confused = False ) self . _context . term ( ) self . _context = None self . _poll = None", "nl": "Closes socket and terminates context"}}
{"translation": {"code": "def acquire ( self ) : self . start ( test_connection = False ) while True : str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) response = str_response . split ( LockerServer . DELIMITER ) if response [ 0 ] == LockerServer . GO : return True elif response [ 0 ] == LockerServer . LOCK_ERROR and retries > 0 : # Message was sent but Server response was lost and we tried again self . _logger . error ( str_response + '; Probably due to retry' ) return True elif response [ 0 ] == LockerServer . WAIT : time . sleep ( self . SLEEP ) else : raise RuntimeError ( 'Response `%s` not understood' % response )", "nl": "Acquires lock and returns True"}}
{"translation": {"code": "def send_done ( self ) : self . start ( test_connection = False ) self . _logger . debug ( 'Sending shutdown signal' ) self . _req_rep ( ZMQServer . DONE )", "nl": "Notifies the Server to shutdown"}}
{"translation": {"code": "def vars ( self ) : if self . _vars is None : self . _vars = NNTreeNodeVars ( self ) return self . _vars", "nl": "Alternative naming you can use node . vars . name instead of node . v_name"}}
{"translation": {"code": "def listen ( self ) : count = 0 self . _start ( ) while True : result = self . _socket . recv_pyobj ( ) if isinstance ( result , tuple ) : request , data = result else : request = result data = None if request == self . SPACE : if self . queue . qsize ( ) + count < self . queue_maxsize : self . _socket . send_string ( self . SPACE_AVAILABLE ) count += 1 else : self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) elif request == self . PING : self . _socket . send_string ( self . PONG ) elif request == self . DATA : self . _socket . send_string ( self . STORING ) self . queue . put ( data ) count -= 1 elif request == self . DONE : self . _socket . send_string ( ZMQServer . CLOSED ) self . queue . put ( ( 'DONE' , [ ] , { } ) ) self . _close ( ) break else : raise RuntimeError ( 'I did not understand your request %s' % request )", "nl": "Handles listening requests from the client ."}}
{"translation": {"code": "def put ( self , data , block = True ) : self . start ( test_connection = False ) while True : response = self . _req_rep ( QueuingServerMessageListener . SPACE ) if response == QueuingServerMessageListener . SPACE_AVAILABLE : self . _req_rep ( ( QueuingServerMessageListener . DATA , data ) ) break else : time . sleep ( 0.01 )", "nl": "If there is space it sends data to server"}}
{"translation": {"code": "def _handle_sigint ( self , signum , frame ) : if self . hit : prompt = 'Exiting immediately!' raise KeyboardInterrupt ( prompt ) else : self . hit = True prompt = ( '\\nYou killed the process(es) via `SIGINT` (`CTRL+C`). ' 'I am trying to exit ' 'gracefully. Using `SIGINT` (`CTRL+C`) ' 'again will cause an immediate exit.\\n' ) sys . stderr . write ( prompt )", "nl": "Handler of SIGINT"}}
{"translation": {"code": "def kids ( self ) : if self . _kids is None : self . _kids = NNTreeNodeKids ( self ) return self . _kids", "nl": "Alternative naming you can use node . kids . name instead of node . name for easier tab completion ."}}
{"translation": {"code": "def _run_network ( self , traj ) : self . build ( traj ) self . _pretty_print_explored_parameters ( traj ) # We need to construct a network object in case one was not pre-run if not self . _pre_run : self . _network = self . _network_constructor ( * self . _brian_list ) # Start the experimental run self . network_runner . execute_network_run ( traj , self . _network , self . _network_dict , self . components , self . analysers ) self . _logger . info ( '\\n-----------------------------\\n' 'Network Simulation successful\\n' '-----------------------------' )", "nl": "Starts a single run carried out by a NetworkRunner ."}}
{"translation": {"code": "def run_network ( self , traj ) : # Check if the network was pre-built if self . _pre_built : if self . _pre_run and hasattr ( self . _network , 'restore' ) : self . _network . restore ( 'pre_run' ) # Temprorary fix for https://github.com/brian-team/brian2/issues/681 self . _network . store ( 'pre_run' ) self . _run_network ( traj ) else : self . _run_network ( traj )", "nl": "Top - level simulation function pass this to the environment"}}
{"translation": {"code": "def execute_network_pre_run ( self , traj , network , network_dict , component_list , analyser_list ) : self . _execute_network_run ( traj , network , network_dict , component_list , analyser_list , pre_run = True )", "nl": "Runs a network before the actual experiment ."}}
{"translation": {"code": "def analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : if len ( subrun_list ) == 0 : traj . f_add_result ( Brian2MonitorResult , 'monitors.spikes_e' , self . spike_monitor , comment = 'The spiketimes of the excitatory population' ) traj . f_add_result ( Brian2MonitorResult , 'monitors.V' , self . V_monitor , comment = 'Membrane voltage of four neurons from 2 clusters' ) traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_e' , self . I_syn_e_monitor , comment = 'I_syn_e of four neurons from 2 clusters' ) traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_i' , self . I_syn_i_monitor , comment = 'I_syn_i of four neurons from 2 clusters' ) print ( 'Plotting' ) if traj . parameters . analysis . make_plots : self . _print_graphs ( traj )", "nl": "Extracts monitor data and plots ."}}
{"translation": {"code": "def _print_graphs ( self , traj ) : print_folder = self . _make_folder ( traj ) # If we use BRIAN's own raster_plot functionality we # need to sue the SpikeMonitor directly plt . figure ( ) plt . scatter ( self . spike_monitor . t , self . spike_monitor . i , s = 1 ) plt . xlabel ( 't' ) plt . ylabel ( 'Exc. Neurons' ) plt . title ( 'Spike Raster Plot' ) filename = os . path . join ( print_folder , 'spike.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) fig = plt . figure ( ) self . _plot_result ( traj , 'monitors.V' ) filename = os . path . join ( print_folder , 'V.png' ) print ( 'Current plot: %s ' % filename ) fig . savefig ( filename ) plt . close ( ) plt . figure ( ) self . _plot_result ( traj , 'monitors.I_syn_e' ) filename = os . path . join ( print_folder , 'I_syn_e.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) plt . figure ( ) self . _plot_result ( traj , 'monitors.I_syn_i' ) filename = os . path . join ( print_folder , 'I_syn_i.png' ) print ( 'Current plot: %s ' % filename ) plt . savefig ( filename ) plt . close ( ) if not traj . analysis . show_plots : plt . close ( 'all' ) else : plt . show ( )", "nl": "Makes some plots and stores them into subfolders"}}
{"translation": {"code": "def _plot_result ( self , traj , result_name ) : result = traj . f_get ( result_name ) varname = result . record_variables [ 0 ] values = result [ varname ] times = result . t record = result . record for idx , celia_neuron in enumerate ( record ) : plt . subplot ( len ( record ) , 1 , idx + 1 ) plt . plot ( times , values [ idx , : ] ) if idx == 0 : plt . title ( '%s' % varname ) if idx == 1 : plt . ylabel ( '%s' % ( varname ) ) if idx == len ( record ) - 1 : plt . xlabel ( 't' )", "nl": "Plots a state variable graph for several neurons into one figure"}}
{"translation": {"code": "def add_params ( traj ) : # We set the BrianParameter to be the standard parameter traj . v_standard_parameter = Brian2Parameter traj . v_fast_access = True # Add parameters we need for our network traj . f_add_parameter ( 'Net.C' , 281 * pF ) traj . f_add_parameter ( 'Net.gL' , 30 * nS ) traj . f_add_parameter ( 'Net.EL' , - 70.6 * mV ) traj . f_add_parameter ( 'Net.VT' , - 50.4 * mV ) traj . f_add_parameter ( 'Net.DeltaT' , 2 * mV ) traj . f_add_parameter ( 'Net.tauw' , 40 * ms ) traj . f_add_parameter ( 'Net.a' , 4 * nS ) traj . f_add_parameter ( 'Net.b' , 0.08 * nA ) traj . f_add_parameter ( 'Net.I' , .8 * nA ) traj . f_add_parameter ( 'Net.Vcut' , 'vm > 0*mV' ) # practical threshold condition traj . f_add_parameter ( 'Net.N' , 50 ) eqs = '''\n    dvm/dt=(gL*(EL-vm)+gL*DeltaT*exp((vm-VT)/DeltaT)+I-w)/C : volt\n    dw/dt=(a*(vm-EL)-w)/tauw : amp\n    Vr:volt\n    ''' traj . f_add_parameter ( 'Net.eqs' , eqs ) traj . f_add_parameter ( 'reset' , 'vm=Vr;w+=b' )", "nl": "Adds all necessary parameters to traj ."}}
{"translation": {"code": "def _add_monitors ( self , traj , network , network_dict ) : neurons_e = network_dict [ 'neurons_e' ] monitor_list = [ ] # Spiketimes self . spike_monitor = SpikeMonitor ( neurons_e ) monitor_list . append ( self . spike_monitor ) # Membrane Potential self . V_monitor = StateMonitor ( neurons_e , 'V' , record = list ( traj . neuron_records ) ) monitor_list . append ( self . V_monitor ) # Exc. syn .Current self . I_syn_e_monitor = StateMonitor ( neurons_e , 'I_syn_e' , record = list ( traj . neuron_records ) ) monitor_list . append ( self . I_syn_e_monitor ) # Inh. syn. Current self . I_syn_i_monitor = StateMonitor ( neurons_e , 'I_syn_i' , record = list ( traj . neuron_records ) ) monitor_list . append ( self . I_syn_i_monitor ) # Add monitors to network and dictionary network . add ( * monitor_list ) network_dict [ 'monitors' ] = monitor_list", "nl": "Adds monitors to the network"}}
{"translation": {"code": "def _compute_mean_fano_factor ( neuron_ids , spike_res , time_window , start_time , end_time ) : ffs = np . zeros ( len ( neuron_ids ) ) for idx , neuron_id in enumerate ( neuron_ids ) : ff = CNFanoFactorComputer . _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) ffs [ idx ] = ff mean_ff = np . mean ( ffs ) return mean_ff", "nl": "Computes average Fano Factor over many neurons ."}}
{"translation": {"code": "def _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : assert ( end_time >= start_time + time_window ) # Number of time bins bins = ( end_time - start_time ) / time_window bins = int ( np . floor ( bins ) ) # Arrays for binning of spike counts binned_spikes = np . zeros ( bins ) # DataFrame only containing spikes of the particular neuron spike_array_neuron = spike_res . t [ spike_res . i == neuron_id ] for bin in range ( bins ) : # We iterate over the bins to calculate the spike counts lower_time = start_time + time_window * bin upper_time = start_time + time_window * ( bin + 1 ) # Filter the spikes spike_array_interval = spike_array_neuron [ spike_array_neuron >= lower_time ] spike_array_interval = spike_array_interval [ spike_array_interval < upper_time ] # Add count to bins spikes = len ( spike_array_interval ) binned_spikes [ bin ] = spikes var = np . var ( binned_spikes ) avg = np . mean ( binned_spikes ) if avg > 0 : return var / float ( avg ) else : return 0", "nl": "Computes Fano Factor for one neuron ."}}
{"translation": {"code": "def _build_model ( self , traj , brian_list , network_dict ) : model = traj . parameters . model # Create the equations for both models eqs_dict = self . _build_model_eqs ( traj ) # Create inhibitory neurons eqs_i = eqs_dict [ 'i' ] neurons_i = NeuronGroup ( N = model . N_i , model = eqs_i , threshold = model . V_th , reset = model . reset_func , refractory = model . refractory , method = 'Euler' ) # Create excitatory neurons eqs_e = eqs_dict [ 'e' ] neurons_e = NeuronGroup ( N = model . N_e , model = eqs_e , threshold = model . V_th , reset = model . reset_func , refractory = model . refractory , method = 'Euler' ) # Set the bias terms neurons_e . mu = rand ( model . N_e ) * ( model . mu_e_max - model . mu_e_min ) + model . mu_e_min neurons_i . mu = rand ( model . N_i ) * ( model . mu_i_max - model . mu_i_min ) + model . mu_i_min # Set initial membrane potentials neurons_e . V = rand ( model . N_e ) neurons_i . V = rand ( model . N_i ) # Add both groups to the `brian_list` and the `network_dict` brian_list . append ( neurons_i ) brian_list . append ( neurons_e ) network_dict [ 'neurons_e' ] = neurons_e network_dict [ 'neurons_i' ] = neurons_i", "nl": "Builds the neuron groups from traj ."}}
{"translation": {"code": "def pre_build ( self , traj , brian_list , network_dict ) : self . _pre_build = not _explored_parameters_in_group ( traj , traj . parameters . model ) if self . _pre_build : self . _build_model ( traj , brian_list , network_dict )", "nl": "Pre - builds the neuron groups ."}}
{"translation": {"code": "def _build_model_eqs ( traj ) : model_eqs = traj . model . eqs post_eqs = { } for name_post in [ 'i' , 'e' ] : variables_dict = { } new_model_eqs = model_eqs . replace ( 'POST' , name_post ) for name_pre in [ 'i' , 'e' ] : conn_eqs = traj . model . synaptic . eqs new_conn_eqs = conn_eqs . replace ( 'PRE' , name_pre ) new_model_eqs += new_conn_eqs tau1 = traj . model . synaptic [ 'tau1' ] tau2 = traj . model . synaptic [ 'tau2_' + name_pre ] normalization = ( tau1 - tau2 ) / tau2 invtau1 = 1.0 / tau1 invtau2 = 1.0 / tau2 variables_dict [ 'invtau1_' + name_pre ] = invtau1 variables_dict [ 'invtau2_' + name_pre ] = invtau2 variables_dict [ 'normalization_' + name_pre ] = normalization variables_dict [ 'tau1_' + name_pre ] = tau1 variables_dict [ 'tau2_' + name_pre ] = tau2 variables_dict [ 'tau_' + name_post ] = traj . model [ 'tau_' + name_post ] post_eqs [ name_post ] = Equations ( new_model_eqs , * * variables_dict ) return post_eqs", "nl": "Computes model equations for the excitatory and inhibitory population ."}}
{"translation": {"code": "def add_to_network ( self , traj , network , current_subrun , subrun_list , network_dict ) : if current_subrun . v_annotations . order == 1 : self . _add_monitors ( traj , network , network_dict )", "nl": "Adds monitors to the network if the measurement run is carried out ."}}
{"translation": {"code": "def _get_argspec ( func ) : if inspect . isclass ( func ) : func = func . __init__ if not inspect . isfunction ( func ) : # Init function not existing return [ ] , False parameters = inspect . signature ( func ) . parameters args = [ ] uses_starstar = False for par in parameters . values ( ) : if ( par . kind == inspect . Parameter . POSITIONAL_OR_KEYWORD or par . kind == inspect . Parameter . KEYWORD_ONLY ) : args . append ( par . name ) elif par . kind == inspect . Parameter . VAR_KEYWORD : uses_starstar = True return args , uses_starstar", "nl": "Helper function to support both Python versions"}}