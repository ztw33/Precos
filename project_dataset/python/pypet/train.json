{"translation": {"code": "def cartesian_product ( parameter_dict , combined_parameters = ( ) ) : if not combined_parameters : combined_parameters = list ( parameter_dict ) else : combined_parameters = list ( combined_parameters ) for idx , item in enumerate ( combined_parameters ) : if isinstance ( item , str ) : combined_parameters [ idx ] = ( item , ) iterator_list = [ ] for item_tuple in combined_parameters : inner_iterator_list = [ parameter_dict [ key ] for key in item_tuple ] zipped_iterator = zip ( * inner_iterator_list ) iterator_list . append ( zipped_iterator ) result_dict = { } for key in parameter_dict : result_dict [ key ] = [ ] cartesian_iterator = itools . product ( * iterator_list ) for cartesian_tuple in cartesian_iterator : for idx , item_tuple in enumerate ( combined_parameters ) : for inneridx , key in enumerate ( item_tuple ) : result_dict [ key ] . append ( cartesian_tuple [ idx ] [ inneridx ] ) return result_dict", "nl": "Generates a Cartesian product of the input parameter dictionary ."}}
{"translation": {"code": "def _ann_load_annotations ( self , item_with_annotations , node ) : annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) if annotated : annotations = item_with_annotations . v_annotations # You can only load into non-empty annotations, to prevent overwriting data in RAM if not annotations . f_is_empty ( ) : raise TypeError ( 'Loading into non-empty annotations!' ) current_attrs = node . _v_attrs for attr_name in current_attrs . _v_attrnames : if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : key = attr_name key = key . replace ( HDF5StorageService . ANNOTATION_PREFIX , '' ) data = getattr ( current_attrs , attr_name ) setattr ( annotations , key , data )", "nl": "Loads annotations from disk ."}}
{"translation": {"code": "def f_load_child ( self , name , recursive = False , load_data = pypetconstants . LOAD_DATA , max_depth = None ) : traj = self . _nn_interface . _root_instance storage_service = traj . v_storage_service storage_service . load ( pypetconstants . TREE , self , name , trajectory_name = traj . v_name , load_data = load_data , recursive = recursive , max_depth = max_depth ) return self . f_get ( name , shortcuts = False )", "nl": "Loads a child or recursively a subtree from disk ."}}
{"translation": {"code": "def _grp_store_group ( self , traj_group , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , _hdf5_group = None , _newly_created = False ) : if store_data == pypetconstants . STORE_NOTHING : return elif store_data == pypetconstants . STORE_DATA_SKIPPING and traj_group . _stored : self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % traj_group . v_full_name ) elif not recursive : if _hdf5_group is None : _hdf5_group , _newly_created = self . _all_create_or_get_groups ( traj_group . v_full_name ) overwrite = store_data == pypetconstants . OVERWRITE_DATA if ( traj_group . v_comment != '' and ( HDF5StorageService . COMMENT not in _hdf5_group . _v_attrs or overwrite ) ) : setattr ( _hdf5_group . _v_attrs , HDF5StorageService . COMMENT , traj_group . v_comment ) if ( ( _newly_created or overwrite ) and type ( traj_group ) not in ( nn . NNGroupNode , nn . ConfigGroup , nn . ParameterGroup , nn . DerivedParameterGroup , nn . ResultGroup ) ) : # We only store the name of the class if it is not one of the standard groups, # that are always used. setattr ( _hdf5_group . _v_attrs , HDF5StorageService . CLASS_NAME , traj_group . f_get_class_name ( ) ) self . _ann_store_annotations ( traj_group , _hdf5_group , overwrite = overwrite ) self . _hdf5file . flush ( ) traj_group . _stored = True # Signal completed node loading self . _node_processing_timer . signal_update ( ) if recursive : parent_traj_group = traj_group . f_get_parent ( ) parent_hdf5_group = self . _all_create_or_get_groups ( parent_traj_group . v_full_name ) [ 0 ] self . _tree_store_nodes_dfs ( parent_traj_group , traj_group . v_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , parent_hdf5_group = parent_hdf5_group )", "nl": "Stores a group node ."}}
{"translation": {"code": "def f_store_child ( self , name , recursive = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : if not self . f_contains ( name , shortcuts = False ) : raise ValueError ( 'Your group `%s` does not (directly) contain the child `%s`. ' 'Please not that shortcuts are not allowed for `f_store_child`.' % ( self . v_full_name , name ) ) traj = self . _nn_interface . _root_instance storage_service = traj . v_storage_service storage_service . store ( pypetconstants . TREE , self , name , trajectory_name = traj . v_name , recursive = recursive , store_data = store_data , max_depth = max_depth )", "nl": "Stores a child or recursively a subtree to disk ."}}
{"translation": {"code": "def _prm_extract_missing_flags ( data_dict , flags_dict ) : for key , data in data_dict . items ( ) : if not key in flags_dict : dtype = type ( data ) if ( dtype is np . ndarray or dtype is dict ) and len ( data ) == 0 : # Empty containers are stored as an Array # No need to ask for tuple or list, because they are always # stored as arrays. flags_dict [ key ] = HDF5StorageService . ARRAY continue else : try : flags_dict [ key ] = HDF5StorageService . TYPE_FLAG_MAPPING [ dtype ] except KeyError : raise pex . NoSuchServiceError ( 'I cannot store `%s`, I do not understand the' 'type `%s`.' % ( key , str ( dtype ) ) )", "nl": "Extracts storage flags for data in data_dict if they were not specified in flags_dict ."}}
{"translation": {"code": "def _prm_add_meta_info ( self , instance , group , overwrite = False ) : if overwrite : flags = ( ) else : flags = ( HDF5StorageService . ADD_ROW , ) definitely_store_comment = True try : # Check if we need to store the comment. Maybe update the overview tables # accordingly if the current run index is lower than the one in the table. definitely_store_comment = self . _prm_meta_add_summary ( instance ) try : # Update the summary overview table table_name = instance . v_branch + '_overview' table = getattr ( self . _overview_group , table_name ) if len ( table ) < pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH : self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) except pt . NoSuchNodeError : pass except Exception as exc : self . _logger . error ( 'Could not store information table due to `%s`.' % repr ( exc ) ) if ( ( not self . _purge_duplicate_comments or definitely_store_comment ) and instance . v_comment != '' ) : # Only add the comment if necessary setattr ( group . _v_attrs , HDF5StorageService . COMMENT , instance . v_comment ) # Add class name and whether node is a leaf to the HDF5 attributes setattr ( group . _v_attrs , HDF5StorageService . CLASS_NAME , instance . f_get_class_name ( ) ) setattr ( group . _v_attrs , HDF5StorageService . LEAF , True ) if instance . v_is_parameter and instance . v_explored : # If the stored parameter was an explored one we need to mark this in the # explored overview table try : tablename = 'explored_parameters_overview' table = getattr ( self . _overview_group , tablename ) if len ( table ) < pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH : self . _all_store_param_or_result_table_entry ( instance , table , flags = flags ) except pt . NoSuchNodeError : pass except Exception as exc : self . _logger . error ( 'Could not store information ' 'table due to `%s`.' % repr ( exc ) )", "nl": "Adds information to overview tables and meta information to the instance s hdf5 group ."}}
{"translation": {"code": "def f_get_children ( self , copy = True ) : if copy : return self . _children . copy ( ) else : return self . _children", "nl": "Returns a children dictionary ."}}
{"translation": {"code": "def _prm_store_parameter_or_result ( self , instance , store_data = pypetconstants . STORE_DATA , store_flags = None , overwrite = None , with_links = False , recursive = False , _hdf5_group = None , _newly_created = False , * * kwargs ) : if store_data == pypetconstants . STORE_NOTHING : return elif store_data == pypetconstants . STORE_DATA_SKIPPING and instance . _stored : self . _logger . debug ( 'Already found `%s` on disk I will not store it!' % instance . v_full_name ) return elif store_data == pypetconstants . OVERWRITE_DATA : if not overwrite : overwrite = True fullname = instance . v_full_name self . _logger . debug ( 'Storing `%s`.' % fullname ) if _hdf5_group is None : # If no group is provided we might need to create one _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) # kwargs_flags = {} # Dictionary to change settings # old_kwargs = {} store_dict = { } # If the user did not supply storage flags, we need to set it to the empty dictionary if store_flags is None : store_flags = { } try : # Get the data to store from the instance if not instance . f_is_empty ( ) : store_dict = instance . _store ( ) try : # Ask the instance for storage flags instance_flags = instance . _store_flags ( ) . copy ( ) # copy to avoid modifying the # original data except AttributeError : # If it does not provide any, set it to the empty dictionary instance_flags = { } # User specified flags have priority over the flags from the instance instance_flags . update ( store_flags ) store_flags = instance_flags # If we still have data in `store_dict` about which we do not know how to store # it, pick default storage flags self . _prm_extract_missing_flags ( store_dict , store_flags ) if overwrite : if isinstance ( overwrite , str ) : overwrite = [ overwrite ] if overwrite is True : to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] self . _all_delete_parameter_or_result_or_group ( instance , delete_only = to_delete , _hdf5_group = _hdf5_group ) elif isinstance ( overwrite , ( list , tuple ) ) : overwrite_set = set ( overwrite ) key_set = set ( store_dict . keys ( ) ) stuff_not_to_be_overwritten = overwrite_set - key_set if overwrite != 'v_annotations' and len ( stuff_not_to_be_overwritten ) > 0 : self . _logger . warning ( 'Cannot overwrite `%s`, these items are not supposed to ' 'be stored by the leaf node.' % str ( stuff_not_to_be_overwritten ) ) stuff_to_overwrite = overwrite_set & key_set if len ( stuff_to_overwrite ) > 0 : self . _all_delete_parameter_or_result_or_group ( instance , delete_only = list ( stuff_to_overwrite ) ) else : raise ValueError ( 'Your value of overwrite `%s` is not understood. ' 'Please pass `True` of a list of strings to fine grain ' 'overwriting.' % str ( overwrite ) ) self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) # Store annotations self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) if _newly_created or overwrite is True : # If we created a new group or the parameter was extended we need to # update the meta information and summary tables self . _prm_add_meta_info ( instance , _hdf5_group , overwrite = not _newly_created ) instance . _stored = True #self._logger.debug('Finished Storing `%s`.' % fullname) # Signal completed node loading self . _node_processing_timer . signal_update ( ) except : # I anything fails, we want to remove the data of the parameter again self . _logger . error ( 'Failed storing leaf `%s`. I will remove the hdf5 data I added  again.' % fullname ) # Delete data for key in store_dict . keys ( ) : if key in _hdf5_group : hdf5_child = _hdf5_group . _f_get_child ( key ) hdf5_child . _f_remove ( recursive = True ) # If no data left delete the whole parameter if _hdf5_group . _v_nchildren == 0 : _hdf5_group . _f_remove ( recursive = True ) raise", "nl": "Stores a parameter or result to hdf5 ."}}
{"translation": {"code": "def f_contains ( self , item , with_links = True , shortcuts = False , max_depth = None ) : # Check if an instance or a name was supplied by the user try : search_string = item . v_full_name parent_full_name = self . v_full_name if not search_string . startswith ( parent_full_name ) : return False if parent_full_name != '' : search_string = search_string [ len ( parent_full_name ) + 1 : ] else : search_string = search_string shortcuts = False # if we search for a particular item we do not allow shortcuts except AttributeError : search_string = item item = None if search_string == '' : return False # To allow to search for nodes wit name = '', which are never part # of the trajectory try : result = self . f_get ( search_string , shortcuts = shortcuts , max_depth = max_depth , with_links = with_links ) except AttributeError : return False if item is not None : return id ( item ) == id ( result ) else : return True", "nl": "Checks if the node contains a specific parameter or result ."}}
{"translation": {"code": "def f_remove_child ( self , name , recursive = False , predicate = None ) : if name not in self . _children : raise ValueError ( 'Your group `%s` does not contain the child `%s`.' % ( self . v_full_name , name ) ) else : child = self . _children [ name ] if ( name not in self . _links and not child . v_is_leaf and child . f_has_children ( ) and not recursive ) : raise TypeError ( 'Cannot remove child. It is a group with children. Use' ' f_remove with ``recursive = True``' ) else : self . _nn_interface . _remove_subtree ( self , name , predicate )", "nl": "Removes a child of the group ."}}
{"translation": {"code": "def _prm_write_dict_as_table ( self , key , data_to_store , group , fullname , * * kwargs ) : if key in group : raise ValueError ( 'Dictionary `%s` already exists in `%s`. Appending is not supported (yet).' ) if key in group : raise ValueError ( 'Dict `%s` already exists in `%s`. Appending is not supported (yet).' ) temp_dict = { } for innerkey in data_to_store : val = data_to_store [ innerkey ] temp_dict [ innerkey ] = [ val ] # Convert dictionary to object table objtable = ObjectTable ( data = temp_dict ) # Then store the object table self . _prm_write_into_pytable ( key , objtable , group , fullname , * * kwargs ) new_table = group . _f_get_child ( key ) # Remember that the Object Table represents a dictionary self . _all_set_attributes_to_recall_natives ( temp_dict , new_table , HDF5StorageService . DATA_PREFIX ) setattr ( new_table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . DICT ) self . _hdf5file . flush ( )", "nl": "Stores a python dictionary as pytable"}}
{"translation": {"code": "def _prm_write_pandas_data ( self , key , data , group , fullname , flag , * * kwargs ) : try : if 'filters' not in kwargs : filters = self . _all_get_filters ( kwargs ) kwargs [ 'filters' ] = filters if 'format' not in kwargs : kwargs [ 'format' ] = self . pandas_format if 'encoding' not in kwargs : kwargs [ 'encoding' ] = self . encoding overwrite = kwargs . pop ( 'overwrite' , False ) if key in group and not ( overwrite or kwargs . get ( 'append' , False ) ) : raise ValueError ( 'DataFrame `%s` already exists in `%s`. ' 'To append pass ``append=`True```.' % ( key , fullname ) ) else : self . _logger . debug ( 'Appending to pandas data `%s` in `%s`' % ( key , fullname ) ) if data is not None and ( kwargs [ 'format' ] == 'f' or kwargs [ 'format' ] == 'fixed' ) : kwargs [ 'expectedrows' ] = data . shape [ 0 ] name = group . _v_pathname + '/' + key self . _hdf5store . put ( name , data , * * kwargs ) self . _hdf5store . flush ( ) self . _hdf5file . flush ( ) frame_group = group . _f_get_child ( key ) setattr ( frame_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) self . _hdf5file . flush ( ) except : self . _logger . error ( 'Failed storing pandas data `%s` of `%s`.' % ( key , fullname ) ) raise", "nl": "Stores a pandas DataFrame into hdf5 ."}}
{"translation": {"code": "def _prm_write_into_other_array ( self , key , data , group , fullname , flag , * * kwargs ) : try : if flag == HDF5StorageService . CARRAY : factory = self . _hdf5file . create_carray elif flag == HDF5StorageService . EARRAY : factory = self . _hdf5file . create_earray elif flag == HDF5StorageService . VLARRAY : factory = self . _hdf5file . create_vlarray else : raise RuntimeError ( 'You shall not pass!' ) if key in group : raise ValueError ( 'CArray `%s` already exists in `%s`. Appending is not supported (yet).' ) if 'filters' in kwargs : filters = kwargs . pop ( 'filters' ) else : filters = self . _all_get_filters ( kwargs ) try : other_array = factory ( where = group , name = key , obj = data , filters = filters , * * kwargs ) except ( ValueError , TypeError ) as exc : try : conv_data = data [ : ] conv_data = np . core . defchararray . encode ( conv_data , self . encoding ) other_array = factory ( where = group , name = key , obj = conv_data , filters = filters , * * kwargs ) except Exception : # Re-raise original Error raise exc if data is not None : # Remember the types of the original data to recall them on loading self . _all_set_attributes_to_recall_natives ( data , other_array , HDF5StorageService . DATA_PREFIX ) setattr ( other_array . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) self . _hdf5file . flush ( ) except : self . _logger . error ( 'Failed storing %s `%s` of `%s`.' % ( flag , key , fullname ) ) raise", "nl": "Stores data as carray earray or vlarray depending on flag ."}}
{"translation": {"code": "def _prm_write_into_array ( self , key , data , group , fullname , * * kwargs ) : try : if key in group : raise ValueError ( 'Array `%s` already exists in `%s`. Appending is not supported (yet).' ) try : array = self . _hdf5file . create_array ( where = group , name = key , obj = data , * * kwargs ) except ( TypeError , ValueError ) as exc : try : if type ( data ) is dict and len ( data ) == 0 : # We cannot store an empty dictionary, # but we can use an empty tuple as a dummy. conv_data = ( ) elif isinstance ( data , str ) : conv_data = data . encode ( self . _encoding ) elif isinstance ( data , int ) : conv_data = np . int64 ( data ) else : conv_data = [ ] for string in data : conv_data . append ( string . encode ( self . _encoding ) ) array = self . _hdf5file . create_array ( where = group , name = key , obj = conv_data , * * kwargs ) except Exception : # Re-raise original error raise exc if data is not None : # Remember the types of the original data to recall them on loading self . _all_set_attributes_to_recall_natives ( data , array , HDF5StorageService . DATA_PREFIX ) setattr ( array . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . ARRAY ) self . _hdf5file . flush ( ) except : self . _logger . error ( 'Failed storing array `%s` of `%s`.' % ( key , fullname ) ) raise", "nl": "Stores data as array ."}}
{"translation": {"code": "def _all_delete_parameter_or_result_or_group ( self , instance , delete_only = None , remove_from_item = False , recursive = False , _hdf5_group = None ) : split_name = instance . v_location . split ( '.' ) if _hdf5_group is None : where = '/' + self . _trajectory_name + '/' + '/' . join ( split_name ) node_name = instance . v_name _hdf5_group = self . _hdf5file . get_node ( where = where , name = node_name ) if delete_only is None : if instance . v_is_group and not recursive and len ( _hdf5_group . _v_children ) != 0 : raise TypeError ( 'You cannot remove the group `%s`, it has children, please ' 'use `recursive=True` to enforce removal.' % instance . v_full_name ) _hdf5_group . _f_remove ( recursive = True ) else : if not instance . v_is_leaf : raise ValueError ( 'You can only choose `delete_only` mode for leafs.' ) if isinstance ( delete_only , str ) : delete_only = [ delete_only ] for delete_item in delete_only : if ( remove_from_item and hasattr ( instance , '__contains__' ) and hasattr ( instance , '__delattr__' ) and delete_item in instance ) : delattr ( instance , delete_item ) try : _hdf5_sub_group = self . _hdf5file . get_node ( where = _hdf5_group , name = delete_item ) _hdf5_sub_group . _f_remove ( recursive = True ) except pt . NoSuchNodeError : self . _logger . warning ( 'Could not delete `%s` from `%s`. HDF5 node not found!' % ( delete_item , instance . v_full_name ) )", "nl": "Removes a parameter or result or group from the hdf5 file ."}}
{"translation": {"code": "def _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , * * kwargs ) : datasize = data . shape [ 0 ] try : # Get a new pytables description from the data and create a new table description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) description_dicts = [ { } ] if len ( description_dict ) > ptpa . MAX_COLUMNS : # For optimization we want to store the original data types into another table # and split the tables into several ones new_table_group = self . _hdf5file . create_group ( where = hdf5_group , name = tablename , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) count = 0 for innerkey in description_dict : val = description_dict [ innerkey ] if count == ptpa . MAX_COLUMNS : description_dicts . append ( { } ) count = 0 description_dicts [ - 1 ] [ innerkey ] = val count += 1 setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , 1 ) hdf5_group = new_table_group else : description_dicts = [ description_dict ] for idx , descr_dict in enumerate ( description_dicts ) : if idx == 0 : tblname = tablename else : tblname = tablename + '_%d' % idx table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = datasize , filters = self . _all_get_filters ( kwargs . copy ( ) ) ) row = table . row for n in range ( datasize ) : # Fill the columns with data, note if the parameter was extended nstart!=0 for key in descr_dict : row [ key ] = data [ key ] [ n ] row . append ( ) # Remember the original types of the data for perfect recall if idx == 0 and len ( description_dict ) <= ptpa . MAX_COLUMNS : # We only have a single table and # we can store the original data types as attributes for field_name in data_type_dict : type_description = data_type_dict [ field_name ] self . _all_set_attr ( table , field_name , type_description ) setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . TABLE ) table . flush ( ) self . _hdf5file . flush ( ) if len ( description_dict ) > ptpa . MAX_COLUMNS : # We have potentially many split tables and the data types are # stored into an additional table for performance reasons tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , description = descr_dict , title = tblname , expectedrows = len ( field_names ) , filters = self . _all_get_filters ( kwargs ) ) row = table . row for n in range ( len ( field_names ) ) : # Fill the columns with data for key in data_type_table_dict : row [ key ] = data_type_table_dict [ key ] [ n ] row . append ( ) setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , 1 ) table . flush ( ) self . _hdf5file . flush ( ) except : self . _logger . error ( 'Failed storing table `%s` of `%s`.' % ( tablename , fullname ) ) raise", "nl": "Stores data as pytable ."}}
{"translation": {"code": "def _prm_make_description ( self , data , fullname ) : def _convert_lists_and_tuples ( series_of_data ) : \"\"\"Converts lists and tuples to numpy arrays\"\"\" if isinstance ( series_of_data [ 0 ] , ( list , tuple ) ) : # and not isinstance(series_of_data[0], np.ndarray): # If the first data item is a list, the rest must be as well, since # data has to be homogeneous for idx , item in enumerate ( series_of_data ) : series_of_data [ idx ] = np . array ( item ) descriptiondict = { } # dictionary containing the description to build a pytables table original_data_type_dict = { } # dictionary containing the original data types for key in data : val = data [ key ] # remember the original data types self . _all_set_attributes_to_recall_natives ( val [ 0 ] , PTItemMock ( original_data_type_dict ) , HDF5StorageService . FORMATTED_COLUMN_PREFIX % key ) _convert_lists_and_tuples ( val ) # get a pytables column from the data col = self . _all_get_table_col ( key , val , fullname ) descriptiondict [ key ] = col return descriptiondict , original_data_type_dict", "nl": "Returns a description dictionary for pytables table creation"}}
{"translation": {"code": "def _add_to_tree ( self , start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs ) : # Then walk iteratively from the start node as specified by the new name and create # new empty groups on the fly try : act_node = start_node last_idx = len ( split_names ) - 1 add_link = type_name == LINK link_added = False # last_name = start_node.v_crun for idx , name in enumerate ( split_names ) : if name not in act_node . _children : if idx == last_idx : if add_link : new_node = self . _create_link ( act_node , name , instance ) link_added = True elif group_type_name != type_name : # We are at the end of the chain and we add a leaf node new_node = self . _create_any_param_or_result ( act_node , name , type_name , instance , constructor , args , kwargs ) self . _flat_leaf_storage_dict [ new_node . v_full_name ] = new_node else : # We add a group as desired new_node = self . _create_any_group ( act_node , name , group_type_name , instance , constructor , args , kwargs ) else : # We add a group on the fly new_node = self . _create_any_group ( act_node , name , group_type_name ) if name in self . _root_instance . _run_information : self . _root_instance . _run_parent_groups [ act_node . v_full_name ] = act_node if self . _root_instance . _is_run : if link_added : self . _root_instance . _new_links [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) else : self . _root_instance . _new_nodes [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) else : if name in act_node . _links : raise AttributeError ( 'You cannot hop over links when adding ' 'data to the tree. ' 'There is a link called `%s` under `%s`.' % ( name , act_node . v_full_name ) ) if idx == last_idx : if self . _root_instance . _no_clobber : self . _logger . warning ( 'You already have a group/instance/link `%s` ' 'under `%s`. ' 'However, you set `v_no_clobber=True`, ' 'so I will ignore your addition of ' 'data.' % ( name , act_node . v_full_name ) ) else : raise AttributeError ( 'You already have a group/instance/link `%s` ' 'under `%s`' % ( name , act_node . v_full_name ) ) act_node = act_node . _children [ name ] return act_node except : self . _logger . error ( 'Failed adding `%s` under `%s`.' % ( name , start_node . v_full_name ) ) raise", "nl": "Adds a new item to the tree ."}}
{"translation": {"code": "def load ( self , msg , stuff_to_load , * args , * * kwargs ) : opened = True try : opened = self . _srvc_opening_routine ( 'r' , kwargs = kwargs ) if msg == pypetconstants . TRAJECTORY : self . _trj_load_trajectory ( stuff_to_load , * args , * * kwargs ) elif msg == pypetconstants . LEAF : self . _prm_load_parameter_or_result ( stuff_to_load , * args , * * kwargs ) elif msg == pypetconstants . GROUP : self . _grp_load_group ( stuff_to_load , * args , * * kwargs ) elif msg == pypetconstants . TREE : self . _tree_load_sub_branch ( stuff_to_load , * args , * * kwargs ) elif msg == pypetconstants . LIST : self . _srvc_load_several_items ( stuff_to_load , * args , * * kwargs ) else : raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) except pt . NoSuchNodeError as exc : self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) raise pex . DataNotInStorageError ( repr ( exc ) ) except : self . _logger . error ( 'Failed loading  `%s`' % str ( stuff_to_load ) ) raise finally : self . _srvc_closing_routine ( opened )", "nl": "Loads a particular item from disk ."}}
{"translation": {"code": "def _check_names ( self , split_names , parent_node = None ) : faulty_names = '' if parent_node is not None and parent_node . v_is_root and split_names [ 0 ] == 'overview' : faulty_names = '%s `overview` cannot be added directly under the root node ' 'this is a reserved keyword,' % ( faulty_names ) for split_name in split_names : if len ( split_name ) == 0 : faulty_names = '%s `%s` contains no characters, please use at least 1,' % ( faulty_names , split_name ) elif split_name . startswith ( '_' ) : faulty_names = '%s `%s` starts with a leading underscore,' % ( faulty_names , split_name ) elif re . match ( CHECK_REGEXP , split_name ) is None : faulty_names = '%s `%s` contains non-admissible characters ' '(use only [A-Za-z0-9_-]),' % ( faulty_names , split_name ) elif '$' in split_name : if split_name not in self . _root_instance . _wildcard_keys : faulty_names = '%s `%s` contains `$` but has no associated ' 'wildcard function,' % ( faulty_names , split_name ) elif split_name in self . _not_admissible_names : warnings . warn ( '`%s` is a method/attribute of the ' 'trajectory/treenode/naminginterface, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) elif split_name in self . _python_keywords : warnings . warn ( '`%s` is a python keyword, you may not be ' 'able to access it via natural naming but only by using ' '`[]` square bracket notation. ' % split_name , category = SyntaxWarning ) name = split_names [ - 1 ] if len ( name ) >= pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH : faulty_names = '%s `%s` is too long the name can only have %d characters but it has ' '%d,' % ( faulty_names , name , len ( name ) , pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) return faulty_names", "nl": "Checks if a list contains strings with invalid names ."}}
{"translation": {"code": "def _create_any_group ( self , parent_node , name , type_name , instance = None , constructor = None , args = None , kwargs = None ) : if args is None : args = [ ] if kwargs is None : kwargs = { } full_name = self . _make_full_name ( parent_node . v_full_name , name ) if instance is None : if constructor is None : if type_name == RESULT_GROUP : constructor = ResultGroup elif type_name == PARAMETER_GROUP : constructor = ParameterGroup elif type_name == CONFIG_GROUP : constructor = ConfigGroup elif type_name == DERIVED_PARAMETER_GROUP : constructor = DerivedParameterGroup elif type_name == GROUP : constructor = NNGroupNode else : raise RuntimeError ( 'You shall not pass!' ) instance = self . _root_instance . _construct_instance ( constructor , full_name , * args , * * kwargs ) else : instance . _rename ( full_name ) # Check if someone tries to add a particular standard group to a branch where # it does not belong: if type_name == RESULT_GROUP : if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under results' % str ( type ( instance ) ) ) elif type_name == PARAMETER_GROUP : if type ( instance ) in ( NNGroupNode , ResultGroup , ConfigGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under parameters' % str ( type ( instance ) ) ) elif type_name == CONFIG_GROUP : if type ( instance ) in ( NNGroupNode , ParameterGroup , ResultGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under config' % str ( type ( instance ) ) ) elif type_name == DERIVED_PARAMETER_GROUP : if type ( instance ) in ( NNGroupNode , ParameterGroup , ConfigGroup , ResultGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under derived ' 'parameters' % str ( type ( instance ) ) ) elif type_name == GROUP : if type ( instance ) in ( ResultGroup , ParameterGroup , ConfigGroup , DerivedParameterGroup ) : raise TypeError ( 'You cannot add a `%s` type of group under other data' % str ( type ( instance ) ) ) else : raise RuntimeError ( 'You shall not pass!' ) self . _set_details_tree_node ( parent_node , name , instance ) instance . _nn_interface = self self . _root_instance . _all_groups [ instance . v_full_name ] = instance self . _add_to_nodes_and_leaves ( instance ) parent_node . _children [ name ] = instance parent_node . _groups [ name ] = instance return instance", "nl": "Generically creates a new group inferring from the type_name ."}}
{"translation": {"code": "def _create_any_param_or_result ( self , parent_node , name , type_name , instance , constructor , args , kwargs ) : root = self . _root_instance full_name = self . _make_full_name ( parent_node . v_full_name , name ) if instance is None : if constructor is None : if type_name == RESULT : constructor = root . _standard_result elif type_name in [ PARAMETER , CONFIG , DERIVED_PARAMETER ] : constructor = root . _standard_parameter else : constructor = root . _standard_leaf instance = root . _construct_instance ( constructor , full_name , * args , * * kwargs ) else : instance . _rename ( full_name ) self . _set_details_tree_node ( parent_node , name , instance ) where_dict = self . _map_type_to_dict ( type_name ) full_name = instance . _full_name if full_name in where_dict : raise AttributeError ( full_name + ' is already part of trajectory,' ) if type_name != RESULT and full_name in root . _changed_default_parameters : self . _logger . info ( 'You have marked parameter %s for change before, so here you go!' % full_name ) change_args , change_kwargs = root . _changed_default_parameters . pop ( full_name ) instance . f_set ( * change_args , * * change_kwargs ) where_dict [ full_name ] = instance self . _add_to_nodes_and_leaves ( instance ) parent_node . _children [ name ] = instance parent_node . _leaves [ name ] = instance if full_name in self . _root_instance . _explored_parameters : instance . _explored = True # Mark this parameter as explored. self . _root_instance . _explored_parameters [ full_name ] = instance self . _logger . debug ( 'Added `%s` to trajectory.' % full_name ) return instance", "nl": "Generically creates a novel parameter or result instance inferring from the type_name ."}}
{"translation": {"code": "def _prm_read_array ( self , array , full_name ) : try : result = self . _svrc_read_array ( array ) # Recall original data types result , dummy = self . _all_recall_native_type ( result , array , HDF5StorageService . DATA_PREFIX ) return result except : self . _logger . error ( 'Failed loading `%s` of `%s`.' % ( array . _v_name , full_name ) ) raise", "nl": "Reads data from an array or carray"}}
{"translation": {"code": "def f_add_parameter_group ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = PARAMETER_GROUP , group_type_name = PARAMETER_GROUP , args = args , kwargs = kwargs )", "nl": "Adds an empty parameter group under the current node ."}}
{"translation": {"code": "def _iter_nodes ( self , node , recursive = False , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : def _run_predicate ( x , run_name_set ) : branch = x . v_run_branch return branch == 'trajectory' or branch in run_name_set if max_depth is None : max_depth = float ( 'inf' ) if predicate is None : predicate = lambda x : True elif isinstance ( predicate , ( tuple , list ) ) : # Create a predicate from a list of run names or run indices run_list = predicate run_name_set = set ( ) for item in run_list : if item == - 1 : run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - 1 ) ) elif isinstance ( item , int ) : run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) else : run_name_set . add ( item ) predicate = lambda x : _run_predicate ( x , run_name_set ) if recursive : return NaturalNamingInterface . _recursive_traversal_bfs ( node , self . _root_instance . _linked_by , max_depth , with_links , in_search , predicate ) else : iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if predicate ( x [ 2 ] ) ) if in_search : return iterator # Here we return tuples: (depth, name, object) else : return ( x [ 2 ] for x in iterator )", "nl": "Returns an iterator over nodes hanging below a given start node ."}}
{"translation": {"code": "def _search ( self , node , key , max_depth = float ( 'inf' ) , with_links = True , crun = None ) : # If we find it directly there is no need for an exhaustive search if key in node . _children and ( with_links or key not in node . _links ) : return node . _children [ key ] , 1 # First the very fast search is tried that does not need tree traversal. try : result = self . _very_fast_search ( node , key , max_depth , with_links , crun ) if result : return result except pex . TooManyGroupsError : pass except pex . NotUniqueNodeError : pass # Slowly traverse the entire tree nodes_iterator = self . _iter_nodes ( node , recursive = True , max_depth = max_depth , in_search = True , with_links = with_links ) result_node = None result_depth = float ( 'inf' ) for depth , name , child in nodes_iterator : if depth > result_depth : # We can break here because we enter a deeper stage of the tree and we # cannot find matching node of the same depth as the one we found break if key == name : # If result_node is not None means that we care about uniqueness and the search # has found more than a single solution. if result_node is not None : raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once within ' 'the same depth %d. ' 'Full name of first occurrence is `%s` and of ' 'second `%s`' % ( key , child . v_depth , result_node . v_full_name , child . v_full_name ) ) result_node = child result_depth = depth return result_node , result_depth", "nl": "Searches for an item in the tree below node"}}
{"translation": {"code": "def _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : result_list = [ ] # Result list of all found items full_name_set = set ( ) # Set containing full names of all found items to avoid finding items # twice due to links colon_name = '.' . join ( split_name ) key = split_name [ - 1 ] candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) parent_full_name = start_node . v_full_name split_length = len ( split_name ) for candidate_name in candidate_dict : # Check if candidate startswith the parent's name candidate = candidate_dict [ candidate_name ] if key != candidate . v_name or candidate . v_full_name in full_name_set : # If this is not the case we do have link, that we need to skip continue if candidate_name . startswith ( parent_full_name ) : if parent_full_name != '' : reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] else : reduced_candidate_name = candidate_name candidate_split_name = reduced_candidate_name . split ( '.' ) if len ( candidate_split_name ) > max_depth : break if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : result_list . append ( candidate ) full_name_set . add ( candidate . v_full_name ) elif shortcuts : candidate_set = set ( candidate_split_name ) climbing = True for name in split_name : if name not in candidate_set : climbing = False break if climbing : count = 0 candidate_length = len ( candidate_split_name ) for idx in range ( candidate_length ) : if idx + split_length - count > candidate_length : break if split_name [ count ] == candidate_split_name [ idx ] : count += 1 if count == len ( split_name ) : result_list . append ( candidate ) full_name_set . add ( candidate . v_full_name ) break return result_list", "nl": "Performs a backwards search from the terminal node back to the start node"}}
{"translation": {"code": "def _prm_read_table ( self , table_or_group , full_name ) : try : result_table = None if self . _all_get_from_attrs ( table_or_group , HDF5StorageService . SPLIT_TABLE ) : table_name = table_or_group . _v_name data_type_table_name = table_name + '__' + HDF5StorageService . STORAGE_TYPE data_type_table = table_or_group . _v_children [ data_type_table_name ] data_type_dict = { } for row in data_type_table : fieldname = row [ 'field_name' ] . decode ( 'utf-8' ) data_type_dict [ fieldname ] = row [ 'data_type' ] . decode ( 'utf-8' ) for sub_table in table_or_group : sub_table_name = sub_table . _v_name if sub_table_name == data_type_table_name : continue for colname in sub_table . colnames : # Read Data column by column col = sub_table . col ( colname ) data_list = list ( col ) prefix = HDF5StorageService . FORMATTED_COLUMN_PREFIX % colname for idx , data in enumerate ( data_list ) : # Recall original type of data data , type_changed = self . _all_recall_native_type ( data , PTItemMock ( data_type_dict ) , prefix ) if type_changed : data_list [ idx ] = data else : break # Construct or insert into an ObjectTable if result_table is None : result_table = ObjectTable ( data = { colname : data_list } ) else : result_table [ colname ] = data_list else : for colname in table_or_group . colnames : # Read Data column by column col = table_or_group . col ( colname ) data_list = list ( col ) prefix = HDF5StorageService . FORMATTED_COLUMN_PREFIX % colname for idx , data in enumerate ( data_list ) : # Recall original type of data data , type_changed = self . _all_recall_native_type ( data , table_or_group , prefix ) if type_changed : data_list [ idx ] = data else : break # Construct or insert into an ObjectTable if result_table is None : result_table = ObjectTable ( data = { colname : data_list } ) else : result_table [ colname ] = data_list return result_table except : self . _logger . error ( 'Failed loading `%s` of `%s`.' % ( table_or_group . _v_name , full_name ) ) raise", "nl": "Reads a non - nested PyTables table column by column and created a new ObjectTable for the loaded data ."}}
{"translation": {"code": "def _prm_read_dictionary ( self , leaf , full_name ) : try : # Load as Pbject Table temp_table = self . _prm_read_table ( leaf , full_name ) # Turn the ObjectTable into a dictionary of lists (with length 1). temp_dict = temp_table . to_dict ( 'list' ) innder_dict = { } # Turn the dictionary of lists into a normal dictionary for innerkey , vallist in temp_dict . items ( ) : innder_dict [ innerkey ] = vallist [ 0 ] return innder_dict except : self . _logger . error ( 'Failed loading `%s` of `%s`.' % ( leaf . _v_name , full_name ) ) raise", "nl": "Loads data that was originally a dictionary when stored"}}
{"translation": {"code": "def _prm_get_longest_stringsize ( string_list ) : maxlength = 1 for stringar in string_list : if isinstance ( stringar , np . ndarray ) : if stringar . ndim > 0 : for string in stringar . ravel ( ) : maxlength = max ( len ( string ) , maxlength ) else : maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) else : maxlength = max ( len ( stringar ) , maxlength ) # Make the string Col longer than needed in order to allow later on slightly larger strings return int ( maxlength * 1.5 )", "nl": "Returns the longest string size for a string entry across data ."}}
{"translation": {"code": "def _all_get_table_col ( self , key , column , fullname ) : val = column [ 0 ] try : # # We do not want to loose int_ if type ( val ) is int : return pt . IntCol ( ) if isinstance ( val , ( str , bytes ) ) : itemsize = int ( self . _prm_get_longest_stringsize ( column ) ) return pt . StringCol ( itemsize ) if isinstance ( val , np . ndarray ) : if ( np . issubdtype ( val . dtype , str ) or np . issubdtype ( val . dtype , bytes ) ) : itemsize = int ( self . _prm_get_longest_stringsize ( column ) ) return pt . StringCol ( itemsize , shape = val . shape ) else : return pt . Col . from_dtype ( np . dtype ( ( val . dtype , val . shape ) ) ) else : return pt . Col . from_dtype ( np . dtype ( type ( val ) ) ) except Exception : self . _logger . error ( 'Failure in storing `%s` of Parameter/Result `%s`.' ' Its type was `%s`.' % ( key , fullname , repr ( type ( val ) ) ) ) raise", "nl": "Creates a pytables column instance ."}}
{"translation": {"code": "def _very_fast_search ( self , node , key , max_depth , with_links , crun ) : if key in self . _links_count : return parent_full_name = node . v_full_name starting_depth = node . v_depth candidate_dict = self . _get_candidate_dict ( key , crun ) # If there are to many potential candidates sequential search might be too slow if with_links : upper_bound = 1 else : upper_bound = FAST_UPPER_BOUND if len ( candidate_dict ) > upper_bound : raise pex . TooManyGroupsError ( 'Too many nodes' ) # Next check if the found candidates could be reached from the parent node result_node = None for goal_name in candidate_dict : # Check if we have found a matching node if goal_name . startswith ( parent_full_name ) : candidate = candidate_dict [ goal_name ] if candidate . v_depth - starting_depth <= max_depth : # In case of several solutions raise an error: if result_node is not None : raise pex . NotUniqueNodeError ( 'Node `%s` has been found more than once, ' 'full name of first occurrence is `%s` and of' 'second `%s`' % ( key , goal_name , result_node . v_full_name ) ) result_node = candidate if result_node is not None : return result_node , result_node . v_depth", "nl": "Fast search for a node in the tree ."}}
{"translation": {"code": "def _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , add_prefix = True , check_naming = True ) : args = list ( args ) create_new = True name = '' instance = None constructor = None add_link = type_name == LINK # First check if the item is already a given instance or we want to add a link if add_link : name = args [ 0 ] instance = args [ 1 ] create_new = False elif len ( args ) == 1 and len ( kwargs ) == 0 : item = args [ 0 ] try : name = item . v_full_name instance = item create_new = False except AttributeError : pass # If the item is not an instance yet, check if args[0] is a class and args[1] is # a string describing the new name of the instance. # If args[0] is not a class it is assumed to be the name of the new instance. if create_new : if len ( args ) > 0 and inspect . isclass ( args [ 0 ] ) : constructor = args . pop ( 0 ) if len ( args ) > 0 and isinstance ( args [ 0 ] , str ) : name = args . pop ( 0 ) elif 'name' in kwargs : name = kwargs . pop ( 'name' ) elif 'full_name' in kwargs : name = kwargs . pop ( 'full_name' ) else : raise ValueError ( 'Could not determine a name of the new item you want to add. ' 'Either pass the name as positional argument or as a keyword ' 'argument `name`.' ) split_names = name . split ( '.' ) if check_naming : for idx , name in enumerate ( split_names ) : translated_shortcut , name = self . _translate_shortcut ( name ) replaced , name = self . _replace_wildcards ( name ) if translated_shortcut or replaced : split_names [ idx ] = name # First check if the naming of the new item is appropriate faulty_names = self . _check_names ( split_names , start_node ) if faulty_names : full_name = '.' . join ( split_names ) raise ValueError ( 'Your Parameter/Result/Node `%s` contains the following not admissible names: ' '%s please choose other names.' % ( full_name , faulty_names ) ) if add_link : if instance is None : raise ValueError ( 'You must provide an instance to link to!' ) if instance . v_is_root : raise ValueError ( 'You cannot create a link to the root node' ) if start_node . v_is_root and name in SUBTREE_MAPPING : raise ValueError ( '`%s` is a reserved name for a group under root.' % name ) if not self . _root_instance . f_contains ( instance , with_links = False , shortcuts = False ) : raise ValueError ( 'You can only link to items within the trajectory tree!' ) # Check if the name fulfils the prefix conditions, if not change the name accordingly. if add_prefix : split_names = self . _add_prefix ( split_names , start_node , group_type_name ) if group_type_name == GROUP : add_leaf = type_name != group_type_name and not add_link # If this is equal we add a group node group_type_name , type_name = self . _determine_types ( start_node , split_names [ 0 ] , add_leaf , add_link ) # Check if we are allowed to add the data if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : raise TypeError ( 'You are not allowed to add config or parameter data or groups ' 'during a single run.' ) return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , constructor , args , kwargs )", "nl": "Adds a given item to the tree irrespective of the subtree ."}}
{"translation": {"code": "def f_add_parameter ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = PARAMETER , group_type_name = PARAMETER_GROUP , args = args , kwargs = kwargs )", "nl": "Adds a parameter under the current node ."}}
{"translation": {"code": "def f_add_result ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = RESULT , group_type_name = RESULT_GROUP , args = args , kwargs = kwargs )", "nl": "Adds a result under the current node ."}}
{"translation": {"code": "def store ( self , msg , stuff_to_store , * args , * * kwargs ) : opened = True try : opened = self . _srvc_opening_routine ( 'a' , msg , kwargs ) if msg == pypetconstants . MERGE : self . _trj_merge_trajectories ( * args , * * kwargs ) elif msg == pypetconstants . BACKUP : self . _trj_backup_trajectory ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . PREPARE_MERGE : self . _trj_prepare_merge ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . TRAJECTORY : self . _trj_store_trajectory ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . SINGLE_RUN : self . _srn_store_single_run ( stuff_to_store , * args , * * kwargs ) elif msg in pypetconstants . LEAF : self . _prm_store_parameter_or_result ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . DELETE : self . _all_delete_parameter_or_result_or_group ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . GROUP : self . _grp_store_group ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . TREE : self . _tree_store_sub_branch ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . DELETE_LINK : self . _lnk_delete_link ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . LIST : self . _srvc_store_several_items ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . ACCESS_DATA : return self . _hdf5_interact_with_data ( stuff_to_store , * args , * * kwargs ) elif msg == pypetconstants . OPEN_FILE : opened = False # Wee need to keep the file open to allow later interaction self . _keep_open = True self . _node_processing_timer . active = False # This might be open quite long # so we don't want to display horribly long opening times elif msg == pypetconstants . CLOSE_FILE : opened = True # Simply conduct the closing routine afterwards self . _keep_open = False elif msg == pypetconstants . FLUSH : self . _hdf5file . flush ( ) else : raise pex . NoSuchServiceError ( 'I do not know how to handle `%s`' % msg ) except : self . _logger . error ( 'Failed storing `%s`' % str ( stuff_to_store ) ) raise finally : self . _srvc_closing_routine ( opened )", "nl": "Stores a particular item to disk ."}}
{"translation": {"code": "def _srvc_load_several_items ( self , iterable , * args , * * kwargs ) : for input_tuple in iterable : msg = input_tuple [ 0 ] item = input_tuple [ 1 ] if len ( input_tuple ) > 2 : args = input_tuple [ 2 ] if len ( input_tuple ) > 3 : kwargs = input_tuple [ 3 ] if len ( input_tuple ) > 4 : raise RuntimeError ( 'You shall not pass!' ) self . load ( msg , item , * args , * * kwargs )", "nl": "Loads several items from an iterable"}}
{"translation": {"code": "def _srvc_store_several_items ( self , iterable , * args , * * kwargs ) : for input_tuple in iterable : msg = input_tuple [ 0 ] item = input_tuple [ 1 ] if len ( input_tuple ) > 2 : args = input_tuple [ 2 ] if len ( input_tuple ) > 3 : kwargs = input_tuple [ 3 ] if len ( input_tuple ) > 4 : raise RuntimeError ( 'You shall not pass!' ) self . store ( msg , item , * args , * * kwargs )", "nl": "Stores several items from an iterable"}}
{"translation": {"code": "def _srvc_closing_routine ( self , closing ) : if ( not self . _keep_open and closing and self . is_open ) : f_fd = self . _hdf5file . fileno ( ) self . _hdf5file . flush ( ) try : os . fsync ( f_fd ) try : self . _hdf5store . flush ( fsync = True ) except TypeError : f_fd = self . _hdf5store . _handle . fileno ( ) self . _hdf5store . flush ( ) os . fsync ( f_fd ) except OSError as exc : # This seems to be the only way to avoid an OSError under Windows errmsg = ( 'Encountered OSError while flushing file.' 'If you are using Windows, don`t worry! ' 'I will ignore the error and try to close the file. ' 'Original error: %s' % repr ( exc ) ) self . _logger . debug ( errmsg ) self . _hdf5store . close ( ) if self . _hdf5file . isopen : self . _logger . error ( 'Could not close HDF5 file!' ) self . _hdf5file = None self . _hdf5store = None self . _trajectory_group = None self . _trajectory_name = None self . _trajectory_index = None self . _overview_group_ = None self . _logger . debug ( 'Closing HDF5 file' ) return True else : return False", "nl": "Routine to close an hdf5 file"}}
{"translation": {"code": "def _srvc_extract_file_information ( self , kwargs ) : if 'filename' in kwargs : self . _filename = kwargs . pop ( 'filename' ) if 'file_title' in kwargs : self . _file_title = kwargs . pop ( 'file_title' ) if 'trajectory_name' in kwargs : self . _trajectory_name = kwargs . pop ( 'trajectory_name' ) if 'trajectory_index' in kwargs : self . _trajectory_index = kwargs . pop ( 'trajectory_index' )", "nl": "Extracts file information from kwargs ."}}
{"translation": {"code": "def _trj_backup_trajectory ( self , traj , backup_filename = None ) : self . _logger . info ( 'Storing backup of %s.' % traj . v_name ) mypath , _ = os . path . split ( self . _filename ) if backup_filename is None : backup_filename = os . path . join ( '%s' % mypath , 'backup_%s.hdf5' % traj . v_name ) backup_hdf5file = pt . open_file ( filename = backup_filename , mode = 'a' , title = backup_filename ) if '/' + self . _trajectory_name in backup_hdf5file : raise ValueError ( 'I cannot backup  `%s` into file `%s`, there is already a ' 'trajectory with that name.' % ( traj . v_name , backup_filename ) ) backup_root = backup_hdf5file . root self . _trajectory_group . _f_copy ( newparent = backup_root , recursive = True ) backup_hdf5file . flush ( ) backup_hdf5file . close ( ) self . _logger . info ( 'Finished backup of %s.' % traj . v_name )", "nl": "Backs up a trajectory ."}}
{"translation": {"code": "def _trj_prepare_merge ( self , traj , changed_parameters , old_length ) : if not traj . _stored : traj . f_store ( ) # Update meta information infotable = getattr ( self . _overview_group , 'info' ) insert_dict = self . _all_extract_insert_dict ( traj , infotable . colnames ) self . _all_add_or_modify_row ( traj . v_name , insert_dict , infotable , index = 0 , flags = ( HDF5StorageService . MODIFY_ROW , ) ) # Store extended parameters for param_name in changed_parameters : param = traj . f_get ( param_name ) try : self . _all_delete_parameter_or_result_or_group ( param ) except pt . NoSuchNodeError : pass # We are fine and the node did not exist in the first place # Increase the run table by the number of new runs run_table = getattr ( self . _overview_group , 'runs' ) actual_rows = run_table . nrows self . _trj_fill_run_table ( traj , actual_rows , len ( traj ) ) # Extract parameter summary and if necessary create new explored parameter tables # in the result groups for idx in range ( old_length , len ( traj ) ) : run_name = traj . f_idx_to_run ( idx ) run_info = traj . f_get_run_information ( run_name ) run_info [ 'name' ] = run_name traj . _set_explored_parameters_to_idx ( idx ) run_summary = self . _srn_summarize_explored_parameters ( list ( traj . _explored_parameters . values ( ) ) ) run_info [ 'parameter_summary' ] = run_summary self . _all_add_or_modify_row ( run_name , run_info , run_table , index = idx , flags = ( HDF5StorageService . MODIFY_ROW , ) ) traj . f_restore_default ( )", "nl": "Prepares a trajectory for merging ."}}
{"translation": {"code": "def _trj_load_meta_data ( self , traj , load_data , as_new , with_run_information , force ) : metatable = self . _overview_group . info metarow = metatable [ 0 ] try : version = metarow [ 'version' ] . decode ( 'utf-8' ) except ( IndexError , ValueError ) as ke : self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) version = '`COULD NOT BE LOADED`' try : python = metarow [ 'python' ] . decode ( 'utf-8' ) except ( IndexError , ValueError ) as ke : self . _logger . error ( 'Could not check version due to: %s' % str ( ke ) ) python = '`COULD NOT BE LOADED`' self . _trj_check_version ( version , python , force ) # Load the skeleton information self . _grp_load_group ( traj , load_data = load_data , with_links = False , recursive = False , _traj = traj , _as_new = as_new , _hdf5_group = self . _trajectory_group ) if as_new : length = int ( metarow [ 'length' ] ) for irun in range ( length ) : traj . _add_run_info ( irun ) else : traj . _comment = metarow [ 'comment' ] . decode ( 'utf-8' ) traj . _timestamp = float ( metarow [ 'timestamp' ] ) traj . _trajectory_timestamp = traj . _timestamp traj . _time = metarow [ 'time' ] . decode ( 'utf-8' ) traj . _trajectory_time = traj . _time traj . _name = metarow [ 'name' ] . decode ( 'utf-8' ) traj . _trajectory_name = traj . _name traj . _version = version traj . _python = python single_run_table = self . _overview_group . runs if with_run_information : for row in single_run_table . iterrows ( ) : name = row [ 'name' ] . decode ( 'utf-8' ) idx = int ( row [ 'idx' ] ) timestamp = float ( row [ 'timestamp' ] ) time_ = row [ 'time' ] . decode ( 'utf-8' ) completed = int ( row [ 'completed' ] ) summary = row [ 'parameter_summary' ] . decode ( 'utf-8' ) hexsha = row [ 'short_environment_hexsha' ] . decode ( 'utf-8' ) # To allow backwards compatibility we need this try catch block try : runtime = row [ 'runtime' ] . decode ( 'utf-8' ) finish_timestamp = float ( row [ 'finish_timestamp' ] ) except ( IndexError , ValueError ) as ke : runtime = '' finish_timestamp = 0.0 self . _logger . debug ( 'Could not load runtime, ' + repr ( ke ) ) info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time_ , 'completed' : completed , 'name' : name , 'parameter_summary' : summary , 'short_environment_hexsha' : hexsha } traj . _add_run_info ( * * info_dict ) else : traj . _length = single_run_table . nrows # Load explorations self . _trj_load_exploration ( traj ) # Load the hdf5 config data: self . _srvc_load_hdf5_settings ( )", "nl": "Loads meta information about the trajectory"}}
{"translation": {"code": "def _tree_load_sub_branch ( self , traj_node , branch_name , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _trajectory = None , _as_new = False , _hdf5_group = None ) : if load_data == pypetconstants . LOAD_NOTHING : return if max_depth is None : max_depth = float ( 'inf' ) if _trajectory is None : _trajectory = traj_node . v_root if _hdf5_group is None : hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) # Get child node to load if hdf5_group_name == '' : _hdf5_group = self . _trajectory_group else : try : _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_group_name ) except pt . NoSuchNodeError : self . _logger . error ( 'Cannot find `%s` the hdf5 node `%s` does not exist!' % ( traj_node . v_full_name , hdf5_group_name ) ) raise split_names = branch_name . split ( '.' ) final_group_name = split_names . pop ( ) current_depth = 1 for name in split_names : if current_depth > max_depth : return # First load along the branch _hdf5_group = getattr ( _hdf5_group , name ) self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group ) current_depth += 1 traj_node = traj_node . _children [ name ] if current_depth <= max_depth : # Then load recursively all data in the last group and below _hdf5_group = getattr ( _hdf5_group , final_group_name ) self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , trajectory = _trajectory , as_new = _as_new , hdf5_group = _hdf5_group )", "nl": "Loads data starting from a node along a branch and starts recursively loading all data at end of branch ."}}
{"translation": {"code": "def _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : for table_name in tables_to_make : # Prepare the tables desciptions, depending on which overview table we create # we need different columns paramdescriptiondict = { } expectedrows = 0 # Every overview table has a name and location column paramdescriptiondict [ 'location' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , pos = 0 ) paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , pos = 1 ) paramdescriptiondict [ 'comment' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) paramdescriptiondict [ 'value' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) if table_name == 'config_overview' : if traj is not None : expectedrows = len ( traj . _config ) if table_name == 'parameters_overview' : if traj is not None : expectedrows = len ( traj . _parameters ) if table_name == 'explored_parameters_overview' : paramdescriptiondict [ 'range' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) paramdescriptiondict [ 'length' ] = pt . IntCol ( ) if traj is not None : expectedrows = len ( traj . _explored_parameters ) if table_name . endswith ( 'summary' ) : paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) # Check if the user provided an estimate of the amount of results per run # This can help to speed up storing if table_name == 'derived_parameters_overview' : expectedrows = self . _derived_parameters_per_run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . _derived_parameters ) if table_name == 'results_overview' : expectedrows = self . _results_per_run if traj is not None : expectedrows *= len ( traj ) expectedrows += len ( traj . _results ) if expectedrows > 0 : paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict , expectedrows = expectedrows ) else : paramtable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = table_name , description = paramdescriptiondict ) paramtable . flush ( )", "nl": "Creates the overview tables in overview group"}}
{"translation": {"code": "def _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : if not only_init : self . _logger . info ( 'Start storing Trajectory `%s`.' % self . _trajectory_name ) else : self . _logger . info ( 'Initialising storage or updating meta data of Trajectory `%s`.' % self . _trajectory_name ) store_data = pypetconstants . STORE_NOTHING # In case we accidentally chose a trajectory name that already exist # We do not want to mess up the stored trajectory but raise an Error if not traj . _stored and self . _trajectory_group is not None : raise RuntimeError ( 'You want to store a completely new trajectory with name' ' `%s` but this trajectory is already found in file `%s`.' 'Did you try to accidentally overwrite existing data? If ' 'you DO want to override existing data, use `overwrite_file=True`.' 'Note that this deletes the whole HDF5 file not just the particular ' 'trajectroy therein! ' % ( traj . v_name , self . _filename ) ) # Extract HDF5 settings from the trajectory self . _srvc_check_hdf_properties ( traj ) # Store the trajectory for the first time if necessary: if self . _trajectory_group is None : self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , name = self . _trajectory_name , title = self . _trajectory_name , filters = self . _all_get_filters ( ) ) # Store meta information self . _trj_store_meta_data ( traj ) # # Store recursively the config subtree # self._tree_store_recursively(pypetconstants.LEAF,traj.config,self._trajectory_group) if store_data in ( pypetconstants . STORE_DATA_SKIPPING , pypetconstants . STORE_DATA , pypetconstants . OVERWRITE_DATA ) : counter = 0 maximum_display_other = 10 name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) for child_name in traj . _children : if child_name in name_set : self . _logger . info ( 'Storing branch `%s`.' % child_name ) else : if counter < maximum_display_other : self . _logger . info ( 'Storing branch/node `%s`.' % child_name ) elif counter == maximum_display_other : self . _logger . info ( 'To many branches or nodes at root for display. ' 'I will not inform you about storing anymore. ' 'Branches are stored silently in the background. ' 'Do not worry, I will not freeze! Pinky promise!!!' ) counter += 1 # Store recursively the elements self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , with_links = True , recursive = True , max_depth = max_depth , hdf5_group = self . _trajectory_group ) self . _logger . info ( 'Finished storing Trajectory `%s`.' % self . _trajectory_name ) else : self . _logger . info ( 'Finished init or meta data update for `%s`.' % self . _trajectory_name ) traj . _stored = True", "nl": "Stores a trajectory to an hdf5 file"}}
{"translation": {"code": "def _tree_store_sub_branch ( self , traj_node , branch_name , store_data = pypetconstants . STORE_DATA , with_links = True , recursive = False , max_depth = None , hdf5_group = None ) : if store_data == pypetconstants . STORE_NOTHING : return if max_depth is None : max_depth = float ( 'inf' ) if hdf5_group is None : # Get parent hdf5 node location = traj_node . v_full_name hdf5_location = location . replace ( '.' , '/' ) try : if location == '' : hdf5_group = self . _trajectory_group else : hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , name = hdf5_location ) except pt . NoSuchNodeError : self . _logger . debug ( 'Cannot store `%s` the parental hdf5 node with path `%s` does ' 'not exist on disk.' % ( traj_node . v_name , hdf5_location ) ) if traj_node . v_is_leaf : self . _logger . error ( 'Cannot store `%s` the parental hdf5 ' 'node with path `%s` does ' 'not exist on disk! The child ' 'you want to store is a leaf node,' 'that cannot be stored without ' 'the parental node existing on ' 'disk.' % ( traj_node . v_name , hdf5_location ) ) raise else : self . _logger . debug ( 'I will try to store the path from trajectory root to ' 'the child now.' ) self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , traj_node . v_full_name + '.' + branch_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth + traj_node . v_depth , hdf5_group = self . _trajectory_group ) return current_depth = 1 split_names = branch_name . split ( '.' ) leaf_name = split_names . pop ( ) for name in split_names : if current_depth > max_depth : return # Store along a branch self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group ) current_depth += 1 traj_node = traj_node . _children [ name ] hdf5_group = getattr ( hdf5_group , name ) # Store final group and recursively everything below it if current_depth <= max_depth : self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = current_depth , parent_hdf5_group = hdf5_group )", "nl": "Stores data starting from a node along a branch and starts recursively loading all data at end of branch ."}}
{"translation": {"code": "def _tree_load_nodes_dfs ( self , parent_traj_node , load_data , with_links , recursive , max_depth , current_depth , trajectory , as_new , hdf5_group ) : if max_depth is None : max_depth = float ( 'inf' ) loading_list = [ ( parent_traj_node , current_depth , hdf5_group ) ] while loading_list : parent_traj_node , current_depth , hdf5_group = loading_list . pop ( ) if isinstance ( hdf5_group , pt . link . SoftLink ) : if with_links : # We end up here when auto-loading a soft link self . _tree_load_link ( parent_traj_node , load_data = load_data , traj = trajectory , as_new = as_new , hdf5_soft_link = hdf5_group ) continue name = hdf5_group . _v_name is_leaf = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . LEAF ) in_trajectory = name in parent_traj_node . _children if is_leaf : # In case we have a leaf node, we need to check if we have to create a new # parameter or result if in_trajectory : instance = parent_traj_node . _children [ name ] # Otherwise we need to create a new instance else : instance = self . _tree_create_leaf ( name , trajectory , hdf5_group ) # Add the instance to the trajectory tree parent_traj_node . _add_leaf_from_storage ( args = ( instance , ) , kwargs = { } ) self . _prm_load_parameter_or_result ( instance , load_data = load_data , _hdf5_group = hdf5_group ) if as_new : instance . _stored = False else : if in_trajectory : traj_group = parent_traj_node . _children [ name ] if load_data == pypetconstants . OVERWRITE_DATA : traj_group . v_annotations . f_empty ( ) traj_group . v_comment = '' else : if HDF5StorageService . CLASS_NAME in hdf5_group . _v_attrs : class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) class_constructor = trajectory . _create_class ( class_name ) instance = trajectory . _construct_instance ( class_constructor , name ) args = ( instance , ) else : args = ( name , ) # If the group does not exist create it' traj_group = parent_traj_node . _add_group_from_storage ( args = args , kwargs = { } ) # Load annotations and comment self . _grp_load_group ( traj_group , load_data = load_data , with_links = with_links , recursive = False , max_depth = max_depth , _traj = trajectory , _as_new = as_new , _hdf5_group = hdf5_group ) if recursive and current_depth < max_depth : new_depth = current_depth + 1 for children in ( hdf5_group . _v_groups , hdf5_group . _v_links ) : for new_hdf5_group_name in children : new_hdf5_group = children [ new_hdf5_group_name ] loading_list . append ( ( traj_group , new_depth , new_hdf5_group ) )", "nl": "Loads a node from hdf5 file and if desired recursively everything below"}}
{"translation": {"code": "def _tree_store_nodes_dfs ( self , parent_traj_node , name , store_data , with_links , recursive , max_depth , current_depth , parent_hdf5_group ) : if max_depth is None : max_depth = float ( 'inf' ) store_list = [ ( parent_traj_node , name , current_depth , parent_hdf5_group ) ] while store_list : parent_traj_node , name , current_depth , parent_hdf5_group = store_list . pop ( ) # Check if we create a link if name in parent_traj_node . _links : if with_links : self . _tree_store_link ( parent_traj_node , name , parent_hdf5_group ) continue traj_node = parent_traj_node . _children [ name ] # If the node does not exist in the hdf5 file create it if not hasattr ( parent_hdf5_group , name ) : newly_created = True new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , filters = self . _all_get_filters ( ) ) else : newly_created = False new_hdf5_group = getattr ( parent_hdf5_group , name ) if traj_node . v_is_leaf : self . _prm_store_parameter_or_result ( traj_node , store_data = store_data , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) else : self . _grp_store_group ( traj_node , store_data = store_data , with_links = with_links , recursive = False , max_depth = max_depth , _hdf5_group = new_hdf5_group , _newly_created = newly_created ) if recursive and current_depth < max_depth : for child in traj_node . _children . keys ( ) : store_list . append ( ( traj_node , child , current_depth + 1 , new_hdf5_group ) )", "nl": "Stores a node to hdf5 and if desired stores recursively everything below it ."}}
{"translation": {"code": "def f_ann_to_str ( self ) : resstr = '' for key in sorted ( self . _dict . keys ( ) ) : resstr += '%s=%s; ' % ( key , str ( self . _dict [ key ] ) ) return resstr [ : - 2 ]", "nl": "Returns all annotations lexicographically sorted as a concatenated string ."}}
{"translation": {"code": "def _all_store_param_or_result_table_entry ( self , instance , table , flags , additional_info = None ) : # assert isinstance(table, pt.Table) location = instance . v_location name = instance . v_name fullname = instance . v_full_name if ( flags == ( HDF5StorageService . ADD_ROW , ) and table . nrows < 2 and 'location' in table . colnames ) : # We add the modify row option here because you cannot delete the very first # row of the table, so there is the rare condition, that the row might already # exist. # We also need to check if 'location' is in the columns in order to avoid # confusion with the smaller explored parameter overviews flags = ( HDF5StorageService . ADD_ROW , HDF5StorageService . MODIFY_ROW ) if flags == ( HDF5StorageService . ADD_ROW , ) : # If we are sure we only want to add a row we do not need to search! condvars = None condition = None else : # Condition to search for an entry condvars = { 'namecol' : table . cols . name , 'locationcol' : table . cols . location , 'name' : name , 'location' : location } condition = \"\"\"(namecol == name) & (locationcol == location)\"\"\" if HDF5StorageService . REMOVE_ROW in flags : # If we want to remove a row, we don't need to extract information insert_dict = { } else : # Extract information to insert from the instance and the additional info dict colnames = set ( table . colnames ) insert_dict = self . _all_extract_insert_dict ( instance , colnames , additional_info ) # Write the table entry self . _all_add_or_modify_row ( fullname , insert_dict , table , condition = condition , condvars = condvars , flags = flags )", "nl": "Stores a single row into an overview table"}}
{"translation": {"code": "def f_to_dict ( self , copy = True ) : if copy : return self . _dict . copy ( ) else : return self . _dict", "nl": "Returns annotations as dictionary ."}}
{"translation": {"code": "def f_add_derived_parameter_group ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = DERIVED_PARAMETER_GROUP , group_type_name = DERIVED_PARAMETER_GROUP , args = args , kwargs = kwargs )", "nl": "Adds an empty derived parameter group under the current node ."}}
{"translation": {"code": "def f_add_derived_parameter ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = DERIVED_PARAMETER , group_type_name = DERIVED_PARAMETER_GROUP , args = args , kwargs = kwargs )", "nl": "Adds a derived parameter under the current group ."}}
{"translation": {"code": "def f_add_config_group ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = CONFIG_GROUP , group_type_name = CONFIG_GROUP , args = args , kwargs = kwargs )", "nl": "Adds an empty config group under the current node ."}}
{"translation": {"code": "def f_add_config ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = CONFIG , group_type_name = CONFIG_GROUP , args = args , kwargs = kwargs )", "nl": "Adds a config parameter under the current group ."}}
{"translation": {"code": "def _ann_store_annotations ( self , item_with_annotations , node , overwrite = False ) : # If we overwrite delete all annotations first if overwrite is True or overwrite == 'v_annotations' : annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) if annotated : current_attrs = node . _v_attrs for attr_name in current_attrs . _v_attrnames : if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : delattr ( current_attrs , attr_name ) delattr ( current_attrs , HDF5StorageService . ANNOTATED ) self . _hdf5file . flush ( ) # Only store annotations if the item has some if not item_with_annotations . v_annotations . f_is_empty ( ) : anno_dict = item_with_annotations . v_annotations . _dict current_attrs = node . _v_attrs changed = False for field_name in anno_dict : val = anno_dict [ field_name ] field_name_with_prefix = HDF5StorageService . ANNOTATION_PREFIX + field_name if field_name_with_prefix not in current_attrs : # Only store *new* annotations, if they already exist on disk, skip storage setattr ( current_attrs , field_name_with_prefix , val ) changed = True if changed : setattr ( current_attrs , HDF5StorageService . ANNOTATED , True ) self . _hdf5file . flush ( )", "nl": "Stores annotations into an hdf5 file ."}}
{"translation": {"code": "def flatten_dictionary ( nested_dict , separator ) : flat_dict = { } for key , val in nested_dict . items ( ) : if isinstance ( val , dict ) : new_flat_dict = flatten_dictionary ( val , separator ) for flat_key , inval in new_flat_dict . items ( ) : new_key = key + separator + flat_key flat_dict [ new_key ] = inval else : flat_dict [ key ] = val return flat_dict", "nl": "Flattens a nested dictionary ."}}
{"translation": {"code": "def f_add_result_group ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = RESULT_GROUP , group_type_name = RESULT_GROUP , args = args , kwargs = kwargs )", "nl": "Adds an empty result group under the current node ."}}
{"translation": {"code": "def nest_dictionary ( flat_dict , separator ) : nested_dict = { } for key , val in flat_dict . items ( ) : split_key = key . split ( separator ) act_dict = nested_dict final_key = split_key . pop ( ) for new_key in split_key : if not new_key in act_dict : act_dict [ new_key ] = { } act_dict = act_dict [ new_key ] act_dict [ final_key ] = val return nested_dict", "nl": "Nests a given flat dictionary ."}}
{"translation": {"code": "def _all_extract_insert_dict ( self , item , colnames , additional_info = None ) : insert_dict = { } if 'length' in colnames : insert_dict [ 'length' ] = len ( item ) if 'comment' in colnames : comment = self . _all_cut_string ( item . v_comment . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH , self . _logger ) insert_dict [ 'comment' ] = comment if 'location' in colnames : insert_dict [ 'location' ] = item . v_location . encode ( 'utf-8' ) if 'name' in colnames : name = item . _name if ( not item . v_is_root or not item . v_is_run ) else item . _crun insert_dict [ 'name' ] = name . encode ( 'utf-8' ) if 'class_name' in colnames : insert_dict [ 'class_name' ] = item . f_get_class_name ( ) . encode ( 'utf-8' ) if 'value' in colnames : insert_dict [ 'value' ] = self . _all_cut_string ( item . f_val_to_str ( ) . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , self . _logger ) if 'hexdigest' in colnames : insert_dict [ 'hexdigest' ] = additional_info [ 'hexdigest' ] if 'idx' in colnames : insert_dict [ 'idx' ] = item . v_idx if 'time' in colnames : time_ = item . _time insert_dict [ 'time' ] = time_ . encode ( 'utf-8' ) if 'timestamp' in colnames : timestamp = item . _timestamp insert_dict [ 'timestamp' ] = timestamp if 'range' in colnames : third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 item_range = itools . islice ( item . f_get_range ( copy = False ) , 0 , third_length ) range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) insert_dict [ 'range' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) # To allow backwards compatibility if 'array' in colnames : third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 item_range = itools . islice ( item . f_get_range ( copy = False ) , 0 , third_length ) range_string = ', ' . join ( [ repr ( x ) for x in item_range ] ) insert_dict [ 'array' ] = self . _all_cut_string ( range_string . encode ( 'utf-8' ) , pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , self . _logger ) if 'version' in colnames : insert_dict [ 'version' ] = item . v_version . encode ( 'utf-8' ) if 'python' in colnames : insert_dict [ 'python' ] = item . v_python . encode ( 'utf-8' ) if 'finish_timestamp' in colnames : insert_dict [ 'finish_timestamp' ] = item . _finish_timestamp_run return insert_dict", "nl": "Extracts information from a given item to be stored into a pytable row ."}}
{"translation": {"code": "def _all_insert_into_row ( self , row , insert_dict ) : for key , val in insert_dict . items ( ) : try : row [ key ] = val except KeyError as ke : self . _logger . warning ( 'Could not write `%s` into a table, ' % key + repr ( ke ) )", "nl": "Copies data from insert_dict into a pytables row ."}}
{"translation": {"code": "def _all_add_or_modify_row ( self , item_name , insert_dict , table , index = None , condition = None , condvars = None , flags = ( ADD_ROW , MODIFY_ROW , ) ) : if len ( flags ) == 0 : # No flags means no-op return # You can only specify either an index or a condition not both if index is not None and condition is not None : raise ValueError ( 'Please give either a condition or an index or none!' ) elif condition is not None : row_iterator = table . where ( condition , condvars = condvars ) elif index is not None : row_iterator = table . iterrows ( index , index + 1 ) else : row_iterator = None try : row = next ( row_iterator ) except TypeError : row = None except StopIteration : row = None # multiple_entries = [] if ( ( HDF5StorageService . MODIFY_ROW in flags or HDF5StorageService . ADD_ROW in flags ) and HDF5StorageService . REMOVE_ROW in flags ) : # You cannot remove and modify or add at the same time raise ValueError ( 'You cannot add or modify and remove a row at the same time.' ) if row is None and HDF5StorageService . ADD_ROW in flags : # Here we add a new row row = table . row self . _all_insert_into_row ( row , insert_dict ) row . append ( ) elif row is not None and HDF5StorageService . MODIFY_ROW in flags : # Here we modify an existing row self . _all_insert_into_row ( row , insert_dict ) row . update ( ) elif HDF5StorageService . REMOVE_ROW in flags : # Here we delete an existing row if row is not None : # Only delete if the row does exist otherwise we do not have to do anything row_number = row . nrow try : table . remove_rows ( start = row_number , stop = row_number + 1 ) except NotImplementedError : pass # We get here if we try to remove the last row of a table # there is nothing we can do but keep it :-( else : raise ValueError ( 'Something is wrong, you might not have found ' 'a row, or your flags are not set appropriately' ) self . _all_kill_iterator ( row_iterator ) table . flush ( ) if HDF5StorageService . REMOVE_ROW not in flags and row is None : raise RuntimeError ( 'Could not add or modify entries of `%s` in ' 'table %s' % ( item_name , table . _v_name ) )", "nl": "Adds or changes a row in a pytable ."}}
{"translation": {"code": "def _all_recall_native_type ( self , data , ptitem , prefix ) : typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) type_changed = False # Check what the original data type was from the hdf5 node attributes if colltype == HDF5StorageService . COLL_SCALAR : # Here data item was a scalar if isinstance ( data , np . ndarray ) : # If we recall a numpy scalar, pytables loads a 1d array :-/ # So we have to change it to a real scalar value data = np . array ( [ data ] ) [ 0 ] type_changed = True if not typestr is None : # Check if current type and stored type match # if not convert the data if typestr != type ( data ) . __name__ : if typestr == str . __name__ : data = data . decode ( self . _encoding ) else : try : data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) except KeyError : # For compatibility with files from older pypet versions data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) type_changed = True elif ( colltype == HDF5StorageService . COLL_TUPLE or colltype == HDF5StorageService . COLL_LIST ) : # Here data item was originally a tuple or a list if type ( data ) is not list and type is not tuple : # If the original type cannot be recalled, first convert it to a list type_changed = True data = list ( data ) if len ( data ) > 0 : first_item = data [ 0 ] # Check if the type of the first item was conserved if not typestr == type ( first_item ) . __name__ : if not isinstance ( data , list ) : data = list ( data ) # If type was not conserved we need to convert all items # in the list or tuple for idx , item in enumerate ( data ) : if typestr == str . __name__ : data [ idx ] = data [ idx ] . decode ( self . _encoding ) else : try : data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) except KeyError : # For compatibility with files from older pypet versions: data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) type_changed = True if colltype == HDF5StorageService . COLL_TUPLE : # If it was originally a tuple we need to convert it back to tuple if type ( data ) is not tuple : data = tuple ( data ) type_changed = True elif colltype == HDF5StorageService . COLL_EMPTY_DICT : data = { } type_changed = True elif isinstance ( data , np . ndarray ) : if typestr == str . __name__ : data = np . core . defchararray . decode ( data , self . _encoding ) type_changed = True if colltype == HDF5StorageService . COLL_MATRIX : # Here data item was originally a matrix data = np . matrix ( data ) type_changed = True return data , type_changed", "nl": "Checks if loaded data has the type it was stored in . If not converts it ."}}
{"translation": {"code": "def _all_set_attributes_to_recall_natives ( data , ptitem , prefix ) : # If `data` is a container, remember the container type if type ( data ) is tuple : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_TUPLE ) elif type ( data ) is list : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_LIST ) elif type ( data ) is np . ndarray : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_NDARRAY ) elif type ( data ) is np . matrix : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_MATRIX ) elif type ( data ) in pypetconstants . PARAMETER_SUPPORTED_DATA : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_SCALAR ) strtype = type ( data ) . __name__ if not strtype in pypetconstants . PARAMETERTYPEDICT : raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) elif type ( data ) is dict : if len ( data ) > 0 : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_DICT ) else : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , HDF5StorageService . COLL_EMPTY_DICT ) else : raise TypeError ( 'I do not know how to handle `%s` its type is `%s`.' % ( str ( data ) , repr ( type ( data ) ) ) ) if type ( data ) in ( list , tuple ) : # If data is a list or tuple we need to remember the data type of the elements # in the list or tuple. # We do NOT need to remember the elements of `dict` explicitly, though. # `dict` is stored # as an `ObjectTable` and thus types are already conserved. if len ( data ) > 0 : strtype = type ( data [ 0 ] ) . __name__ if not strtype in pypetconstants . PARAMETERTYPEDICT : raise TypeError ( 'I do not know how to handle `%s` its type is ' '`%s`.' % ( str ( data ) , strtype ) ) HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) elif ( type ( data ) in ( np . ndarray , np . matrix ) and np . issubdtype ( data . dtype , str ) ) : HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , str . __name__ )", "nl": "Stores original data type to hdf5 node attributes for preserving the data type ."}}
{"translation": {"code": "def _all_get_or_create_table ( self , where , tablename , description , expectedrows = None ) : where_node = self . _hdf5file . get_node ( where ) if not tablename in where_node : if not expectedrows is None : table = self . _hdf5file . create_table ( where = where_node , name = tablename , description = description , title = tablename , expectedrows = expectedrows , filters = self . _all_get_filters ( ) ) else : table = self . _hdf5file . create_table ( where = where_node , name = tablename , description = description , title = tablename , filters = self . _all_get_filters ( ) ) else : table = where_node . _f_get_child ( tablename ) return table", "nl": "Creates a new table or if the table already exists returns it ."}}
{"translation": {"code": "def _all_create_or_get_groups ( self , key , start_hdf5_group = None ) : if start_hdf5_group is None : newhdf5_group = self . _trajectory_group else : newhdf5_group = start_hdf5_group created = False if key == '' : return newhdf5_group , created split_key = key . split ( '.' ) for name in split_key : newhdf5_group , created = self . _all_create_or_get_group ( name , newhdf5_group ) return newhdf5_group , created", "nl": "Creates new or follows existing group nodes along a given colon separated key ."}}
{"translation": {"code": "def _determine_types ( start_node , first_name , add_leaf , add_link ) : if start_node . v_is_root : where = first_name else : where = start_node . _branch if where in SUBTREE_MAPPING : type_tuple = SUBTREE_MAPPING [ where ] else : type_tuple = ( GROUP , LEAF ) if add_link : return type_tuple [ 0 ] , LINK if add_leaf : return type_tuple else : return type_tuple [ 0 ] , type_tuple [ 0 ]", "nl": "Determines types for generic additions"}}
{"translation": {"code": "def f_add_to_dynamic_imports ( self , dynamic_imports ) : if not isinstance ( dynamic_imports , ( list , tuple ) ) : dynamic_imports = [ dynamic_imports ] for item in dynamic_imports : if not ( isinstance ( item , str ) or inspect . isclass ( item ) ) : raise TypeError ( 'Your dynamic import `%s` is neither a class nor a string.' % str ( item ) ) self . _dynamic_imports . extend ( dynamic_imports )", "nl": "Adds classes or paths to classes to the trajectory to create custom parameters ."}}
{"translation": {"code": "def _translate_shortcut ( self , name ) : if isinstance ( name , int ) : return True , self . _root_instance . f_wildcard ( '$' , name ) if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : split_name = name . split ( '_' ) if len ( split_name ) == 2 : index = split_name [ 1 ] if index . isdigit ( ) : return True , self . _root_instance . f_wildcard ( '$' , int ( index ) ) elif index == 'A' : return True , self . _root_instance . f_wildcard ( '$' , - 1 ) if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : split_name = name . split ( '_' ) if len ( split_name ) == 2 : index = split_name [ 1 ] if index . isdigit ( ) : return True , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) elif index == 'A' : return True , self . _root_instance . f_wildcard ( '$set' , - 1 ) if name in SHORTCUT_SET : if name == 'par' : return True , 'parameters' elif name == 'dpar' : return True , 'derived_parameters' elif name == 'res' : return True , 'results' elif name == 'conf' : return True , 'config' else : raise RuntimeError ( 'You shall not pass!' ) return False , name", "nl": "Maps a given shortcut to corresponding name"}}
{"translation": {"code": "def f_set_single ( self , name , item ) : if self . v_stored : self . _logger . debug ( 'You are changing an already stored result. If ' 'you not explicitly overwrite the data on disk, this change ' 'might be lost and not propagated to disk.' ) if self . _supports ( item ) : # self._check_if_empty(item, name) # No longer needed if name in self . _data : self . _logger . debug ( 'Replacing `%s` in result `%s`.' % ( name , self . v_full_name ) ) self . _data [ name ] = item else : raise TypeError ( 'Your result `%s` of type `%s` is not supported.' % ( name , str ( type ( item ) ) ) )", "nl": "Sets a single data item of the result ."}}
{"translation": {"code": "def f_get ( self , * args ) : if len ( args ) == 0 : if len ( self . _data ) == 1 : return list ( self . _data . values ( ) ) [ 0 ] elif len ( self . _data ) > 1 : raise ValueError ( 'Your result `%s` contains more than one entry: ' '`%s` Please use >>f_get<< with one of these.' % ( self . v_full_name , str ( list ( self . _data . keys ( ) ) ) ) ) else : raise AttributeError ( 'Your result `%s` is empty, cannot access data.' % self . v_full_name ) result_list = [ ] for name in args : name = self . f_translate_key ( name ) if not name in self . _data : if name == 'data' and len ( self . _data ) == 1 : return self . _data [ list ( self . _data . keys ( ) ) [ 0 ] ] else : raise AttributeError ( '`%s` is not part of your result `%s`.' % ( name , self . v_full_name ) ) result_list . append ( self . _data [ name ] ) if len ( args ) == 1 : return result_list [ 0 ] else : return result_list", "nl": "Returns items handled by the result ."}}
{"translation": {"code": "def f_store_items ( self , iterator , * args , * * kwargs ) : if not self . _stored : raise TypeError ( 'Cannot store stuff for a trajectory that has never been ' 'stored to disk. Please call traj.f_store(only_init=True) first.' ) fetched_items = self . _nn_interface . _fetch_items ( STORE , iterator , args , kwargs ) if fetched_items : self . _storage_service . store ( pypetconstants . LIST , fetched_items , trajectory_name = self . v_name ) else : raise ValueError ( 'Your storage was not successful, could not find a single item ' 'to store.' )", "nl": "Stores individual items to disk ."}}
{"translation": {"code": "def f_load_items ( self , iterator , * args , * * kwargs ) : if not self . _stored : raise TypeError ( 'Cannot load stuff from disk for a trajectory that has never been stored.' ) fetched_items = self . _nn_interface . _fetch_items ( LOAD , iterator , args , kwargs ) if fetched_items : self . _storage_service . load ( pypetconstants . LIST , fetched_items , trajectory_name = self . v_name ) else : self . _logger . warning ( 'Your loading was not successful, could not find a single item ' 'to load.' )", "nl": "Loads parameters and results specified in iterator ."}}
{"translation": {"code": "def f_set ( self , * args , * * kwargs ) : if args and self . v_name is None : raise AttributeError ( 'Cannot set positional value because I do not have a name!' ) for idx , arg in enumerate ( args ) : valstr = self . f_translate_key ( idx ) self . f_set_single ( valstr , arg ) for key , arg in kwargs . items ( ) : self . f_set_single ( key , arg )", "nl": "Method to put data into the result ."}}
{"translation": {"code": "def f_to_dict ( self , copy = True ) : if copy : return self . _data . copy ( ) else : return self . _data", "nl": "Returns all handled data as a dictionary ."}}
{"translation": {"code": "def f_remove_items ( self , iterator , recursive = False ) : # Will format the request in a form that is understood by the storage service # aka (msg, item, args, kwargs) fetched_items = self . _nn_interface . _fetch_items ( REMOVE , iterator , ( ) , { } ) if fetched_items : for _ , item , dummy1 , dummy2 in fetched_items : self . _nn_interface . _remove_node_or_leaf ( item , recursive = recursive ) else : self . _logger . warning ( 'Your removal was not successful, could not find a single ' 'item to remove.' )", "nl": "Removes parameters results or groups from the trajectory ."}}
{"translation": {"code": "def f_delete_links ( self , iterator_of_links , remove_from_trajectory = False ) : to_delete_links = [ ] group_link_pairs = [ ] for elem in iterator_of_links : if isinstance ( elem , str ) : split_names = elem . split ( '.' ) parent_name = '.' . join ( split_names [ : - 1 ] ) link = split_names [ - 1 ] parent_node = self . f_get ( parent_name ) if parent_name != '' else self link_name = parent_node . v_full_name + '.' + link if parent_name != '' else link to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) group_link_pairs . append ( ( parent_node , link ) ) else : link_name = elem [ 0 ] . v_full_name + '.' + elem [ 1 ] to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) group_link_pairs . append ( elem ) try : self . _storage_service . store ( pypetconstants . LIST , to_delete_links , trajectory_name = self . v_name ) except : self . _logger . error ( 'Could not remove `%s` from the trajectory. Maybe the' ' item(s) was/were never stored to disk.' % str ( to_delete_links ) ) raise if remove_from_trajectory : for group , link in group_link_pairs : group . f_remove_link ( link )", "nl": "Deletes several links from the hard disk ."}}
{"translation": {"code": "def _add_prefix ( self , split_names , start_node , group_type_name ) : root = self . _root_instance # If the start node of our insertion is root or one below root # we might need to add prefixes. # In case of derived parameters and results we also need to add prefixes containing the # subbranch and the current run in case of a single run. # For instance, a prefix could be 'results.runs.run_00000007'. prepend = [ ] if start_node . v_depth < 3 and not group_type_name == GROUP : if start_node . v_depth == 0 : if group_type_name == DERIVED_PARAMETER_GROUP : if split_names [ 0 ] == 'derived_parameters' : return split_names else : prepend += [ 'derived_parameters' ] elif group_type_name == RESULT_GROUP : if split_names [ 0 ] == 'results' : return split_names else : prepend += [ 'results' ] elif group_type_name == CONFIG_GROUP : if split_names [ 0 ] == 'config' : return split_names else : prepend += [ 'config' ] elif group_type_name == PARAMETER_GROUP : if split_names [ 0 ] == 'parameters' : return split_names [ 0 ] else : prepend += [ 'parameters' ] else : raise RuntimeError ( 'Why are you here?' ) # Check if we have to add a prefix containing the current run if root . _is_run and root . _auto_run_prepend : dummy = root . f_wildcard ( '$' , - 1 ) crun = root . f_wildcard ( '$' ) if any ( name in root . _run_information for name in split_names ) : pass elif any ( name == dummy for name in split_names ) : pass elif ( group_type_name == RESULT_GROUP or group_type_name == DERIVED_PARAMETER_GROUP ) : if start_node . v_depth == 0 : prepend += [ 'runs' , crun ] elif start_node . v_depth == 1 : if len ( split_names ) == 1 and split_names [ 0 ] == 'runs' : return split_names else : prepend += [ 'runs' , crun ] elif start_node . v_depth == 2 and start_node . v_name == 'runs' : prepend += [ crun ] if prepend : split_names = prepend + split_names return split_names", "nl": "Adds the correct sub branch prefix to a given name ."}}
{"translation": {"code": "def f_val_to_str ( self ) : resstrlist = [ ] strlen = 0 for key in self . _data : val = self . _data [ key ] resstr = '%s=%s, ' % ( key , repr ( val ) ) resstrlist . append ( resstr ) strlen += len ( resstr ) if strlen > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : break return_string = \"\" . join ( resstrlist ) if len ( return_string ) > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : return_string = return_string [ 0 : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3 ] + '...' else : return_string = return_string [ 0 : - 2 ] # Delete the last `, ` return return_string", "nl": "Summarizes data handled by the result as a string ."}}
{"translation": {"code": "def _load ( self , load_dict ) : if self . v_locked : raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) if 'data' in load_dict : dump = load_dict [ 'data' ] self . _data = pickle . loads ( dump ) else : self . _logger . warning ( 'Your parameter `%s` is empty, ' 'I did not find any data on disk.' % self . v_full_name ) try : self . v_protocol = load_dict [ PickleParameter . PROTOCOL ] except KeyError : # For backwards compatibility self . v_protocol = PickleParameter . _get_protocol ( dump ) if 'explored_data' in load_dict : explore_table = load_dict [ 'explored_data' ] name_col = explore_table [ 'idx' ] explore_list = [ ] for name_id in name_col : arrayname = self . _build_name ( name_id ) loaded = pickle . loads ( load_dict [ arrayname ] ) explore_list . append ( loaded ) self . _explored_range = explore_list self . _explored = True self . _default = self . _data self . _locked = True", "nl": "Reconstructs objects from the pickle dumps in load_dict ."}}
{"translation": {"code": "def _return_item_dictionary ( param_dict , fast_access , copy ) : if not copy and fast_access : raise ValueError ( 'You cannot access the original dictionary and use fast access at the' ' same time!' ) if not fast_access : if copy : return param_dict . copy ( ) else : return param_dict else : resdict = { } for key in param_dict : param = param_dict [ key ] val = param . f_get ( ) resdict [ key ] = val return resdict", "nl": "Returns a dictionary containing either all parameters all explored parameters all config all derived parameters or all results ."}}
{"translation": {"code": "def f_delete_items ( self , iterator , * args , * * kwargs ) : remove_from_trajectory = kwargs . pop ( 'remove_from_trajectory' , False ) recursive = kwargs . get ( 'recursive' , False ) # Will format the request in a form that is understood by the storage service # aka (msg, item, args, kwargs) fetched_items = self . _nn_interface . _fetch_items ( REMOVE , iterator , args , kwargs ) if fetched_items : try : self . _storage_service . store ( pypetconstants . LIST , fetched_items , trajectory_name = self . v_name ) except : self . _logger . error ( 'Could not remove `%s` from the trajectory. Maybe the' ' item(s) was/were never stored to disk.' % str ( fetched_items ) ) raise for _ , item , dummy1 , dummy2 in fetched_items : if remove_from_trajectory : self . _nn_interface . _remove_node_or_leaf ( item , recursive = recursive ) else : item . _stored = False else : self . _logger . warning ( 'Your removal was not successful, could not find a single ' 'item to remove.' )", "nl": "Deletes items from storage on disk ."}}
{"translation": {"code": "def f_get_range_length ( self ) : if not self . f_has_range ( ) : raise TypeError ( 'Not applicable, parameter does not have a range' ) elif hasattr ( self , '__len__' ) : return len ( self ) else : raise NotImplementedError ( \"Should have implemented this.\" )", "nl": "Returns the length of the parameter range ."}}
{"translation": {"code": "def f_val_to_str ( self ) : old_locked = self . _locked try : return repr ( self . f_get ( ) ) except Exception : return 'No Evaluation possible (yet)!' finally : self . _locked = old_locked", "nl": "String summary of the value handled by the parameter ."}}
{"translation": {"code": "def _store ( self ) : store_dict = { } if self . _data is not None : dump = pickle . dumps ( self . _data , protocol = self . v_protocol ) store_dict [ 'data' ] = dump store_dict [ PickleParameter . PROTOCOL ] = self . v_protocol if self . f_has_range ( ) : store_dict [ 'explored_data' ] = ObjectTable ( columns = [ 'idx' ] , index = list ( range ( len ( self ) ) ) ) smart_dict = { } count = 0 for idx , val in enumerate ( self . _explored_range ) : obj_id = id ( val ) if obj_id in smart_dict : name_id = smart_dict [ obj_id ] add = False else : name_id = count add = True name = self . _build_name ( name_id ) store_dict [ 'explored_data' ] [ 'idx' ] [ idx ] = name_id if add : store_dict [ name ] = pickle . dumps ( val , protocol = self . v_protocol ) smart_dict [ obj_id ] = name_id count += 1 self . _locked = True return store_dict", "nl": "Returns a dictionary for storage ."}}
{"translation": {"code": "def _load ( self , load_dict ) : if self . v_locked : raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) try : self . _data = load_dict [ 'data' + ArrayParameter . IDENTIFIER ] if 'explored_data' + ArrayParameter . IDENTIFIER in load_dict : explore_table = load_dict [ 'explored_data' + ArrayParameter . IDENTIFIER ] idx = explore_table [ 'idx' ] explore_list = [ ] # Recall the arrays in the order stored in the ObjectTable 'explored_data__rr__' for name_idx in idx : arrayname = self . _build_name ( name_idx ) explore_list . append ( load_dict [ arrayname ] ) self . _explored_range = [ x for x in explore_list ] self . _explored = True except KeyError : super ( ArrayParameter , self ) . _load ( load_dict ) self . _default = self . _data self . _locked = True", "nl": "Reconstructs the data and exploration array ."}}
{"translation": {"code": "def _equal_values ( self , val1 , val2 ) : if self . f_supports ( val1 ) != self . f_supports ( val2 ) : return False if not self . f_supports ( val1 ) and not self . f_supports ( val2 ) : raise TypeError ( 'I do not support the types of both inputs (`%s` and `%s`), ' 'therefore I cannot judge whether ' 'the two are equal.' % ( str ( type ( val1 ) ) , str ( type ( val2 ) ) ) ) if not self . _values_of_same_type ( val1 , val2 ) : return False return comparisons . nested_equal ( val1 , val2 )", "nl": "Checks if the parameter considers two values as equal ."}}
{"translation": {"code": "def f_get_range ( self , copy = True ) : if not self . f_has_range ( ) : raise TypeError ( 'Your parameter `%s` is not array, so cannot return array.' % self . v_full_name ) elif copy : return self . _explored_range [ : ] else : return self . _explored_range", "nl": "Returns a python iterable containing the exploration range ."}}
{"translation": {"code": "def _explore ( self , explore_iterable ) : if self . v_locked : raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) if self . f_has_range ( ) : raise TypeError ( 'Your parameter `%s` is already explored, ' 'cannot _explore it further!' % self . _name ) if self . _data is None : raise TypeError ( 'Your parameter `%s` has no default value, please specify one ' 'via `f_set` before exploration. ' % self . v_full_name ) data_list = self . _data_sanity_checks ( explore_iterable ) self . _explored_range = data_list self . _explored = True self . f_lock ( )", "nl": "Explores the parameter according to the iterable ."}}
{"translation": {"code": "def _load ( self , load_dict ) : if self . v_locked : raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) if 'data' in load_dict : self . _data = load_dict [ 'data' ] [ 'data' ] [ 0 ] self . _default = self . _data else : self . _logger . warning ( 'Your parameter `%s` is empty, ' 'I did not find any data on disk.' % self . v_full_name ) if 'explored_data' in load_dict : self . _explored_range = [ x for x in load_dict [ 'explored_data' ] [ 'data' ] . tolist ( ) ] self . _explored = True self . _locked = True", "nl": "Loads the data and exploration range from the load_dict ."}}
{"translation": {"code": "def _expand ( self , explore_iterable ) : if self . v_locked : raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) if not self . f_has_range ( ) : raise TypeError ( 'Your Parameter `%s` is not an array and can therefore ' 'not be expanded.' % self . _name ) data_list = self . _data_sanity_checks ( explore_iterable ) self . _explored_range . extend ( data_list ) self . f_lock ( )", "nl": "Explores the parameter according to the iterable and appends to the exploration range ."}}
{"translation": {"code": "def parameters_equal ( a , b ) : if ( not b . v_is_parameter and not a . v_is_parameter ) : raise ValueError ( 'Both inputs are not parameters' ) if ( not b . v_is_parameter or not a . v_is_parameter ) : return False if a . v_full_name != b . v_full_name : return False if a . f_is_empty ( ) and b . f_is_empty ( ) : return True if a . f_is_empty ( ) != b . f_is_empty ( ) : return False if not a . _values_of_same_type ( a . f_get ( ) , b . f_get ( ) ) : return False if not a . _equal_values ( a . f_get ( ) , b . f_get ( ) ) : return False if a . f_has_range ( ) != b . f_has_range ( ) : return False if a . f_has_range ( ) : if a . f_get_range_length ( ) != b . f_get_range_length ( ) : return False for myitem , bitem in zip ( a . f_get_range ( copy = False ) , b . f_get_range ( copy = False ) ) : if not a . _values_of_same_type ( myitem , bitem ) : return False if not a . _equal_values ( myitem , bitem ) : return False return True", "nl": "Compares two parameter instances"}}
{"translation": {"code": "def results_equal ( a , b ) : if a . v_is_parameter and b . v_is_parameter : raise ValueError ( 'Both inputs are not results.' ) if a . v_is_parameter or b . v_is_parameter : return False if a . v_full_name != b . v_full_name : return False if hasattr ( a , '_data' ) and not hasattr ( b , '_data' ) : return False if hasattr ( a , '_data' ) : akeyset = set ( a . _data . keys ( ) ) bkeyset = set ( b . _data . keys ( ) ) if akeyset != bkeyset : return False for key in a . _data : val = a . _data [ key ] bval = b . _data [ key ] if not nested_equal ( val , bval ) : return False return True", "nl": "Compares two result instances"}}
{"translation": {"code": "def _single_run ( kwargs ) : pypet_root_logger = logging . getLogger ( 'pypet' ) traj = kwargs [ 'traj' ] runfunc = kwargs [ 'runfunc' ] runargs = kwargs [ 'runargs' ] kwrunparams = kwargs [ 'runkwargs' ] clean_up_after_run = kwargs [ 'clean_up_runs' ] automatic_storing = kwargs [ 'automatic_storing' ] wrap_mode = kwargs [ 'wrap_mode' ] idx = traj . v_idx total_runs = len ( traj ) pypet_root_logger . info ( '\\n=========================================\\n ' 'Starting single run #%d of %d ' '\\n=========================================\\n' % ( idx , total_runs ) ) # Measure start time traj . f_start_run ( turn_into_run = True ) # Run the job function of the user result = runfunc ( traj , * runargs , * * kwrunparams ) # Store data if desired if automatic_storing : traj . f_store ( ) # Add the index to the result and the run information if wrap_mode == pypetconstants . WRAP_MODE_LOCAL : result = ( ( traj . v_idx , result ) , traj . f_get_run_information ( traj . v_idx , copy = False ) , traj . v_storage_service . references ) traj . v_storage_service . free_references ( ) else : result = ( ( traj . v_idx , result ) , traj . f_get_run_information ( traj . v_idx , copy = False ) ) # Measure time of finishing traj . f_finalize_run ( store_meta_data = False , clean_up = clean_up_after_run ) pypet_root_logger . info ( '\\n=========================================\\n ' 'Finished single run #%d of %d ' '\\n=========================================\\n' % ( idx , total_runs ) ) return result", "nl": "Performs a single run of the experiment ."}}
{"translation": {"code": "def _data_sanity_checks ( self , explore_iterable ) : data_list = [ ] for val in explore_iterable : if not self . f_supports ( val ) : raise TypeError ( '%s is of not supported type %s.' % ( repr ( val ) , str ( type ( val ) ) ) ) if not self . _values_of_same_type ( val , self . _default ) : raise TypeError ( 'Data of `%s` is not of the same type as the original entry value, ' 'new type is %s vs old type %s.' % ( self . v_full_name , str ( type ( val ) ) , str ( type ( self . _default ) ) ) ) data_list . append ( val ) if len ( data_list ) == 0 : raise ValueError ( 'Cannot explore an empty list!' ) return data_list", "nl": "Checks if data values are valid ."}}
{"translation": {"code": "def _set_finish ( self ) : run_info_dict = self . _run_information [ self . v_crun ] timestamp_run = run_info_dict [ 'timestamp' ] run_summary = self . _summarize_explored_parameters ( ) finish_timestamp_run = time . time ( ) findatetime = datetime . datetime . fromtimestamp ( finish_timestamp_run ) startdatetime = datetime . datetime . fromtimestamp ( timestamp_run ) runtime_run = str ( findatetime - startdatetime ) run_info_dict [ 'parameter_summary' ] = run_summary run_info_dict [ 'completed' ] = 1 run_info_dict [ 'finish_timestamp' ] = finish_timestamp_run run_info_dict [ 'runtime' ] = runtime_run", "nl": "Sets the finish time and computes the runtime in human readable format"}}
{"translation": {"code": "def f_get_run_information ( self , name_or_idx = None , copy = True ) : if name_or_idx is None : if copy : return cp . deepcopy ( self . _run_information ) else : return self . _run_information try : if copy : # Since the information dictionaries only contain immutable items # (float, int, str) # the normal copy operation is sufficient return self . _run_information [ name_or_idx ] . copy ( ) else : return self . _run_information [ name_or_idx ] except KeyError : # Maybe the user provided an idx, this would yield a key error and we # have to convert it to a run name name_or_idx = self . f_idx_to_run ( name_or_idx ) if copy : return self . _run_information [ name_or_idx ] . copy ( ) else : return self . _run_information [ name_or_idx ]", "nl": "Returns a dictionary containing information about a single run ."}}
{"translation": {"code": "def _remove_along_branch ( self , actual_node , split_name , recursive = False ) : # If the names list is empty, we have reached the node we want to delete. if len ( split_name ) == 0 : if actual_node . v_is_group and actual_node . f_has_children ( ) : if recursive : for child in list ( actual_node . _children . keys ( ) ) : actual_node . f_remove_child ( child , recursive = True ) else : raise TypeError ( 'Cannot remove group `%s` it contains children. Please ' 'remove with `recursive=True`.' % actual_node . v_full_name ) self . _delete_node ( actual_node ) return True # Otherwise get the next node by using the first name in the list name = split_name . popleft ( ) if name in actual_node . _links : if len ( split_name ) > 0 : raise RuntimeError ( 'You cannot remove nodes while hopping over links!' ) actual_node . f_remove_link ( name ) else : child = actual_node . _children [ name ] if self . _remove_along_branch ( child , split_name , recursive = recursive ) : del actual_node . _children [ name ] if name in actual_node . _groups : del actual_node . _groups [ name ] elif name in actual_node . _leaves : del actual_node . _leaves [ name ] else : raise RuntimeError ( 'You shall not pass!' ) del child return False", "nl": "Removes a given node from the tree ."}}
{"translation": {"code": "def _remove_node_or_leaf ( self , instance , recursive = False ) : full_name = instance . v_full_name split_name = deque ( full_name . split ( '.' ) ) self . _remove_along_branch ( self . _root_instance , split_name , recursive )", "nl": "Removes a single node from the tree ."}}
{"translation": {"code": "def _delete_node ( self , node ) : full_name = node . v_full_name root = self . _root_instance if full_name == '' : # You cannot delete root return if node . v_is_leaf : if full_name in root . _parameters : del root . _parameters [ full_name ] elif full_name in root . _config : del root . _config [ full_name ] elif full_name in root . _derived_parameters : del root . _derived_parameters [ full_name ] elif full_name in root . _results : del root . _results [ full_name ] elif full_name in root . _other_leaves : del root . _other_leaves [ full_name ] if full_name in root . _explored_parameters : if root . _stored : # We always keep the explored parameters in case the trajectory was stored root . _explored_parameters [ full_name ] = None else : del root . _explored_parameters [ full_name ] if len ( root . _explored_parameters ) == 0 : root . f_shrink ( ) del self . _flat_leaf_storage_dict [ full_name ] else : del root . _all_groups [ full_name ] if full_name in root . _run_parent_groups : del root . _run_parent_groups [ full_name ] # Delete all links to the node if full_name in root . _linked_by : linking = root . _linked_by [ full_name ] for linking_name in list ( linking . keys ( ) ) : linking_group , link_set = linking [ linking_name ] for link in list ( link_set ) : linking_group . f_remove_link ( link ) if ( node . v_location , node . v_name ) in self . _root_instance . _new_nodes : del self . _root_instance . _new_nodes [ ( node . v_location , node . v_name ) ] # Finally remove all references in the dictionaries for fast search self . _remove_from_nodes_and_leaves ( node ) # Remove circular references node . _vars = None node . _func = None", "nl": "Deletes a single node from the tree ."}}
{"translation": {"code": "def _remove_subtree ( self , start_node , name , predicate = None ) : def _delete_from_children ( node , child_name ) : del node . _children [ child_name ] if child_name in node . _groups : del node . _groups [ child_name ] elif child_name in node . _leaves : del node . _leaves [ child_name ] else : raise RuntimeError ( 'You shall not pass!' ) def _remove_subtree_inner ( node , predicate ) : if not predicate ( node ) : return False elif node . v_is_group : for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , list ( node . _groups . keys ( ) ) ) : child_ = node . _children [ name_ ] child_deleted = _remove_subtree_inner ( child_ , predicate ) if child_deleted : _delete_from_children ( node , name_ ) del child_ for link_ in list ( node . _links . keys ( ) ) : node . f_remove_link ( link_ ) if len ( node . _children ) == 0 : self . _delete_node ( node ) return True else : return False else : self . _delete_node ( node ) return True if name in start_node . _links : start_node . f_remove_link ( name ) else : child = start_node . _children [ name ] if predicate is None : predicate = lambda x : True if _remove_subtree_inner ( child , predicate ) : _delete_from_children ( start_node , name ) del child return True else : return False", "nl": "Removes a subtree from the trajectory tree ."}}
{"translation": {"code": "def _node_to_msg ( store_load , node ) : if node . v_is_leaf : if store_load == STORE : return pypetconstants . LEAF elif store_load == LOAD : return pypetconstants . LEAF elif store_load == REMOVE : return pypetconstants . DELETE else : if store_load == STORE : return pypetconstants . GROUP elif store_load == LOAD : return pypetconstants . GROUP elif store_load == REMOVE : return pypetconstants . DELETE", "nl": "Maps a given node and a store_load constant to the message that is understood by the storage service ."}}
{"translation": {"code": "def f_set_crun ( self , name_or_idx ) : if ( name_or_idx is None or name_or_idx == self . f_wildcard ( '$' , - 1 ) or name_or_idx == - 1 ) : self . f_restore_default ( ) else : if isinstance ( name_or_idx , str ) : self . _idx = self . f_idx_to_run ( name_or_idx ) self . _crun = name_or_idx else : self . _crun = self . f_idx_to_run ( name_or_idx ) self . _idx = name_or_idx self . _set_explored_parameters_to_idx ( self . v_idx )", "nl": "Can make the trajectory behave as during a particular single run ."}}
{"translation": {"code": "def _load ( self , load_dict ) : try : self . v_protocol = load_dict . pop ( PickleParameter . PROTOCOL ) except KeyError : # For backwards compatibility dump = next ( load_dict . values ( ) ) self . v_protocol = PickleParameter . _get_protocol ( dump ) for key in load_dict : val = load_dict [ key ] self . _data [ key ] = pickle . loads ( val )", "nl": "Reconstructs all items from the pickle dumps in load_dict ."}}
{"translation": {"code": "def _store ( self ) : store_dict = { } for key , val in self . _data . items ( ) : store_dict [ key ] = pickle . dumps ( val , protocol = self . v_protocol ) store_dict [ PickleResult . PROTOCOL ] = self . v_protocol return store_dict", "nl": "Returns a dictionary containing pickle dumps"}}
{"translation": {"code": "def f_set_single ( self , name , item ) : if self . v_stored : self . _logger . debug ( 'You are changing an already stored result. If ' 'you not explicitly overwrite the data on disk, this change ' 'might be lost and not propagated to disk.' ) if name == PickleResult . PROTOCOL : raise AttributeError ( 'You cannot name an entry `%s`' % PickleResult . PROTOCOL ) self . _data [ name ] = item", "nl": "Adds a single data item to the pickle result ."}}
{"translation": {"code": "def f_iter_runs ( self , start = 0 , stop = None , step = 1 , yields = 'name' ) : if stop is None : stop = len ( self ) elif stop > len ( self ) : raise ValueError ( 'Stop cannot be larger than the trajectory lenght.' ) yields = yields . lower ( ) if yields == 'name' : yield_func = lambda x : self . f_idx_to_run ( x ) elif yields == 'idx' : yield_func = lambda x : x elif yields == 'self' : yield_func = lambda x : self elif yields == 'copy' : yield_func = lambda x : self . __copy__ ( ) else : raise ValueError ( 'Please choose yields among: `name`, `idx`, or `self`.' ) for idx in range ( start , stop , step ) : self . f_set_crun ( idx ) yield yield_func ( idx ) self . f_set_crun ( None )", "nl": "Makes the trajectory iterate over all runs ."}}
{"translation": {"code": "def f_shrink ( self , force = False ) : if self . _stored and not force : raise TypeError ( 'Your trajectory is already stored to disk or database, shrinking is ' 'not allowed.' ) for param in self . _explored_parameters . values ( ) : param . f_unlock ( ) try : param . _shrink ( ) except Exception as exc : self . _logger . error ( 'Could not shrink `%s` because of:`%s`' % ( param . v_full_name , repr ( exc ) ) ) # If we shrink, we do not have any explored parameters left and we can erase all # run information, and the length of the trajectory is 1 again. self . _explored_parameters = { } self . _run_information = { } self . _single_run_ids = { } self . _add_run_info ( 0 ) self . _test_run_addition ( 1 )", "nl": "Shrinks the trajectory and removes all exploration ranges from the parameters . Only possible if the trajectory has not been stored to disk before or was loaded as new ."}}
{"translation": {"code": "def f_find_idx ( self , name_list , predicate ) : if self . _is_run and not self . v_full_copy : raise TypeError ( 'You cannot use this function during a multiprocessing signle run and ' 'not having ``v_full_copy=True``.' ) if isinstance ( name_list , str ) : name_list = [ name_list ] # First create a list of iterators, each over the range of the matched parameters iter_list = [ ] for name in name_list : param = self . f_get ( name ) if not param . v_is_parameter : raise TypeError ( '`%s` is not a parameter it is a %s, find idx is not applicable' % ( name , str ( type ( param ) ) ) ) if param . f_has_range ( ) : iter_list . append ( iter ( param . f_get_range ( copy = False ) ) ) else : iter_list . append ( itools . repeat ( param . f_get ( ) , len ( self ) ) ) # Create a logical iterator returning `True` or `False` # whether the user's predicate matches the parameter data logic_iter = map ( predicate , * iter_list ) # Now the run indices are the the indices where `logic_iter` evaluates to `True` for idx , item in enumerate ( logic_iter ) : if item : yield idx", "nl": "Finds a single run index given a particular condition on parameters ."}}
{"translation": {"code": "def f_preset_parameter ( self , param_name , * args , * * kwargs ) : if not param_name . startswith ( 'parameters.' ) : param_name = 'parameters.' + param_name self . _preset ( param_name , args , kwargs )", "nl": "Presets parameter value before a parameter is added ."}}
{"translation": {"code": "def _add_run_info ( self , idx , name = '' , timestamp = 42.0 , finish_timestamp = 1.337 , runtime = 'forever and ever' , time = '>>Maybe time`s gone on strike' , completed = 0 , parameter_summary = 'Not yet my friend!' , short_environment_hexsha = 'N/A' ) : if idx in self . _single_run_ids : # Delete old entries, they might be replaced by a new name old_name = self . _single_run_ids [ idx ] del self . _single_run_ids [ old_name ] del self . _single_run_ids [ idx ] del self . _run_information [ old_name ] if name == '' : name = self . f_wildcard ( '$' , idx ) # The `_single_run_ids` dict is bidirectional and maps indices to run names and vice versa self . _single_run_ids [ name ] = idx self . _single_run_ids [ idx ] = name info_dict = { 'idx' : idx , 'timestamp' : timestamp , 'finish_timestamp' : finish_timestamp , 'runtime' : runtime , 'time' : time , 'completed' : completed , 'name' : name , 'parameter_summary' : parameter_summary , 'short_environment_hexsha' : short_environment_hexsha } self . _run_information [ name ] = info_dict self . _length = len ( self . _run_information )", "nl": "Adds a new run to the _run_information dict ."}}
{"translation": {"code": "def _finalize ( self , store_meta_data = True ) : self . _is_run = False self . f_set_crun ( None ) if store_meta_data : self . f_store ( only_init = True )", "nl": "Final rollback initiated by the environment"}}
{"translation": {"code": "def f_load_skeleton ( self ) : self . f_load ( self . v_name , as_new = False , load_parameters = pypetconstants . LOAD_SKELETON , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , with_run_information = False )", "nl": "Loads the full skeleton from the storage service ."}}
{"translation": {"code": "def f_load ( self , name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = True , load_data = None , max_depth = None , force = False , dynamic_imports = None , with_run_information = True , with_meta_data = True , storage_service = None , * * kwargs ) : # Do some argument validity checks first if name is None and index is None : name = self . v_name if as_new : load_parameters = pypetconstants . LOAD_DATA load_derived_parameters = pypetconstants . LOAD_NOTHING load_results = pypetconstants . LOAD_NOTHING load_other_data = pypetconstants . LOAD_NOTHING unused_kwargs = set ( kwargs . keys ( ) ) if self . v_storage_service is None or storage_service is not None or len ( kwargs ) > 0 : self . _storage_service , unused_kwargs = storage_factory ( storage_service = storage_service , trajectory = self , * * kwargs ) if len ( unused_kwargs ) > 0 : raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) if dynamic_imports is not None : self . f_add_to_dynamic_imports ( dynamic_imports ) if load_data is not None : load_parameters = load_data load_derived_parameters = load_data load_results = load_data load_other_data = load_data self . _storage_service . load ( pypetconstants . TRAJECTORY , self , trajectory_name = name , trajectory_index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , max_depth = max_depth , with_run_information = with_run_information , with_meta_data = with_meta_data , force = force ) # If a trajectory is newly loaded, all parameters are unlocked. if as_new : for param in self . _parameters . values ( ) : param . f_unlock ( )", "nl": "Loads a trajectory via the storage service ."}}
{"translation": {"code": "def f_backup ( self , * * kwargs ) : self . _storage_service . store ( pypetconstants . BACKUP , self , trajectory_name = self . v_name , * * kwargs )", "nl": "Backs up the trajectory with the given storage service ."}}
{"translation": {"code": "def _merge_derived_parameters ( self , other_trajectory , used_runs , rename_dict , allowed_translations , ignore_data ) : other_derived_parameters = other_trajectory . _derived_parameters . copy ( ) # get first run_idx new_first_run_idx = min ( used_runs . values ( ) ) run_name_dummy = other_trajectory . f_wildcard ( '$' , - 1 ) for param_name in other_derived_parameters : if param_name in ignore_data : continue split_name = param_name . split ( '.' ) if not any ( x in run_name_dummy for x in split_name ) : continue ignore_data . add ( param_name ) param = other_derived_parameters [ param_name ] new_param_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) if new_param_name in self : my_param = self . f_get ( new_param_name , fast_access = False ) if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : continue first_new_param_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = new_first_run_idx ) rename_dict [ param_name ] = first_new_param_name comment = param . v_comment param_type = param . f_get_class_name ( ) param_type = self . _create_class ( param_type ) first_param = self . f_add_leaf ( param_type , first_new_param_name , comment = comment ) for run_idx in used_runs . values ( ) : if run_idx == new_first_run_idx : continue next_name = self . _rename_full_name ( param_name , other_trajectory , new_run_idx = run_idx ) split_name = next_name . split ( '.' ) link_name = split_name . pop ( ) location_name = '.' . join ( split_name ) if not self . f_contains ( location_name , shortcuts = False ) : the_group = self . f_add_group ( location_name ) else : the_group = self . f_get ( location_name ) the_group . f_add_link ( link_name , first_param ) for param_name in other_derived_parameters : if param_name in ignore_data : continue split_name = param_name . split ( '.' ) ignore_data . add ( param_name ) if any ( x in other_trajectory . _reversed_wildcards and x not in allowed_translations for x in split_name ) : continue new_name = self . _rename_full_name ( param_name , other_trajectory , used_runs = used_runs ) if self . f_contains ( new_name ) : my_param = self . f_get ( new_name , fast_access = False ) param = other_derived_parameters [ param_name ] if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : continue else : self . _logger . error ( 'Could not merge parameter `%s`. ' 'I will ignore it!' % new_name ) rename_dict [ param_name ] = new_name", "nl": "Merges derived parameters that have the run_ALL in a name ."}}
{"translation": {"code": "def _merge_links ( self , other_trajectory , used_runs , allowed_translations , ignore_data ) : linked_items = other_trajectory . _linked_by run_name_dummys = set ( [ f ( - 1 ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) if len ( linked_items ) > 0 : self . _logger . info ( 'Merging potential links!' ) for old_linked_name in other_trajectory . _linked_by : if old_linked_name in ignore_data : continue split_name = old_linked_name . split ( '.' ) if any ( x in run_name_dummys for x in split_name ) : self . _logger . warning ( 'Ignoring all links linking to `%s` because ' 'I don`t know how to resolve links under `%s` nodes.' % ( old_linked_name , str ( run_name_dummys ) ) ) continue old_link_dict = other_trajectory . _linked_by [ old_linked_name ] split_name = old_linked_name . split ( '.' ) if all ( x in allowed_translations for x in split_name ) : new_linked_full_name = self . _rename_full_name ( old_linked_name , other_trajectory , used_runs = used_runs ) else : new_linked_full_name = old_linked_name for linking_node , link_set in old_link_dict . values ( ) : linking_full_name = linking_node . v_full_name split_name = linking_full_name . split ( '.' ) if any ( x in run_name_dummys for x in split_name ) : self . _logger . warning ( 'Ignoring links under `%s` because ' 'I don`t know how to resolve links ' 'under a `%s` node.' % ( linking_full_name , str ( run_name_dummys ) ) ) split_name = linking_full_name . split ( '.' ) if any ( x in allowed_translations for x in split_name ) : new_linking_full_name = self . _rename_full_name ( linking_full_name , other_trajectory , used_runs = used_runs ) else : new_linking_full_name = linking_full_name for link in link_set : if ( linking_full_name + '.' + link ) in ignore_data : continue if link in run_name_dummys : self . _logger . warning ( 'Ignoring link `%s` under `%s` because ' 'I don`t know how to resolve ' 'links named as `%s`.' % ( link , linking_full_name , str ( run_name_dummys ) ) ) continue try : new_linked_item = self . f_get ( new_linked_full_name , shortcuts = False ) if self . f_contains ( new_linking_full_name ) : new_linking_item = self . f_get ( new_linking_full_name , shortcuts = False ) else : new_linking_item = self . f_add_group ( new_linking_full_name ) if link in allowed_translations : run_indices , wildcards = other_trajectory . _reversed_wildcards [ link ] link = self . f_wildcard ( wildcards [ 0 ] , used_runs [ run_indices [ 0 ] ] ) if not link in new_linking_item . _links : new_linking_item . f_add_link ( link , new_linked_item ) else : self . _logger . debug ( 'Link `%s` exists already under `%s`.' % ( link , new_linked_item . v_full_name ) ) except ( AttributeError , ValueError ) as exc : self . _logger . error ( 'Could not copy link `%s` under `%s` linking ' 'to `%s` due to `%s`' % ( link , linking_full_name , old_linked_name , repr ( exc ) ) )", "nl": "Merges all links"}}
{"translation": {"code": "def _merge_results ( self , other_trajectory , rename_dict , used_runs , allowed_translations , ignore_data ) : other_results = other_trajectory . _results . copy ( ) for result_name in other_results : if result_name in ignore_data : continue split_name = result_name . split ( '.' ) ignore_data . add ( result_name ) if any ( x in other_trajectory . _reversed_wildcards and x not in allowed_translations for x in split_name ) : continue new_name = self . _rename_full_name ( result_name , other_trajectory , used_runs = used_runs ) if self . f_contains ( new_name ) : self . _logger . warning ( 'I found result `%s` already, I will ignore it.' % new_name ) continue rename_dict [ result_name ] = new_name", "nl": "Merges all results ."}}
{"translation": {"code": "def f_migrate ( self , new_name = None , in_store = False , new_storage_service = None , * * kwargs ) : if new_name is not None : self . _name = new_name unused_kwargs = set ( kwargs . keys ( ) ) if new_storage_service is not None or len ( kwargs ) > 0 : self . _storage_service , unused_kwargs = storage_factory ( storage_service = new_storage_service , trajectory = self , * * kwargs ) if len ( unused_kwargs ) > 0 : raise ValueError ( 'The following keyword arguments were not used: `%s`' % str ( unused_kwargs ) ) self . _stored = in_store", "nl": "Can be called to rename and relocate the trajectory ."}}
{"translation": {"code": "def f_store ( self , only_init = False , store_data = pypetconstants . STORE_DATA , max_depth = None ) : if self . _is_run : if self . _new_nodes or self . _new_links : self . _storage_service . store ( pypetconstants . SINGLE_RUN , self , trajectory_name = self . v_name , recursive = not only_init , store_data = store_data , max_depth = max_depth ) else : self . _storage_service . store ( pypetconstants . TRAJECTORY , self , trajectory_name = self . v_name , only_init = only_init , store_data = store_data , max_depth = max_depth ) self . _stored = True", "nl": "Stores the trajectory to disk and recursively all data in the tree ."}}
{"translation": {"code": "def f_get_run_names ( self , sort = True ) : if sort : return [ self . f_idx_to_run ( idx ) for idx in range ( len ( self ) ) ] else : return list ( self . _run_information . keys ( ) )", "nl": "Returns a list of run names ."}}
{"translation": {"code": "def f_explore ( self , build_dict ) : for run_idx in range ( len ( self ) ) : if self . f_is_completed ( run_idx ) : raise TypeError ( 'You cannot explore a trajectory which has been explored before, ' 'please use `f_expand` instead.' ) added_explored_parameters = [ ] try : length = len ( self ) for key , builditerable in build_dict . items ( ) : act_param = self . f_get ( key ) if not act_param . v_is_leaf or not act_param . v_is_parameter : raise ValueError ( '%s is not an appropriate search string for a parameter.' % key ) act_param . f_unlock ( ) act_param . _explore ( builditerable ) added_explored_parameters . append ( act_param ) full_name = act_param . v_full_name self . _explored_parameters [ full_name ] = act_param act_param . _explored = True # Compare the length of two consecutive parameters in the `build_dict` if len ( self . _explored_parameters ) == 1 : length = act_param . f_get_range_length ( ) elif not length == act_param . f_get_range_length ( ) : raise ValueError ( 'The parameters to explore have not the same size!' ) for irun in range ( length ) : self . _add_run_info ( irun ) self . _test_run_addition ( length ) except Exception : # Remove the added parameters again for param in added_explored_parameters : param . f_unlock ( ) param . _shrink ( ) param . _explored = False full_name = param . v_full_name del self . _explored_parameters [ full_name ] if len ( self . _explored_parameters ) == 0 : self . f_shrink ( force = True ) raise", "nl": "Prepares the trajectory to explore the parameter space ."}}
{"translation": {"code": "def _store ( self ) : if self . _data is not None : store_dict = { 'data' : ObjectTable ( data = { 'data' : [ self . _data ] } ) } if self . f_has_range ( ) : store_dict [ 'explored_data' ] = ObjectTable ( data = { 'data' : self . _explored_range } ) self . _locked = True return store_dict", "nl": "Returns a dictionary of formatted data understood by the storage service ."}}
{"translation": {"code": "def diff_lorenz ( value_array , sigma , beta , rho ) : diff_array = np . zeros ( 3 ) diff_array [ 0 ] = sigma * ( value_array [ 1 ] - value_array [ 0 ] ) diff_array [ 1 ] = value_array [ 0 ] * ( rho - value_array [ 2 ] ) - value_array [ 1 ] diff_array [ 2 ] = value_array [ 0 ] * value_array [ 1 ] - beta * value_array [ 2 ] return diff_array", "nl": "The Lorenz attractor differential equation"}}
{"translation": {"code": "def euler_scheme ( traj , diff_func ) : steps = traj . steps initial_conditions = traj . initial_conditions dimension = len ( initial_conditions ) # This array will collect the results result_array = np . zeros ( ( steps , dimension ) ) # Get the function parameters stored into `traj` as a dictionary # with the (short) names as keys : func_params_dict = traj . func_params . f_to_dict ( short_names = True , fast_access = True ) # Take initial conditions as first result result_array [ 0 ] = initial_conditions # Now we compute the Euler Scheme steps-1 times for idx in range ( 1 , steps ) : result_array [ idx ] = diff_func ( result_array [ idx - 1 ] , * * func_params_dict ) * traj . dt + result_array [ idx - 1 ] # Note the **func_params_dict unzips the dictionary, it's the reverse of **kwargs in function # definitions! #Finally we want to keep the results traj . f_add_result ( 'euler_evolution' , data = result_array , comment = 'Our time series data!' )", "nl": "Simulation function for Euler integration ."}}
{"translation": {"code": "def diff_roessler ( value_array , a , c ) : b = a diff_array = np . zeros ( 3 ) diff_array [ 0 ] = - value_array [ 1 ] - value_array [ 2 ] diff_array [ 1 ] = value_array [ 0 ] + a * value_array [ 1 ] diff_array [ 2 ] = b + value_array [ 2 ] * ( value_array [ 0 ] - c ) return diff_array", "nl": "The Roessler attractor differential equation"}}
{"translation": {"code": "def add_parameters ( traj ) : traj . f_add_parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) # Here we want to add the initial conditions as an array parameter, since we will simulate # a 3-D differential equation, that is the Roessler attractor # (https://en.wikipedia.org/wiki/R%C3%B6ssler_attractor) traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) # Per default we choose the name `'diff_lorenz'` as in the last example traj . f_add_parameter ( 'diff_name' , 'diff_lorenz' , comment = 'Name of our differential equation' ) # We want some control flow depending on which name we really choose if traj . diff_name == 'diff_lorenz' : # These parameters are for the Lorenz differential equation traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) traj . f_add_parameter ( 'func_params.rho' , 28.0 ) elif traj . diff_name == 'diff_roessler' : # If we use the Roessler system we need different parameters traj . f_add_parameter ( 'func_params.a' , 0.1 ) traj . f_add_parameter ( 'func_params.c' , 14.0 ) else : raise ValueError ( 'I don\\'t know what %s is.' % traj . diff_name )", "nl": "Adds all necessary parameters to the traj container ."}}
{"translation": {"code": "def add_parameters ( traj ) : traj . f_add_parameter ( 'steps' , 10000 , comment = 'Number of time steps to simulate' ) traj . f_add_parameter ( 'dt' , 0.01 , comment = 'Step size' ) # Here we want to add the initial conditions as an array parameter. We will simulate # a 3-D differential equation, the Lorenz attractor. traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , comment = 'Our initial conditions, as default we will start from' ' origin!' ) # We will group all parameters of the Lorenz differential equation into the group 'func_params' traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) traj . f_add_parameter ( 'func_params.rho' , 28.0 ) #For the fun of it we will annotate the  group traj . func_params . v_annotations . info = 'This group contains as default the original values chosen ' 'by Edward Lorenz in 1963. Check it out on wikipedia ' '(https://en.wikipedia.org/wiki/Lorenz_attractor)!'", "nl": "Adds all necessary parameters to the traj container"}}
{"translation": {"code": "def _rename ( self , full_name ) : self . _full_name = full_name if full_name : self . _name = full_name . rsplit ( '.' , 1 ) [ - 1 ]", "nl": "Renames the tree node"}}
{"translation": {"code": "def _make_child_iterator ( node , with_links , current_depth = 0 ) : cdp1 = current_depth + 1 if with_links : iterator = ( ( cdp1 , x [ 0 ] , x [ 1 ] ) for x in node . _children . items ( ) ) else : leaves = ( ( cdp1 , x [ 0 ] , x [ 1 ] ) for x in node . _leaves . items ( ) ) groups = ( ( cdp1 , y [ 0 ] , y [ 1 ] ) for y in node . _groups . items ( ) ) iterator = itools . chain ( groups , leaves ) return iterator", "nl": "Returns an iterator over a node s children ."}}
{"translation": {"code": "def _recursive_traversal_bfs ( node , linked_by = None , max_depth = float ( 'inf' ) , with_links = True , in_search = False , predicate = None ) : if predicate is None : predicate = lambda x : True iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) #iterator_queue = iter([(0, node.v_name, node)]) start = True visited_linked_nodes = set ( [ ] ) while True : try : depth , name , item = next ( iterator_queue ) full_name = item . _full_name if start or predicate ( item ) : if full_name in visited_linked_nodes : if in_search : # We need to return the node again to check if a link to the node # has to be found yield depth , name , item elif depth <= max_depth : if start : start = False else : if in_search : yield depth , name , item else : yield item if full_name in linked_by : visited_linked_nodes . add ( full_name ) if not item . _is_leaf and depth < max_depth : child_iterator = NaturalNamingInterface . _make_child_iterator ( item , with_links , current_depth = depth ) iterator_queue . add ( child_iterator ) #iterator_queue = itools.chain(iterator_queue, child_iterator) except StopIteration : break", "nl": "Iterator function traversing the tree below node in breadth first search manner ."}}
{"translation": {"code": "def _prm_meta_add_summary ( self , instance ) : if instance . v_comment == '' : return False where = instance . v_branch definitely_store_comment = True # Get the hexdigest of the comment to see if such a comment has been stored before bytes_comment = instance . v_comment . encode ( 'utf-8' ) hexdigest = hashlib . sha1 ( bytes_comment ) . hexdigest ( ) hexdigest = hexdigest . encode ( 'utf-8' ) # Get the overview table table_name = where + '_summary' # Check if the overview table exists, otherwise skip the rest of # the meta adding if table_name in self . _overview_group : table = getattr ( self . _overview_group , table_name ) else : return definitely_store_comment try : condvars = { 'hexdigestcol' : table . cols . hexdigest , 'hexdigest' : hexdigest } condition = \"\"\"(hexdigestcol == hexdigest)\"\"\" row_iterator = table . where ( condition , condvars = condvars ) row = None try : row = next ( row_iterator ) except StopIteration : pass if row is None : self . _all_store_param_or_result_table_entry ( instance , table , flags = ( HDF5StorageService . ADD_ROW , ) , additional_info = { 'hexdigest' : hexdigest } ) definitely_store_comment = True else : definitely_store_comment = False self . _all_kill_iterator ( row_iterator ) except pt . NoSuchNodeError : definitely_store_comment = True return definitely_store_comment", "nl": "Adds data to the summary tables and returns if instance s comment has to be stored ."}}
{"translation": {"code": "def _overview_group ( self ) : if self . _overview_group_ is None : self . _overview_group_ = self . _all_create_or_get_groups ( 'overview' ) [ 0 ] return self . _overview_group_", "nl": "Direct link to the overview group"}}
{"translation": {"code": "def _all_cut_string ( string , max_length , logger ) : if len ( string ) > max_length : logger . debug ( 'The string `%s` was too long I truncated it to' ' %d characters' % ( string , max_length ) ) string = string [ 0 : max_length - 3 ] + '...' . encode ( 'utf-8' ) return string", "nl": "Cuts string data to the maximum length allowed in a pytables column if string is too long ."}}
{"translation": {"code": "def add_commit_variables ( traj , commit ) : git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) git_short_name = str ( commit . hexsha [ 0 : 7 ] ) git_commit_name = 'commit_%s_' % git_short_name git_commit_name = 'git.' + git_commit_name + git_time_value if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = False ) : git_commit_name += '.' # Add the hexsha traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , comment = 'SHA-1 hash of commit' ) # Add the description string traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , comment = 'String describing the commits hex sha based on ' 'the closest Reference' ) # Add unix epoch traj . f_add_config ( git_commit_name + 'committed_date' , commit . committed_date , comment = 'Date of commit as unix epoch seconds' ) # Add commit message traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , comment = 'The commit message' )", "nl": "Adds commit information to the trajectory ."}}
{"translation": {"code": "def _load ( self , load_dict ) : if self . v_locked : raise pex . ParameterLockedException ( 'Parameter `%s` is locked!' % self . v_full_name ) try : is_dia = load_dict [ 'data%sis_dia' % SparseParameter . IDENTIFIER ] name_list = self . _get_name_list ( is_dia ) rename_list = [ 'data%s%s' % ( SparseParameter . IDENTIFIER , name ) for name in name_list ] data_list = [ load_dict [ name ] for name in rename_list ] self . _data = self . _reconstruct_matrix ( data_list ) if 'explored_data' + SparseParameter . IDENTIFIER in load_dict : explore_table = load_dict [ 'explored_data' + SparseParameter . IDENTIFIER ] idx_col = explore_table [ 'idx' ] dia_col = explore_table [ 'is_dia' ] explore_list = [ ] for irun , name_id in enumerate ( idx_col ) : is_dia = dia_col [ irun ] # To make everything work with the old format we have the try catch block try : name_list = self . _build_names ( name_id , is_dia ) data_list = [ load_dict [ name ] for name in name_list ] except KeyError : name_list = self . _build_names_old ( name_id , is_dia ) data_list = [ load_dict [ name ] for name in name_list ] matrix = self . _reconstruct_matrix ( data_list ) explore_list . append ( matrix ) self . _explored_range = explore_list self . _explored = True except KeyError : super ( SparseParameter , self ) . _load ( load_dict ) self . _default = self . _data self . _locked = True", "nl": "Reconstructs the data and exploration array"}}
{"translation": {"code": "def _reconstruct_matrix ( data_list ) : matrix_format = data_list [ 0 ] data = data_list [ 1 ] is_empty = isinstance ( data , str ) and data == '__empty__' if matrix_format == 'csc' : if is_empty : return spsp . csc_matrix ( data_list [ 4 ] ) else : return spsp . csc_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) elif matrix_format == 'csr' : if is_empty : return spsp . csr_matrix ( data_list [ 4 ] ) else : return spsp . csr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) elif matrix_format == 'bsr' : if is_empty : # We have an empty matrix, that cannot be build as in elee case return spsp . bsr_matrix ( data_list [ 4 ] ) else : return spsp . bsr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) elif matrix_format == 'dia' : if is_empty : return spsp . dia_matrix ( data_list [ 3 ] ) else : return spsp . dia_matrix ( tuple ( data_list [ 1 : 3 ] ) , shape = data_list [ 3 ] ) else : raise RuntimeError ( 'You shall not pass!' )", "nl": "Reconstructs a matrix from a list containing sparse matrix extracted properties"}}
{"translation": {"code": "def _build_names ( self , name_idx , is_dia ) : name_list = self . _get_name_list ( is_dia ) return tuple ( [ 'explored%s.set_%05d.xspm_%s_%08d' % ( SparseParameter . IDENTIFIER , name_idx // 200 , name , name_idx ) for name in name_list ] )", "nl": "Formats a name for storage"}}
{"translation": {"code": "def _serialize_matrix ( matrix ) : if ( spsp . isspmatrix_csc ( matrix ) or spsp . isspmatrix_csr ( matrix ) or spsp . isspmatrix_bsr ( matrix ) ) : if matrix . size > 0 : return_list = [ matrix . data , matrix . indices , matrix . indptr , matrix . shape ] else : # For empty matrices we only need the shape return_list = [ '__empty__' , ( ) , ( ) , matrix . shape ] return_names = SparseParameter . OTHER_NAME_LIST if spsp . isspmatrix_csc ( matrix ) : return_list = [ 'csc' ] + return_list elif spsp . isspmatrix_csr ( matrix ) : return_list = [ 'csr' ] + return_list elif spsp . isspmatrix_bsr ( matrix ) : return_list = [ 'bsr' ] + return_list else : raise RuntimeError ( 'You shall not pass!' ) elif spsp . isspmatrix_dia ( matrix ) : if matrix . size > 0 : return_list = [ 'dia' , matrix . data , matrix . offsets , matrix . shape ] else : # For empty matrices we only need the shape return_list = [ 'dia' , '__empty__' , ( ) , matrix . shape ] return_names = SparseParameter . DIA_NAME_LIST else : raise RuntimeError ( 'You shall not pass!' ) hash_list = [ ] # Extract the `data` property of a read-only numpy array in order to have something # hashable. for item in return_list : if type ( item ) is np . ndarray : # item.flags.writeable = False hash_list . append ( HashArray ( item ) ) else : hash_list . append ( item ) return return_list , return_names , tuple ( hash_list )", "nl": "Extracts data from a sparse matrix to make it serializable in a human readable format ."}}
{"translation": {"code": "def _is_supported_matrix ( data ) : return ( spsp . isspmatrix_csc ( data ) or spsp . isspmatrix_csr ( data ) or spsp . isspmatrix_bsr ( data ) or spsp . isspmatrix_dia ( data ) )", "nl": "Checks if a data is csr csc bsr or dia Scipy sparse matrix"}}
{"translation": {"code": "def _equal_values ( self , val1 , val2 ) : if self . _is_supported_matrix ( val1 ) : if self . _is_supported_matrix ( val2 ) : _ , _ , hash_tuple_1 = self . _serialize_matrix ( val1 ) _ , _ , hash_tuple_2 = self . _serialize_matrix ( val2 ) return hash ( hash_tuple_1 ) == hash ( hash_tuple_2 ) else : return False else : return super ( SparseParameter , self ) . _equal_values ( val1 , val2 )", "nl": "Matrices are equal if they hash to the same value ."}}
{"translation": {"code": "def make_git_commit ( environment , git_repository , user_message , git_fail ) : # Import GitPython, we do it here to allow also users not having GitPython installed # to use the normal environment # Open the repository repo = git . Repo ( git_repository ) index = repo . index traj = environment . v_trajectory # Create the commit message and append the trajectory name and comment if traj . v_comment : commentstr = ', Comment: `%s`' % traj . v_comment else : commentstr = '' if user_message : user_message += ' -- ' message = '%sTrajectory: `%s`, Time: `%s`, %s' % ( user_message , traj . v_name , traj . v_time , commentstr ) # Detect changes: diff = index . diff ( None ) if diff : if git_fail : # User requested fail instead of a new commit raise pex . GitDiffError ( 'Found not committed changes!' ) # Make the commit repo . git . add ( '-u' ) commit = index . commit ( message ) new_commit = True else : # Take old commit commit = repo . commit ( None ) new_commit = False # Add the commit info to the trajectory add_commit_variables ( traj , commit ) return new_commit , commit . hexsha", "nl": "Makes a commit and returns if a new commit was triggered and the SHA_1 code of the commit ."}}
{"translation": {"code": "def f_get_config ( self , fast_access = False , copy = True ) : return self . _return_item_dictionary ( self . _config , fast_access , copy )", "nl": "Returns a dictionary containing the full config names as keys and the config parameters or the config parameter data items as values ."}}
{"translation": {"code": "def _trj_check_version ( self , version , python , force ) : curr_python = pypetconstants . python_version_string if ( version != VERSION or curr_python != python ) and not force : raise pex . VersionMismatchError ( 'Current pypet version is %s used under python %s ' '  but your trajectory' ' was created with version %s and python %s.' ' Use >>force=True<< to perform your load regardless' ' of version mismatch.' % ( VERSION , curr_python , version , python ) ) elif version != VERSION or curr_python != python : self . _logger . warning ( 'Current pypet version is %s with python %s but your trajectory' ' was created with version %s under python %s.' ' Yet, you enforced the load, so I will' ' handle the trajectory despite the' ' version mismatch.' % ( VERSION , curr_python , version , python ) )", "nl": "Checks for version mismatch"}}
{"translation": {"code": "def _trj_read_out_row ( colnames , row ) : result_dict = { } for colname in colnames : result_dict [ colname ] = row [ colname ] return result_dict", "nl": "Reads out a row and returns a dictionary containing the row content ."}}
{"translation": {"code": "def _store ( self ) : store_dict = { } for key in self . _data : val = self . _data [ key ] if SparseParameter . _is_supported_matrix ( val ) : data_list , name_list , hash_tuple = SparseParameter . _serialize_matrix ( val ) rename_list = [ '%s%s%s' % ( key , SparseParameter . IDENTIFIER , name ) for name in name_list ] is_dia = int ( len ( rename_list ) == 4 ) store_dict [ key + SparseResult . IDENTIFIER + 'is_dia' ] = is_dia for idx , name in enumerate ( rename_list ) : store_dict [ name ] = data_list [ idx ] else : store_dict [ key ] = val return store_dict", "nl": "Returns a storage dictionary understood by the storage service ."}}
{"translation": {"code": "def _load ( self , load_dict ) : for key in list ( load_dict . keys ( ) ) : # We delete keys over time: if key in load_dict : if SparseResult . IDENTIFIER in key : new_key = key . split ( SparseResult . IDENTIFIER ) [ 0 ] is_dia = load_dict . pop ( new_key + SparseResult . IDENTIFIER + 'is_dia' ) name_list = SparseParameter . _get_name_list ( is_dia ) rename_list = [ '%s%s%s' % ( new_key , SparseResult . IDENTIFIER , name ) for name in name_list ] data_list = [ load_dict . pop ( name ) for name in rename_list ] matrix = SparseParameter . _reconstruct_matrix ( data_list ) self . _data [ new_key ] = matrix else : self . _data [ key ] = load_dict [ key ]", "nl": "Loads data from load_dict"}}
{"translation": {"code": "def _supports ( self , item ) : if SparseParameter . _is_supported_matrix ( item ) : return True else : return super ( SparseResult , self ) . _supports ( item )", "nl": "Supports everything of parent class and csr csc bsr and dia sparse matrices ."}}
{"translation": {"code": "def deprecated ( msg = '' ) : def wrapper ( func ) : @ functools . wraps ( func ) def new_func ( * args , * * kwargs ) : warning_string = \"Call to deprecated function or property `%s`.\" % func . __name__ warning_string = warning_string + ' ' + msg warnings . warn ( warning_string , category = DeprecationWarning , ) return func ( * args , * * kwargs ) return new_func return wrapper", "nl": "This is a decorator which can be used to mark functions as deprecated . It will result in a warning being emitted when the function is used ."}}
{"translation": {"code": "def _wrap_handling ( kwargs ) : _configure_logging ( kwargs , extract = False ) # Main job, make the listener to the queue start receiving message for writing to disk. handler = kwargs [ 'handler' ] graceful_exit = kwargs [ 'graceful_exit' ] # import cProfile as profile # profiler = profile.Profile() # profiler.enable() if graceful_exit : sigint_handling . start ( ) handler . run ( )", "nl": "Starts running a queue handler and creates a log file for the queue ."}}
{"translation": {"code": "def _merge_config ( self , other_trajectory ) : self . _logger . info ( 'Merging config!' ) # Merge git commit meta data if 'config.git' in other_trajectory : self . _logger . info ( 'Merging git commits!' ) git_node = other_trajectory . f_get ( 'config.git' ) param_list = [ ] for param in git_node . f_iter_leaves ( with_links = False ) : if not self . f_contains ( param . v_full_name , shortcuts = False ) : param_list . append ( self . f_add_config ( param ) ) if param_list : self . f_store_items ( param_list ) self . _logger . info ( 'Merging git commits successful!' ) # Merge environment meta data if 'config.environment' in other_trajectory : self . _logger . info ( 'Merging environment config!' ) env_node = other_trajectory . f_get ( 'config.environment' ) param_list = [ ] for param in env_node . f_iter_leaves ( with_links = False ) : if not self . f_contains ( param . v_full_name , shortcuts = False ) : param_list . append ( self . f_add_config ( param ) ) if param_list : self . f_store_items ( param_list ) self . _logger . info ( 'Merging config successful!' ) # Merge meta data of previous merges if 'config.merge' in other_trajectory : self . _logger . info ( 'Merging merge config!' ) merge_node = other_trajectory . f_get ( 'config.merge' ) param_list = [ ] for param in merge_node . f_iter_leaves ( with_links = False ) : if not self . f_contains ( param . v_full_name , shortcuts = False ) : param_list . append ( self . f_add_config ( param ) ) if param_list : self . f_store_items ( param_list ) self . _logger . info ( 'Merging config successful!' )", "nl": "Merges meta data about previous merges git commits and environment settings of the other trajectory into the current one ."}}
{"translation": {"code": "def f_get_results ( self , fast_access = False , copy = True ) : return self . _return_item_dictionary ( self . _results , fast_access , copy )", "nl": "Returns a dictionary containing the full result names as keys and the corresponding result objects or result data items as values ."}}
{"translation": {"code": "def _trj_fill_run_table ( self , traj , start , stop ) : def _make_row ( info_dict ) : row = ( info_dict [ 'idx' ] , info_dict [ 'name' ] , info_dict [ 'time' ] , info_dict [ 'timestamp' ] , info_dict [ 'finish_timestamp' ] , info_dict [ 'runtime' ] , info_dict [ 'parameter_summary' ] , info_dict [ 'short_environment_hexsha' ] , info_dict [ 'completed' ] ) return row runtable = getattr ( self . _overview_group , 'runs' ) rows = [ ] updated_run_information = traj . _updated_run_information for idx in range ( start , stop ) : info_dict = traj . _run_information [ traj . _single_run_ids [ idx ] ] rows . append ( _make_row ( info_dict ) ) updated_run_information . discard ( idx ) if rows : runtable . append ( rows ) runtable . flush ( ) # Store all runs that are updated and that have not been stored yet rows = [ ] indices = [ ] for idx in updated_run_information : info_dict = traj . f_get_run_information ( idx , copy = False ) rows . append ( _make_row ( info_dict ) ) indices . append ( idx ) if rows : runtable . modify_coordinates ( indices , rows ) traj . _updated_run_information = set ( )", "nl": "Fills the run overview table with information ."}}
{"translation": {"code": "def _all_get_node_by_name ( self , name ) : path_name = name . replace ( '.' , '/' ) where = '/%s/%s' % ( self . _trajectory_name , path_name ) return self . _hdf5file . get_node ( where = where )", "nl": "Returns an HDF5 node by the path specified in name"}}
{"translation": {"code": "def _merge_slowly ( self , other_trajectory , rename_dict ) : for other_key in rename_dict : new_key = rename_dict [ other_key ] other_instance = other_trajectory . f_get ( other_key ) if other_instance . f_is_empty ( ) : # To suppress warnings if nothing needs to be loaded with self . _nn_interface . _disable_logging : other_trajectory . f_load_item ( other_instance ) if not self . f_contains ( new_key ) : class_name = other_instance . f_get_class_name ( ) class_ = self . _create_class ( class_name ) my_instance = self . f_add_leaf ( class_ , new_key ) else : my_instance = self . f_get ( new_key , shortcuts = False ) if not my_instance . f_is_empty ( ) : raise RuntimeError ( 'Something is wrong! Your item `%s` should be empty.' % new_key ) load_dict = other_instance . _store ( ) my_instance . _load ( load_dict ) my_instance . f_set_annotations ( * * other_instance . v_annotations . f_to_dict ( copy = False ) ) my_instance . v_comment = other_instance . v_comment self . f_store_item ( my_instance ) # We do not want to blow up the RAM Memory if other_instance . v_is_parameter : other_instance . f_unlock ( ) my_instance . f_unlock ( ) other_instance . f_empty ( ) my_instance . f_empty ( )", "nl": "Merges trajectories by loading iteratively items of the other trajectory and store it into the current trajectory ."}}
{"translation": {"code": "def _prepare_experiment ( self ) : if len ( self . _changed_default_parameters ) : raise pex . PresettingError ( 'The following parameters were supposed to replace a ' 'default value, but it was never tried to ' 'add default values with these names: %s' % str ( self . _changed_default_parameters ) ) self . f_lock_parameters ( ) self . f_lock_derived_parameters ( )", "nl": "Called by the environment to make some initial configurations before performing the individual runs ."}}
{"translation": {"code": "def f_get_from_runs ( self , name , include_default_run = True , use_indices = False , fast_access = False , with_links = True , shortcuts = True , max_depth = None , auto_load = False ) : result_dict = OrderedDict ( ) old_crun = self . v_crun try : if len ( self . _run_parent_groups ) > 0 : for run_name in self . f_iter_runs ( ) : # Iterate over all runs value = None already_found = False for run_parent_group in self . _run_parent_groups . values ( ) : if run_name not in run_parent_group . _children : continue try : value = run_parent_group . f_get ( run_name + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) if already_found : raise pex . NotUniqueNodeError ( '`%s` has been found several times ' 'in one run.' % name ) else : already_found = True except ( AttributeError , pex . DataNotInStorageError ) : pass if value is None and include_default_run : for run_parent_group in self . _run_parent_groups . values ( ) : try : value = run_parent_group . f_get ( self . f_wildcard ( '$' , - 1 ) + '.' + name , fast_access = False , with_links = with_links , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load ) if already_found : raise pex . NotUniqueNodeError ( '`%s` has been found several ' 'times in one run.' % name ) else : already_found = True except ( AttributeError , pex . DataNotInStorageError ) : pass if value is not None : if value . v_is_leaf : value = self . _nn_interface . _apply_fast_access ( value , fast_access ) if use_indices : key = self . f_idx_to_run ( run_name ) else : key = run_name result_dict [ key ] = value return result_dict finally : self . v_crun = old_crun", "nl": "Searches for all occurrences of name in each run ."}}
{"translation": {"code": "def _is_completed ( self , name_or_id = None ) : if name_or_id is None : return all ( ( runinfo [ 'completed' ] for runinfo in self . _run_information . values ( ) ) ) else : return self . f_get_run_information ( name_or_id , copy = False ) [ 'completed' ]", "nl": "Private function such that it can still be called by the environment during a single run"}}
{"translation": {"code": "def _debug ( self ) : class Bunch ( object ) : \"\"\"Dummy container class\"\"\" pass debug_tree = Bunch ( ) if not self . v_annotations . f_is_empty ( ) : debug_tree . v_annotations = self . v_annotations if not self . v_comment == '' : debug_tree . v_comment = self . v_comment for leaf_name in self . _leaves : leaf = self . _leaves [ leaf_name ] setattr ( debug_tree , leaf_name , leaf ) for link_name in self . _links : linked_node = self . _links [ link_name ] setattr ( debug_tree , link_name , 'Link to `%s`' % linked_node . v_full_name ) for group_name in self . _groups : group = self . _groups [ group_name ] setattr ( debug_tree , group_name , group . _debug ( ) ) return debug_tree", "nl": "Creates a dummy object containing the whole tree to make unfolding easier ."}}
{"translation": {"code": "def f_add_group ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = GROUP , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = False )", "nl": "Adds an empty generic group under the current node ."}}
{"translation": {"code": "def f_add_leaf ( self , * args , * * kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = LEAF , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = False )", "nl": "Adds an empty generic leaf under the current node ."}}
{"translation": {"code": "def _srvc_check_hdf_properties ( self , traj ) : for attr_name in HDF5StorageService . ATTR_LIST : try : config = traj . f_get ( 'config.hdf5.' + attr_name ) . f_get ( ) setattr ( self , attr_name , config ) except AttributeError : self . _logger . debug ( 'Could not find `%s` in traj config, ' 'using (default) value `%s`.' % ( attr_name , str ( getattr ( self , attr_name ) ) ) ) for attr_name , table_name in HDF5StorageService . NAME_TABLE_MAPPING . items ( ) : try : if table_name in ( 'parameters' , 'config' ) : table_name += '_overview' config = traj . f_get ( 'config.hdf5.overview.' + table_name ) . f_get ( ) setattr ( self , attr_name , config ) except AttributeError : self . _logger . debug ( 'Could not find `%s` in traj config, ' 'using (default) value `%s`.' % ( table_name , str ( getattr ( self , attr_name ) ) ) ) for attr_name , name in HDF5StorageService . PR_ATTR_NAME_MAPPING . items ( ) : try : config = traj . f_get ( 'config.hdf5.' + name ) . f_get ( ) setattr ( self , attr_name , config ) except AttributeError : self . _logger . debug ( 'Could not find `%s` in traj config, ' 'using (default) value `%s`.' % ( name , str ( getattr ( self , attr_name ) ) ) ) if ( ( not self . _overview_results_summary or not self . _overview_derived_parameters_summary ) and self . _purge_duplicate_comments ) : raise RuntimeError ( 'You chose to purge duplicate comments but disabled a summary ' 'table. You can only use the purging if you enable ' 'the summary tables.' ) self . _filters = None", "nl": "Reads out the properties for storing new data into the hdf5file"}}
{"translation": {"code": "def _merge_single_runs ( self , other_trajectory , used_runs ) : count = len ( self ) # Variable to count the increasing new run indices and create # new run names run_indices = range ( len ( other_trajectory ) ) run_name_dict = OrderedDict ( ) to_store_groups_with_annotations = [ ] for idx in run_indices : # Iterate through all used runs and store annotated groups and mark results and # derived parameters for merging if idx in used_runs : # Update the run information dict of the current trajectory other_info_dict = other_trajectory . f_get_run_information ( idx ) time_ = other_info_dict [ 'time' ] timestamp = other_info_dict [ 'timestamp' ] completed = other_info_dict [ 'completed' ] short_environment_hexsha = other_info_dict [ 'short_environment_hexsha' ] finish_timestamp = other_info_dict [ 'finish_timestamp' ] runtime = other_info_dict [ 'runtime' ] new_idx = used_runs [ idx ] new_runname = self . f_wildcard ( '$' , new_idx ) run_name_dict [ idx ] = new_runname info_dict = dict ( idx = new_idx , time = time_ , timestamp = timestamp , completed = completed , short_environment_hexsha = short_environment_hexsha , finish_timestamp = finish_timestamp , runtime = runtime ) self . _add_run_info ( * * info_dict )", "nl": "Updates the run_information of the current trajectory ."}}
{"translation": {"code": "def _set_explored_parameters_to_idx ( self , idx ) : for param in self . _explored_parameters . values ( ) : if param is not None : param . _set_parameter_access ( idx )", "nl": "Notifies the explored parameters what current point in the parameter space they should represent ."}}
{"translation": {"code": "def add_parameters ( traj ) : print ( 'Adding Parameters' ) traj . f_add_parameter ( 'neuron.V_init' , 0.0 , comment = 'The initial condition for the ' 'membrane potential' ) traj . f_add_parameter ( 'neuron.I' , 0.0 , comment = 'The externally applied current.' ) traj . f_add_parameter ( 'neuron.tau_V' , 10.0 , comment = 'The membrane time constant in milliseconds' ) traj . f_add_parameter ( 'neuron.tau_ref' , 5.0 , comment = 'The refractory period in milliseconds ' 'where the membrane potnetial ' 'is clamped.' ) traj . f_add_parameter ( 'simulation.duration' , 1000.0 , comment = 'The duration of the experiment in ' 'milliseconds.' ) traj . f_add_parameter ( 'simulation.dt' , 0.1 , comment = 'The step size of an Euler integration step.' )", "nl": "Adds all parameters to traj"}}
{"translation": {"code": "def neuron_postproc ( traj , result_list ) : # Let's create a pandas DataFrame to sort the computed firing rate according to the # parameters. We could have also used a 2D numpy array. # But a pandas DataFrame has the advantage that we can index into directly with # the parameter values without translating these into integer indices. I_range = traj . par . neuron . f_get ( 'I' ) . f_get_range ( ) ref_range = traj . par . neuron . f_get ( 'tau_ref' ) . f_get_range ( ) I_index = sorted ( set ( I_range ) ) ref_index = sorted ( set ( ref_range ) ) rates_frame = pd . DataFrame ( columns = ref_index , index = I_index ) # This frame is basically a two dimensional table that we can index with our # parameters # Now iterate over the results. The result list is a list of tuples, with the # run index at first position and our result at the second for result_tuple in result_list : run_idx = result_tuple [ 0 ] firing_rates = result_tuple [ 1 ] I_val = I_range [ run_idx ] ref_val = ref_range [ run_idx ] rates_frame . loc [ I_val , ref_val ] = firing_rates # Put the firing rate into the # data frame # Finally we going to store our new firing rate table into the trajectory traj . f_add_result ( 'summary.firing_rates' , rates_frame = rates_frame , comment = 'Contains a pandas data frame with all firing rates.' )", "nl": "Postprocessing sorts computed firing rates into a table"}}
{"translation": {"code": "def run_neuron ( traj ) : # Extract all parameters from `traj` V_init = traj . par . neuron . V_init I = traj . par . neuron . I tau_V = traj . par . neuron . tau_V tau_ref = traj . par . neuron . tau_ref dt = traj . par . simulation . dt duration = traj . par . simulation . duration steps = int ( duration / float ( dt ) ) # Create some containers for the Euler integration V_array = np . zeros ( steps ) V_array [ 0 ] = V_init spiketimes = [ ] # List to collect all times of action potentials # Do the Euler integration: print ( 'Starting Euler Integration' ) for step in range ( 1 , steps ) : if V_array [ step - 1 ] >= 1 : # The membrane potential crossed the threshold and we mark this as # an action potential V_array [ step ] = 0 spiketimes . append ( ( step - 1 ) * dt ) elif spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : # We are in the refractory period, so we simply clamp the voltage # to 0 V_array [ step ] = 0 else : # Euler Integration step: dV = - 1 / tau_V * V_array [ step - 1 ] + I V_array [ step ] = V_array [ step - 1 ] + dV * dt print ( 'Finished Euler Integration' ) # Add the voltage trace and spike times traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , comment = 'Contains the development of the membrane potential over time ' 'as well as the number of spikes.' ) # This result will be renamed to `traj.results.neuron.run_XXXXXXXX`. # And finally we return the estimate of the firing rate return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000", "nl": "Runs a simulation of a model neuron ."}}
{"translation": {"code": "def add_exploration ( traj ) : print ( 'Adding exploration of I and tau_ref' ) explore_dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 'neuron.tau_ref' : [ 5.0 , 7.5 , 10.0 ] } explore_dict = cartesian_product ( explore_dict , ( 'neuron.tau_ref' , 'neuron.I' ) ) # The second argument, the tuple, specifies the order of the cartesian product, # The variable on the right most side changes fastest and defines the # 'inner for-loop' of the cartesian product traj . f_explore ( explore_dict )", "nl": "Explores different values of I and tau_ref ."}}
{"translation": {"code": "def signal_update ( self ) : if not self . active : return self . _updates += 1 current_time = time . time ( ) dt = current_time - self . _last_time if dt > self . _display_time : dfullt = current_time - self . _start_time seconds = int ( dfullt ) % 60 minutes = int ( dfullt ) / 60 if minutes == 0 : formatted_time = '%ds' % seconds else : formatted_time = '%dm%02ds' % ( minutes , seconds ) nodespersecond = self . _updates / dfullt message = 'Processed %d nodes in %s (%.2f nodes/s).' % ( self . _updates , formatted_time , nodespersecond ) self . _logger . info ( message ) self . _last_time = current_time", "nl": "Signals the process timer ."}}
{"translation": {"code": "def write ( self , buf ) : if not self . _recursion : self . _recursion = True try : for line in buf . rstrip ( ) . splitlines ( ) : self . _logger . log ( self . _log_level , line . rstrip ( ) ) finally : self . _recursion = False else : # If stderr is redirected to stdout we can avoid further recursion by sys . __stderr__ . write ( 'ERROR: Recursion in Stream redirection!' )", "nl": "Writes data from buffer to logger"}}
{"translation": {"code": "def kwargs_api_change ( old_name , new_name = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new_func ( * args , * * kwargs ) : if old_name in kwargs : if new_name is None : warning_string = 'Using deprecated keyword argument `%s` in function `%s`. ' 'This keyword is no longer supported, please don`t use it ' 'anymore.' % ( old_name , func . __name__ ) else : warning_string = 'Using deprecated keyword argument `%s` in function `%s`, ' 'please use keyword `%s` instead.' % ( old_name , func . __name__ , new_name ) warnings . warn ( warning_string , category = DeprecationWarning ) value = kwargs . pop ( old_name ) if new_name is not None : kwargs [ new_name ] = value return func ( * args , * * kwargs ) return new_func return wrapper", "nl": "This is a decorator which can be used if a kwarg has changed its name over versions to also support the old argument name ."}}
{"translation": {"code": "def f_get_default ( self , name , default = None , fast_access = True , with_links = True , shortcuts = True , max_depth = None , auto_load = False ) : try : return self . f_get ( name , fast_access = fast_access , shortcuts = shortcuts , max_depth = max_depth , auto_load = auto_load , with_links = with_links ) except ( AttributeError , pex . DataNotInStorageError ) : return default", "nl": "Similar to f_get but returns the default value if name is not found in the trajectory ."}}
{"translation": {"code": "def progressbar ( index , total , percentage_step = 10 , logger = 'print' , log_level = logging . INFO , reprint = True , time = True , length = 20 , fmt_string = None , reset = False ) : return _progressbar ( index = index , total = total , percentage_step = percentage_step , logger = logger , log_level = log_level , reprint = reprint , time = time , length = length , fmt_string = fmt_string , reset = reset )", "nl": "Plots a progress bar to the given logger for large for loops ."}}
{"translation": {"code": "def _reset ( self , index , total , percentage_step , length ) : self . _start_time = datetime . datetime . now ( ) self . _start_index = index self . _current_index = index self . _percentage_step = percentage_step self . _total = float ( total ) self . _total_minus_one = total - 1 self . _length = length self . _norm_factor = total * percentage_step / 100.0 self . _current_interval = int ( ( index + 1.0 ) / self . _norm_factor )", "nl": "Resets to the progressbar to start a new one"}}
{"translation": {"code": "def f_add_link ( self , name_or_item , full_name_or_item = None ) : if isinstance ( name_or_item , str ) : name = name_or_item if isinstance ( full_name_or_item , str ) : instance = self . v_root . f_get ( full_name_or_item ) else : instance = full_name_or_item else : instance = name_or_item name = instance . v_name return self . _nn_interface . _add_generic ( self , type_name = LINK , group_type_name = GROUP , args = ( name , instance ) , kwargs = { } , add_prefix = False )", "nl": "Adds a link to an existing node ."}}
{"translation": {"code": "def _create_link ( self , act_node , name , instance ) : act_node . _links [ name ] = instance act_node . _children [ name ] = instance full_name = instance . v_full_name if full_name not in self . _root_instance . _linked_by : self . _root_instance . _linked_by [ full_name ] = { } linking = self . _root_instance . _linked_by [ full_name ] if act_node . v_full_name not in linking : linking [ act_node . v_full_name ] = ( act_node , set ( ) ) linking [ act_node . v_full_name ] [ 1 ] . add ( name ) if name not in self . _links_count : self . _links_count [ name ] = 0 self . _links_count [ name ] = self . _links_count [ name ] + 1 self . _logger . debug ( 'Added link `%s` under `%s` pointing ' 'to `%s`.' % ( name , act_node . v_full_name , instance . v_full_name ) ) return instance", "nl": "Creates a link and checks if names are appropriate"}}
{"translation": {"code": "def f_remove_link ( self , name ) : if name not in self . _links : raise ValueError ( 'No link with name `%s` found under `%s`.' % ( name , self . _full_name ) ) self . _nn_interface . _remove_link ( self , name )", "nl": "Removes a link from from the current group node with a given name ."}}
{"translation": {"code": "def _lnk_delete_link ( self , link_name ) : translated_name = '/' + self . _trajectory_name + '/' + link_name . replace ( '.' , '/' ) link = self . _hdf5file . get_node ( where = translated_name ) link . _f_remove ( )", "nl": "Removes a link from disk"}}
{"translation": {"code": "def f_get_links ( self , copy = True ) : if copy : return self . _links . copy ( ) else : return self . _links", "nl": "Returns a link dictionary ."}}
{"translation": {"code": "def _finalize_run ( self ) : self . _run_information [ self . v_crun ] [ 'completed' ] = 1 while len ( self . _new_links ) : name_pair , child_parent_pair = self . _new_links . popitem ( last = False ) parent_node , _ = child_parent_pair _ , link = name_pair parent_node . f_remove_child ( link ) while len ( self . _new_nodes ) : _ , child_parent_pair = self . _new_nodes . popitem ( last = False ) parent , child = child_parent_pair child_name = child . v_name parent . f_remove_child ( child_name , recursive = True )", "nl": "Called by the environment after storing to perform some rollback operations ."}}
{"translation": {"code": "def f_get_leaves ( self , copy = True ) : if copy : return self . _leaves . copy ( ) else : return self . _leaves", "nl": "Returns a dictionary of all leaves hanging immediately below this group ."}}
{"translation": {"code": "def f_get_groups ( self , copy = True ) : if copy : return self . _groups . copy ( ) else : return self . _groups", "nl": "Returns a dictionary of groups hanging immediately below this group ."}}
{"translation": {"code": "def f_lock_derived_parameters ( self ) : for par in self . _derived_parameters . values ( ) : if not par . f_is_empty ( ) : par . f_lock ( )", "nl": "Locks all non - empty derived parameters"}}
{"translation": {"code": "def f_lock_parameters ( self ) : for par in self . _parameters . values ( ) : if not par . f_is_empty ( ) : par . f_lock ( )", "nl": "Locks all non - empty parameters"}}
{"translation": {"code": "def f_restore_default ( self ) : self . _idx = - 1 self . _crun = None for param in self . _explored_parameters . values ( ) : if param is not None : param . _restore_default ( )", "nl": "Restores the default value in all explored parameters and sets the v_idx property back to - 1 and v_crun to None ."}}
{"translation": {"code": "def _preset ( self , name , args , kwargs ) : if self . f_contains ( name , shortcuts = False ) : raise ValueError ( 'Parameter `%s` is already part of your trajectory, use the normal' 'accessing routine to change config.' % name ) else : self . _changed_default_parameters [ name ] = ( args , kwargs )", "nl": "Generic preset function marks a parameter or config for presetting ."}}
{"translation": {"code": "def _make_single_run ( self ) : self . _is_run = False # to be able to use f_set_crun self . _new_nodes = OrderedDict ( ) self . _new_links = OrderedDict ( ) self . _is_run = True return self", "nl": "Modifies the trajectory for single runs executed by the environment"}}
{"translation": {"code": "def _set_start ( self ) : init_time = time . time ( ) formatted_time = datetime . datetime . fromtimestamp ( init_time ) . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' ) run_info_dict = self . _run_information [ self . v_crun ] run_info_dict [ 'timestamp' ] = init_time run_info_dict [ 'time' ] = formatted_time if self . _environment_hexsha is not None : run_info_dict [ 'short_environment_hexsha' ] = self . _environment_hexsha [ 0 : 7 ]", "nl": "Sets the start timestamp and formatted time to the current time ."}}
{"translation": {"code": "def load_trajectory ( name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , load_derived_parameters = pypetconstants . LOAD_SKELETON , load_results = pypetconstants . LOAD_SKELETON , load_other_data = pypetconstants . LOAD_SKELETON , recursive = True , load_data = None , max_depth = None , force = False , dynamic_imports = None , new_name = 'my_trajectory' , add_time = True , wildcard_functions = None , with_run_information = True , storage_service = storage . HDF5StorageService , * * kwargs ) : if name is None and index is None : raise ValueError ( 'Please specify either a name or an index' ) elif name is not None and index is not None : raise ValueError ( 'Please specify either a name or an index' ) traj = Trajectory ( name = new_name , add_time = add_time , dynamic_imports = dynamic_imports , wildcard_functions = wildcard_functions ) traj . f_load ( name = name , index = index , as_new = as_new , load_parameters = load_parameters , load_derived_parameters = load_derived_parameters , load_results = load_results , load_other_data = load_other_data , recursive = recursive , load_data = load_data , max_depth = max_depth , force = force , with_run_information = with_run_information , storage_service = storage_service , * * kwargs ) return traj", "nl": "Helper function that creates a novel trajectory and loads it from disk ."}}
{"translation": {"code": "def _construct_instance ( self , constructor , full_name , * args , * * kwargs ) : if getattr ( constructor , 'KNOWS_TRAJECTORY' , False ) : return constructor ( full_name , self , * args , * * kwargs ) else : return constructor ( full_name , * args , * * kwargs )", "nl": "Creates a new node . Checks if the new node needs to know the trajectory ."}}
{"translation": {"code": "def _prm_write_shared_table ( self , key , hdf5_group , fullname , * * kwargs ) : first_row = None description = None if 'first_row' in kwargs : first_row = kwargs . pop ( 'first_row' ) if not 'description' in kwargs : description = { } for colname in first_row : data = first_row [ colname ] column = self . _all_get_table_col ( key , [ data ] , fullname ) description [ colname ] = column if 'description' in kwargs : description = kwargs . pop ( 'description' ) if 'filters' in kwargs : filters = kwargs . pop ( 'filters' ) else : filters = self . _all_get_filters ( kwargs ) table = self . _hdf5file . create_table ( where = hdf5_group , name = key , description = description , filters = filters , * * kwargs ) table . flush ( ) if first_row is not None : row = table . row for key in description : row [ key ] = first_row [ key ] row . append ( ) table . flush ( )", "nl": "Creates a new empty table"}}
{"translation": {"code": "def _prm_write_shared_array ( self , key , data , hdf5_group , full_name , flag , * * kwargs ) : if flag == HDF5StorageService . ARRAY : self . _prm_write_into_array ( key , data , hdf5_group , full_name , * * kwargs ) elif flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : self . _prm_write_into_other_array ( key , data , hdf5_group , full_name , flag = flag , * * kwargs ) else : raise RuntimeError ( 'Flag `%s` of hdf5 data `%s` of `%s` not understood' % ( flag , key , full_name ) ) self . _hdf5file . flush ( )", "nl": "Creates and array that can be used with an HDF5 array object"}}
{"translation": {"code": "def compact_hdf5_file ( filename , name = None , index = None , keep_backup = True ) : if name is None and index is None : index = - 1 tmp_traj = load_trajectory ( name , index , as_new = False , load_all = pypetconstants . LOAD_NOTHING , force = True , filename = filename ) service = tmp_traj . v_storage_service complevel = service . complevel complib = service . complib shuffle = service . shuffle fletcher32 = service . fletcher32 name_wo_ext , ext = os . path . splitext ( filename ) tmp_filename = name_wo_ext + '_tmp' + ext abs_filename = os . path . abspath ( filename ) abs_tmp_filename = os . path . abspath ( tmp_filename ) command = [ 'ptrepack' , '-v' , '--complib' , complib , '--complevel' , str ( complevel ) , '--shuffle' , str ( int ( shuffle ) ) , '--fletcher32' , str ( int ( fletcher32 ) ) , abs_filename , abs_tmp_filename ] str_command = ' ' . join ( command ) print ( 'Executing command `%s`' % str_command ) retcode = subprocess . call ( command ) if retcode != 0 : print ( '#### ERROR: Compacting `%s` failed with errorcode %s! ####' % ( filename , str ( retcode ) ) ) else : print ( '#### Compacting successful ####' ) print ( 'Renaming files' ) if keep_backup : backup_file_name = name_wo_ext + '_backup' + ext os . rename ( filename , backup_file_name ) else : os . remove ( filename ) os . rename ( tmp_filename , filename ) print ( '### Compacting and Renaming finished ####' ) return retcode", "nl": "Can compress an HDF5 to reduce file size ."}}
{"translation": {"code": "def get_data_node ( self ) : if not self . _storage_service . is_open : warnings . warn ( 'You requesting the data item but your store is not open, ' 'the item itself will be closed, too!' , category = RuntimeWarning ) return self . _request_data ( '__thenode__' )", "nl": "Returns the actula node of the underlying data ."}}
{"translation": {"code": "def get_matching_kwargs ( func , kwargs ) : args , uses_startstar = _get_argspec ( func ) if uses_startstar : return kwargs . copy ( ) else : matching_kwargs = dict ( ( k , kwargs [ k ] ) for k in args if k in kwargs ) return matching_kwargs", "nl": "Takes a function and keyword arguments and returns the ones that can be passed ."}}
{"translation": {"code": "def storage_factory ( storage_service , trajectory = None , * * kwargs ) : if 'filename' in kwargs and storage_service is None : filename = kwargs [ 'filename' ] _ , ext = os . path . splitext ( filename ) if ext in ( '.hdf' , '.h4' , '.hdf4' , '.he2' , '.h5' , '.hdf5' , '.he5' ) : storage_service = HDF5StorageService else : raise ValueError ( 'Extension `%s` of filename `%s` not understood.' % ( ext , filename ) ) elif isinstance ( storage_service , str ) : class_name = storage_service . split ( '.' ) [ - 1 ] storage_service = create_class ( class_name , [ storage_service , HDF5StorageService ] ) if inspect . isclass ( storage_service ) : return _create_storage ( storage_service , trajectory , * * kwargs ) else : return storage_service , set ( kwargs . keys ( ) )", "nl": "Creates a storage service to be extended if new storage services are added"}}
{"translation": {"code": "def _create_storage ( storage_service , trajectory = None , * * kwargs ) : kwargs_copy = kwargs . copy ( ) kwargs_copy [ 'trajectory' ] = trajectory matching_kwargs = get_matching_kwargs ( storage_service , kwargs_copy ) storage_service = storage_service ( * * matching_kwargs ) unused_kwargs = set ( kwargs . keys ( ) ) - set ( matching_kwargs . keys ( ) ) return storage_service , unused_kwargs", "nl": "Creates a service from a constructor and checks which kwargs are not used"}}
{"translation": {"code": "def create_shared_data ( self , * * kwargs ) : if 'flag' not in kwargs : kwargs [ 'flag' ] = self . FLAG if 'data' in kwargs : kwargs [ 'obj' ] = kwargs . pop ( 'data' ) if 'trajectory' in kwargs : self . traj = kwargs . pop ( 'trajectory' ) if 'traj' in kwargs : self . traj = kwargs . pop ( 'traj' ) if 'name' in kwargs : self . name = kwargs . pop [ 'name' ] if 'parent' in kwargs : self . parent = kwargs . pop ( 'parent' ) if self . name is not None : self . parent [ self . name ] = self return self . _request_data ( 'create_shared_data' , kwargs = kwargs )", "nl": "Creates shared data on disk with a StorageService on disk ."}}
{"translation": {"code": "def _request_data ( self , request , args = None , kwargs = None ) : return self . _storage_service . store ( pypetconstants . ACCESS_DATA , self . parent . v_full_name , self . name , request , args , kwargs , trajectory_name = self . traj . v_name )", "nl": "Interface with the underlying storage ."}}
{"translation": {"code": "def make_shared_result ( result , key , trajectory , new_class = None ) : data = result . f_get ( key ) if new_class is None : if isinstance ( data , ObjectTable ) : new_class = SharedTable elif isinstance ( data , pd . DataFrame ) : new_class = SharedPandasFrame elif isinstance ( data , ( tuple , list ) ) : new_class = SharedArray elif isinstance ( data , ( np . ndarray , np . matrix ) ) : new_class = SharedCArray else : raise RuntimeError ( 'Your data `%s` is not understood.' % key ) shared_data = new_class ( result . f_translate_key ( key ) , result , trajectory = trajectory ) result [ key ] = shared_data shared_data . _request_data ( 'make_shared' ) return result", "nl": "Turns an ordinary data item into a shared one ."}}
{"translation": {"code": "def _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = True , recursive = False , max_depth = None , _traj = None , _as_new = False , _hdf5_group = None ) : if _hdf5_group is None : _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) _traj = traj_group . v_root if recursive : parent_traj_node = traj_group . f_get_parent ( ) self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , recursive = recursive , max_depth = max_depth , current_depth = 0 , trajectory = _traj , as_new = _as_new , hdf5_group = _hdf5_group ) else : if load_data == pypetconstants . LOAD_NOTHING : return elif load_data == pypetconstants . OVERWRITE_DATA : traj_group . v_annotations . f_empty ( ) traj_group . v_comment = '' self . _all_load_skeleton ( traj_group , _hdf5_group ) traj_group . _stored = not _as_new # Signal completed node loading self . _node_processing_timer . signal_update ( )", "nl": "Loads a group node and potentially everything recursively below"}}
{"translation": {"code": "def _all_load_skeleton ( self , traj_node , hdf5_group ) : if traj_node . v_annotations . f_is_empty ( ) : self . _ann_load_annotations ( traj_node , hdf5_group ) if traj_node . v_comment == '' : comment = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . COMMENT ) if comment is None : comment = '' traj_node . v_comment = comment", "nl": "Reloads skeleton data of a tree node"}}
{"translation": {"code": "def f_set_properties ( self , * * kwargs ) : for name in kwargs : val = kwargs [ name ] if not name . startswith ( 'v_' ) : name = 'v_' + name if not name in self . _nn_interface . _not_admissible_names : raise AttributeError ( 'Cannot set property `%s` does not exist.' % name ) else : setattr ( self , name , val )", "nl": "Sets properties like v_fast_access ."}}
{"translation": {"code": "def f_load ( self , recursive = True , load_data = pypetconstants . LOAD_DATA , max_depth = None ) : traj = self . _nn_interface . _root_instance storage_service = traj . v_storage_service storage_service . load ( pypetconstants . GROUP , self , trajectory_name = traj . v_name , load_data = load_data , recursive = recursive , max_depth = max_depth ) return self", "nl": "Loads a group from disk ."}}
{"translation": {"code": "def f_store ( self , recursive = True , store_data = pypetconstants . STORE_DATA , max_depth = None ) : traj = self . _nn_interface . _root_instance storage_service = traj . v_storage_service storage_service . store ( pypetconstants . GROUP , self , trajectory_name = traj . v_name , recursive = recursive , store_data = store_data , max_depth = max_depth )", "nl": "Stores a group node to disk"}}
{"translation": {"code": "def _tree_create_leaf ( self , name , trajectory , hdf5_group ) : class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) # Create the instance with the appropriate constructor class_constructor = trajectory . _create_class ( class_name ) instance = trajectory . _construct_instance ( class_constructor , name ) return instance", "nl": "Creates a new pypet leaf instance ."}}
{"translation": {"code": "def f_translate_key ( self , key ) : if isinstance ( key , int ) : if key == 0 : key = self . v_name else : key = self . v_name + '_%d' % key return key", "nl": "Translates integer indices into the appropriate names"}}
{"translation": {"code": "def _remove_exploration ( self ) : for param in self . _explored_parameters . values ( ) : if param . _stored : try : self . f_delete_item ( param ) except Exception : self . _logger . exception ( 'Could not delete expanded parameter `%s` ' 'from disk.' % param . v_full_name )", "nl": "Called if trajectory is expanded deletes all explored parameters from disk"}}
{"translation": {"code": "def manipulate_multiproc_safe ( traj ) : # Manipulate the data in the trajectory traj . last_process_name = mp . current_process ( ) . name # Store the manipulated data traj . results . f_store ( store_data = 3 )", "nl": "Target function that manipulates the trajectory ."}}
{"translation": {"code": "def convert_rule ( rule_number ) : binary_rule = [ ( rule_number // pow ( 2 , i ) ) % 2 for i in range ( 8 ) ] return np . array ( binary_rule )", "nl": "Converts a rule given as an integer into a binary list representation ."}}
{"translation": {"code": "def main ( ) : rules_to_test = [ 10 , 30 , 90 , 110 , 184 ] # rules we want to explore: steps = 250 # cell iterations ncells = 400 # number of cells seed = 100042 # RNG seed initial_states = [ 'single' , 'random' ] # Initial states we want to explore # create a folder for the plots and the data folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_original' ) if not os . path . isdir ( folder ) : os . makedirs ( folder ) filename = os . path . join ( folder , 'all_patterns.p' ) print ( 'Computing all patterns' ) all_patterns = [ ] # list containing the simulation results for idx , rule_number in enumerate ( rules_to_test ) : # iterate over all rules for initial_name in initial_states : # iterate over the initial states # make the initial state initial_state = make_initial_state ( initial_name , ncells , seed = seed ) # simulate the automaton pattern = cellular_automaton_1D ( initial_state , rule_number , steps ) # keep the resulting pattern all_patterns . append ( ( rule_number , initial_name , pattern ) ) # Print a progressbar, because I am always impatient #  (ok that's already from pypet, but it's really handy!) progressbar ( idx , len ( rules_to_test ) , reprint = True ) # Store all patterns to disk with open ( filename , 'wb' ) as file : pickle . dump ( all_patterns , file = file ) # Finally print all patterns print ( 'Plotting all patterns' ) for idx , pattern_tuple in enumerate ( all_patterns ) : rule_number , initial_name , pattern = pattern_tuple # Plot the pattern filename = os . path . join ( folder , 'rule_%s_%s.png' % ( str ( rule_number ) , initial_name ) ) plot_pattern ( pattern , rule_number , filename ) progressbar ( idx , len ( all_patterns ) , reprint = True )", "nl": "Main simulation function"}}
{"translation": {"code": "def cellular_automaton_1D ( initial_state , rule_number , steps ) : ncells = len ( initial_state ) # Create an array for the full pattern pattern = np . zeros ( ( steps , ncells ) ) # Pass initial state: pattern [ 0 , : ] = initial_state # Get the binary rule list binary_rule = convert_rule ( rule_number ) # Conversion list to get the position in the binary rule list neighbourhood_factors = np . array ( [ 1 , 2 , 4 ] ) # Iterate over all steps to compute the CA all_cells = range ( ncells ) for step in range ( steps - 1 ) : current_row = pattern [ step , : ] next_row = pattern [ step + 1 , : ] for irun in all_cells : # Get the neighbourhood neighbour_indices = range ( irun - 1 , irun + 2 ) neighbourhood = np . take ( current_row , neighbour_indices , mode = 'wrap' ) # Convert neighborhood to decimal decimal_neighborhood = int ( np . sum ( neighbourhood * neighbourhood_factors ) ) # Get next state from rule book next_state = binary_rule [ decimal_neighborhood ] # Update next state of cell next_row [ irun ] = next_state return pattern", "nl": "Simulates a 1 dimensional cellular automaton ."}}
{"translation": {"code": "def plot_pattern ( pattern , rule_number , filename ) : plt . figure ( ) plt . imshow ( pattern ) plt . xlabel ( 'Cell No.' ) plt . ylabel ( 'Time Step' ) plt . title ( 'CA with Rule %s' % str ( rule_number ) ) plt . savefig ( filename ) #plt.show() plt . close ( )", "nl": "Plots an automaton pattern and stores the image under a given filename ."}}
{"translation": {"code": "def make_initial_state ( name , ncells , seed = 42 ) : if name == 'single' : just_one_cell = np . zeros ( ncells ) just_one_cell [ int ( ncells / 2 ) ] = 1.0 return just_one_cell elif name == 'random' : np . random . seed ( seed ) random_init = np . random . randint ( 2 , size = ncells ) return random_init else : raise ValueError ( 'I cannot handel your initial state `%s`.' % name )", "nl": "Creates an initial state for the automaton ."}}
{"translation": {"code": "def find_unique_points ( explored_parameters ) : ranges = [ param . f_get_range ( copy = False ) for param in explored_parameters ] zipped_tuples = list ( zip ( * ranges ) ) try : unique_elements = OrderedDict ( ) for idx , val_tuple in enumerate ( zipped_tuples ) : if val_tuple not in unique_elements : unique_elements [ val_tuple ] = [ ] unique_elements [ val_tuple ] . append ( idx ) return list ( unique_elements . items ( ) ) except TypeError : logger = logging . getLogger ( 'pypet.find_unique' ) logger . error ( 'Your parameter entries could not be hashed, ' 'now I am sorting slowly in O(N**2).' ) unique_elements = [ ] for idx , val_tuple in enumerate ( zipped_tuples ) : matches = False for added_tuple , pos_list in unique_elements : matches = True for idx2 , val in enumerate ( added_tuple ) : if not explored_parameters [ idx2 ] . _equal_values ( val_tuple [ idx2 ] , val ) : matches = False break if matches : pos_list . append ( idx ) break if not matches : unique_elements . append ( ( val_tuple , [ idx ] ) ) return unique_elements", "nl": "Takes a list of explored parameters and finds unique parameter combinations ."}}
{"translation": {"code": "def make_filename ( traj ) : explored_parameters = traj . f_get_explored_parameters ( ) filename = '' for param in explored_parameters . values ( ) : short_name = param . v_name val = param . f_get ( ) filename += '%s_%s__' % ( short_name , str ( val ) ) return filename [ : - 2 ] + '.png'", "nl": "Function to create generic filenames based on what has been explored"}}
{"translation": {"code": "def f_remove ( self , recursive = True , predicate = None ) : if not recursive : raise ValueError ( 'Nice try ;-)' ) for child in list ( self . _children . keys ( ) ) : self . f_remove_child ( child , recursive = True , predicate = predicate )", "nl": "Recursively removes all children of the trajectory"}}
{"translation": {"code": "def f_remove ( self , recursive = True , predicate = None ) : parent = self . f_get_parent ( ) parent . f_remove_child ( self . v_name , recursive = recursive , predicate = predicate )", "nl": "Recursively removes the group and all it s children ."}}
{"translation": {"code": "def next ( self ) : while True : # We need this loop because some iterators may already be empty. # We keep on popping from the left until next succeeds and as long # as there are iterators available try : return next ( self . _current ) except StopIteration : try : self . _current = iter ( self . _chain . popleft ( ) ) except IndexError : # If we run out of iterators we are sure that # there can be no more element raise StopIteration ( 'Reached end of iterator chain' )", "nl": "Returns next element from chain ."}}
{"translation": {"code": "def f_get_parent ( self ) : if self . v_is_root : raise TypeError ( 'Root does not have a parent' ) elif self . v_location == '' : return self . v_root else : return self . v_root . f_get ( self . v_location , fast_access = False , shortcuts = False )", "nl": "Returns the parent of the node ."}}
{"translation": {"code": "def kwargs_mutual_exclusive ( param1_name , param2_name , map2to1 = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new_func ( * args , * * kwargs ) : if param2_name in kwargs : if param1_name in kwargs : raise ValueError ( 'You cannot specify `%s` and `%s` at the same time, ' 'they are mutually exclusive.' % ( param1_name , param2_name ) ) param2 = kwargs . pop ( param2_name ) if map2to1 is not None : param1 = map2to1 ( param2 ) else : param1 = param2 kwargs [ param1_name ] = param1 return func ( * args , * * kwargs ) return new_func return wrapper", "nl": "If there exist mutually exclusive parameters checks for them and maps param2 to 1 ."}}
{"translation": {"code": "def load_class ( full_class_string ) : class_data = full_class_string . split ( \".\" ) module_path = \".\" . join ( class_data [ : - 1 ] ) class_str = class_data [ - 1 ] module = importlib . import_module ( module_path ) # We retrieve the Class from the module return getattr ( module , class_str )", "nl": "Loads a class from a string naming the module and class name ."}}
{"translation": {"code": "def create_class ( class_name , dynamic_imports ) : try : new_class = globals ( ) [ class_name ] if not inspect . isclass ( new_class ) : raise TypeError ( 'Not a class!' ) return new_class except ( KeyError , TypeError ) : for dynamic_class in dynamic_imports : # Dynamic classes can be provided directly as a Class instance, # for example as `MyCustomParameter`, # or as a string describing where to import the class from, # for instance as `'mypackage.mymodule.MyCustomParameter'`. if inspect . isclass ( dynamic_class ) : if class_name == dynamic_class . __name__ : return dynamic_class else : # The class name is always the last in an import string, # e.g. `'mypackage.mymodule.MyCustomParameter'` class_name_to_test = dynamic_class . split ( '.' ) [ - 1 ] if class_name == class_name_to_test : new_class = load_class ( dynamic_class ) return new_class raise ImportError ( 'Could not create the class named `%s`.' % class_name )", "nl": "Dynamically creates a class ."}}
{"translation": {"code": "def _handle_config_parsing ( self , log_config ) : parser = NoInterpolationParser ( ) parser . readfp ( log_config ) rename_func = lambda string : rename_log_file ( string , env_name = self . env_name , traj_name = self . traj_name , set_name = self . set_name , run_name = self . run_name ) sections = parser . sections ( ) for section in sections : options = parser . options ( section ) for option in options : if option == 'args' : self . _check_and_replace_parser_args ( parser , section , option , rename_func = rename_func ) return parser", "nl": "Checks for filenames within a config file and translates them ."}}
{"translation": {"code": "def _handle_dict_config ( self , log_config ) : new_dict = dict ( ) for key in log_config . keys ( ) : if key == 'filename' : filename = log_config [ key ] filename = rename_log_file ( filename , env_name = self . env_name , traj_name = self . traj_name , set_name = self . set_name , run_name = self . run_name ) new_dict [ key ] = filename try_make_dirs ( filename ) elif isinstance ( log_config [ key ] , dict ) : inner_dict = self . _handle_dict_config ( log_config [ key ] ) new_dict [ key ] = inner_dict else : new_dict [ key ] = log_config [ key ] return new_dict", "nl": "Recursively walks and copies the log_config dict and searches for filenames ."}}
{"translation": {"code": "def make_logging_handlers_and_tools ( self , multiproc = False ) : log_stdout = self . log_stdout if sys . stdout is self . _stdout_to_logger : # If we already redirected stdout we don't neet to redo it again log_stdout = False if self . log_config : if multiproc : proc_log_config = self . _mp_config else : proc_log_config = self . _sp_config if proc_log_config : if isinstance ( proc_log_config , dict ) : new_dict = self . _handle_dict_config ( proc_log_config ) dictConfig ( new_dict ) else : parser = self . _handle_config_parsing ( proc_log_config ) memory_file = self . _parser_to_string_io ( parser ) fileConfig ( memory_file , disable_existing_loggers = False ) if log_stdout : #  Create a logging mock for stdout std_name , std_level = self . log_stdout stdout = StdoutToLogger ( std_name , log_level = std_level ) stdout . start ( ) self . _tools . append ( stdout )", "nl": "Creates logging handlers and redirects stdout ."}}
{"translation": {"code": "def start ( self ) : if sys . stdout is not self : self . _original_steam = sys . stdout sys . stdout = self self . _redirection = True if self . _redirection : print ( 'Established redirection of `stdout`.' )", "nl": "Starts redirection of stdout"}}
{"translation": {"code": "def simple_logging_config ( func ) : @ functools . wraps ( func ) def new_func ( self , * args , * * kwargs ) : if use_simple_logging ( kwargs ) : if 'log_config' in kwargs : raise ValueError ( 'Please do not specify `log_config` ' 'if you want to use the simple ' 'way of providing logging configuration ' '(i.e using `log_folder`, `logger_names` and/or `log_levels`).' ) _change_logging_kwargs ( kwargs ) return func ( self , * args , * * kwargs ) return new_func", "nl": "Decorator to allow a simple logging configuration ."}}
{"translation": {"code": "def _change_logging_kwargs ( kwargs ) : log_levels = kwargs . pop ( 'log_level' , None ) log_folder = kwargs . pop ( 'log_folder' , 'logs' ) logger_names = kwargs . pop ( 'logger_names' , '' ) if log_levels is None : log_levels = kwargs . pop ( 'log_levels' , logging . INFO ) log_multiproc = kwargs . pop ( 'log_multiproc' , True ) if not isinstance ( logger_names , ( tuple , list ) ) : logger_names = [ logger_names ] if not isinstance ( log_levels , ( tuple , list ) ) : log_levels = [ log_levels ] if len ( log_levels ) == 1 : log_levels = [ log_levels [ 0 ] for _ in logger_names ] # We don't want to manipulate the original dictionary dictionary = copy . deepcopy ( LOGGING_DICT ) prefixes = [ '' ] if not log_multiproc : for key in list ( dictionary . keys ( ) ) : if key . startswith ( 'multiproc_' ) : del dictionary [ key ] else : prefixes . append ( 'multiproc_' ) # Add all handlers to all loggers for prefix in prefixes : for handler_dict in dictionary [ prefix + 'handlers' ] . values ( ) : if 'filename' in handler_dict : filename = os . path . join ( log_folder , handler_dict [ 'filename' ] ) filename = os . path . normpath ( filename ) handler_dict [ 'filename' ] = filename dictionary [ prefix + 'loggers' ] = { } logger_dict = dictionary [ prefix + 'loggers' ] for idx , logger_name in enumerate ( logger_names ) : logger_dict [ logger_name ] = { 'level' : log_levels [ idx ] , 'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) } kwargs [ 'log_config' ] = dictionary", "nl": "Helper function to turn the simple logging kwargs into a log_config ."}}
{"translation": {"code": "def _configure_logging ( kwargs , extract = True ) : try : logging_manager = kwargs [ 'logging_manager' ] if extract : logging_manager . extract_replacements ( kwargs [ 'traj' ] ) logging_manager . make_logging_handlers_and_tools ( multiproc = True ) except Exception as exc : sys . stderr . write ( 'Could not configure logging system because of: %s' % repr ( exc ) ) traceback . print_exc ( )", "nl": "Requests the logging manager to configure logging ."}}
{"translation": {"code": "def try_make_dirs ( filename ) : try : dirname = os . path . dirname ( os . path . normpath ( filename ) ) racedirs ( dirname ) except Exception as exc : sys . stderr . write ( 'ERROR during log config file handling, could not create dirs for ' 'filename `%s` because of: %s' % ( filename , repr ( exc ) ) )", "nl": "Tries to make directories for a given filename ."}}
{"translation": {"code": "def rename_log_file ( filename , trajectory = None , env_name = None , traj_name = None , set_name = None , run_name = None , process_name = None , host_name = None ) : if pypetconstants . LOG_ENV in filename : if env_name is None : env_name = trajectory . v_environment_name filename = filename . replace ( pypetconstants . LOG_ENV , env_name ) if pypetconstants . LOG_TRAJ in filename : if traj_name is None : traj_name = trajectory . v_name filename = filename . replace ( pypetconstants . LOG_TRAJ , traj_name ) if pypetconstants . LOG_RUN in filename : if run_name is None : run_name = trajectory . f_wildcard ( '$' ) filename = filename . replace ( pypetconstants . LOG_RUN , run_name ) if pypetconstants . LOG_SET in filename : if set_name is None : set_name = trajectory . f_wildcard ( '$set' ) filename = filename . replace ( pypetconstants . LOG_SET , set_name ) if pypetconstants . LOG_PROC in filename : if process_name is None : process_name = multip . current_process ( ) . name + '-' + str ( os . getpid ( ) ) filename = filename . replace ( pypetconstants . LOG_PROC , process_name ) if pypetconstants . LOG_HOST in filename : if host_name is None : host_name = socket . getfqdn ( ) . replace ( '.' , '-' ) filename = filename . replace ( pypetconstants . LOG_HOST , host_name ) return filename", "nl": "Renames a given filename with valid wildcard placements ."}}
{"translation": {"code": "def finalize ( self , remove_all_handlers = True ) : for tool in self . _tools : tool . finalize ( ) self . _tools = [ ] self . _stdout_to_logger = None for config in ( self . _sp_config , self . _mp_config ) : if hasattr ( config , 'close' ) : config . close ( ) self . _sp_config = None self . _mp_config = None if remove_all_handlers : self . tabula_rasa ( )", "nl": "Finalizes the manager closes and removes all handlers if desired ."}}
{"translation": {"code": "def _check_and_replace_parser_args ( parser , section , option , rename_func , make_dirs = True ) : args = parser . get ( section , option , raw = True ) strings = get_strings ( args ) replace = False for string in strings : isfilename = any ( x in string for x in FILENAME_INDICATORS ) if isfilename : newstring = rename_func ( string ) if make_dirs : try_make_dirs ( newstring ) # To work with windows path specifications we need this replacement: raw_string = string . replace ( '\\\\' , '\\\\\\\\' ) raw_newstring = newstring . replace ( '\\\\' , '\\\\\\\\' ) args = args . replace ( raw_string , raw_newstring ) replace = True if replace : parser . set ( section , option , args )", "nl": "Searches for parser settings that define filenames ."}}
{"translation": {"code": "def _find_multiproc_options ( parser ) : sections = parser . sections ( ) if not any ( section . startswith ( 'multiproc_' ) for section in sections ) : return None mp_parser = NoInterpolationParser ( ) for section in sections : if section . startswith ( 'multiproc_' ) : new_section = section . replace ( 'multiproc_' , '' ) mp_parser . add_section ( new_section ) options = parser . options ( section ) for option in options : val = parser . get ( section , option , raw = True ) mp_parser . set ( new_section , option , val ) return mp_parser", "nl": "Searches for multiprocessing options within a ConfigParser ."}}
{"translation": {"code": "def _parser_to_string_io ( parser ) : memory_file = StringIO ( ) parser . write ( memory_file ) memory_file . flush ( ) memory_file . seek ( 0 ) return memory_file", "nl": "Turns a ConfigParser into a StringIO stream ."}}
{"translation": {"code": "def check_log_config ( self ) : if self . report_progress : if self . report_progress is True : self . report_progress = ( 5 , 'pypet' , logging . INFO ) elif isinstance ( self . report_progress , ( int , float ) ) : self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) elif isinstance ( self . report_progress , str ) : self . report_progress = ( 5 , self . report_progress , logging . INFO ) elif len ( self . report_progress ) == 2 : self . report_progress = ( self . report_progress [ 0 ] , self . report_progress [ 1 ] , logging . INFO ) if self . log_config : if self . log_config == pypetconstants . DEFAULT_LOGGING : pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) init_path = os . path . join ( pypet_path , 'logging' ) self . log_config = os . path . join ( init_path , 'default.ini' ) if isinstance ( self . log_config , str ) : if not os . path . isfile ( self . log_config ) : raise ValueError ( 'Could not find the logger init file ' '`%s`.' % self . log_config ) parser = NoInterpolationParser ( ) parser . read ( self . log_config ) elif isinstance ( self . log_config , cp . RawConfigParser ) : parser = self . log_config else : parser = None if parser is not None : self . _sp_config = self . _parser_to_string_io ( parser ) self . _mp_config = self . _find_multiproc_options ( parser ) if self . _mp_config is not None : self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) elif isinstance ( self . log_config , dict ) : self . _sp_config = self . log_config self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) if self . log_stdout : if self . log_stdout is True : self . log_stdout = ( 'STDOUT' , logging . INFO ) if isinstance ( self . log_stdout , str ) : self . log_stdout = ( self . log_stdout , logging . INFO ) if isinstance ( self . log_stdout , int ) : self . log_stdout = ( 'STDOUT' , self . log_stdout )", "nl": "Checks and converts all settings if necessary passed to the Manager ."}}
{"translation": {"code": "def _find_multiproc_dict ( dictionary ) : if not any ( key . startswith ( 'multiproc_' ) for key in dictionary . keys ( ) ) : return None mp_dictionary = { } for key in dictionary . keys ( ) : if key . startswith ( 'multiproc_' ) : new_key = key . replace ( 'multiproc_' , '' ) mp_dictionary [ new_key ] = dictionary [ key ] mp_dictionary [ 'version' ] = dictionary [ 'version' ] if 'disable_existing_loggers' in dictionary : mp_dictionary [ 'disable_existing_loggers' ] = dictionary [ 'disable_existing_loggers' ] return mp_dictionary", "nl": "Searches for multiprocessing options in a given dictionary ."}}
{"translation": {"code": "def _set_logger ( self , name = None ) : if name is None : cls = self . __class__ name = '%s.%s' % ( cls . __module__ , cls . __name__ ) self . _logger = logging . getLogger ( name )", "nl": "Adds a logger with a given name ."}}
{"translation": {"code": "def show_progress ( self , n , total_runs ) : if self . report_progress : percentage , logger_name , log_level = self . report_progress if logger_name == 'print' : logger = 'print' else : logger = logging . getLogger ( logger_name ) if n == - 1 : # Compute the number of digits and avoid log10(0) digits = int ( math . log10 ( total_runs + 0.1 ) ) + 1 self . _format_string = 'PROGRESS: Finished %' + '%d' % digits + 'd/%d runs ' fmt_string = self . _format_string % ( n + 1 , total_runs ) + '%s' reprint = log_level == 0 progressbar ( n , total_runs , percentage_step = percentage , logger = logger , log_level = log_level , fmt_string = fmt_string , reprint = reprint )", "nl": "Displays a progressbar"}}
{"translation": {"code": "def _set_details ( self , depth , branch , run_branch ) : self . _depth = depth self . _branch = branch self . _run_branch = run_branch", "nl": "Sets some details for internal handling ."}}
{"translation": {"code": "def make_set_name ( idx ) : GROUPSIZE = 1000 set_idx = idx // GROUPSIZE if set_idx >= 0 : return pypetconstants . FORMATTED_SET_NAME % set_idx else : return pypetconstants . SET_NAME_DUMMY", "nl": "Creates a run set name based on idx"}}
{"translation": {"code": "def _set_details_tree_node ( self , parent_node , name , instance ) : depth = parent_node . _depth + 1 if parent_node . v_is_root : branch = name # We add below root else : branch = parent_node . _branch if name in self . _root_instance . _run_information : run_branch = name else : run_branch = parent_node . _run_branch instance . _set_details ( depth , branch , run_branch )", "nl": "Renames a given instance based on parent_node and name ."}}
{"translation": {"code": "def _trj_store_explorations ( self , traj ) : nexplored = len ( traj . _explored_parameters ) if nexplored > 0 : if hasattr ( self . _overview_group , 'explorations' ) : explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) if len ( explorations_table ) != nexplored : self . _hdf5file . remove_node ( where = self . _overview_group , name = 'explorations' ) if not hasattr ( self . _overview_group , 'explorations' ) : explored_list = list ( traj . _explored_parameters . keys ( ) ) if explored_list : string_col = self . _all_get_table_col ( 'explorations' , explored_list , 'overview.explorations' ) else : string_col = pt . StringCol ( 1 ) description = { 'explorations' : string_col } explorations_table = self . _hdf5file . create_table ( where = self . _overview_group , name = 'explorations' , description = description ) rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored_list ] if rows : explorations_table . append ( rows ) explorations_table . flush ( )", "nl": "Stores a all explored parameter names for internal recall"}}
{"translation": {"code": "def _trj_load_exploration ( self , traj ) : if hasattr ( self . _overview_group , 'explorations' ) : explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) for row in explorations_table . iterrows ( ) : param_name = row [ 'explorations' ] . decode ( 'utf-8' ) if param_name not in traj . _explored_parameters : traj . _explored_parameters [ param_name ] = None else : # This is for backwards compatibility for what in ( 'parameters' , 'derived_parameters' ) : if hasattr ( self . _trajectory_group , what ) : parameters = self . _trajectory_group . _f_get_child ( what ) for group in parameters . _f_walk_groups ( ) : if self . _all_get_from_attrs ( group , HDF5StorageService . LENGTH ) : group_location = group . _v_pathname full_name = '.' . join ( group_location . split ( '/' ) [ 2 : ] ) traj . _explored_parameters [ full_name ] = None", "nl": "Recalls names of all explored parameters"}}
{"translation": {"code": "def _rename_full_name ( self , full_name , other_trajectory , used_runs = None , new_run_idx = None ) : split_name = full_name . split ( '.' ) for idx , name in enumerate ( split_name ) : if name in other_trajectory . _reversed_wildcards : run_indices , wildcards = other_trajectory . _reversed_wildcards [ name ] if new_run_idx is None : # We can safely take the first index of the index list that matches run_idx = None for run_jdx in run_indices : if run_jdx in used_runs : run_idx = used_runs [ run_jdx ] break elif run_jdx == - 1 : run_idx = - 1 break if run_idx is None : raise RuntimeError ( 'You shall not pass!' ) else : run_idx = new_run_idx new_name = self . f_wildcard ( wildcards [ 0 ] , run_idx ) split_name [ idx ] = new_name full_name = '.' . join ( split_name ) return full_name", "nl": "Renames a full name based on the wildcards and a particular run"}}
{"translation": {"code": "def _make_reversed_wildcards ( self , old_length = - 1 ) : if len ( self . _reversed_wildcards ) > 0 : # We already created reversed wildcards, so we don't need to do all of them # again start = old_length else : start = - 1 for wildcards , func in self . _wildcard_functions . items ( ) : for irun in range ( start , len ( self ) ) : translated_name = func ( irun ) if not translated_name in self . _reversed_wildcards : self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) self . _reversed_wildcards [ translated_name ] [ 0 ] . append ( irun )", "nl": "Creates a full mapping from all wildcard translations to the corresponding wildcards"}}
{"translation": {"code": "def _add_leaf_from_storage ( self , args , kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = LEAF , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = False , check_naming = False )", "nl": "Can be called from storage service to create a new leaf to bypass name checking"}}
{"translation": {"code": "def _add_group_from_storage ( self , args , kwargs ) : return self . _nn_interface . _add_generic ( self , type_name = GROUP , group_type_name = GROUP , args = args , kwargs = kwargs , add_prefix = False , check_naming = False )", "nl": "Can be called from storage service to create a new group to bypass name checking"}}
{"translation": {"code": "def add_parameters ( self , traj ) : if self . config_file : parameters = self . _collect_section ( 'parameters' ) for name in parameters : value = parameters [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f_add_parameter ( name , * value ) config = self . _collect_section ( 'config' ) for name in config : value = config [ name ] if not isinstance ( value , tuple ) : value = ( value , ) traj . f_add_config ( name , * value )", "nl": "Adds parameters and config from the . ini file to the trajectory"}}
{"translation": {"code": "def interpret ( self ) : if self . config_file : new_kwargs = self . _collect_config ( ) for key in new_kwargs : # Already specified kwargs take precedence over the ini file if key not in self . kwargs : self . kwargs [ key ] = new_kwargs [ key ] if not use_simple_logging ( self . kwargs ) and 'log_config' not in self . kwargs : self . kwargs [ 'log_config' ] = self . config_file return self . kwargs", "nl": "Copies parsed arguments into the kwargs passed to the environment"}}
{"translation": {"code": "def _collect_config ( self ) : kwargs = { } sections = ( 'storage_service' , 'trajectory' , 'environment' ) for section in sections : kwargs . update ( self . _collect_section ( section ) ) return kwargs", "nl": "Collects all info from three sections"}}
{"translation": {"code": "def _collect_section ( self , section ) : kwargs = { } try : if self . parser . has_section ( section ) : options = self . parser . options ( section ) for option in options : str_val = self . parser . get ( section , option ) val = ast . literal_eval ( str_val ) kwargs [ option ] = val return kwargs except : raise", "nl": "Collects all settings within a section"}}
{"translation": {"code": "def parse_config ( init_func ) : @ functools . wraps ( init_func ) def new_func ( env , * args , * * kwargs ) : config_interpreter = ConfigInterpreter ( kwargs ) # Pass the config data to the kwargs new_kwargs = config_interpreter . interpret ( ) init_func ( env , * args , * * new_kwargs ) # Add parameters and config data from the `.ini` file config_interpreter . add_parameters ( env . traj ) return new_func", "nl": "Decorator wrapping the environment to use a config file"}}
{"translation": {"code": "def retry ( n , errors , wait = 0.0 , logger_name = None ) : def wrapper ( func ) : @ functools . wraps ( func ) def new_func ( * args , * * kwargs ) : retries = 0 while True : try : result = func ( * args , * * kwargs ) if retries and logger_name : logger = logging . getLogger ( logger_name ) logger . debug ( 'Retry of `%s` successful' % func . __name__ ) return result except errors : if retries >= n : if logger_name : logger = logging . getLogger ( logger_name ) logger . exception ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) raise elif logger_name : logger = logging . getLogger ( logger_name ) logger . debug ( 'I could not execute `%s` with args %s and kwargs %s, ' 'starting next try. ' % ( func . __name__ , str ( args ) , str ( kwargs ) ) ) retries += 1 if wait : time . sleep ( wait ) return new_func return wrapper", "nl": "This is a decorator that retries a function ."}}
{"translation": {"code": "def f_remove ( self , key ) : key = self . _translate_key ( key ) try : del self . _dict [ key ] except KeyError : raise AttributeError ( 'Your annotations do not contain %s' % key )", "nl": "Removes key from annotations"}}
{"translation": {"code": "def make_ordinary_result ( result , key , trajectory = None , reload = True ) : shared_data = result . f_get ( key ) if trajectory is not None : shared_data . traj = trajectory shared_data . _request_data ( 'make_ordinary' ) result . f_remove ( key ) if reload : trajectory . f_load_item ( result , load_data = pypetconstants . OVERWRITE_DATA ) return result", "nl": "Turns a given shared data item into a an ordinary one ."}}
{"translation": {"code": "def _prm_read_shared_data ( self , shared_node , instance ) : try : data_type = self . _all_get_from_attrs ( shared_node , HDF5StorageService . SHARED_DATA_TYPE ) constructor = shared . FLAG_CLASS_MAPPING [ data_type ] name = shared_node . _v_name result = constructor ( name = name , parent = instance ) return result except : self . _logger . error ( 'Failed loading `%s` of `%s`.' % ( shared_node . _v_name , instance . v_full_name ) ) raise", "nl": "Reads shared data and constructs the appropraite class ."}}
{"translation": {"code": "def create_shared_data ( self , name = None , * * kwargs ) : if name is None : item = self . f_get ( ) else : item = self . f_get ( name ) return item . create_shared_data ( * * kwargs )", "nl": "Calls the corresponding function of the shared data item"}}
{"translation": {"code": "def _supports ( self , item ) : result = super ( SharedResult , self ) . _supports ( item ) result = result or type ( item ) in SharedResult . SUPPORTED_DATA return result", "nl": "Checks if outer data structure is supported ."}}
{"translation": {"code": "def _get_remaining ( self , index ) : try : current_time = datetime . datetime . now ( ) time_delta = current_time - self . _start_time try : total_seconds = time_delta . total_seconds ( ) except AttributeError : # for backwards-compatibility # Python 2.6 does not support `total_seconds` total_seconds = ( ( time_delta . microseconds + ( time_delta . seconds + time_delta . days * 24 * 3600 ) * 10 ** 6 ) / 10.0 ** 6 ) remaining_seconds = int ( ( self . _total - self . _start_index - 1.0 ) * total_seconds / float ( index - self . _start_index ) - total_seconds ) remaining_delta = datetime . timedelta ( seconds = remaining_seconds ) remaining_str = ', remaining: ' + str ( remaining_delta ) except ZeroDivisionError : remaining_str = '' return remaining_str", "nl": "Calculates remaining time as a string"}}
{"translation": {"code": "def _process_single_run ( kwargs ) : _configure_niceness ( kwargs ) _configure_logging ( kwargs ) result_queue = kwargs [ 'result_queue' ] result = _sigint_handling_single_run ( kwargs ) result_queue . put ( result ) result_queue . close ( )", "nl": "Wrapper function that first configures logging and starts a single run afterwards ."}}
{"translation": {"code": "def _configure_pool ( kwargs ) : _pool_single_run . storage_service = kwargs [ 'storage_service' ] _configure_niceness ( kwargs ) _configure_logging ( kwargs , extract = False )", "nl": "Configures the pool and keeps the storage service"}}
{"translation": {"code": "def _pool_single_run ( kwargs ) : wrap_mode = kwargs [ 'wrap_mode' ] traj = kwargs [ 'traj' ] traj . v_storage_service = _pool_single_run . storage_service if wrap_mode == pypetconstants . WRAP_MODE_LOCAL : # Free references from previous runs traj . v_storage_service . free_references ( ) return _sigint_handling_single_run ( kwargs )", "nl": "Starts a pool single run and passes the storage service"}}
{"translation": {"code": "def _configure_frozen_pool ( kwargs ) : _frozen_pool_single_run . kwargs = kwargs _configure_niceness ( kwargs ) _configure_logging ( kwargs , extract = False ) # Reset full copy to it's old value traj = kwargs [ 'traj' ] traj . v_full_copy = kwargs [ 'full_copy' ]", "nl": "Configures the frozen pool and keeps all kwargs"}}
{"translation": {"code": "def _frozen_pool_single_run ( kwargs ) : idx = kwargs . pop ( 'idx' ) frozen_kwargs = _frozen_pool_single_run . kwargs frozen_kwargs . update ( kwargs ) # in case of `run_map` # we need to update job's args and kwargs traj = frozen_kwargs [ 'traj' ] traj . f_set_crun ( idx ) return _sigint_handling_single_run ( frozen_kwargs )", "nl": "Single run wrapper for the frozen pool makes a single run and passes kwargs"}}
{"translation": {"code": "def merge_all_in_folder ( folder , ext = '.hdf5' , dynamic_imports = None , storage_service = None , force = False , ignore_data = ( ) , move_data = False , delete_other_files = False , keep_info = True , keep_other_trajectory_info = True , merge_config = True , backup = True ) : in_dir = os . listdir ( folder ) all_files = [ ] # Find all files with matching extension for file in in_dir : full_file = os . path . join ( folder , file ) if os . path . isfile ( full_file ) : _ , extension = os . path . splitext ( full_file ) if extension == ext : all_files . append ( full_file ) all_files = sorted ( all_files ) # Open all trajectories trajs = [ ] for full_file in all_files : traj = load_trajectory ( index = - 1 , storage_service = storage_service , filename = full_file , load_data = 0 , force = force , dynamic_imports = dynamic_imports ) trajs . append ( traj ) # Merge all trajectories first_traj = trajs . pop ( 0 ) first_traj . f_merge_many ( trajs , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = False , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = backup ) if delete_other_files : # Delete all but the first file for file in all_files [ 1 : ] : os . remove ( file ) return first_traj", "nl": "Merges all files in a given folder ."}}
{"translation": {"code": "def f_merge_many ( self , other_trajectories , ignore_data = ( ) , move_data = False , delete_other_trajectory = False , keep_info = True , keep_other_trajectory_info = True , merge_config = True , backup = True ) : other_length = len ( other_trajectories ) self . _logger . info ( 'Merging %d trajectories into the current one.' % other_length ) self . f_load_skeleton ( ) if backup : self . f_backup ( ) for idx , other in enumerate ( other_trajectories ) : self . f_merge ( other , ignore_data = ignore_data , move_data = move_data , delete_other_trajectory = delete_other_trajectory , keep_info = keep_info , keep_other_trajectory_info = keep_other_trajectory_info , merge_config = merge_config , backup = False , consecutive_merge = True ) self . _logger . log ( 21 , 'Merged %d out of %d' % ( idx + 1 , other_length ) ) self . _logger . info ( 'Storing data to disk' ) self . _reversed_wildcards = { } self . f_store ( ) self . _logger . info ( 'Finished final storage' )", "nl": "Can be used to merge several other_trajectories into your current one ."}}
{"translation": {"code": "def _update_run_information ( self , run_information_dict ) : idx = run_information_dict [ 'idx' ] name = run_information_dict [ 'name' ] self . _run_information [ name ] = run_information_dict self . _updated_run_information . add ( idx )", "nl": "Overwrites the run information of a particular run"}}
{"translation": {"code": "def f_finalize_run ( self , store_meta_data = True , clean_up = True ) : if not self . _run_started : return self self . _set_finish ( ) if clean_up and self . _is_run : self . _finalize_run ( ) self . _is_run = False self . _run_started = False self . _updated_run_information . add ( self . v_idx ) if store_meta_data : self . f_store ( only_init = True ) return self", "nl": "Can be called to finish a run if manually started ."}}
{"translation": {"code": "def manual_run ( turn_into_run = True , store_meta_data = True , clean_up = True ) : def wrapper ( func ) : @ functools . wraps ( func ) def new_func ( traj , * args , * * kwargs ) : do_wrap = not traj . _run_by_environment if do_wrap : traj . f_start_run ( turn_into_run = turn_into_run ) result = func ( traj , * args , * * kwargs ) if do_wrap : traj . f_finalize_run ( store_meta_data = store_meta_data , clean_up = clean_up ) return result return new_func return wrapper", "nl": "Can be used to decorate a function as a manual run function ."}}
{"translation": {"code": "def eval_one_max ( traj , individual ) : traj . f_add_result ( '$set.$.individual' , list ( individual ) ) fitness = sum ( individual ) traj . f_add_result ( '$set.$.fitness' , fitness ) traj . f_store ( ) return ( fitness , )", "nl": "The fitness function"}}
{"translation": {"code": "def f_start_run ( self , run_name_or_idx = None , turn_into_run = True ) : if self . _run_started : return self if run_name_or_idx is None : if self . v_idx == - 1 : raise ValueError ( 'Cannot start run if trajectory is not set to a particular run' ) else : self . f_set_crun ( run_name_or_idx ) self . _run_started = True if turn_into_run : self . _make_single_run ( ) self . _set_start ( ) return self", "nl": "Can be used to manually allow running of an experiment without using an environment ."}}
{"translation": {"code": "def _copy_from ( self , node , copy_leaves = True , overwrite = False , with_links = True ) : def _copy_skeleton ( node_in , node_out ) : \"\"\"Copies the skeleton of from `node_out` to `node_in`\"\"\" new_annotations = node_out . v_annotations node_in . _annotations = new_annotations node_in . v_comment = node_out . v_comment def _add_leaf ( leaf ) : \"\"\"Adds a leaf to the trajectory\"\"\" leaf_full_name = leaf . v_full_name try : found_leaf = self . f_get ( leaf_full_name , with_links = False , shortcuts = False , auto_load = False ) if overwrite : found_leaf . __setstate__ ( leaf . __getstate__ ( ) ) return found_leaf except AttributeError : pass if copy_leaves is True or ( copy_leaves == 'explored' and leaf . v_is_parameter and leaf . v_explored ) : new_leaf = self . f_add_leaf ( cp . copy ( leaf ) ) else : new_leaf = self . f_add_leaf ( leaf ) if new_leaf . v_is_parameter and new_leaf . v_explored : self . _explored_parameters [ new_leaf . v_full_name ] = new_leaf return new_leaf def _add_group ( group ) : \"\"\"Adds a new group to the trajectory\"\"\" group_full_name = group . v_full_name try : found_group = self . f_get ( group_full_name , with_links = False , shortcuts = False , auto_load = False ) if overwrite : _copy_skeleton ( found_group , group ) return found_group except AttributeError : pass new_group = self . f_add_group ( group_full_name ) _copy_skeleton ( new_group , group ) return new_group is_run = self . _is_run self . _is_run = False # So that we can copy Config Groups and Config Data try : if node . v_is_leaf : return _add_leaf ( node ) elif node . v_is_group : other_root = node . v_root if other_root is self : raise RuntimeError ( 'You cannot copy a given tree to itself!' ) result = _add_group ( node ) nodes_iterator = node . f_iter_nodes ( recursive = True , with_links = with_links ) has_links = [ ] if node . _links : has_links . append ( node ) for child in nodes_iterator : if child . v_is_leaf : _add_leaf ( child ) else : _add_group ( child ) if child . _links : has_links . append ( child ) if with_links : for current in has_links : mine = self . f_get ( current . v_full_name , with_links = False , shortcuts = False , auto_load = False ) my_link_set = set ( mine . _links . keys ( ) ) other_link_set = set ( current . _links . keys ( ) ) new_links = other_link_set - my_link_set for link in new_links : where_full_name = current . _links [ link ] . v_full_name mine . f_add_link ( link , where_full_name ) return result else : raise RuntimeError ( 'You shall not pass!' ) except Exception : self . _is_run = is_run", "nl": "Pass a node to insert the full tree to the trajectory ."}}
{"translation": {"code": "def _configure_niceness ( kwargs ) : niceness = kwargs [ 'niceness' ] if niceness is not None : try : try : current = os . nice ( 0 ) if niceness - current > 0 : # Under Linux you cannot decrement niceness if set elsewhere os . nice ( niceness - current ) except AttributeError : # Fall back on psutil under Windows psutil . Process ( ) . nice ( niceness ) except Exception as exc : sys . stderr . write ( 'Could not configure niceness because of: %s' % repr ( exc ) ) traceback . print_exc ( )", "nl": "Sets niceness of a process"}}