{"translation": {"code": "def multiply ( traj ) : z = traj . x * traj . y traj . f_add_result ( 'z' , z = z , comment = 'I am the product of two reals!' )", "nl": "Sophisticated simulation of multiplication"}}
{"translation": {"code": "def _all_create_or_get_group ( self , name , parent_hdf5_group = None ) : if not name in parent_hdf5_group : new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , name = name , title = name , filters = self . _all_get_filters ( ) ) return new_hdf5_group , True else : new_hdf5_group = parent_hdf5_group . _f_get_child ( name ) return new_hdf5_group , False", "nl": "Creates or returns a group"}}
{"translation": {"code": "def _prm_store_from_dict ( self , fullname , store_dict , hdf5_group , store_flags , kwargs ) : for key , data_to_store in store_dict . items ( ) : # self._logger.log(1, 'SUB-Storing %s [%s]', key, str(store_dict[key])) original_hdf5_group = None flag = store_flags [ key ] if '.' in key : original_hdf5_group = hdf5_group split_key = key . split ( '.' ) key = split_key . pop ( ) for inner_key in split_key : hdf5_group , newly_created = self . _all_create_or_get_group ( inner_key , hdf5_group ) if newly_created : setattr ( hdf5_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , HDF5StorageService . NESTED_GROUP ) else : store_type = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . STORAGE_TYPE ) if store_type != HDF5StorageService . NESTED_GROUP : raise ValueError ( 'You want to nested results but `%s` is already ' 'of type `%s`!' % ( hdf5_group . _v_name , store_type ) ) # Iterate through the data and store according to the storage flags if key in hdf5_group : # We won't change any data that is found on disk self . _logger . debug ( 'Found %s already in hdf5 node of %s, so I will ignore it.' % ( key , fullname ) ) continue if flag == HDF5StorageService . TABLE : # self._logger.log(1, 'SUB-Storing %s TABLE', key) self . _prm_write_into_pytable ( key , data_to_store , hdf5_group , fullname , * * kwargs ) elif flag == HDF5StorageService . DICT : # self._logger.log(1, 'SUB-Storing %s DICT', key) self . _prm_write_dict_as_table ( key , data_to_store , hdf5_group , fullname , * * kwargs ) elif flag == HDF5StorageService . ARRAY : # self._logger.log(1, 'SUB-Storing %s ARRAY', key) self . _prm_write_into_array ( key , data_to_store , hdf5_group , fullname , * * kwargs ) elif flag in ( HDF5StorageService . CARRAY , HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : self . _prm_write_into_other_array ( key , data_to_store , hdf5_group , fullname , flag = flag , * * kwargs ) elif flag in ( HDF5StorageService . SERIES , HDF5StorageService . FRAME , #  HDF5StorageService.PANEL ) : # self._logger.log(1, 'SUB-Storing %s PANDAS', key) self . _prm_write_pandas_data ( key , data_to_store , hdf5_group , fullname , flag , * * kwargs ) elif flag == HDF5StorageService . SHARED_DATA : pass # Shared data needs to be explicitly created and is not stored on # the fly else : raise RuntimeError ( 'You shall not pass!' ) if original_hdf5_group is not None : hdf5_group = original_hdf5_group", "nl": "Stores a store_dict"}}
{"translation": {"code": "def upload_file ( filename , session ) : print ( 'Uploading file %s' % filename ) outfilesource = os . path . join ( os . getcwd ( ) , filename ) outfiletarget = 'sftp://' + ADDRESS + WORKING_DIR out = saga . filesystem . File ( outfilesource , session = session , flags = OVERWRITE ) out . copy ( outfiletarget ) print ( 'Transfer of `%s` to `%s` successful' % ( filename , outfiletarget ) )", "nl": "Uploads a file"}}
{"translation": {"code": "def create_session ( ) : ctx = saga . Context ( \"UserPass\" ) ctx . user_id = USER ctx . user_pass = PASSWORD session = saga . Session ( ) session . add_context ( ctx ) return session", "nl": "Creates and returns a new SAGA session"}}
{"translation": {"code": "def merge_trajectories ( session ) : jd = saga . job . Description ( ) jd . executable = 'python' jd . arguments = [ 'merge_trajs.py' ] jd . output = \"mysagajob_merge.stdout\" jd . error = \"mysagajob_merge.stderr\" jd . working_directory = WORKING_DIR js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) myjob = js . create_job ( jd ) print ( \"\\n...starting job...\\n\" ) # Now we can start our job. myjob . run ( ) print ( \"Job ID    : %s\" % ( myjob . id ) ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"\\n...waiting for job...\\n\" ) # wait for the job to either finish or fail myjob . wait ( ) print ( \"Job State : %s\" % ( myjob . state ) ) print ( \"Exitcode  : %s\" % ( myjob . exit_code ) )", "nl": "Merges all trajectories found in the working directory"}}
{"translation": {"code": "def explore_batch ( traj , batch ) : explore_dict = { } explore_dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + 1 ) , 1.0 ) . tolist ( ) # for batch = 0 explores sigma in [0.0, 1.0, 2.0, ..., 9.0], # for batch = 1 explores sigma in [10.0, 11.0, 12.0, ..., 19.0] # and so on traj . f_explore ( explore_dict )", "nl": "Chooses exploration according to batch"}}
{"translation": {"code": "def _configure_frozen_scoop ( kwargs ) : def _delete_old_scoop_rev_data ( old_scoop_rev ) : if old_scoop_rev is not None : try : elements = shared . elements for key in elements : var_dict = elements [ key ] if old_scoop_rev in var_dict : del var_dict [ old_scoop_rev ] logging . getLogger ( 'pypet.scoop' ) . debug ( 'Deleted old SCOOP data from ' 'revolution `%s`.' % old_scoop_rev ) except AttributeError : logging . getLogger ( 'pypet.scoop' ) . error ( 'Could not delete old SCOOP data from ' 'revolution `%s`.' % old_scoop_rev ) scoop_rev = kwargs . pop ( 'scoop_rev' ) # Check if we need to reconfigure SCOOP try : old_scoop_rev = _frozen_scoop_single_run . kwargs [ 'scoop_rev' ] configured = old_scoop_rev == scoop_rev except ( AttributeError , KeyError ) : old_scoop_rev = None configured = False if not configured : _frozen_scoop_single_run . kwargs = shared . getConst ( scoop_rev , timeout = 424.2 ) frozen_kwargs = _frozen_scoop_single_run . kwargs frozen_kwargs [ 'scoop_rev' ] = scoop_rev frozen_kwargs [ 'traj' ] . v_full_copy = frozen_kwargs [ 'full_copy' ] if not scoop . IS_ORIGIN : _configure_niceness ( frozen_kwargs ) _configure_logging ( frozen_kwargs , extract = False ) _delete_old_scoop_rev_data ( old_scoop_rev ) logging . getLogger ( 'pypet.scoop' ) . info ( 'Configured Worker %s' % str ( scoop . worker ) )", "nl": "Wrapper function that configures a frozen SCOOP set up ."}}
{"translation": {"code": "def _handle_data ( self , msg , args , kwargs ) : stop = False try : if msg == 'DONE' : stop = True elif msg == 'STORE' : if 'msg' in kwargs : store_msg = kwargs . pop ( 'msg' ) else : store_msg = args [ 0 ] args = args [ 1 : ] if 'stuff_to_store' in kwargs : stuff_to_store = kwargs . pop ( 'stuff_to_store' ) else : stuff_to_store = args [ 0 ] args = args [ 1 : ] trajectory_name = kwargs [ 'trajectory_name' ] if self . _trajectory_name != trajectory_name : if self . _storage_service . is_open : self . _close_file ( ) self . _trajectory_name = trajectory_name self . _open_file ( ) self . _storage_service . store ( store_msg , stuff_to_store , * args , * * kwargs ) self . _storage_service . store ( pypetconstants . FLUSH , None ) self . _check_and_collect_garbage ( ) else : raise RuntimeError ( 'You queued something that was not ' 'intended to be queued. I did not understand message ' '`%s`.' % msg ) except Exception : self . _logger . exception ( 'ERROR occurred during storing!' ) time . sleep ( 0.01 ) pass # We don't want to kill the queue process in case of an error return stop", "nl": "Handles data and returns True or False if everything is done ."}}
{"translation": {"code": "def _receive_data ( self ) : result = self . queue . get ( block = True ) if hasattr ( self . queue , 'task_done' ) : self . queue . task_done ( ) return result", "nl": "Gets data from queue"}}
{"translation": {"code": "def _receive_data ( self ) : while True : while len ( self . _buffer ) < self . max_size and self . conn . poll ( ) : data = self . _read_chunks ( ) if data is not None : self . _buffer . append ( data ) if len ( self . _buffer ) > 0 : return self . _buffer . popleft ( )", "nl": "Gets data from pipe"}}
{"translation": {"code": "def store ( self , * args , * * kwargs ) : try : self . acquire_lock ( ) return self . _storage_service . store ( * args , * * kwargs ) finally : if self . lock is not None : try : self . release_lock ( ) except RuntimeError : self . _logger . error ( 'Could not release lock `%s`!' % str ( self . lock ) )", "nl": "Acquires a lock before storage and releases it afterwards ."}}
{"translation": {"code": "def _lock ( self , name , client_id , request_id ) : if name in self . _locks : other_client_id , other_request_id = self . _locks [ name ] if other_client_id == client_id : response = ( self . LOCK_ERROR + self . DELIMITER + 'Re-request of lock `%s` (old request id `%s`) by `%s` ' '(request id `%s`)' % ( name , client_id , other_request_id , request_id ) ) self . _logger . warning ( response ) return response else : return self . WAIT else : self . _locks [ name ] = ( client_id , request_id ) return self . GO", "nl": "Hanldes locking of locks"}}
{"translation": {"code": "def port_to_tcp ( port = None ) : #address = 'tcp://' + socket.gethostbyname(socket.getfqdn()) domain_name = socket . getfqdn ( ) try : addr_list = socket . getaddrinfo ( domain_name , None ) except Exception : addr_list = socket . getaddrinfo ( '127.0.0.1' , None ) family , socktype , proto , canonname , sockaddr = addr_list [ 0 ] host = convert_ipv6 ( sockaddr [ 0 ] ) address = 'tcp://' + host if port is None : port = ( ) if not isinstance ( port , int ) : # determine port automatically context = zmq . Context ( ) try : socket_ = context . socket ( zmq . REP ) socket_ . ipv6 = is_ipv6 ( address ) port = socket_ . bind_to_random_port ( address , * port ) except Exception : print ( 'Could not connect to {} using {}' . format ( address , addr_list ) ) pypet_root_logger = logging . getLogger ( 'pypet' ) pypet_root_logger . exception ( 'Could not connect to {}' . format ( address ) ) raise socket_ . close ( ) context . term ( ) return address + ':' + str ( port )", "nl": "Returns local tcp address for a given port automatic port if None"}}
{"translation": {"code": "def _detect_fork ( self ) : if self . _pid is None : self . _pid = os . getpid ( ) if self . _context is not None : current_pid = os . getpid ( ) if current_pid != self . _pid : self . _logger . debug ( 'Fork detected: My pid `%s` != os pid `%s`. ' 'Restarting connection.' % ( str ( self . _pid ) , str ( current_pid ) ) ) self . _context = None self . _pid = current_pid", "nl": "Detects if lock client was forked ."}}
{"translation": {"code": "def start ( self , test_connection = True ) : if self . _context is None : self . _logger . debug ( 'Starting Client' ) self . _context = zmq . Context ( ) self . _poll = zmq . Poller ( ) self . _start_socket ( ) if test_connection : self . test_ping ( )", "nl": "Starts connection to server if not existent ."}}
{"translation": {"code": "def _req_rep_retry ( self , request ) : retries_left = self . RETRIES while retries_left : self . _logger . log ( 1 , 'Sending REQ `%s`' , request ) self . _send_request ( request ) socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) if socks . get ( self . _socket ) == zmq . POLLIN : response = self . _receive_response ( ) self . _logger . log ( 1 , 'Received REP `%s`' , response ) return response , self . RETRIES - retries_left else : self . _logger . debug ( 'No response from server (%d retries left)' % retries_left ) self . _close_socket ( confused = True ) retries_left -= 1 if retries_left == 0 : raise RuntimeError ( 'Server seems to be offline!' ) time . sleep ( self . SLEEP ) self . _start_socket ( )", "nl": "Returns response and number of retries"}}
{"translation": {"code": "def prefix_naming ( cls ) : if hasattr ( cls , '__getattr__' ) : raise TypeError ( '__getattr__ already defined' ) cls . __getattr__ = _prfx_getattr_ cls . __setattr__ = _prfx_setattr_ return cls", "nl": "Decorate that adds the prefix naming scheme"}}
{"translation": {"code": "def func ( self ) : if self . _func is None : self . _func = NNTreeNodeFunc ( self ) return self . _func", "nl": "Alternative naming you can use node . func . name instead of node . f_func"}}
{"translation": {"code": "def f_dir_data ( self ) : if ( self . _nn_interface is not None and self . _nn_interface . _root_instance is not None and self . v_root . v_auto_load ) : try : if self . v_is_root : self . f_load ( recursive = True , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON , with_meta_data = False , with_run_information = False ) else : self . f_load ( recursive = True , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON ) except Exception as exc : pass return list ( self . _children . keys ( ) )", "nl": "Returns a list of all children names"}}
{"translation": {"code": "def racedirs ( path ) : if os . path . isfile ( path ) : raise IOError ( 'Path `%s` is already a file not a directory' ) while True : try : if os . path . isdir ( path ) : # only break if full path has been created or exists break os . makedirs ( path ) except EnvironmentError as exc : # Part of the directory path already exist if exc . errno != 17 : # This error won't be any good raise", "nl": "Like os . makedirs but takes care about race conditions"}}
{"translation": {"code": "def _sigint_handling_single_run ( kwargs ) : try : graceful_exit = kwargs [ 'graceful_exit' ] if graceful_exit : sigint_handling . start ( ) if sigint_handling . hit : result = ( sigint_handling . SIGINT , None ) else : result = _single_run ( kwargs ) if sigint_handling . hit : result = ( sigint_handling . SIGINT , result ) return result return _single_run ( kwargs ) except : # Log traceback of exception pypet_root_logger = logging . getLogger ( 'pypet' ) pypet_root_logger . exception ( 'ERROR occurred during a single run! ' ) raise", "nl": "Wrapper that allow graceful exits of single runs"}}
{"translation": {"code": "def pre_run_network ( self , traj ) : self . pre_build ( traj ) self . _logger . info ( '\\n------------------------\\n' 'Pre-Running the Network\\n' '------------------------' ) self . _network = self . _network_constructor ( * self . _brian_list ) self . network_runner . execute_network_pre_run ( traj , self . _network , self . _network_dict , self . components , self . analysers ) self . _logger . info ( '\\n-----------------------------\\n' 'Network Simulation successful\\n' '-----------------------------' ) self . _pre_run = True if hasattr ( self . _network , 'store' ) : self . _network . store ( 'pre_run' )", "nl": "Starts a network run before the individual run ."}}
{"translation": {"code": "def add_parameters ( self , traj ) : self . _logger . info ( 'Adding Parameters of Components' ) for component in self . components : component . add_parameters ( traj ) if self . analysers : self . _logger . info ( 'Adding Parameters of Analysers' ) for analyser in self . analysers : analyser . add_parameters ( traj ) self . _logger . info ( 'Adding Parameters of Runner' ) self . network_runner . add_parameters ( traj )", "nl": "Adds parameters for a network simulation ."}}
{"translation": {"code": "def _execute_network_run ( self , traj , network , network_dict , component_list , analyser_list , pre_run = False ) : # Initially extract the `subrun_list` subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) # counter for subruns subrun_number = 0 # Execute all subruns in order while len ( subrun_list ) > 0 : # Get the next subrun current_subrun = subrun_list . pop ( 0 ) # 1. Call `add` of all normal components for component in component_list : component . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) # 2. Call `add` of all analyser components for analyser in analyser_list : analyser . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) # 3. Call `add` of the network runner itself self . add_to_network ( traj , network , current_subrun , subrun_list , network_dict ) # 4. Run the network self . _logger . info ( 'STARTING subrun `%s` (#%d) lasting %s.' % ( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) network . run ( duration = current_subrun . f_get ( ) , report = self . _report , report_period = self . _report_period ) # 5. Call `analyse` of all analyser components for analyser in analyser_list : analyser . analyse ( traj , network , current_subrun , subrun_list , network_dict ) # 6. Call `remove` of the network runner itself self . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) # 7. Call `remove` for all analyser components for analyser in analyser_list : analyser . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) # 8. Call `remove` for all normal components for component in component_list : component . remove_from_network ( traj , network , current_subrun , subrun_list , network_dict ) subrun_number += 1", "nl": "Generic execute_network_run function handles experimental runs as well as pre - runs ."}}
{"translation": {"code": "def _extract_subruns ( self , traj , pre_run = False ) : if pre_run : durations_list = traj . f_get_all ( self . _pre_durations_group_name ) else : durations_list = traj . f_get_all ( self . _durations_group_name ) subruns = { } orders = [ ] for durations in durations_list : for duration_param in durations . f_iter_leaves ( with_links = False ) : if 'order' in duration_param . v_annotations : order = duration_param . v_annotations . order else : raise RuntimeError ( 'Your duration parameter %s has no order. Please add ' 'an order in `v_annotations.order`.' % duration_param . v_full_name ) if order in subruns : raise RuntimeError ( 'Your durations must differ in their order, there are two ' 'with order %d.' % order ) else : subruns [ order ] = duration_param orders . append ( order ) return [ subruns [ order ] for order in sorted ( orders ) ]", "nl": "Extracts subruns from the trajectory ."}}
{"translation": {"code": "def execute_network_run ( self , traj , network , network_dict , component_list , analyser_list ) : self . _execute_network_run ( traj , network , network_dict , component_list , analyser_list , pre_run = False )", "nl": "Runs a network in an experimental run ."}}
{"translation": {"code": "def analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : #Check if we finished all subruns if len ( subrun_list ) == 0 : spikes_e = traj . results . monitors . spikes_e time_window = traj . parameters . analysis . statistics . time_window start_time = traj . parameters . simulation . durations . initial_run end_time = start_time + traj . parameters . simulation . durations . measurement_run neuron_ids = traj . parameters . analysis . statistics . neuron_ids mean_ff = self . _compute_mean_fano_factor ( neuron_ids , spikes_e , time_window , start_time , end_time ) traj . f_add_result ( 'statistics.mean_fano_factor' , mean_ff , comment = 'Average Fano ' 'Factor over all ' 'exc neurons' ) print ( 'R_ee: %f, Mean FF: %f' % ( traj . R_ee , mean_ff ) )", "nl": "Calculates average Fano Factor of a network ."}}
{"translation": {"code": "def add_parameters ( self , traj ) : par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.initial_run' , 500 * ms , comment = 'Initialisation run for more realistic ' 'measurement conditions.' ) par . v_annotations . order = 0 par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.measurement_run' , 1500 * ms , comment = 'Measurement run that is considered for ' 'statistical evaluation' ) par . v_annotations . order = 1", "nl": "Adds all necessary parameters to traj container ."}}
{"translation": {"code": "def build ( self , traj , brian_list , network_dict ) : if not hasattr ( self , '_pre_build' ) or not self . _pre_build : self . _build_connections ( traj , brian_list , network_dict )", "nl": "Builds the connections ."}}
{"translation": {"code": "def pre_build ( self , traj , brian_list , network_dict ) : self . _pre_build = not _explored_parameters_in_group ( traj , traj . parameters . connections ) self . _pre_build = ( self . _pre_build and 'neurons_i' in network_dict and 'neurons_e' in network_dict ) if self . _pre_build : self . _build_connections ( traj , brian_list , network_dict )", "nl": "Pre - builds the connections ."}}
{"translation": {"code": "def build ( self , traj , brian_list , network_dict ) : if not hasattr ( self , '_pre_build' ) or not self . _pre_build : self . _build_model ( traj , brian_list , network_dict )", "nl": "Builds the neuron groups ."}}
{"translation": {"code": "def _explored_parameters_in_group ( traj , group_node ) : explored = False for param in traj . f_get_explored_parameters ( ) : if param in group_node : explored = True break return explored", "nl": "Checks if one the parameters in group_node is explored ."}}
{"translation": {"code": "def run_net ( traj ) : eqs = traj . eqs # Create a namespace dictionairy namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) # Create the Neuron Group neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , namespace = namespace ) neuron . vm = traj . EL neuron . w = traj . a * ( neuron . vm - traj . EL ) neuron . Vr = linspace ( - 48.3 * mV , - 47.7 * mV , traj . N ) # bifurcation parameter # Run the network initially for 100 milliseconds print ( 'Initial Run' ) net = Network ( neuron ) net . run ( 100 * ms , report = 'text' ) # we discard the first spikes # Create a Spike Monitor MSpike = SpikeMonitor ( neuron ) net . add ( MSpike ) # Create a State Monitor for the membrane voltage, record from neurons 1-3 MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) net . add ( MStateV ) # Now record for 500 milliseconds print ( 'Measurement run' ) net . run ( 500 * ms , report = 'text' ) # Add the BRAIN monitors traj . v_standard_result = Brian2MonitorResult traj . f_add_result ( 'SpikeMonitor' , MSpike ) traj . f_add_result ( 'StateMonitorV' , MStateV )", "nl": "Creates and runs BRIAN network based on the parameters in traj ."}}
{"translation": {"code": "def _make_folder ( self , traj ) : print_folder = os . path . join ( traj . analysis . plot_folder , traj . v_name , traj . v_crun ) print_folder = os . path . abspath ( print_folder ) if not os . path . isdir ( print_folder ) : os . makedirs ( print_folder ) return print_folder", "nl": "Makes a subfolder for plots ."}}