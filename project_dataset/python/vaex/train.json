{"translation": {"code": "def os_open ( document ) : osname = platform . system ( ) . lower ( ) if osname == \"darwin\" : os . system ( \"open \\\"\" + document + \"\\\"\" ) if osname == \"linux\" : cmd = \"xdg-open \\\"\" + document + \"\\\"&\" os . system ( cmd ) if osname == \"windows\" : os . system ( \"start \\\"\" + document + \"\\\"\" )", "nl": "Open document by the default handler of the OS could be a url opened by a browser a text file by an editor etc"}}
{"translation": {"code": "def getinfo ( filename , seek = None ) : DESC = '=I4sII' # struct formatting string HEAD = '=I6I6dddii6iiiddddii6ii60xI' # struct formatting string keys = ( 'Npart' , 'Massarr' , 'Time' , 'Redshift' , 'FlagSfr' , 'FlagFeedback' , 'Nall' , 'FlagCooling' , 'NumFiles' , 'BoxSize' , 'Omega0' , 'OmegaLambda' , 'HubbleParam' , 'FlagAge' , 'FlagMetals' , 'NallHW' , 'flag_entr_ics' , 'filename' ) f = open ( filename , 'rb' ) \"\"\"Detects Gadget file type (type 1 or 2; resp. without or with the 16\n\tbyte block headers).\"\"\" firstbytes = struct . unpack ( 'I' , f . read ( 4 ) ) if firstbytes [ 0 ] == 8 : gtype = 2 else : gtype = 1 if gtype == 2 : f . seek ( 16 ) else : f . seek ( 0 ) if seek is not None : f . seek ( seek ) raw = struct . unpack ( HEAD , f . read ( 264 ) ) [ 1 : - 1 ] values = ( raw [ : 6 ] , raw [ 6 : 12 ] ) + raw [ 12 : 16 ] + ( raw [ 16 : 22 ] , ) + raw [ 22 : 30 ] + ( raw [ 30 : 36 ] , raw [ 36 ] , filename ) header = dict ( list ( zip ( keys , values ) ) ) f . close ( ) if gtype == 2 : posoffset = ( 2 * 16 + ( 8 + 256 ) ) else : posoffset = ( 8 + 256 ) Npart = sum ( header [ 'Npart' ] ) if gtype == 2 : veloffset = 3 * 16 + ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) else : veloffset = ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) return Npart , posoffset + 4 , veloffset + 4 , header", "nl": "Read header data from Gadget data file filename with Gadget file type gtype . Returns offsets of positions and velocities ."}}
{"translation": {"code": "def zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = 1 , seed = None ) : import vaex . file return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale )", "nl": "Creates a zeldovich DataFrame ."}}
{"translation": {"code": "def open_many ( filenames ) : dfs = [ ] for filename in filenames : filename = filename . strip ( ) if filename and filename [ 0 ] != \"#\" : dfs . append ( open ( filename ) ) return vaex . dataframe . DataFrameConcatenated ( dfs = dfs )", "nl": "Open a list of filenames and return a DataFrame with all DataFrames cocatenated ."}}
{"translation": {"code": "def server ( url , * * kwargs ) : from vaex . remote import ServerRest url = urlparse ( url ) if url . scheme == \"ws\" : websocket = True else : websocket = False assert url . scheme in [ \"ws\" , \"http\" ] port = url . port base_path = url . path hostname = url . hostname return vaex . remote . ServerRest ( hostname , base_path = base_path , port = port , websocket = websocket , * * kwargs )", "nl": "Connect to hostname supporting the vaex web api ."}}
{"translation": {"code": "def app ( * args , * * kwargs ) : import vaex . ui . main return vaex . ui . main . VaexApp ( )", "nl": "Create a vaex app the QApplication mainloop must be started ."}}
{"translation": {"code": "def from_arrays ( * * arrays ) : import numpy as np import six from . column import Column df = vaex . dataframe . DataFrameArrays ( \"array\" ) for name , array in arrays . items ( ) : if isinstance ( array , Column ) : df . add_column ( name , array ) else : array = np . asanyarray ( array ) df . add_column ( name , array ) return df", "nl": "Create an in memory DataFrame from numpy arrays ."}}
{"translation": {"code": "def from_samp ( username = None , password = None ) : print ( \"Waiting for SAMP message...\" ) import vaex . samp t = vaex . samp . single_table ( username = username , password = password ) return from_astropy_table ( t . to_table ( ) )", "nl": "Connect to a SAMP Hub and wait for a single table load event disconnect download the table and return the DataFrame ."}}
{"translation": {"code": "def delayed ( f ) : def wrapped ( * args , * * kwargs ) : # print \"calling\", f, \"with\", kwargs # key_values = kwargs.items() key_promise = list ( [ ( key , promisify ( value ) ) for key , value in kwargs . items ( ) ] ) # key_promise = [(key, promisify(value)) for key, value in key_values] arg_promises = list ( [ promisify ( value ) for value in args ] ) kwarg_promises = list ( [ promise for key , promise in key_promise ] ) promises = arg_promises + kwarg_promises for promise in promises : def echo_error ( exc , promise = promise ) : print ( \"error with \" , promise , \"exception is\" , exc ) # raise exc def echo ( value , promise = promise ) : print ( \"done with \" , repr ( promise ) , \"value is\" , value ) # promise.then(echo, echo_error) # print promises allarguments = aplus . listPromise ( * promises ) def call ( _ ) : kwargs_real = { key : promise . get ( ) for key , promise in key_promise } args_real = list ( [ promise . get ( ) for promise in arg_promises ] ) return f ( * args_real , * * kwargs_real ) def error ( exc ) : print ( \"error\" , exc ) raise exc return allarguments . then ( call , error ) return wrapped", "nl": "Decorator to transparantly accept delayed computation ."}}
{"translation": {"code": "def from_scalars ( * * kwargs ) : import numpy as np return from_arrays ( * * { k : np . array ( [ v ] ) for k , v in kwargs . items ( ) } )", "nl": "Similar to from_arrays but convenient for a DataFrame of length 1 ."}}
{"translation": {"code": "def from_astropy_table ( table ) : import vaex . file . other return vaex . file . other . DatasetAstropyTable ( table = table )", "nl": "Create a vaex DataFrame from an Astropy Table ."}}
{"translation": {"code": "def get_autotype ( arr ) : try : narr = arr . astype ( 'float' ) if ( narr < sys . maxsize ) . all ( ) and ( narr % 1 ) . sum ( ) == 0 : return narr . astype ( 'int' ) else : return narr except ValueError : return arr", "nl": "Attempts to return a numpy array converted to the most sensible dtype Value errors will be caught and simply return the original array Tries to make dtype int then float then no change"}}
{"translation": {"code": "def as_recarray ( self ) : dtype = [ ( k , v . dtype ) for k , v in self . __dict__ . iteritems ( ) ] R = numpy . recarray ( len ( self . __dict__ [ k ] ) , dtype = dtype ) for key in self . __dict__ : R [ key ] = self . __dict__ [ key ] return R", "nl": "Convert into numpy recordarray"}}
{"translation": {"code": "def iter_properties ( fh , comments = False ) : for line in _property_lines ( fh ) : key , value = _split_key_value ( line ) if key is not COMMENT : key = _unescape ( key ) elif not comments : continue yield key , _unescape ( value )", "nl": "Incrementally read properties from a Java . properties file ."}}
{"translation": {"code": "def store_properties ( fh , props , comment = None , timestamp = True ) : if comment is not None : write_comment ( fh , comment ) if timestamp : write_comment ( fh , time . strftime ( '%a %b %d %H:%M:%S %Z %Y' ) ) if hasattr ( props , 'keys' ) : for key in props : write_property ( fh , key , props [ key ] ) else : for key , value in props : write_property ( fh , key , value )", "nl": "Writes properties to the file in Java properties format ."}}
{"translation": {"code": "def write_comment ( fh , comment ) : _require_string ( comment , 'comments' ) fh . write ( _escape_comment ( comment ) ) fh . write ( b'\\n' )", "nl": "Writes a comment to the file in Java properties format ."}}
{"translation": {"code": "def write_property ( fh , key , value ) : if key is COMMENT : write_comment ( fh , value ) return _require_string ( key , 'keys' ) _require_string ( value , 'values' ) fh . write ( _escape_key ( key ) ) fh . write ( b'=' ) fh . write ( _escape_value ( value ) ) fh . write ( b'\\n' )", "nl": "Write a single property to the file in Java properties format ."}}
{"translation": {"code": "def _universal_newlines ( fp ) : # if file was opened with universal newline support we don't need to convert if 'U' in getattr ( fp , 'mode' , '' ) : for line in fp : yield line else : for line in fp : line = line . replace ( b'\\r\\n' , b'\\n' ) . replace ( b'\\r' , b'\\n' ) for piece in line . split ( b'\\n' ) : yield piece", "nl": "Wrap a file to convert newlines regardless of whether the file was opened with the universal newlines option or not ."}}
{"translation": {"code": "def monochrome ( I , color , vmin = None , vmax = None ) : if vmin is None : vmin = np . nanmin ( I ) if vmax is None : vmax = np . nanmax ( I ) normalized = ( I - vmin ) / ( vmax - vmin ) return np . clip ( normalized [ ... , np . newaxis ] , 0 , 1 ) * np . array ( color )", "nl": "Turns a intensity array to a monochrome image by replacing each intensity by a scaled color"}}
{"translation": {"code": "def polychrome ( I , colors , vmin = None , vmax = None , axis = - 1 ) : axes_length = len ( I . shape ) allaxes = list ( range ( axes_length ) ) otheraxes = list ( allaxes ) otheraxes . remove ( ( axis + axes_length ) % axes_length ) otheraxes = tuple ( otheraxes ) if vmin is None : vmin = np . nanmin ( I , axis = otheraxes ) if vmax is None : vmax = np . nanmax ( I , axis = otheraxes ) normalized = ( I - vmin ) / ( vmax - vmin ) return np . clip ( normalized , 0 , 1 ) . dot ( colors )", "nl": "Similar to monochrome but now do it for multiple colors"}}
{"translation": {"code": "def from_pandas ( df , name = \"pandas\" , copy_index = True , index_name = \"index\" ) : import six vaex_df = vaex . dataframe . DataFrameArrays ( name ) def add ( name , column ) : values = column . values try : vaex_df . add_column ( name , values ) except Exception as e : print ( \"could not convert column %s, error: %r, will try to convert it to string\" % ( name , e ) ) try : values = values . astype ( \"S\" ) vaex_df . add_column ( name , values ) except Exception as e : print ( \"Giving up column %s, error: %r\" % ( name , e ) ) for name in df . columns : add ( name , df [ name ] ) if copy_index : add ( index_name , df . index ) return vaex_df", "nl": "Create an in memory DataFrame from a pandas DataFrame ."}}
{"translation": {"code": "def write_to ( f , mode ) : if hasattr ( f , 'write' ) : yield f else : f = open ( f , mode ) yield f f . close ( )", "nl": "Flexible writing where f can be a filename or f object if filename closed after writing"}}
{"translation": {"code": "def add_virtual_columns_cartesian_velocities_to_pmvr ( self , x = \"x\" , y = \"y\" , z = \"z\" , vx = \"vx\" , vy = \"vy\" , vz = \"vz\" , vr = \"vr\" , pm_long = \"pm_long\" , pm_lat = \"pm_lat\" , distance = None ) : if distance is None : distance = \"sqrt({x}**2+{y}**2+{z}**2)\" . format ( * * locals ( ) ) k = 4.74057 self . add_variable ( \"k\" , k , overwrite = False ) self . add_virtual_column ( vr , \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\" . format ( * * locals ( ) ) ) self . add_virtual_column ( pm_long , \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)/{distance}/k\" . format ( * * locals ( ) ) ) self . add_virtual_column ( pm_lat , \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( ({x}**2+{y}**2+{z}**2) * sqrt({x}**2+{y}**2) )/k\" . format ( * * locals ( ) ) )", "nl": "Concert velocities from a cartesian system to proper motions and radial velocities"}}
{"translation": {"code": "def patch ( f ) : name = f . __name__ Dataset . __hidden__ [ name ] = f return f", "nl": "Adds method f to the Dataset class"}}
{"translation": {"code": "def add_virtual_columns_proper_motion2vperpendicular ( self , distance = \"distance\" , pm_long = \"pm_l\" , pm_lat = \"pm_b\" , vl = \"vl\" , vb = \"vb\" , propagate_uncertainties = False , radians = False ) : k = 4.74057 self . add_variable ( \"k\" , k , overwrite = False ) self . add_virtual_column ( vl , \"k*{pm_long}*{distance}\" . format ( * * locals ( ) ) ) self . add_virtual_column ( vb , \"k* {pm_lat}*{distance}\" . format ( * * locals ( ) ) ) if propagate_uncertainties : self . propagate_uncertainties ( [ self [ vl ] , self [ vb ] ] )", "nl": "Convert proper motion to perpendicular velocities ."}}
{"translation": {"code": "def patch ( f ) : name = f . __name__ setattr ( DataFrame , name , f ) return f", "nl": "Adds method f to the DataFrame class"}}
{"translation": {"code": "def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = False ) : expression = _ensure_strings_from_expressions ( expression ) result = self . server . _call_dataset ( \"evaluate\" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) # TODO: we ignore out return result", "nl": "basic support for evaluate at server at least to run some unittest do not expect this to work from strings"}}
{"translation": {"code": "def from_csv ( filename_or_buffer , copy_index = True , * * kwargs ) : import pandas as pd return from_pandas ( pd . read_csv ( filename_or_buffer , * * kwargs ) , copy_index = copy_index )", "nl": "Shortcut to read a csv file using pandas and convert to a DataFrame directly ."}}
{"translation": {"code": "def _task ( self , task , progressbar = False ) : if self . delay : # should return a task or a promise nesting it return self . executor . schedule ( task ) else : import vaex . utils callback = None try : if progressbar == True : def update ( fraction ) : bar . update ( fraction ) return True bar = vaex . utils . progressbar ( task . name ) callback = self . executor . signal_progress . connect ( update ) elif progressbar : callback = self . executor . signal_progress . connect ( progressbar ) result = self . executor . run ( task ) if progressbar == True : bar . finish ( ) sys . stdout . write ( '\\n' ) return result finally : if callback : self . executor . signal_progress . disconnect ( callback )", "nl": "Helper function for returning tasks results result when immediate is True otherwise the task itself which is a promise"}}
{"translation": {"code": "def open ( self , path ) : logger . debug ( \"open dataset: %r\" , path ) if path . startswith ( \"http\" ) or path . startswith ( \"ws\" ) : dataset = vaex . open ( path , thread_mover = self . call_in_main_thread ) else : dataset = vaex . open ( path ) self . add_recently_opened ( path ) self . dataset_selector . add ( dataset ) return dataset", "nl": "Add a dataset and add it to the UI"}}
{"translation": {"code": "def sort ( self , Ncol , order ) : self . emit ( QtCore . SIGNAL ( \"layoutAboutToBeChanged()\" ) ) if Ncol == 0 : print ( \"by name\" ) # get indices, sorted by pair name sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) print ( sortlist ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) print ( sortlist ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) print ( ( self . indices ) ) if Ncol == 1 : # get indices, sorted by ranking, or no sorting if None not in self . ranking : sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) sortlist . sort ( key = operator . itemgetter ( 0 ) ) self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) else : self . indices = list ( range ( len ( self . pairs ) ) ) print ( ( self . indices ) ) if order == QtCore . Qt . DescendingOrder : self . indices . reverse ( ) print ( ( self . indices ) ) self . emit ( QtCore . SIGNAL ( \"layoutChanged()\" ) )", "nl": "Sort table by given column number ."}}
{"translation": {"code": "def clear ( self , event ) : if self . useblit : self . background = ( self . canvas . copy_from_bbox ( self . canvas . figure . bbox ) ) for line in self . vlines + self . hlines : line . set_visible ( False ) self . ellipse . set_visible ( False )", "nl": "clear the cursor"}}
{"translation": {"code": "def _wait ( self ) : logger . debug ( \"will wait for last plot to finish\" ) self . _plot_event = threading . Event ( ) self . queue_update . _wait ( ) self . queue_replot . _wait ( ) self . queue_redraw . _wait ( ) qt_app = QtCore . QCoreApplication . instance ( ) sleep = 10 while not self . _plot_event . is_set ( ) : logger . debug ( \"waiting for last plot to finish\" ) qt_app . processEvents ( ) QtTest . QTest . qSleep ( sleep ) logger . debug ( \"waiting for plot finished\" )", "nl": "Used for unittesting to make sure the plots are all done"}}
{"translation": {"code": "def concat ( dfs ) : ds = reduce ( ( lambda x , y : x . concat ( y ) ) , dfs ) return ds", "nl": "Concatenate a list of DataFrames ."}}
{"translation": {"code": "def arrow_table_from_vaex_df ( ds , column_names = None , selection = None , strings = True , virtual = False ) : names = [ ] arrays = [ ] for name , array in ds . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : names . append ( name ) arrays . append ( arrow_array_from_numpy_array ( array ) ) return pyarrow . Table . from_arrays ( arrays , names )", "nl": "Implementation of Dataset . to_arrow_table"}}
{"translation": {"code": "def selection_can_redo ( self , name = \"default\" ) : return ( self . selection_history_indices [ name ] + 1 ) < len ( self . selection_histories [ name ] )", "nl": "Can selection name be redone?"}}
{"translation": {"code": "def selection_redo ( self , name = \"default\" , executor = None ) : logger . debug ( \"redo\" ) executor = executor or self . executor assert self . selection_can_redo ( name = name ) selection_history = self . selection_histories [ name ] index = self . selection_history_indices [ name ] next = selection_history [ index + 1 ] self . selection_history_indices [ name ] += 1 self . signal_selection_changed . emit ( self ) logger . debug ( \"redo: selection history is %r, index is %r\" , selection_history , index )", "nl": "Redo selection for the name ."}}
{"translation": {"code": "def selection_undo ( self , name = \"default\" , executor = None ) : logger . debug ( \"undo\" ) executor = executor or self . executor assert self . selection_can_undo ( name = name ) selection_history = self . selection_histories [ name ] index = self . selection_history_indices [ name ] self . selection_history_indices [ name ] -= 1 self . signal_selection_changed . emit ( self ) logger . debug ( \"undo: selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] )", "nl": "Undo selection for the name ."}}
{"translation": {"code": "def materialize ( self , virtual_column , inplace = False ) : df = self . trim ( inplace = inplace ) virtual_column = _ensure_string_from_expression ( virtual_column ) if virtual_column not in df . virtual_columns : raise KeyError ( 'Virtual column not found: %r' % virtual_column ) ar = df . evaluate ( virtual_column , filtered = False ) del df [ virtual_column ] df . add_column ( virtual_column , ar ) return df", "nl": "Returns a new DataFrame where the virtual column is turned into an in memory numpy array ."}}
{"translation": {"code": "def sort ( self , by , ascending = True , kind = 'quicksort' ) : self = self . trim ( ) values = self . evaluate ( by , filtered = False ) indices = np . argsort ( values , kind = kind ) if not ascending : indices = indices [ : : - 1 ] . copy ( ) # this may be used a lot, so copy for performance return self . take ( indices )", "nl": "Return a sorted DataFrame sorted by the expression by"}}
{"translation": {"code": "def split_random ( self , frac , random_state = None ) : self = self . extract ( ) if type ( random_state ) == int or random_state is None : random_state = np . random . RandomState ( seed = random_state ) indices = random_state . choice ( len ( self ) , len ( self ) , replace = False ) return self . take ( indices ) . split ( frac )", "nl": "Returns a list containing random portions of the DataFrame ."}}
{"translation": {"code": "def sample ( self , n = None , frac = None , replace = False , weights = None , random_state = None ) : self = self . extract ( ) if type ( random_state ) == int or random_state is None : random_state = np . random . RandomState ( seed = random_state ) if n is None and frac is None : n = 1 elif frac is not None : n = int ( round ( frac * len ( self ) ) ) weights_values = None if weights is not None : weights_values = self . evaluate ( weights ) weights_values = weights_values / self . sum ( weights ) indices = random_state . choice ( len ( self ) , n , replace = replace , p = weights_values ) return self . take ( indices )", "nl": "Returns a DataFrame with a random set of rows"}}
{"translation": {"code": "def head_and_tail_print ( self , n = 5 ) : from IPython import display display . display ( display . HTML ( self . _head_and_tail_table ( n ) ) )", "nl": "Display the first and last n elements of a DataFrame ."}}
{"translation": {"code": "def take ( self , indices ) : df = self . copy ( ) # if the columns in ds already have a ColumnIndex # we could do, direct_indices = df.column['bla'].indices[indices] # which should be shared among multiple ColumnIndex'es, so we store # them in this dict direct_indices_map = { } indices = np . array ( indices ) for name in df : column = df . columns . get ( name ) if column is not None : # we optimize this somewhere, so we don't do multiple # levels of indirection if isinstance ( column , ColumnIndexed ) : # TODO: think about what happpens when the indices are masked.. ? if id ( column . indices ) not in direct_indices_map : direct_indices = column . indices [ indices ] direct_indices_map [ id ( column . indices ) ] = direct_indices else : direct_indices = direct_indices_map [ id ( column . indices ) ] df . columns [ name ] = ColumnIndexed ( column . df , direct_indices , column . name ) else : df . columns [ name ] = ColumnIndexed ( self , indices , name ) df . _length_original = len ( indices ) df . _length_unfiltered = df . _length_original df . set_selection ( None , name = FILTER_SELECTION_NAME ) return df", "nl": "Returns a DataFrame containing only rows indexed by indices"}}
{"translation": {"code": "def trim ( self , inplace = False ) : df = self if inplace else self . copy ( ) for name in df : column = df . columns . get ( name ) if column is not None : if self . _index_start == 0 and len ( column ) == self . _index_end : pass # we already assigned it in .copy else : if isinstance ( column , np . ndarray ) : # real array df . columns [ name ] = column [ self . _index_start : self . _index_end ] else : df . columns [ name ] = column . trim ( self . _index_start , self . _index_end ) df . _length_original = self . length_unfiltered ( ) df . _length_unfiltered = df . _length_original df . _index_start = 0 df . _index_end = df . _length_original df . _active_fraction = 1 return df", "nl": "Return a DataFrame where all columns are trimmed by the active range ."}}
{"translation": {"code": "def get_column_names ( self , virtual = True , strings = True , hidden = False , regex = None ) : def column_filter ( name ) : '''Return True if column with specified name should be returned''' if regex and not re . match ( regex , name ) : return False if not virtual and name in self . virtual_columns : return False if not strings and ( self . dtype ( name ) == str_type or self . dtype ( name ) . type == np . string_ ) : return False if not hidden and name . startswith ( '__' ) : return False return True return [ name for name in self . column_names if column_filter ( name ) ]", "nl": "Return a list of column names"}}
{"translation": {"code": "def set_current_row ( self , value ) : if ( value is not None ) and ( ( value < 0 ) or ( value >= len ( self ) ) ) : raise IndexError ( \"index %d out of range [0,%d]\" % ( value , len ( self ) ) ) self . _current_row = value self . signal_pick . emit ( self , value )", "nl": "Set the current row and emit the signal signal_pick ."}}
{"translation": {"code": "def cat ( self , i1 , i2 , format = 'html' ) : from IPython import display if format == 'html' : output = self . _as_html_table ( i1 , i2 ) display . display ( display . HTML ( output ) ) else : output = self . _as_table ( i1 , i2 , format = format ) print ( output )", "nl": "Display the DataFrame from row i1 till i2"}}
{"translation": {"code": "def select ( self , boolean_expression , mode = \"replace\" , name = \"default\" , executor = None ) : boolean_expression = _ensure_string_from_expression ( boolean_expression ) if boolean_expression is None and not self . has_selection ( name = name ) : pass # we don't want to pollute the history with many None selections self . signal_selection_changed . emit ( self ) # TODO: unittest want to know, does this make sense? else : def create ( current ) : return selections . SelectionExpression ( boolean_expression , current , mode ) if boolean_expression else None self . _selection ( create , name )", "nl": "Perform a selection defined by the boolean expression and combined with the previous selection using the given mode ."}}
{"translation": {"code": "def tail ( self , n = 10 ) : N = len ( self ) # self.cat(i1=max(0, N-n), i2=min(len(self), N)) return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ]", "nl": "Return a shallow copy a DataFrame with the last n rows ."}}
{"translation": {"code": "def extract ( self ) : trimmed = self . trim ( ) if trimmed . filtered : indices = trimmed . _filtered_range_to_unfiltered_indices ( 0 , len ( trimmed ) ) return trimmed . take ( indices ) else : return trimmed", "nl": "Return a DataFrame containing only the filtered rows ."}}
{"translation": {"code": "def select_non_missing ( self , drop_nan = True , drop_masked = True , column_names = None , mode = \"replace\" , name = \"default\" ) : column_names = column_names or self . get_column_names ( virtual = False ) def create ( current ) : return selections . SelectionDropNa ( drop_nan , drop_masked , column_names , current , mode ) self . _selection ( create , name )", "nl": "Create a selection that selects rows having non missing values for all columns in column_names ."}}
{"translation": {"code": "def select_inverse ( self , name = \"default\" , executor = None ) : def create ( current ) : return selections . SelectionInvert ( current ) self . _selection ( create , name , executor = executor )", "nl": "Invert the selection i . e . what is selected will not be and vice versa"}}
{"translation": {"code": "def select_rectangle ( self , x , y , limits , mode = \"replace\" , name = \"default\" ) : self . select_box ( [ x , y ] , limits , mode = mode , name = name )", "nl": "Select a 2d rectangular box in the space given by x and y bounds by limits ."}}
{"translation": {"code": "def add_column ( self , name , data ) : # assert _is_array_type_ok(data), \"dtype not supported: %r, %r\" % (data.dtype, data.dtype.type) # self._length = len(data) # if self._length_unfiltered is None: #     self._length_unfiltered = len(data) #     self._length_original = len(data) #     self._index_end = self._length_unfiltered super ( DataFrameArrays , self ) . add_column ( name , data ) self . _length_unfiltered = int ( round ( self . _length_original * self . _active_fraction ) )", "nl": "Add a column to the DataFrame"}}
{"translation": {"code": "def export_hdf5 ( self , path , column_names = None , byteorder = \"=\" , shuffle = False , selection = False , progress = None , virtual = False , sort = None , ascending = True ) : import vaex . export vaex . export . export_hdf5 ( self , path , column_names , byteorder , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending )", "nl": "Exports the DataFrame to a vaex hdf5 file"}}
{"translation": {"code": "def concat ( self , other ) : dfs = [ ] if isinstance ( self , DataFrameConcatenated ) : dfs . extend ( self . dfs ) else : dfs . extend ( [ self ] ) if isinstance ( other , DataFrameConcatenated ) : dfs . extend ( other . dfs ) else : dfs . extend ( [ other ] ) return DataFrameConcatenated ( dfs )", "nl": "Concatenates two DataFrames adding the rows of one the other DataFrame to the current returned in a new DataFrame ."}}
{"translation": {"code": "def _hstack ( self , other , prefix = None ) : assert len ( self ) == len ( other ) , \"does not make sense to horizontally stack DataFrames with different lengths\" for name in other . get_column_names ( ) : if prefix : new_name = prefix + name else : new_name = name self . add_column ( new_name , other . columns [ name ] )", "nl": "Join the columns of the other DataFrame to this one assuming the ordering is the same"}}
{"translation": {"code": "def length ( self , selection = False ) : if selection : return 0 if self . mask is None else np . sum ( self . mask ) else : return len ( self )", "nl": "Get the length of the DataFrames for the selection of the whole DataFrame ."}}
{"translation": {"code": "def data ( self ) : class Datas ( object ) : pass datas = Datas ( ) for name , array in self . columns . items ( ) : setattr ( datas , name , array ) return datas", "nl": "Gives direct access to the data as numpy arrays ."}}
{"translation": {"code": "def ordinal_encode ( self , column , values = None , inplace = False ) : column = _ensure_string_from_expression ( column ) df = self if inplace else self . copy ( ) # for the codes, we need to work on the unfiltered dataset, since the filter # may change, and we also cannot add an array that is smaller in length df_unfiltered = df . copy ( ) # maybe we need some filter manipulation methods df_unfiltered . select_nothing ( name = FILTER_SELECTION_NAME ) df_unfiltered . _length_unfiltered = df . _length_original df_unfiltered . set_active_range ( 0 , df . _length_original ) # codes point to the index of found_values # meaning: found_values[codes[0]] == ds[column].values[0] found_values , codes = df_unfiltered . unique ( column , return_inverse = True ) if values is None : values = found_values else : # we have specified which values we should support, anything # not found will be masked translation = np . zeros ( len ( found_values ) , dtype = np . uint64 ) # mark values that are in the column, but not in values with a special value missing_value = len ( found_values ) for i , found_value in enumerate ( found_values ) : try : found_value = found_value . decode ( 'ascii' ) except : pass if found_value not in values : # not present, we need a missing value translation [ i ] = missing_value else : translation [ i ] = values . index ( found_value ) codes = translation [ codes ] if missing_value in translation : # all special values will be marked as missing codes = np . ma . masked_array ( codes , codes == missing_value ) original_column = df . rename_column ( column , '__original_' + column , unique = True ) labels = [ str ( k ) for k in values ] df . add_column ( column , codes ) df . _categories [ column ] = dict ( labels = labels , N = len ( values ) , values = values ) return df", "nl": "Encode column as ordinal values and mark it as categorical ."}}
{"translation": {"code": "def dropna ( self , drop_nan = True , drop_masked = True , column_names = None ) : copy = self . copy ( ) copy . select_non_missing ( drop_nan = drop_nan , drop_masked = drop_masked , column_names = column_names , name = FILTER_SELECTION_NAME , mode = 'and' ) return copy", "nl": "Create a shallow copy of a DataFrame with filtering set using select_non_missing ."}}
{"translation": {"code": "def categorize ( self , column , labels = None , check = True ) : column = _ensure_string_from_expression ( column ) if check : vmin , vmax = self . minmax ( column ) if labels is None : N = int ( vmax + 1 ) labels = list ( map ( str , range ( N ) ) ) if ( vmax - vmin ) >= len ( labels ) : raise ValueError ( 'value of {} found, which is larger than number of labels {}' . format ( vmax , len ( labels ) ) ) self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) )", "nl": "Mark column as categorical with given labels assuming zero indexing"}}
{"translation": {"code": "def _selection ( self , create_selection , name , executor = None , execute_fully = False ) : selection_history = self . selection_histories [ name ] previous_index = self . selection_history_indices [ name ] current = selection_history [ previous_index ] if selection_history else None selection = create_selection ( current ) executor = executor or self . executor selection_history . append ( selection ) self . selection_history_indices [ name ] += 1 # clip any redo history del selection_history [ self . selection_history_indices [ name ] : - 1 ] if 0 : if self . is_local ( ) : if selection : # result = selection.execute(executor=executor, execute_fully=execute_fully) result = vaex . promise . Promise . fulfilled ( None ) self . signal_selection_changed . emit ( self ) else : result = vaex . promise . Promise . fulfilled ( None ) self . signal_selection_changed . emit ( self ) else : self . signal_selection_changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) self . signal_selection_changed . emit ( self ) result = vaex . promise . Promise . fulfilled ( None ) logger . debug ( \"select selection history is %r, index is %r\" , selection_history , self . selection_history_indices [ name ] ) return result", "nl": "select_lasso and select almost share the same code"}}
{"translation": {"code": "def set_selection ( self , selection , name = \"default\" , executor = None ) : def create ( current ) : return selection self . _selection ( create , name , executor = executor , execute_fully = True )", "nl": "Sets the selection object"}}
{"translation": {"code": "def delete_variable ( self , name ) : del self . variables [ name ] self . signal_variable_changed . emit ( self , name , \"delete\" )", "nl": "Deletes a variable from a DataFrame ."}}
{"translation": {"code": "def select_lasso ( self , expression_x , expression_y , xsequence , ysequence , mode = \"replace\" , name = \"default\" , executor = None ) : def create ( current ) : return selections . SelectionLasso ( expression_x , expression_y , xsequence , ysequence , current , mode ) self . _selection ( create , name , executor = executor )", "nl": "For performance reasons a lasso selection is handled differently ."}}
{"translation": {"code": "def select_ellipse ( self , x , y , xc , yc , width , height , angle = 0 , mode = \"replace\" , name = \"default\" , radians = False , inclusive = True ) : # Computing the properties of the ellipse prior to selection if radians : pass else : alpha = np . deg2rad ( angle ) xr = width / 2 yr = height / 2 r = max ( xr , yr ) a = xr / r b = yr / r expr = \"(({x}-{xc})*cos({alpha})+({y}-{yc})*sin({alpha}))**2/{a}**2 + (({x}-{xc})*sin({alpha})-({y}-{yc})*cos({alpha}))**2/{b}**2 <= {r}**2\" . format ( * * locals ( ) ) if inclusive : expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 <= r ** 2 else : expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 < r ** 2 self . select ( boolean_expression = expr , mode = mode , name = name )", "nl": "Select an elliptical region centred on xc yc with a certain width height and angle ."}}
{"translation": {"code": "def select_circle ( self , x , y , xc , yc , r , mode = \"replace\" , name = \"default\" , inclusive = True ) : # expr = \"({x}-{xc})**2 + ({y}-{yc})**2 <={r}**2\".format(**locals()) if inclusive : expr = ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 <= r ** 2 else : expr = ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 < r ** 2 self . select ( boolean_expression = expr , mode = mode , name = name )", "nl": "Select a circular region centred on xc yc with a radius of r ."}}
{"translation": {"code": "def select_box ( self , spaces , limits , mode = \"replace\" , name = \"default\" ) : sorted_limits = [ ( min ( l ) , max ( l ) ) for l in limits ] expressions = [ \"((%s) >= %f) & ((%s) <= %f)\" % ( expression , lmin , expression , lmax ) for ( expression , ( lmin , lmax ) ) in zip ( spaces , sorted_limits ) ] self . select ( \"&\" . join ( expressions ) , mode = mode , name = name )", "nl": "Select a n - dimensional rectangular box bounded by limits ."}}
{"translation": {"code": "def _find_valid_name ( self , initial_name ) : return vaex . utils . find_valid_name ( initial_name , used = self . get_column_names ( hidden = True ) )", "nl": "Finds a non - colliding name by optional postfixing"}}
{"translation": {"code": "def add_variable ( self , name , expression , overwrite = True , unique = True ) : if unique or overwrite or name not in self . variables : existing_names = self . get_column_names ( virtual = False ) + list ( self . variables . keys ( ) ) name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else existing_names ) self . variables [ name ] = expression self . signal_variable_changed . emit ( self , name , \"add\" ) if unique : return name", "nl": "Add a variable to to a DataFrame ."}}
{"translation": {"code": "def split ( self , frac ) : self = self . extract ( ) if _issequence ( frac ) : # make sure it is normalized total = sum ( frac ) frac = [ k / total for k in frac ] else : assert frac <= 1 , \"fraction should be <= 1\" frac = [ frac , 1 - frac ] offsets = np . round ( np . cumsum ( frac ) * len ( self ) ) . astype ( np . int64 ) start = 0 for offset in offsets : yield self [ start : offset ] start = offset", "nl": "Returns a list containing ordered subsets of the DataFrame ."}}
{"translation": {"code": "def add_virtual_column ( self , name , expression , unique = False ) : type = \"change\" if name in self . virtual_columns else \"add\" expression = _ensure_string_from_expression ( expression ) if name in self . get_column_names ( virtual = False ) : renamed = '__' + vaex . utils . find_valid_name ( name , used = self . get_column_names ( ) ) expression = self . _rename ( name , renamed , expression ) [ 0 ] . expression name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else self . get_column_names ( ) ) self . virtual_columns [ name ] = expression self . column_names . append ( name ) self . _save_assign_expression ( name ) self . signal_column_changed . emit ( self , name , \"add\" )", "nl": "Add a virtual column to the DataFrame ."}}
{"translation": {"code": "def remove_virtual_meta ( self ) : dir = self . get_private_dir ( create = True ) path = os . path . join ( dir , \"virtual_meta.yaml\" ) try : if os . path . exists ( path ) : os . remove ( path ) if not os . listdir ( dir ) : os . rmdir ( dir ) except : logger . exception ( \"error while trying to remove %s or %s\" , path , dir )", "nl": "Removes the file with the virtual column etc it does not change the current virtual columns etc ."}}
{"translation": {"code": "def state_set ( self , state , use_active_range = False ) : self . description = state [ 'description' ] if use_active_range : self . _index_start , self . _index_end = state [ 'active_range' ] self . _length_unfiltered = self . _index_end - self . _index_start if 'renamed_columns' in state : for old , new in state [ 'renamed_columns' ] : self . _rename ( old , new ) for name , value in state [ 'functions' ] . items ( ) : self . add_function ( name , vaex . serialize . from_dict ( value ) ) if 'column_names' in state : # we clear all columns, and add them later on, since otherwise self[name] = ... will try # to rename the columns (which is unsupported for remote dfs) self . column_names = [ ] self . virtual_columns = collections . OrderedDict ( ) for name , value in state [ 'virtual_columns' ] . items ( ) : self [ name ] = self . _expr ( value ) # self._save_assign_expression(name) self . column_names = state [ 'column_names' ] else : # old behaviour self . virtual_columns = collections . OrderedDict ( ) for name , value in state [ 'virtual_columns' ] . items ( ) : self [ name ] = self . _expr ( value ) self . variables = state [ 'variables' ] import astropy # TODO: make this dep optional? units = { key : astropy . units . Unit ( value ) for key , value in state [ \"units\" ] . items ( ) } self . units . update ( units ) for name , selection_dict in state [ 'selections' ] . items ( ) : # TODO: make selection use the vaex.serialize framework if selection_dict is None : selection = None else : selection = selections . selection_from_dict ( selection_dict ) self . set_selection ( selection , name = name )", "nl": "Sets the internal state of the df"}}
{"translation": {"code": "def state_get ( self ) : virtual_names = list ( self . virtual_columns . keys ( ) ) + list ( self . variables . keys ( ) ) units = { key : str ( value ) for key , value in self . units . items ( ) } ucds = { key : value for key , value in self . ucds . items ( ) if key in virtual_names } descriptions = { key : value for key , value in self . descriptions . items ( ) } import vaex . serialize def check ( key , value ) : if not vaex . serialize . can_serialize ( value . f ) : warnings . warn ( 'Cannot serialize function for virtual column {} (use vaex.serialize.register)' . format ( key ) ) return False return True def clean ( value ) : return vaex . serialize . to_dict ( value . f ) functions = { key : clean ( value ) for key , value in self . functions . items ( ) if check ( key , value ) } virtual_columns = { key : value for key , value in self . virtual_columns . items ( ) } selections = { name : self . get_selection ( name ) for name , history in self . selection_histories . items ( ) } selections = { name : selection . to_dict ( ) if selection is not None else None for name , selection in selections . items ( ) } # if selection is not None} state = dict ( virtual_columns = virtual_columns , column_names = self . column_names , renamed_columns = self . _renamed_columns , variables = self . variables , functions = functions , selections = selections , ucds = ucds , units = units , descriptions = descriptions , description = self . description , active_range = [ self . _index_start , self . _index_end ] ) return state", "nl": "Return the internal state of the DataFrame in a dictionary"}}
{"translation": {"code": "def get_private_dir ( self , create = False ) : if self . is_local ( ) : name = os . path . abspath ( self . path ) . replace ( os . path . sep , \"_\" ) [ : 250 ] # should not be too long for most os'es name = name . replace ( \":\" , \"_\" ) # for windows drive names else : server = self . server name = \"%s_%s_%s_%s\" % ( server . hostname , server . port , server . base_path . replace ( \"/\" , \"_\" ) , self . name ) dir = os . path . join ( vaex . utils . get_private_dir ( ) , \"dfs\" , name ) if create and not os . path . exists ( dir ) : os . makedirs ( dir ) return dir", "nl": "Each DataFrame has a directory where files are stored for metadata etc ."}}
{"translation": {"code": "def dtype ( self , expression , internal = False ) : expression = _ensure_string_from_expression ( expression ) if expression in self . variables : return np . float64 ( 1 ) . dtype elif expression in self . columns . keys ( ) : column = self . columns [ expression ] data = column [ 0 : 1 ] dtype = data . dtype else : data = self . evaluate ( expression , 0 , 1 , filtered = False ) dtype = data . dtype if not internal : if dtype != str_type : if dtype . kind in 'US' : return str_type if dtype . kind == 'O' : # we lie about arrays containing strings if isinstance ( data [ 0 ] , six . string_types ) : return str_type return dtype", "nl": "Return the numpy dtype for the given expression if not a column the first row will be evaluated to get the dtype ."}}
{"translation": {"code": "def plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = \"count(*)\" , shape = 128 , selection = [ None , True ] , f = None , vcount_limits = None , smooth_pre = None , smooth_post = None , grid_limits = None , normalize = \"normalize\" , colormap = \"afmhot\" , figure_key = None , fig = None , lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level_width = 0.1 , show = True , * * kwargs ) : import vaex . ext . ipyvolume # vaex.ext.ipyvolume. cls = vaex . ext . ipyvolume . PlotDefault plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , smooth_pre = smooth_pre , smooth_post = smooth_post , grid_limits = grid_limits , vcount_limits = vcount_limits , normalize = normalize , colormap = colormap , * * kwargs ) if show : plot3d . show ( ) return plot3d", "nl": "Use at own risk requires ipyvolume"}}
{"translation": {"code": "def healpix_plot ( self , healpix_expression = \"source_id/34359738368\" , healpix_max_level = 12 , healpix_level = 8 , what = \"count(*)\" , selection = None , grid = None , healpix_input = \"equatorial\" , healpix_output = \"galactic\" , f = None , colormap = \"afmhot\" , grid_limits = None , image_size = 800 , nest = True , figsize = None , interactive = False , title = \"\" , smooth = None , show = False , colorbar = True , rotation = ( 0 , 0 , 0 ) , * * kwargs ) : # plot_level = healpix_level #healpix_max_level-reduce_level import healpy as hp import pylab as plt if grid is None : reduce_level = healpix_max_level - healpix_level NSIDE = 2 ** healpix_level nmax = hp . nside2npix ( NSIDE ) # print nmax, np.sqrt(nmax) scaling = 4 ** reduce_level # print nmax epsilon = 1. / scaling / 2 grid = self . _stat ( what = what , binby = \"%s/%s\" % ( healpix_expression , scaling ) , limits = [ - epsilon , nmax - epsilon ] , shape = nmax , selection = selection ) if grid_limits : grid_min , grid_max = grid_limits else : grid_min = grid_max = None f_org = f f = _parse_f ( f ) if smooth : if nest : grid = hp . reorder ( grid , inp = \"NEST\" , out = \"RING\" ) nest = False # grid[np.isnan(grid)] = np.nanmean(grid) grid = hp . smoothing ( grid , sigma = np . radians ( smooth ) ) fgrid = f ( grid ) coord_map = dict ( equatorial = 'C' , galactic = 'G' , ecliptic = \"E\" ) fig = plt . gcf ( ) if figsize is not None : fig . set_size_inches ( * figsize ) what_label = what if f_org : what_label = f_org + \" \" + what_label f = hp . mollzoom if interactive else hp . mollview with warnings . catch_warnings ( ) : warnings . simplefilter ( \"ignore\" ) coord = coord_map [ healpix_input ] , coord_map [ healpix_output ] if coord_map [ healpix_input ] == coord_map [ healpix_output ] : coord = None f ( fgrid , unit = what_label , rot = rotation , nest = nest , title = title , coord = coord , cmap = colormap , hold = True , xsize = image_size , min = grid_min , max = grid_max , cbar = colorbar , * * kwargs ) if show : plt . show ( )", "nl": "Viz data in 2d using a healpix column ."}}
{"translation": {"code": "def healpix_count ( self , expression = None , healpix_expression = None , healpix_max_level = 12 , healpix_level = 8 , binby = None , limits = None , shape = default_shape , delay = False , progress = None , selection = None ) : # if binby is None: import healpy as hp if healpix_expression is None : if self . ucds . get ( \"source_id\" , None ) == 'meta.id;meta.main' : # we now assume we have gaia data healpix_expression = \"source_id/34359738368\" if healpix_expression is None : raise ValueError ( \"no healpix_expression given, and was unable to guess\" ) reduce_level = healpix_max_level - healpix_level NSIDE = 2 ** healpix_level nmax = hp . nside2npix ( NSIDE ) scaling = 4 ** reduce_level expr = \"%s/%s\" % ( healpix_expression , scaling ) binby = [ expr ] + ( [ ] if binby is None else _ensure_list ( binby ) ) shape = ( nmax , ) + _expand_shape ( shape , len ( binby ) - 1 ) epsilon = 1. / scaling / 2 limits = [ [ - epsilon , nmax - epsilon ] ] + ( [ ] if limits is None else limits ) return self . count ( expression , binby = binby , limits = limits , shape = shape , delay = delay , progress = progress , selection = selection )", "nl": "Count non missing value for expression on an array which represents healpix data ."}}
{"translation": {"code": "def plot_widget ( self , x , y , z = None , grid = None , shape = 256 , limits = None , what = \"count(*)\" , figsize = None , f = \"identity\" , figure_key = None , fig = None , axes = None , xlabel = None , ylabel = None , title = None , show = True , selection = [ None , True ] , colormap = \"afmhot\" , grid_limits = None , normalize = \"normalize\" , grid_before = None , what_kwargs = { } , type = \"default\" , scales = None , tool_select = False , bq_cleanup = True , backend = \"bqplot\" , * * kwargs ) : import vaex . jupyter . plot backend = vaex . jupyter . plot . create_backend ( backend ) cls = vaex . jupyter . plot . get_type ( type ) x = _ensure_strings_from_expressions ( x ) y = _ensure_strings_from_expressions ( y ) z = _ensure_strings_from_expressions ( z ) for name in 'vx vy vz' . split ( ) : if name in kwargs : kwargs [ name ] = _ensure_strings_from_expressions ( kwargs [ name ] ) plot2d = cls ( backend = backend , dataset = self , x = x , y = y , z = z , grid = grid , shape = shape , limits = limits , what = what , f = f , figure_key = figure_key , fig = fig , selection = selection , grid_before = grid_before , grid_limits = grid_limits , normalize = normalize , colormap = colormap , what_kwargs = what_kwargs , * * kwargs ) if show : plot2d . show ( ) return plot2d", "nl": "Viz 1d 2d or 3d in a Jupyter notebook"}}
{"translation": {"code": "def median_approx ( self , expression , percentage = 50. , binby = [ ] , limits = None , shape = default_shape , percentile_shape = 256 , percentile_limits = \"minmax\" , selection = False , delay = False ) : return self . percentile_approx ( expression , 50 , binby = binby , limits = limits , shape = shape , percentile_shape = percentile_shape , percentile_limits = percentile_limits , selection = selection , delay = delay )", "nl": "Calculate the median possibly on a grid defined by binby ."}}
{"translation": {"code": "def min ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : return self . _compute_agg ( 'min' , expression , binby , limits , shape , selection , delay , edges , progress ) @ delayed def finish ( result ) : return result [ ... , 0 ] return self . _delay ( delay , finish ( self . minmax ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = delay , progress = progress ) ) )", "nl": "Calculate the minimum for given expressions possibly on a grid defined by binby ."}}
{"translation": {"code": "def minmax ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : # vmin  = self._compute_agg('min', expression, binby, limits, shape, selection, delay, edges, progress) # vmax =  self._compute_agg('max', expression, binby, limits, shape, selection, delay, edges, progress) @ delayed def finish ( * minmax_list ) : value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) value = value . astype ( dtype0 ) return value @ delayed def calculate ( expression , limits ) : task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_MIN_MAX , selection = selection ) self . executor . schedule ( task ) progressbar . add_task ( task , \"minmax for %s\" % expression ) return task @ delayed def finish ( * minmax_list ) : value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) value = value . astype ( dtype0 ) return value expression = _ensure_strings_from_expressions ( expression ) binby = _ensure_strings_from_expressions ( binby ) waslist , [ expressions , ] = vaex . utils . listify ( expression ) dtypes = [ self . dtype ( expr ) for expr in expressions ] dtype0 = dtypes [ 0 ] if not all ( [ k . kind == dtype0 . kind for k in dtypes ] ) : raise ValueError ( \"cannot mix datetime and non-datetime expressions\" ) progressbar = vaex . utils . progressbars ( progress , name = \"minmaxes\" ) limits = self . limits ( binby , limits , selection = selection , delay = True ) all_tasks = [ calculate ( expression , limits ) for expression in expressions ] result = finish ( * all_tasks ) return self . _delay ( delay , result )", "nl": "Calculate the minimum and maximum for expressions possibly on a grid defined by binby ."}}
{"translation": {"code": "def cov ( self , x , y = None , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : selection = _ensure_strings_from_expressions ( selection ) if y is None : if not _issequence ( x ) : raise ValueError ( \"if y argument is not given, x is expected to be sequence, not %r\" , x ) expressions = x else : expressions = [ x , y ] N = len ( expressions ) binby = _ensure_list ( binby ) shape = _expand_shape ( shape , len ( binby ) ) progressbar = vaex . utils . progressbars ( progress ) limits = self . limits ( binby , limits , selection = selection , delay = True ) @ delayed def calculate ( expressions , limits ) : # print('limits', limits) task = tasks . TaskStatistic ( self , binby , shape , limits , weights = expressions , op = tasks . OP_COV , selection = selection ) self . executor . schedule ( task ) progressbar . add_task ( task , \"covariance values for %r\" % expressions ) return task @ delayed def finish ( values ) : N = len ( expressions ) counts = values [ ... , : N ] sums = values [ ... , N : 2 * N ] with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : means = sums / counts # matrix of means * means.T meansxy = means [ ... , None ] * means [ ... , None , : ] counts = values [ ... , 2 * N : 2 * N + N ** 2 ] sums = values [ ... , 2 * N + N ** 2 : ] shape = counts . shape [ : - 1 ] + ( N , N ) counts = counts . reshape ( shape ) sums = sums . reshape ( shape ) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : moments2 = sums / counts cov_matrix = moments2 - meansxy return cov_matrix progressbar = vaex . utils . progressbars ( progress ) values = calculate ( expressions , limits ) cov_matrix = finish ( values ) return self . _delay ( delay , cov_matrix )", "nl": "Calculate the covariance matrix for x and y or more expressions possibly on a grid defined by binby ."}}
{"translation": {"code": "def std ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : @ delayed def finish ( var ) : return var ** 0.5 return self . _delay ( delay , finish ( self . var ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = True , progress = progress ) ) )", "nl": "Calculate the standard deviation for the given expression possible on a grid defined by binby"}}
{"translation": {"code": "def sum ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : return self . _compute_agg ( 'sum' , expression , binby , limits , shape , selection , delay , edges , progress ) @ delayed def finish ( * sums ) : return vaex . utils . unlistify ( waslist , sums ) expression = _ensure_strings_from_expressions ( expression ) binby = _ensure_strings_from_expressions ( binby ) waslist , [ expressions , ] = vaex . utils . listify ( expression ) progressbar = vaex . utils . progressbars ( progress ) limits = self . limits ( binby , limits , delay = True ) # stats = [calculate(expression, limits) for expression in expressions] sums = [ self . _sum_calculation ( expression , binby = binby , limits = limits , shape = shape , selection = selection , progressbar = progressbar ) for expression in expressions ] s = finish ( * sums ) return self . _delay ( delay , s )", "nl": "Calculate the sum for the given expression possible on a grid defined by binby"}}
{"translation": {"code": "def mean ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : return self . _compute_agg ( 'mean' , expression , binby , limits , shape , selection , delay , edges , progress ) logger . debug ( \"mean of %r, with binby=%r, limits=%r, shape=%r, selection=%r, delay=%r\" , expression , binby , limits , shape , selection , delay ) expression = _ensure_strings_from_expressions ( expression ) selection = _ensure_strings_from_expressions ( selection ) binby = _ensure_strings_from_expressions ( binby ) @ delayed def calculate ( expression , limits ) : task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_ADD_WEIGHT_MOMENTS_01 , selection = selection ) self . executor . schedule ( task ) progressbar . add_task ( task , \"mean for %s\" % expression ) return task @ delayed def finish ( * stats_args ) : stats = np . array ( stats_args ) counts = stats [ ... , 0 ] with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : mean = stats [ ... , 1 ] / counts return vaex . utils . unlistify ( waslist , mean ) waslist , [ expressions , ] = vaex . utils . listify ( expression ) progressbar = vaex . utils . progressbars ( progress ) limits = self . limits ( binby , limits , delay = True ) stats = [ calculate ( expression , limits ) for expression in expressions ] var = finish ( * stats ) return self . _delay ( delay , var )", "nl": "Calculate the mean for expression possibly on a grid defined by binby ."}}
{"translation": {"code": "def delete_virtual_column ( self , name ) : del self . virtual_columns [ name ] self . signal_column_changed . emit ( self , name , \"delete\" )", "nl": "Deletes a virtual column from a DataFrame ."}}
{"translation": {"code": "def write_virtual_meta ( self ) : path = os . path . join ( self . get_private_dir ( create = True ) , \"virtual_meta.yaml\" ) virtual_names = list ( self . virtual_columns . keys ( ) ) + list ( self . variables . keys ( ) ) units = { key : str ( value ) for key , value in self . units . items ( ) if key in virtual_names } ucds = { key : value for key , value in self . ucds . items ( ) if key in virtual_names } descriptions = { key : value for key , value in self . descriptions . items ( ) if key in virtual_names } meta_info = dict ( virtual_columns = self . virtual_columns , variables = self . variables , ucds = ucds , units = units , descriptions = descriptions ) vaex . utils . write_json_or_yaml ( path , meta_info )", "nl": "Writes virtual columns variables and their ucd description and units ."}}
{"translation": {"code": "def write_meta ( self ) : # raise NotImplementedError path = os . path . join ( self . get_private_dir ( create = True ) , \"meta.yaml\" ) units = { key : str ( value ) for key , value in self . units . items ( ) } meta_info = dict ( description = self . description , ucds = self . ucds , units = units , descriptions = self . descriptions , ) vaex . utils . write_json_or_yaml ( path , meta_info )", "nl": "Writes all meta data ucd description and units"}}
{"translation": {"code": "def first ( self , expression , order_expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , edges = False , progress = None ) : return self . _compute_agg ( 'first' , expression , binby , limits , shape , selection , delay , edges , progress , extra_expressions = [ order_expression ] ) logger . debug ( \"count(%r, binby=%r, limits=%r)\" , expression , binby , limits ) logger . debug ( \"count(%r, binby=%r, limits=%r)\" , expression , binby , limits ) expression = _ensure_strings_from_expressions ( expression ) order_expression = _ensure_string_from_expression ( order_expression ) binby = _ensure_strings_from_expressions ( binby ) waslist , [ expressions , ] = vaex . utils . listify ( expression ) @ delayed def finish ( * counts ) : counts = np . asarray ( counts ) return vaex . utils . unlistify ( waslist , counts ) progressbar = vaex . utils . progressbars ( progress ) limits = self . limits ( binby , limits , delay = True , shape = shape ) stats = [ self . _first_calculation ( expression , order_expression , binby = binby , limits = limits , shape = shape , selection = selection , edges = edges , progressbar = progressbar ) for expression in expressions ] var = finish ( * stats ) return self . _delay ( delay , var )", "nl": "Return the first element of a binned expression where the values each bin are sorted by order_expression ."}}
{"translation": {"code": "def set_variable ( self , name , expression_or_value , write = True ) : self . variables [ name ] = expression_or_value", "nl": "Set the variable to an expression or value defined by expression_or_value ."}}
{"translation": {"code": "def subspaces ( self , expressions_list = None , dimensions = None , exclude = None , * * kwargs ) : if dimensions is not None : expressions_list = list ( itertools . combinations ( self . get_column_names ( ) , dimensions ) ) if exclude is not None : import six def excluded ( expressions ) : if callable ( exclude ) : return exclude ( expressions ) elif isinstance ( exclude , six . string_types ) : return exclude in expressions elif isinstance ( exclude , ( list , tuple ) ) : # $#expressions = set(expressions) for e in exclude : if isinstance ( e , six . string_types ) : if e in expressions : return True elif isinstance ( e , ( list , tuple ) ) : if set ( e ) . issubset ( expressions ) : return True else : raise ValueError ( \"elements of exclude should contain a string or a sequence of strings\" ) else : raise ValueError ( \"exclude should contain a string, a sequence of strings, or should be a callable\" ) return False # test if any of the elements of exclude are a subset of the expression expressions_list = [ expr for expr in expressions_list if not excluded ( expr ) ] logger . debug ( \"expression list generated: %r\" , expressions_list ) import vaex . legacy return vaex . legacy . Subspaces ( [ self ( * expressions , * * kwargs ) for expressions in expressions_list ] )", "nl": "Generate a Subspaces object based on a custom list of expressions or all possible combinations based on dimension"}}
{"translation": {"code": "def add_virtual_columns_cartesian_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , alpha = \"l\" , delta = \"b\" , distance = \"distance\" , radians = False , center = None , center_name = \"solar_position\" ) : transform = \"\" if radians else \"*180./pi\" if center is not None : self . add_variable ( center_name , center ) if center is not None and center [ 0 ] != 0 : x = \"({x} - {center_name}[0])\" . format ( * * locals ( ) ) if center is not None and center [ 1 ] != 0 : y = \"({y} - {center_name}[1])\" . format ( * * locals ( ) ) if center is not None and center [ 2 ] != 0 : z = \"({z} - {center_name}[2])\" . format ( * * locals ( ) ) self . add_virtual_column ( distance , \"sqrt({x}**2 + {y}**2 + {z}**2)\" . format ( * * locals ( ) ) ) # self.add_virtual_column(alpha, \"((arctan2({y}, {x}) + 2*pi) % (2*pi)){transform}\".format(**locals())) self . add_virtual_column ( alpha , \"arctan2({y}, {x}){transform}\" . format ( * * locals ( ) ) ) self . add_virtual_column ( delta , \"(-arccos({z}/{distance})+pi/2){transform}\" . format ( * * locals ( ) ) )", "nl": "Convert cartesian to spherical coordinates ."}}
{"translation": {"code": "def add_virtual_columns_spherical_to_cartesian ( self , alpha , delta , distance , xname = \"x\" , yname = \"y\" , zname = \"z\" , propagate_uncertainties = False , center = [ 0 , 0 , 0 ] , center_name = \"solar_position\" , radians = False ) : alpha = self . _expr ( alpha ) delta = self . _expr ( delta ) distance = self . _expr ( distance ) if not radians : alpha = alpha * self . _expr ( 'pi' ) / 180 delta = delta * self . _expr ( 'pi' ) / 180 # TODO: use sth like .optimize by default to get rid of the +0 ? if center [ 0 ] : self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance + center [ 0 ] else : self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance if center [ 1 ] : self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance + center [ 1 ] else : self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance if center [ 2 ] : self [ zname ] = np . sin ( delta ) * distance + center [ 2 ] else : self [ zname ] = np . sin ( delta ) * distance if propagate_uncertainties : self . propagate_uncertainties ( [ self [ xname ] , self [ yname ] , self [ zname ] ] )", "nl": "Convert spherical to cartesian coordinates ."}}
{"translation": {"code": "def add_virtual_columns_rotation ( self , x , y , xnew , ynew , angle_degrees , propagate_uncertainties = False ) : x = _ensure_string_from_expression ( x ) y = _ensure_string_from_expression ( y ) theta = np . radians ( angle_degrees ) matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) ] , [ np . sin ( theta ) , np . cos ( theta ) ] ] ) m = matrix_name = x + \"_\" + y + \"_rot\" for i in range ( 2 ) : for j in range ( 2 ) : self . set_variable ( matrix_name + \"_%d%d\" % ( i , j ) , matrix [ i , j ] . item ( ) ) self [ xnew ] = self . _expr ( \"{m}_00 * {x} + {m}_01 * {y}\" . format ( * * locals ( ) ) ) self [ ynew ] = self . _expr ( \"{m}_10 * {x} + {m}_11 * {y}\" . format ( * * locals ( ) ) ) if propagate_uncertainties : self . propagate_uncertainties ( [ self [ xnew ] , self [ ynew ] ] )", "nl": "Rotation in 2d ."}}
{"translation": {"code": "def add_virtual_columns_polar_velocities_to_cartesian ( self , x = 'x' , y = 'y' , azimuth = None , vr = 'vr_polar' , vazimuth = 'vphi_polar' , vx_out = 'vx' , vy_out = 'vy' , propagate_uncertainties = False ) : x = self . _expr ( x ) y = self . _expr ( y ) vr = self . _expr ( vr ) vazimuth = self . _expr ( vazimuth ) if azimuth is not None : azimuth = self . _expr ( azimuth ) azimuth = np . deg2rad ( azimuth ) else : azimuth = np . arctan2 ( y , x ) azimuth = self . _expr ( azimuth ) self [ vx_out ] = vr * np . cos ( azimuth ) - vazimuth * np . sin ( azimuth ) self [ vy_out ] = vr * np . sin ( azimuth ) + vazimuth * np . cos ( azimuth ) if propagate_uncertainties : self . propagate_uncertainties ( [ self [ vx_out ] , self [ vy_out ] ] )", "nl": "Convert cylindrical polar velocities to Cartesian ."}}
{"translation": {"code": "def add_virtual_columns_cartesian_velocities_to_polar ( self , x = \"x\" , y = \"y\" , vx = \"vx\" , radius_polar = None , vy = \"vy\" , vr_out = \"vr_polar\" , vazimuth_out = \"vphi_polar\" , propagate_uncertainties = False , ) : x = self . _expr ( x ) y = self . _expr ( y ) vx = self . _expr ( vx ) vy = self . _expr ( vy ) if radius_polar is None : radius_polar = np . sqrt ( x ** 2 + y ** 2 ) radius_polar = self . _expr ( radius_polar ) self [ vr_out ] = ( x * vx + y * vy ) / radius_polar self [ vazimuth_out ] = ( x * vy - y * vx ) / radius_polar if propagate_uncertainties : self . propagate_uncertainties ( [ self [ vr_out ] , self [ vazimuth_out ] ] )", "nl": "Convert cartesian to polar velocities ."}}
{"translation": {"code": "def add_virtual_columns_cartesian_velocities_to_spherical ( self , x = \"x\" , y = \"y\" , z = \"z\" , vx = \"vx\" , vy = \"vy\" , vz = \"vz\" , vr = \"vr\" , vlong = \"vlong\" , vlat = \"vlat\" , distance = None ) : # see http://www.astrosurf.com/jephem/library/li110spherCart_en.htm if distance is None : distance = \"sqrt({x}**2+{y}**2+{z}**2)\" . format ( * * locals ( ) ) self . add_virtual_column ( vr , \"({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}\" . format ( * * locals ( ) ) ) self . add_virtual_column ( vlong , \"-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)\" . format ( * * locals ( ) ) ) self . add_virtual_column ( vlat , \"-({z}*({x}*{vx}+{y}*{vy}) - ({x}**2+{y}**2)*{vz})/( {distance}*sqrt({x}**2+{y}**2) )\" . format ( * * locals ( ) ) )", "nl": "Concert velocities from a cartesian to a spherical coordinate system"}}
{"translation": {"code": "def add_virtual_columns_cartesian_to_polar ( self , x = \"x\" , y = \"y\" , radius_out = \"r_polar\" , azimuth_out = \"phi_polar\" , propagate_uncertainties = False , radians = False ) : x = self [ x ] y = self [ y ] if radians : to_degrees = \"\" else : to_degrees = \"*180/pi\" r = np . sqrt ( x ** 2 + y ** 2 ) self [ radius_out ] = r phi = np . arctan2 ( y , x ) if not radians : phi = phi * 180 / np . pi self [ azimuth_out ] = phi if propagate_uncertainties : self . propagate_uncertainties ( [ self [ radius_out ] , self [ azimuth_out ] ] )", "nl": "Convert cartesian to polar coordinates"}}
{"translation": {"code": "def describe ( self , strings = True , virtual = True , selection = None ) : import pandas as pd N = len ( self ) columns = { } for feature in self . get_column_names ( strings = strings , virtual = virtual ) [ : ] : dtype = str ( self . dtype ( feature ) ) if self . dtype ( feature ) != str else 'str' if self . dtype ( feature ) == str_type or self . dtype ( feature ) . kind in [ 'S' , 'U' , 'O' ] : count = self . count ( feature , selection = selection , delay = True ) self . execute ( ) count = count . get ( ) columns [ feature ] = ( ( dtype , count , N - count , '--' , '--' , '--' , '--' ) ) else : count = self . count ( feature , selection = selection , delay = True ) mean = self . mean ( feature , selection = selection , delay = True ) std = self . std ( feature , selection = selection , delay = True ) minmax = self . minmax ( feature , selection = selection , delay = True ) self . execute ( ) count , mean , std , minmax = count . get ( ) , mean . get ( ) , std . get ( ) , minmax . get ( ) count = int ( count ) columns [ feature ] = ( ( dtype , count , N - count , mean , std , minmax [ 0 ] , minmax [ 1 ] ) ) return pd . DataFrame ( data = columns , index = [ 'dtype' , 'count' , 'missing' , 'mean' , 'std' , 'min' , 'max' ] )", "nl": "Give a description of the DataFrame ."}}
{"translation": {"code": "def _evaluate_selection_mask ( self , name = \"default\" , i1 = None , i2 = None , selection = None , cache = False ) : i1 = i1 or 0 i2 = i2 or len ( self ) scope = scopes . _BlockScopeSelection ( self , i1 , i2 , selection , cache = cache ) return scope . evaluate ( name )", "nl": "Internal use ignores the filter"}}
{"translation": {"code": "def to_dict ( self , column_names = None , selection = None , strings = True , virtual = False ) : return dict ( self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) )", "nl": "Return a dict containing the ndarray corresponding to the evaluated data"}}
{"translation": {"code": "def to_copy ( self , column_names = None , selection = None , strings = True , virtual = False , selections = True ) : if column_names : column_names = _ensure_strings_from_expressions ( column_names ) df = vaex . from_items ( * self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = False ) ) if virtual : for name , value in self . virtual_columns . items ( ) : df . add_virtual_column ( name , value ) if selections : # the filter selection does not need copying for key , value in self . selection_histories . items ( ) : if key != FILTER_SELECTION_NAME : df . selection_histories [ key ] = list ( value ) for key , value in self . selection_history_indices . items ( ) : if key != FILTER_SELECTION_NAME : df . selection_history_indices [ key ] = value df . functions . update ( self . functions ) df . copy_metadata ( self ) return df", "nl": "Return a copy of the DataFrame if selection is None it does not copy the data it just has a reference"}}
{"translation": {"code": "def rename_column ( self , name , new_name , unique = False , store_in_state = True ) : new_name = vaex . utils . find_valid_name ( new_name , used = [ ] if not unique else list ( self ) ) data = self . columns . get ( name ) if data is not None : del self . columns [ name ] self . column_names [ self . column_names . index ( name ) ] = new_name self . columns [ new_name ] = data else : expression = self . virtual_columns [ name ] del self . virtual_columns [ name ] self . virtual_columns [ new_name ] = expression if store_in_state : self . _renamed_columns . append ( ( name , new_name ) ) for d in [ self . ucds , self . units , self . descriptions ] : if name in d : d [ new_name ] = d [ name ] del d [ name ] return new_name", "nl": "Renames a column not this is only the in memory name this will not be reflected on disk"}}
{"translation": {"code": "def to_pandas_df ( self , column_names = None , selection = None , strings = True , virtual = False , index_name = None ) : import pandas as pd data = self . to_dict ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) if index_name is not None : if index_name in data : index = data . pop ( index_name ) else : index = self . evaluate ( index_name , selection = selection ) else : index = None df = pd . DataFrame ( data = data , index = index ) if index is not None : df . index . name = index_name return df", "nl": "Return a pandas DataFrame containing the ndarray corresponding to the evaluated data"}}
{"translation": {"code": "def evaluate_variable ( self , name ) : if isinstance ( self . variables [ name ] , six . string_types ) : # TODO: this does not allow more than one level deep variable, like a depends on b, b on c, c is a const value = eval ( self . variables [ name ] , expression_namespace , self . variables ) return value else : return self . variables [ name ]", "nl": "Evaluates the variable given by name ."}}
{"translation": {"code": "def to_astropy_table ( self , column_names = None , selection = None , strings = True , virtual = False , index = None ) : from astropy . table import Table , Column , MaskedColumn meta = dict ( ) meta [ \"name\" ] = self . name meta [ \"description\" ] = self . description table = Table ( meta = meta ) for name , data in self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : if self . dtype ( name ) == str_type : # for astropy we convert it to unicode, it seems to ignore object type data = np . array ( data ) . astype ( 'U' ) meta = dict ( ) if name in self . ucds : meta [ \"ucd\" ] = self . ucds [ name ] if np . ma . isMaskedArray ( data ) : cls = MaskedColumn else : cls = Column table [ name ] = cls ( data , unit = self . unit ( name ) , description = self . descriptions . get ( name ) , meta = meta ) return table", "nl": "Returns a astropy table object containing the ndarrays corresponding to the evaluated data"}}
{"translation": {"code": "def add_column ( self , name , f_or_array ) : if isinstance ( f_or_array , ( np . ndarray , Column ) ) : data = ar = f_or_array # it can be None when we have an 'empty' DataFrameArrays if self . _length_original is None : self . _length_unfiltered = _len ( data ) self . _length_original = _len ( data ) self . _index_end = self . _length_unfiltered if _len ( ar ) != self . length_original ( ) : if self . filtered : # give a better warning to avoid confusion if len ( self ) == len ( ar ) : raise ValueError ( \"Array is of length %s, while the length of the DataFrame is %s due to the filtering, the (unfiltered) length is %s.\" % ( len ( ar ) , len ( self ) , self . length_unfiltered ( ) ) ) raise ValueError ( \"array is of length %s, while the length of the DataFrame is %s\" % ( len ( ar ) , self . length_original ( ) ) ) # assert self.length_unfiltered() == len(data), \"columns should be of equal length, length should be %d, while it is %d\" % ( self.length_unfiltered(), len(data)) self . columns [ name ] = f_or_array if name not in self . column_names : self . column_names . append ( name ) else : raise ValueError ( \"functions not yet implemented\" ) self . _save_assign_expression ( name , Expression ( self , name ) )", "nl": "Add an in memory array as a column ."}}
{"translation": {"code": "def to_arrow_table ( self , column_names = None , selection = None , strings = True , virtual = False ) : from vaex_arrow . convert import arrow_table_from_vaex_df return arrow_table_from_vaex_df ( self , column_names , selection , strings , virtual )", "nl": "Returns an arrow Table object containing the arrays corresponding to the evaluated data"}}
{"translation": {"code": "def str_rjust ( x , width , fillchar = ' ' ) : sl = _to_string_sequence ( x ) . pad ( width , fillchar , True , False ) return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl )", "nl": "Fills the left side of string samples with a specified character such that the strings are left - hand justified ."}}
{"translation": {"code": "def _split_and_combine_mask ( arrays ) : masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . isMaskedArray ( block ) ] arrays = [ block . data if np . ma . isMaskedArray ( block ) else block for block in arrays ] mask = None if masks : mask = masks [ 0 ] . copy ( ) for other in masks [ 1 : ] : mask |= other return arrays , mask", "nl": "Combines all masks from a list of arrays and logically ors them into a single mask"}}
{"translation": {"code": "def dt_hour ( x ) : import pandas as pd return pd . Series ( x ) . dt . hour . values", "nl": "Extracts the hour out of a datetime samples ."}}
{"translation": {"code": "def _depending_columns ( self , ds ) : depending = set ( ) for expression in self . expressions : expression = ds . _expr ( expression ) # make sure it is an expression depending |= expression . variables ( ) if self . previous_selection : depending |= self . previous_selection . _depending_columns ( ds ) return depending", "nl": "Find all columns that this selection depends on for df ds"}}
{"translation": {"code": "def fillna ( ar , value , fill_nan = True , fill_masked = True ) : ar = ar if not isinstance ( ar , column . Column ) else ar . to_numpy ( ) if ar . dtype . kind in 'O' and fill_nan : strings = ar . astype ( str ) mask = strings == 'nan' ar = ar . copy ( ) ar [ mask ] = value elif ar . dtype . kind in 'f' and fill_nan : mask = np . isnan ( ar ) if np . any ( mask ) : ar = ar . copy ( ) ar [ mask ] = value if fill_masked and np . ma . isMaskedArray ( ar ) : mask = ar . mask if np . any ( mask ) : ar = ar . data . copy ( ) ar [ mask ] = value return ar", "nl": "Returns an array where missing values are replaced by value ."}}
{"translation": {"code": "def dt_dayofweek ( x ) : import pandas as pd return pd . Series ( x ) . dt . dayofweek . values", "nl": "Obtain the day of the week with Monday = 0 and Sunday = 6"}}
{"translation": {"code": "def dt_weekofyear ( x ) : import pandas as pd return pd . Series ( x ) . dt . weekofyear . values", "nl": "Returns the week ordinal of the year ."}}
{"translation": {"code": "def dt_year ( x ) : import pandas as pd return pd . Series ( x ) . dt . year . values", "nl": "Extracts the year out of a datetime sample ."}}
{"translation": {"code": "def dt_dayofyear ( x ) : import pandas as pd return pd . Series ( x ) . dt . dayofyear . values", "nl": "The ordinal day of the year ."}}
{"translation": {"code": "def value_counts ( self , dropna = False , dropnull = True , ascending = False , progress = False ) : from pandas import Series dtype = self . dtype transient = self . transient or self . ds . filtered or self . ds . is_masked ( self . expression ) if self . dtype == str_type and not transient : # string is a special case, only ColumnString are not transient ar = self . ds . columns [ self . expression ] if not isinstance ( ar , ColumnString ) : transient = True counter_type = counter_type_from_dtype ( self . dtype , transient ) counters = [ None ] * self . ds . executor . thread_pool . nthreads def map ( thread_index , i1 , i2 , ar ) : if counters [ thread_index ] is None : counters [ thread_index ] = counter_type ( ) if dtype == str_type : previous_ar = ar ar = _to_string_sequence ( ar ) if not transient : assert ar is previous_ar . string_sequence if np . ma . isMaskedArray ( ar ) : mask = np . ma . getmaskarray ( ar ) counters [ thread_index ] . update ( ar , mask ) else : counters [ thread_index ] . update ( ar ) return 0 def reduce ( a , b ) : return a + b self . ds . map_reduce ( map , reduce , [ self . expression ] , delay = False , progress = progress , name = 'value_counts' , info = True , to_numpy = False ) counters = [ k for k in counters if k is not None ] counter0 = counters [ 0 ] for other in counters [ 1 : ] : counter0 . merge ( other ) value_counts = counter0 . extract ( ) index = np . array ( list ( value_counts . keys ( ) ) ) counts = np . array ( list ( value_counts . values ( ) ) ) order = np . argsort ( counts ) if not ascending : order = order [ : : - 1 ] counts = counts [ order ] index = index [ order ] if not dropna or not dropnull : index = index . tolist ( ) counts = counts . tolist ( ) if not dropna and counter0 . nan_count : index = [ np . nan ] + index counts = [ counter0 . nan_count ] + counts if not dropnull and counter0 . null_count : index = [ 'null' ] + index counts = [ counter0 . null_count ] + counts return Series ( counts , index = index )", "nl": "Computes counts of unique values ."}}
{"translation": {"code": "def map ( self , mapper , nan_mapping = None , null_mapping = None ) : assert isinstance ( mapper , collectionsAbc . Mapping ) , \"mapper should be a dict like object\" df = self . ds mapper_keys = np . array ( list ( mapper . keys ( ) ) ) # we map the keys to a ordinal values [0, N-1] using the set key_set = df . _set ( self . expression ) found_keys = key_set . keys ( ) mapper_has_nan = any ( [ key != key for key in mapper_keys ] ) # we want all possible values to be converted # so mapper's key should be a superset of the keys found if not set ( mapper_keys ) . issuperset ( found_keys ) : missing = set ( found_keys ) . difference ( mapper_keys ) missing0 = list ( missing ) [ 0 ] if missing0 == missing0 : # safe nan check raise ValueError ( 'Missing values in mapper: %s' % missing ) # and these are the corresponding choices choices = [ mapper [ key ] for key in found_keys ] if key_set . has_nan : if mapper_has_nan : choices = [ mapper [ np . nan ] ] + choices else : choices = [ nan_mapping ] + choices if key_set . has_null : choices = [ null_mapping ] + choices choices = np . array ( choices ) key_set_name = df . add_variable ( 'map_key_set' , key_set , unique = True ) choices_name = df . add_variable ( 'map_choices' , choices , unique = True ) expr = '_choose(_ordinal_values({}, {}), {})' . format ( self , key_set_name , choices_name ) return Expression ( df , expr )", "nl": "Map values of an expression or in memory column accoring to an input dictionary or a custom callable function ."}}